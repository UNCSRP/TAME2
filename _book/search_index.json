[["index.html", "TAME 2.0 Preface", " TAME 2.0 Rager Lab 2024-06-24 Preface Background Research in exposure science, epidemiology, toxicology, and environmental health is becoming increasingly reliant upon data science and computational methods that can more efficiently extract information from complex datasets. These methods can be leveraged to better identify relationships between exposures to stressors in the environment and human disease outcomes. Still, there remains a critical gap surrounding the training of researchers on these in silico methods. Objectives We aimed to address this critical gap by developing the inTelligence And Machine lEarning (TAME) Toolkit, promoting trainee-driven data generation, management, and analysis methods to “TAME” data in environmental health studies. This toolkit encompasses training modules, organized as chapters within this Github Bookdown site. All underlying code (in RMarkdown), input files, and imported graphics for these modules can be found at the parent UNC-SRP Github Page. Module Development Overview Training modules were developed to provide applications-driven examples of data organization and analysis methods that can be used to address environmental health questions. Target audiences for these modules include students and professionals in academia, government, and industry that are interested in expanding their skillset. Modules were developed by study coauthors using annotated script formatted for R/RStudio coding language and interface and were organized into three chapters. The first group of modules focused on introductory data science, which included the following topics: setting up R/RStudio and coding in the R environment; data organization basics; finding and visualizing data trends; high-dimensional data visualizations with heat maps; and Findability, Accessibility, Interoperability, and Reusability (FAIR) data management practices. The second chapter of modules incorporated chemical-biological analyses and predictive modeling, spanning the following methods: dose-response modeling; machine learning and predictive modeling; mixtures analyses; -omics analyses; toxicokinetic modeling; and read-across toxicity predictions. The last chapter of modules was organized to provide examples on environmental health database mining and integration, including chemical exposure, health outcome, and environmental justice data. Please note that these training modules describe example techniques that can be used to carry out these types of data analyses. We encourage participants to review the additional resources listed above, as well as the resources referenced throughout this training module, when designing and completing similar research to meet the unique needs of their study. The overall organization of this TAME toolkit is summarized below. Modules are organized into three chapters, that are listed on the left side of this website. Concluding Remarks Together, this valuable resource provides unique opportunities to obtain introductory-level training on current data analysis methods applicable to 21st century exposure science, toxicology, and environmental health. These modules serve as applications-based examples on how to “TAME” data within the environmental health research field, expanding the toolbox for career development and cross-training of scientists in multiple specialties, as well as supporting the next generation of data scientists. Funding This study was supported by the National Institutes of Health (NIH) from the National Institute of Environmental Health Sciences, including the following grant funds and associated programs: P42ES031007: The University of North Carolina (UNC)-Superfund Research Program (SRP) seeks to develop new solutions for reducing exposure to inorganic arsenic and prevent arsenic-induced diabetes through mechanistic and translational research. The UNC-SRP is Directed by Dr. Rebecca C. Fry. The UNC-SRP Data Analysis and Management Core (UNC-SRP-DMAC) provides the UNC-SRP with critical expertise in bioinformatics, statistics, data management, and data integration. Dr. Julia E. Rager is a Leader of the UNC-SRP-DMAC. T32ES007126: The UNC Curriculum in Toxicology and Environmental Medicine (CiTEM) seeks to provide a cutting edge research and mentoring environment to train students and postdoctoral fellows in environmental health and toxicology. Towards this goal, the CiTEM has a T32 Training Program for Pre- and Postdoctoral Training in Toxicology to support the development of future investigators in environmental health and toxicology. This training program has received supplement funds to expand training efforts centered on data management and data science practices to address current health issues in toxicology and environmental science. The UNC CiTEM is Directed by Dr. Ilona Jaspers. Support was additionally provided through the Institute for Environmental Health Solutions (IEHS) at the University of North Carolina (UNC) Gillings School of Global Public Health. The IEHS is aimed at protecting those who are particularly vulnerable to diseases caused by environmental factors, putting solutions directly into the hands of individuals and communities of North Carolina and beyond. The IEHS is Directed by Dr. Rebecca C. Fry. Author Affiliations Kyle Roell1,†, Lauren Koval1,2,†, Rebecca Boyles3, Grace Patlewicz4, Caroline Ring4, Cynthia Rider5, Cavin Ward-Caviness6, David M. Reif7, Ilona Jaspers1,2,8,9,10, Rebecca C. Fry1,2,8, and Julia E. Rager1,2,8,9 1The Institute for Environmental Health Solutions, Gillings School of Global Public Health, The University of North Carolina at Chapel Hill, Chapel Hill, North Carolina, USA 2Department of Environmental Sciences and Engineering, Gillings School of Global Public Health, The University of North Carolina at Chapel Hill, Chapel Hill, North Carolina, USA 3Research Computing, RTI International, Durham North Carolina, USA 4Center for Computational Toxicology and Exposure, US Environmental Protection Agency, Chapel Hill, North Carolina, USA 5Division of the National Toxicology Program, National Institute of Environmental Health Sciences, Research Triangle Park, North Carolina, USA 6Center for Public Health and Environmental Assessment, US Environmental Protection Agency, Chapel Hill, North Carolina, USA 7Bioinformatics Research Center, Department of Biological Sciences, North Carolina State University, Raleigh, North Carolina, USA 8Curriculum in Toxicology and Environmental Medicine, School of Medicine, University of North Carolina, Chapel Hill, North Carolina, USA 9Center for Environmental Medicine, Asthma and Lung Biology, School of Medicine, University of North Carolina, Chapel Hill, North Carolina, USA 10Department of Pediatrics, Microbiology and Immunology, School of Medicine, University of North Carolina, Chapel Hill, North Carolina, USA †These authors have contributed equally to this work and share first authorship "],["fair-data-management-practices.html", "FAIR Data Management Practices Introduction to Training Module Introduction to FAIR Breaking Down FAIR, Letter-by-Letter What Does This Mean for You? Data Repositories for Sharing of Data Recent Shifts in Regulatory Policies for Data Sharing Additional Training Resources on FAIR", " FAIR Data Management Practices This training module was developed by Rebecca Boyles, with contributions from Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module This training module provides a description of FAIR data management practices, and points participants to important resources to help ensure generated data meet current FAIR guidelines. This training module is descriptive content-based (as opposed to coding-based), in order to present information clearly and serve as an important resource alongside the other scripted training activities. Training Module’s Environmental Heatlh Questions This training module was specifically developed to answer the following questions: What is FAIR? When was FAIR first developed? When making data ‘Findable’, who and what should be able to find your data? When saving/formatting your data, which of the following formats is preferred to meet FAIR principles: .pdf, .csv, or a proprietary output file from your lab instrument? How can I find a suitable data repository for my data? Introduction to FAIR Proper data management is of utmost importance while leading data analyses within the field of environmental health science. A method to ensure proper data management is the implementation of Findability, Accessibility, Interoperability, and Reusability (FAIR) practices. A landmark paper that describes FAIR practices in environmental health research is the following: Wilkinson MD, Dumontier M, Aalbersberg IJ, et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data. 2016 Mar 15. PMID: 26978244. The FAIR principles describe a framework for data management and stewardship aimed at increasing the value of data by enabling sharing and reuse. These principles were originally developed from discussions during the Jointly Designing a Data FAIRport meeting at the Lorentz Center in Leiden, The Netherlands in 2014, which brought together stakeholders to discuss the creation of an environment for virtual computational science. The resulting principles are technology agnostic, discipline independent, community driven, and internationally adopted. Below is a schematic providing an overview of this guiding principle: Answer to Environmental Health Question 1 &amp; 2 With this background, we can answer Environmental Health Question #1 and #2: What is FAIR and when was it first developed? Answer: FAIR is guiding framework that was recently established to promote best data management practices, to ensure that data are Findable, Accessibility, Interoperable, and Reusable. It was first developed in 2014- which means that these principles are very new and continuing to evolve! Breaking Down FAIR, Letter-by-Letter The aspects of the FAIR principles apply to data and metadata with the aim of making the information available to people and computers as described in the seminal paper by Wilkinson et al., 2016. F (Findable) in FAIR The F in FAIR identifies components of the principles needed to make the meta(data) findable through the application of unique persistent identifiers, thoroughly described, reference the unique identifiers, and that the descriptive information (i.e., metadata) could be searched by both humans and computer systems. F1. (Meta)data are assigned a globally unique and persistent identifier Each dataset is assigned a globally unique and persistent identifier (PID), for example a DOI. These identifiers allow to find, cite and track (meta)data. A DOI looks like: https://doi.org/10.1109/5.771073 Action: Ensure that each dataset is assigned a globally unique and persistent identifier. Certain repositories automatically assign identifiers to datasets as a service. If not, obtain a PID via a PID registration service. F2. Data are described with rich metadata Each dataset is thoroughly (see R1) described: these metadata document how the data was generated, under what term (license) and how it can be (re)used and provide the necessary context for proper interpretation. This information needs to be machine-readable. Action: Fully document each dataset in the metadata, which may include descriptive information about the context, quality and condition, or characteristics of the data. Another researcher in any field, or their computer, should be able to properly understand the nature of your dataset. Be as generous as possible with your metadata (see R1). F3. Metadata clearly and explicitly include the identifier of the data it describes Explanation: The metadata and the dataset they describe are separate files. The association between a metadata file and the dataset is obvious thanks to the mention of the dataset’s PID in the metadata. Action: Make sure that the metadata contains the dataset’s PID. F4. (Meta)data are registered or indexed in a searchable resource Explanation: Metadata are used to build easily searchable indexes of datasets. These resources will allow to search for existing datasets similarly to searching for a book in a library. Action: Provide detailed and complete metadata for each dataset (see F2). Answer to Environmental Health Question 3 With this, we can answer Environmental Health Question #3: When making data ‘Findable’, who and what should be able to find your data? Answer: Both humans and computer systems should be able to find your data. A (Accessible) in FAIR The A components are designed to enable meta(data) be available long-term, accessed by humans and machines using standard communication protocols with clearly described limitations on reuse. A1. (Meta)data are retrievable by their identifier using a standardized communications protocol Explanation: If one knows a dataset’s identifier and the location where it is archived, one can access at least the metadata. Furthermore, the user knows how to proceed to get access to the data. Action: Clearly define who can access the actual data and specify how. It is possible that data will not be downloaded, but rather reused in situ. If so, the metadata must specify the conditions under which this is allowed (sometimes versus the conditions needed to fulfill for external usage/“download”). A1.1 The protocol is open, free, and universally implementable Explanation: Anyone with a computer and an internet connection can access at least the metadata. A1.2 The protocol allows for an authentication and authorization procedure, where necessary Explanation: It often makes sense to request users to create a user account on a repository. This allows to authenticate the owner (or contributor) of each dataset, and to potentially set user specific rights. A2. Metadata are accessible, even when the data are no longer available Explanation: Maintaining all datasets in a readily usable state eternally would require an enormous amount of curation work (adapting to new standards for formats, converting to different format if specifically needed software is discontinued, etc). Keeping the metadata describing each dataset accessible, however, can be done with fewer resources. This allows to build comprehensive data indexes including all current, past, and potentially arising datasets. Action: Provide detailed and complete metadata for each dataset (see R1). I (Interoperable) in FAIR The I components of the principles address needs for data exchange and interpretation by humans and machines which includes the use of controlled vocabularies or ontologies to describe meta(data) and to describe provenance relationships through appropriate data citation. I1. (Meta)data use a formal, accessible, shared, and broadly applicable language Explanation: Interoperability typically means that each computer system has at least knowledge of the other system’s formats in which data is exchanged. If (meta)data are to be searchable and if compatible data sources should be combinable in a (semi)automatic way, computer systems need to be able to decide if the content of datasets are comparable. Action: Provide machine readable data and metadata in an accessible language, using a well-established formalism. Data and metadata are annotated with resolvable vocabularies/ontologies/thesauri that are commonly used in the field (see I2). I2. (Meta)data use vocabularies that follow FAIR principles Explanation: The controlled vocabulary (e.g., MESH) used to describe datasets needs to be documented. This documentation needs to be easily findable and accessible by anyone who uses the dataset. Action: The vocabularies/ontologies/thesauri are themselves findable, accessible, interoperable and thoroughly documented, hence FAIR. Lists of these standards can be found at: NCBO BioPortal, FAIRSharing, OBO Foundry. I3. (Meta)data include qualified references to other (meta)data Explanation: If the dataset builds on another dataset, if additional datasets are needed to complete the data, or if complementary information is stored in a different dataset, this needs to be specified. In particular, the scientific link between the datasets needs to be described. Furthermore, all datasets need to be properly cited (i.e. including their persistent identifiers). Action: Properly cite relevant/associated datasets, by providing their persistent identifiers, in the metadata, and describe the scientific link/relation to your dataset. R (Reusable) in FAIR The R components highlight needs for the meta(data) to be reused and support integration such as sufficient description of the data and data use limitations. R1. Meta(data) are richly described with a plurality of accurate and relevant attributes Explanation: Description of a dataset is required at two different levels: Metadata describing the dataset: what does the dataset contain, how was the data generated, how has it been processed, how can it be reused. Metadata describing the data: any needed information to properly use the data, such as definitions of the variable names Action: Provide complete metadata for each data file. Scope of your data: for what purpose was it generated/collected? Particularities or limitations about the data that other users should be aware of. Date of the dataset generation, lab conditions, who prepared the data, parameter settings, name and version of the software used. Variable names are explained or self-explanatory. Version of the archived and/or reused data is clearly specified and documented. What Does This Mean for You? We advise the following as ‘starting-points’ for participants to start meeting FAIR guidances: Learn how to create a Data Management Plan Keep good documentation (project &amp; data-level) while working Do not use proprietary file formats (.csv is a great go-to formats for your data!) When able, use a domain appropriate metadata standard or ontology Ruthlessly document any steps in a project Most of FAIR can be handled by selecting a good data or software repository Don’t forget to include a license! Answer to Environmental Health Question 4 With these, we can answer Environmental Health Question #4: When saving/formatting your data, which of the following formats is preferred to meet FAIR principles: .pdf, .csv, or a proprietary output file from your lab instrument? Answer: A .csv file is preferred to enhance data sharing. Data Repositories for Sharing of Data When you are organizing your data to deposit online, it is important to identify an appropriate repository to publish your dataset it. A good starting place is a repository registry such as FAIRsharing.org or re3data.org. Journals can also provide helpful resources and starting repository lists, such as Nature and PLOS, which both have published a list of recommended repositories. Funding agencies, including the NIH, can also inform specific repositories. Below are some examples of two main categories of data repositories: 1. Domain Agnostic Data Repositories Domain agnostic repositories allow the deposition of any data type. Some examples include the following: Data in Brief Articles (e.g., Elsevier’s Data in Brief Journal) Dryad Figshare The Dataverse Project Zenodo 2. Domain Specific Data Repositories Domain specific repositories allow the deposition of specific types of data, produced from specific types of technologies or within specific domains. Some examples include the following: Database of Genotypes and Phenotypes Gene Expression Omnibus The Immunology Database and Analysis Portal Metabolomics Workbench (National Metabolomics Data Repository) Microphysiology Systems Database Mouse Genome Informatics Mouse Phenome Database OpenNeuro Protein Data Bank ProteomeXchange Rat Genome Database The Database of Genotypes and Phenotypes Zebrafish Model Organism Database and many, many, many others… Answer to Environmental Health Question 5 With these, we can answer Environmental Health Question #5: How can I find a suitable data repository for my data? Answer: I can search through a data repository registry service or look for recommendations from NIH or other funding agencies. Recent Shifts in Regulatory Policies for Data Sharing The NIH Data Management and Sharing Policy NIH’s data management and sharing (DMS) policy became effective January 2023. This policy specifically lists the expectations that investigators must comply with in order to promote the sharing of scientific data. Information about this recent policy can be found through updated NIH websites. Information about writing an official Data Management and Sharing (DMS) plan for your research can be found through NIH’s Guidance on Writing a Data Management &amp; Sharing Plan. The 2018 Evidence Act The Evidence Act, or Foundations for Evidence-Based Policymaking Act of 2018, was signed into U.S. law on January 14, 2019. The Act requires federal agencies to build the capacity to use evidence and data in their decision-making and policymaking. It also requires agencies to: Develop an evidence-building plan as part of their quadrennial strategic plan &amp; Develop an evaluation plan concurrent with their annual performance plan. The Evidence Act also: Mandates that data be “open by default” Specifies that a comprehensive data inventory should be created for each agency’s open data assets How Does the NIH Data Management and Sharing Policy Intersect with the 2018 Evidence Act? Making your data FAIR, by definition, makes it more shareable and reusable. Many of the requirements in the NIH DMS and the Evidence Act policy overlap with the FAIR principles. The CARE Principles for Indigenous Data Governance While we are experiencing increased requirements for the open sharing of data, it is important to recognize that there are circumstances and populations that should, at the same time, be carefully protected. Examples include human clinical or epidemiological data that may become identifiable upon the sharing of sensitive data. Another example includes the consideration of Indigenous populations. A recent article by Carroll et al. 2021 describes in their abstract: As big data, open data, and open science advance to increase access to complex and large datasets for innovation, discovery, and decision-making, Indigenous Peoples’ rights to control and access their data within these data environments remain limited. Operationalizing the FAIR Principles for scientific data with the CARE Principles for Indigenous Data Governance enhances machine actionability and brings people and purpose to the fore to resolve Indigenous Peoples’ rights to and interests in their data across the data lifecycle. Additional Training Resources on FAIR Many organizations, from specific programs to broad organizations, provide training and resources for scientists in FAIR principles. Some of the notable global organizations organizing and providing training that offer opportunities for community involvement are: Committee on Data for Science and Technology (CODATA) Global Alliance for Genomics &amp; Health GoFAIR Force11 Research Data Alliance Example Workshops discussing FAIR: NAS Implementing FAIR Data for People and Machines: Impacts and Implications (2019). Available at: https://www.nationalacademies.org/our-work/implementing-fair-data-for-people-and-machines-impacts-and-implications NIH Catalyzing Knowledge-driven Discovery in Environmental Health Sciences Through a Harmonized Language, Virtual Workshop (2021). Available at: https://www.niehs.nih.gov/news/events/pastmtg/2021/ehslanguage/index.cfm NIH Trustworthy Data Repositories Workshop (2019). Available at: https://datascience.nih.gov/data-ecosystem/trustworthy-data-repositories-workshop NIH Virtual Workshop on Data Metrics (2020). Available at: https://datascience.nih.gov/data-ecosystem/nih-virtual-workshop-on-data-metrics NIH Workshop on the Role of Generalist Repositories to Enhance Data Discoverability and Reuse: Workshop Summary (2020). Available at: https://datascience.nih.gov/data-ecosystem/nih-data-repository-workshop-summary Example Government Report Documents on FAIR: Collins S, Genova F, Harrower N, Hodson S, Jones S, Laaksonen L, Mietchen D, Petrauskaite R, Wittenburg P. Turning FAIR into reality: Final report and action plan from the European Commission expert group on FAIR data: European Union; 2018. Available at: https://www.vdu.lt/cris/handle/20.500.12259/103794. EU. FAIR Data Advanced Use Cases: From Principles to Practice in the Netherlands. 2018. European Union. Available at: doi:10.5281/zenodo.1250535. NIH. Final NIH Policy for Data Management and Sharing and Supplemental Information. National Institutes of Health. Federal Register, vol. 85, 2020-23674, 30 Oct. 2020, pp. 68890–900. Available at: https://www.federalregister.gov/d/2020-23674. NIH. NIH Strategic Plan for Data Science 2018. National Institutes of Health. Available at: https://datascience.nih.gov/strategicplan. NLM. NLM Strategic Plan 2017 to 2027. U.S. National Library of Medicine, Feb. 2018. Available at: https://www.nlm.nih.gov/about/strategic-plan.html. Example Related Publications on FAIR: Comess S, Akbay A, Vasiliou M, Hines RN, Joppa L, Vasiliou V, Kleinstreuer N. Bringing Big Data to Bear in Environmental Public Health: Challenges and Recommendations. Front Artif Intell. 2020 May;3:31. doi: 10.3389/frai.2020.00031. Epub 2020 May 15. PMID: 33184612; PMCID: PMC7654840. Koers H, Bangert D, Hermans E, van Horik R, de Jong M, Mokrane M. Recommendations for Services in a FAIR Data Ecosystem. Patterns (N Y). 2020 Jul 7;1(5):100058. doi: 10.1016/j.patter.2020.100058. Erratum in: Patterns (N Y). 2020 Sep 11;1(6):100104. PMID: 33205119. Kush RD, Warzel D, Kush MA, Sherman A, Navarro EA, Fitzmartin R, Pétavy F, Galvez J, Becnel LB, Zhou FL, Harmon N, Jauregui B, Jackson T, Hudson L. FAIR data sharing: The roles of common data elements and harmonization. J Biomed Inform. 2020 Jul;107:103421. doi: 10.1016/j.jbi.2020.103421. Epub 2020 May 12. PMID: 32407878. Lin D, Crabtree J, Dillo I, Downs RR, Edmunds R, Giaretta D, De Giusti M, L’Hours H, Hugo W, Jenkyns R, Khodiyar V, Martone ME, Mokrane M, Navale V, Petters J, Sierman B, Sokolova DV, Stockhause M, Westbrook J. The TRUST Principles for digital repositories. Sci Data. 2020 May 14;7(1):144. PMID: 32409645. Thessen AE, Grondin CJ, Kulkarni RD, Brander S, Truong L, Vasilevsky NA, Callahan TJ, Chan LE, Westra B, Willis M, Rothenberg SE, Jarabek AM, Burgoon L, Korrick SA, Haendel MA. Community Approaches for Integrating Environmental Exposures into Human Models of Disease. Environ Health Perspect. 2020 Dec;128(12):125002. PMID: 33369481. Roundtable on Environmental Health Sciences, Research, and Medicine; Board on Population Health and Public Health Practice; Health and Medicine Division; National Academies of Sciences, Engineering, and Medicine. Principles and Obstacles for Sharing Data from Environmental Health Research: Workshop Summary. Washington (DC): National Academies Press (US); 2016 Apr 29. PMID: 27227195. Test Your Knowledge Let’s imagine that you’re a researcher who is planning on gathering a lot of data using the zebrafish model. In order to adequately prepare your studies and steps to ensure data are deposited into proper repositories, you have the idea to check repository information obtained in FAIRsharing.org. What are some example repositories and relevant ontology resources that you could use to organize, deposit, and share your zebrafish data (hint: use the search tool)? "],["data-sharing-through-online-repositories.html", "Data Sharing through Online Repositories An Overview and Example with the Dataverse Repository Introduction to Training Module Data Repositories The Dataverse Project What is a Dataverse? Metadata Creating a Dataverse Creating a Dataset Concluding Remarks", " Data Sharing through Online Repositories An Overview and Example with the Dataverse Repository This training module was developed by Kyle R. Roell, Alexis Payton, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Submitting data to publicly available repositories is an essential part of ensuring data meet FAIR guidelines, as discussed in detail in the previous training module. There are many benefits to sharing and submitting your researching, such as: Making more use out of data that are generated in your lab More easily sharing and integrating across datasets Ensuring reproducibility in analysis findings and conclusions Improving the tracking and archiving of data sources, and data updates Increasing the awareness and attention surrounding your research as others locate your data through additional online queries Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: How should I structure my data for upload into online repositories? What does the term ‘metadata’ mean and what does it look like? This module will introduce some of the repositories that are commonly used to deposit data, how to set up metadata files, and how to organize example data in preparation for sharing. We will also provide information surrounding best practices for data organization and sharing through these repositories. Additional resources are also provided throughout, as there are many ways to organize, share, and deposit data depending on your data types and structures and overall research goals. Data Repositories There are many publicly available repositories that we should consider when depositing data. Some general repository registries that are helpful to search through include FAIRsharing.org or re3data.org. Journals can also provide helpful resources and starting repository lists, such as Nature and PLOS, which both have published a list of recommended repositories. As detailed in the FAIR training module, there are two main categories of data repositories: 1. Domain Agnostic Data Repositories Domain agnostic repositories allow the deposition of any data type. Some examples include: Data in Brief Articles (e.g., Elsevier’s Data in Brief Journal) Dryad Figshare The Dataverse Project Zenodo 2. Domain Specific Data Repositories Domain specific repositories allow the deposition of specific types of data, produced from specific types of technologies or within specific domains. Some examples include: Database of Genotypes and Phenotypes Gene Expression Omnibus The Immunology Database and Analysis Portal Metabolomics Workbench (National Metabolomics Data Repository) Microphysiology Systems Database Mouse Genome Informatics Mouse Phenome Database OpenNeuro Protein Data Bank ProteomeXchange Rat Genome Database The Database of Genotypes and Phenotypes Zebrafish Model Organism Database and many, many, many others… This training module focuses on providing an example of how to organize and upload data into the Dataverse; though many of the methods described below pertain to other data repositories as well, and also incorporate general data organization and sharing best practices. The Dataverse Project Dataverse, organized through The Dataverse Project, is a popular repository option that allows for upload of most types of material, without any stringent requirements. The Dataverse organization also provides ample resources on how to organize, upload, and share data through Dataverse. These resources include very thorough, readable, and user guides and best practices. Screenshot of the main page of The Dataverse Project An easier way to think about Dataverse is to interpret it similar to a folder system on your computer. A Dataverse is just an online folder that contains files, data, or datasets that are all related to some topic, project, etc. Although Dataverse was started at Harvard and the base Dataverse lives there, there are many versions of Dataverse that are specific to and supported by various institutions. For example, these training modules are being developed primarily by faculty, staff, and students at the University of North Carolina at Chapel Hill. As such, the examples contained in this module will specifically connect with the UNC Dataverse; though many of the methods outlined here are applicable to other Dataverses and additional online repositories, in general. What is a Dataverse? Remember how we pointed out that a Dataverse is similar to a folder system on a computer? Well, here we are going to show you what that actually looks like. But first, something that can be confusing when starting to work with Dataverse is the fact that the term Dataverse is used for both the overarching repository as well as individual subsections (or folders) in which data are stored. For example, the UNC Dataverse is called a Dataverse, but to upload data, you need to upload it to a specific sub-Dataverse. So, what is the difference between the high level UNC Dataverse and smaller, sub-dataverses? Well, nothing, really. The UNC Dataverse is similar to a large folder that says, these are all the projects and research related to or contained within UNC. From there, we want to be more specific about where we store our research, so we are creating more sub-Dataverses (folders) within that higher, overarching UNC Dataverse. As an example, using the UNC Dataverse, here we can see various sub-Dataverses that have been created as repositories for specific projects or types of data. As another example looking within a specific Dataverse, here we can see the Dataverse that hosts datasets and publications for Dr. Julia Rager’s lab, the Ragerlab-Dataverse. Within this Datavere, we can see various datasets produced by her lab. It is worth noting that the datasets may not necessarily be directly related to each other in terms of exact topic, for example, the Ragerlab-Dataverse hosts data pertaining to wildfire smoke exposure as well as chemical exposures and breast cancer. But they are all pertaining to experiments and analyses run within her specific lab. Let’s now start talking more specifically about how to organize data and format files for Dataverse, create your own “Dataverse”, upload datasets, and what this all means! Dataset Structure Before uploading your data to any data repository, it is important to structure your data efficiently and effectively, making it easy for others to navigate, understand, and utilize. While we will cover this in various sections throughout these training modules, here are some basic tips for data structure and organization. Keep all data for one participant or subject within one column (or row) of your dataset Genomic data and other analytical assays tend to have subjects on columns and genes, expression, etc. as the rows Descriptive and demographic data often tend to have subjects or participants as the rows and each descriptor variable (including demographics and any other subject variables) as columns Create succinct, descriptive variable names For example, do not use something like “This Variable Contains Information Regarding Smoking Status”, and instead just using something like, “Smoking_Status” Be aware of using spacing, special characters, and capitalization within variable names Think about transforming data from wide to long format depending on your specific dataset and general conventions Be sure to follow specific guidelines of repository when appropriate TAME 2.0 Module 1.1 FAIR Data Management Practices and TAME 2.0 Module 1.4 Data Wrangling in Excel are also helpful resources to reference when thinking about organizing your data. A general example of an organized, long format dataset in Excel in provided below: Only .csv or .txt files can be uploaded to dataverse; therefore, the metadata and data tabs in an excel file will need to saved and uploaded as two separate .csv or .txt files. Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question 1: How should I structure my data for upload into online repositories? Answer: It is ideal to have data clearly organized and filled, with succinct and descriptive variable names clearly labeled and values filled in. Most commonly, datasets should be saved as separate .csv or .txt files for upload into data repositories. Metadata There are many different definitions of what a metadata file is. Helpful explanations, for example, are provided by the UNC University Libraries: There are many definitions of metadata, but one of the simplest is data about data. More specifically… Metadata (in terms of data management) describe a dataset: how they were collected; when they were collected; what assumptions were made in their methodology; their geographic scope; if there are multiple files, how they relate to one another; the definitions of individual variables and, if applicable, what possible answers were (i.e., to survey questions); the calibration of any equipment used in data collection; the version of software used for analysis; etc. Very often, a dataset that has no metadata is incomprehensible. Metadata ARE data. They are pieces of information that have some meaning in relation to another piece of information. They can be created, managed, stored, and preserved like any other data. Metadata can be applied to anything. A computer file can be described in the same way that a book or piece of art can be described. For example, both can have a title, an author, and a year created. Metadata should be documented for research outputs of any kind. Metadata generally has little value on their own. Metadata adds value to other information, but are usually not valuable in themselves. There are exceptions to this rule, such as text transcription of an audio file. There are three kinds of metadata: Descriptive metadata consist of information about the content and context of your data. Examples: title, creator, subject keywords, and description (abstract) Structural metadata describe the physical structure of compound data. Examples: camera used, aperture, exposure, file format, and relation to other data or files Administrative metadata are information used to manage your data. Examples: when and how they were created, who can access them, software required to use them, and copyright permissions Therefore, after having organized your primary dataset for submission into online repositories, it is equally important to have a metadata file for easy comprehension and utilization of your data for future researchers or anyone downloading your data. While most repositories capture some metadata on the dataset page (e.g., descripton of data, upload date, contact information), there is generally little information about the specific data values and variables. In this section, we review some general guidelines and tips to better annotate your data. First, keep in mind, depending on the specific repository you are using, you may have to follow their metadata standards. But, if uploading to more generalist repository, this may be up to you to define. Generally, a metadata file consists of a set of descriptors for each variable in the data. If you are uploading data that contains many covariates or descriptive variables, it is essential that you provide a metadata file that describes these covariates. Both a description of the variable as well as any specific levels of any categorical or factor type variables. From the dataset presented previously, here we present an example of an associated metadata file: Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question 2: What does the term ‘metadata’ mean and what does it look like? Answer: Metadata refers to the information that describes and explains data. It looks like an additional dataset that provides context with details such as the source, type, owner, and relationships to other datasets. This file can help users understand the relevance of a specific dataset and provide guidance on how to use it. Creating a Dataverse Now, let’s review how to actually create a Dataverse. First, navigate to the parent Dataverse that you would like to use as your primary host website. For example, our group uses the UNC Dataverse. If you do not already have one, create a username and login. Then, from the home Dataverse page, click “Add Data” and select “New Dataverse”. And fill in the information necessary. And that is it. After creating your Dataverse site, you will need to publish it; however, before it is accessible to the public, note that you can actually create a Dataverse within another Dataverse (similar to a folder within a folder on your computer). This makes sense even when you are creating a new Dataverse at the home, UNC Dataverse level, you are still technically creating a new Dataverse within an existing one (the large UNC Dataverse). Here are some tips as you create your Dataverse: Do not recreate a Dataverse that already exists Choose a name that is specific, but general enough that it doesn’t only pertain to one specific dataset You can add more than one contact email, if necessary Creating a Dataset Creating a dataset creates a page for your data containing information about that data, a citation for the data (something valuable and somewhat unique to Dataverse), as well the place from where you data can be directly accessed or downloaded. First, decide the specific Dataverse your data will live and navigate to that specific Dataverse site. Then carry out the following steps to create a dataset: Navigate to the Dataverse page under which your dataset will live Click “Add Data” and then select “New Dataset” Fill in the necessary information Upload your data and metadata file(s) structured as detailed above Now, you have a dataset within your Dataverse. Again, you will have to publish the dataset for someone to have access to it. The easy part of using a more generalist repository like Dataverse, is that you do not have to have a strict data structure adherence. However, this means it is up to you to make sure your data is readable and useable. Concluding Remarks In this training module, we set out to express the importance of uploading data to online repositories, demonstrate what the upload process may look like using a generalist repository (Dataverse), and give some examples and tips on structuring data for upload and creating metadata files. It is important to choose the appropriate repository for your data based on your field of study and specifications of your work. Test Your Knowledge Try creating your own Dataverse repository, format your files to be uploaded to Dataverse, and upload those files to your new repository! "],["file-management-using-github.html", "File Management using Github Introduction to Training Module Creating an Account Creating a Repository Uploading Code Adding Subfolders in a Repository Updating Code Updating Repository Titles and Structure to Support a Manuscript Tracking Code Changes using Github Branches Concluding Remarks", " File Management using Github This training module was developed by Alexis Payton, Lauren E. Koval, Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Good data practices like file management and code tracking are imperative for data analysis initiatives, especially when working in research teams and/or shared project folders. Often times analyses and manuscripts are edited many times prior to being submitted for a grant or publication. Analysis methods are also shared between members of a research team and to external communities, as further detailed in TAME 2.0 Module 1.1 FAIR Data Management Practices. Therefore, Github has emerged as an effective way to manage, share, and track how code changes over time. Github is an open source or publicly accessible platform designed to facilitate version control and issue tracking of code. It is used by us and many of our colleagues to not only document versions of script written for data analysis and visualization, but to also make our code publicly available for open communication and dissemination of results. This training module serves a launch pad for getting acclimated with Github and includes… Creating an account Uploading code Creating a repository and making it legible for manuscript submission Creating an Account First, users must create their own accounts within github to start uploading/sharing code. To do this, navigate to github.com, click “Sign Up”, and follow the on screen instructions. Creating a Repository A repository, also known as a “repo”, is similar to a project folder that will contain all code pertaining to a specific project (which can be used for specific research programs, grants, or manuscripts, as examples). A repository can be set to public or private. If a repo is initially set to private to keep findings confidential prior to publication, it can always be updated to public once findings are ready for public dissemination. Multiple people can be allowed to work on a project together within a single repository. To access the repositories that are currently available to you through your user account, click the circle in top right-hand corner and click “Your repositories”. To create a new repository, click on the green button that says “New”. Then give your repository a descriptive name. We often edit the repo titles to match the title of specific manuscripts, though specific titling formats are up to the users/team’s preference. For more information, visit Github’s Create a repo documentation. Then click “Add a README file” to initiate the README file, which is important to continually edit to provide analysis-specific background information, and any additional information that would be helpful during and after code is drafted to better facilitate tracking information and project details. We provide further details surrounding specific information that can be included within the README file below. Uploading Code The simplest way to upload code is to first navigate to the repository that you would like to upload your code/associated files to. Note that this could represent a repo that you created or that someone granted you access to. Click “Add file” then click “Upload files”. Drag and drop your file containing your script into github and click “Commit changes”. A more advanced way to upload code is by using the command line, which allows a user to directly interact with the computer or software application. Further documentation can be found here. Adding Subfolders in a Repository To keep the repository organized, it might be necessary to create a new folder (like the folder labeled “1.1. Summary Statistics” in the above screenshot). Files can be grouped into these folders based on the type of analysis. To do so, click on the new file and then click on the pencil icon next to the “Blame” button. Click on the box that contains the title of the file. Write the title of your new folder and then end with a forward slash (/). In the screenshot below, we’re creating a new folder entitled “New Folder”. Click “Commit changes” and your file should now be in a new folder. Updating Code Saving iterations of code can save valuable time later as analyses are constantly being updated and edited. If your code undergoes substantial changes, (e.g., adding/ removing steps or if there’s code that is likely to be beneficial later on, but is no longer relevant to the current analysis), it is helpful to save that version in Github for future reference. To do so, create a subfolder named “Archive” and move the old file into it. If you have multiple versions of a file with the same name, add the current date to prevent the file from being overwritten later on as seen in the screenshot below. Once the old file version has been archived, now upload the most recent version of your code to the main folder. Based on the screenshot above, that would be under “3. ML Visualizations”. Note: If a file is uploaded with the same name it will be overwritten, which can’t be undone! Therefore, put the older file into the archive folder if you’d like it to be saved PRIOR to uploading the new version. Updating Repository Titles and Structure to Support a Manuscript If the code is for a manuscript, it’s helpful to include the table or figure name it pertains to in the manuscript in parentheses. For example, “Baseline Clusters (Figure 3)”. This allows viewers to find find the code for each table or figure faster. Using a README.md file A README.md file is used to describe the overall aims and purpose of the analyses in the repository or a folder within a repository. It is often the first file that someone will look at in a repo/folder, so it is important to include information that would be valuable to an outsider trying to make use of the work. To add a README.md file, click “Add file” and then “Create new file”. Name your file “README.md”. A README.md file uses R markdown syntax. This type of syntax is very helpful as you continue to develop R coding skills, as it provides a mechanism through which your code’s output can be visualized and saved as a rendered file version. There are many helpful resources for R markdown, including some that we find helpful: R Markdown Cheatsheet R Markdown Syntax Overview The final README.md file for the OVERALL repository for manuscript submission should look something like the screenshot below. Always include… The main goal of the project The final manuscript name, year it was published, Pub Med ID (if applicable) Graphical abstract (if needed for publication) Names and brief descriptions of each file Include both the goal of the analysis and the methodology used (ie. Using chi square tests to determine if there are statistically significant differences across demographic groups) If the code was written in the software Jupyter (ie. has the extension .ipynb not .R or .Rmd), NBViewer is a website that can render jupyter notebooks (files). This is helpful, because sometimes the files take too long to render, so link the repository from the NB viewer website. Go to nbviewer.org –&gt; type in the name of the repository –&gt; copy the url and add it to the README.md file The final README.md file for the a subfolder within a repository should look something like the screenshot below. Always include… The name of each file Brief description of each file Include both the goal of the analysis and the methodology used Table or Figure name in the corresponding manuscript (if applicable) Note: That the organization structure for the README.md files are simply recommendations and should be changed based on needs of the project. However, it is important to include information and organize the repository in a way that helps other readers and colleagues navigate it who aren’t familiar with the project. Example Repositories Below are links to repositories that contain code for analyses used in published manuscripts. These are examples of well organized Github repositories. Wildfires and Environmental Justice: Future Wildfire Events Predicted to Disproportionally Impact Socioeconomically Vulnerable Communities in North Carolina Plasma sterols and vitamin D are correlates and predictors of ozone-induced inflammation in the lung: A pilot study Cytokine signature clusters as a tool to compare changes associated with tobacco product use in upper and lower airway samples Tracking Code Changes using Github Branches Github is a useful platform for managing and facilitating code tracking performed by different collaborators through branches. When creating a repository on Github, it automatically creates a default branch entitled “main”. It’s possible to create a new branch which allows a programmer to make changes to files in a repository in isolation from the main branch. This is beneficial, because the same file can be compared across branches, potentially created by different scientists, and merged together to reflect those changes. Note: In order for this to work the file in main branch has to have the same name and the file in the newly created branch. Let’s start by creating a new branch. First, navigate to a repository, select “main” and then “View all branches”. Click “New branch”, give your branch a title, and click “Create new branch”. In the screenshot, you’ll see the new branch entitled “jr-changes”. As a new collaborator interested in comparing and merging code changes to a file, click on the new branch that was just created. Based on the screenshot, that means click “jr-changes”. After uploading the file(s) to this branch, you’ll see a notification that this branch is now a certain number of commits ahead of the main branch. A commit records the number of changes to files in a branch. Based on the screenshot, “jr-changes” is now 2 commits ahead of “main”. Click on “2 commits ahead” and scroll down to compare versions between the “main” and “jr-changes” branches. A pull request will need to be created. A pull request allows other collaborators to see changes made to a file within a branch. These proposed changes can be discussed and amended before merging them into the main branch. For more information, visit Github’s branches, pull requests and comparing branches in pull requests documentation. Go ahead and click on “Create pull request”. Click on “Create pull request” again on the next screen. Select “Merge pull request” and then “Confirm merge”. Concluding Remarks In summary, this training module serves as a basic tutorial for sharing code on Github in a way that is beneficial for scientific research. Concepts discussed include uploading and updating code, making a repository easily readable for manuscript submissions, and tracking code changes across collaborators. We encourage trainees and data scientists to implement code tracking and sharing through Github and to also keep up with current trends in data analysis documentation that continue to evolve over time. Test Your Knowledge Try creating your own Github profile, set up a practice repo with subfolders, and a detailed READ.md file paralleling the suggested formatting and content detailed above for your own data analyses! "],["data-wrangling-in-excel.html", "Data Wrangling in Excel Introduction to Training Module Save a Copy of the Soon-To-Be Organized and Cleaned Dataset as a New File Remove Extraneous White Space Replace Missing Data with “NA” Create a Metadata Tab Abbreviate and Capitalize Categorical Data Alphabetize (Sort) the Data by the Categorical Variable of Interest Create a New Subject Number Column Remove Special Symbols and Dashes Bold all Column Names and Center all Data Create a Subject Identifier Column Separate Subject Demographic Data from Experimental Measurements Convert Data from Wide to Long Format Pivoting Data from a Wide to Long Format Generating Summary-Level Statistics with Pivot Tables Excel vs. R: Which Should You Use? Concluding Remarks", " Data Wrangling in Excel This training module was developed by Alexis Payton, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module This module is intended to be a starting guide to cleaning and organizing an example toxicology dataset in Excel. Data wrangling involves cleaning, removing of erroneous data, and restructuring necessary for to preparing wet lab generated data for downstream analyses. These steps will ensure that: Data are amenable to downstream analyses in R, or your preferred programming language Data are clear and easily interpretable by collaborators, reviewers, and readers Click here for more information on data wrangling. In this training tutorial, we’ll make use of an example dataset that needs to be wrangled. The dataset contains concentration values for molecules that were measured using protein-based ELISA technologies. These molecules specifically span 17 sterols and cytokines, selected based upon their important roles in mediating biological responses. These measures were derived from human serum samples. Demographic information also exists for each subject. The following steps detailed in this training module are by no means exhaustive! Further resources are provided at the end. This module provides example steps that are helpful when wrangling your data in Excel. Datasets often come in many different formats from our wet bench colleagues, therefore some steps will likely need to be added, removed, or amended depending on your specific data. Save a Copy of the Soon-To-Be Organized and Cleaned Dataset as a New File Open Microsoft Excel and prior to ANY edits, click “File” –&gt; “Save As” to save a new version of the file that can serve as the cleaned version of the data. This is very important for file tracking purposes, and can help in the instance that the original version needs to be referred back to (e.g., if data are accidentally deleted or modified during downstream steps). The file needs to be named something indicative of the data it contains followed by the current date (e.g., “Allostatic Mediator Data_061622”). The title should be succinct and descriptive. It is okay to use dashes or underscores in the name of the title. Do not include special characters, such as $, #, @, !, %, &amp;, *, (, ), and +. Special characters tend to generate errors on local hard drives when syncing to cloud-based servers, and they are difficult to upload into programming software. Let’s first view what the dataset currently looks like: Helpful Excel Keyboard Shortcuts The following keyboard shortcuts can help you work more efficiently in Excel: Move to the last cell in use on the sheet Control + Fn + Right arrow key (Mac users) Control + End (PC users) Move to the beginning of the sheet Control + Fn + Left arrow key, then same Control + Fn + Up arrow key (Mac users) Control + Home (PC users) Highlight and grab all data Click on the first cell in the upper left hand corner then click and hold Shift + Command + Down arrow key + Right arrow key (Mac users) Shift + Command + Down arrow key + Right arrow key (PC users) Note: This only works if there are no cells with missing information or gaps in the columns/rows used to define the peripheral area. For more available shortcuts on various operating systems click here. Remove Extraneous White Space Before we can begin organizing the data, we need to remove the entirely blank rows of cells. This reduces the file size and allows for the use of the filter function in Excel, as well as other organizing functions, which will be used in the next few steps. This step also makes the data look more tidy and amenable to import for coding purposes. Excel Trick #1: Select all lines that need to be removed and press Control + minus key for Mac and PC users. (Note that there are other ways to do this for larger datasets, but this works fine for this small example.) Excel Trick #2: An easier way to remove blank rows and cells for larger datasets, includes clicking “Find &amp; Select”–&gt; “Special” –&gt; “Blanks” –&gt; click “OK” to select all blank rows and cells. Click “Delete” within the home tab –&gt; “Delete sheet rows”. After removing the blank rows, the file should look like the screenshot below. Replace Missing Data with “NA” There are many ways missing data can be encoded in datasets. This includes values like “blank”, “N/A”, “NA”, or leaving a cell blank. Replacing all missing values with “NA” values is done for 2 reasons: To confirm that the data is indeed missing R reads in “NA” values as missing values To check for missing values, the filter function can be used on each column and only select cells with missing values. You may need to scroll to the bottom of the filter pop up window for numerical data. Enter “NA” into the cell of the filtered column. Double click the bottom right corner of the cell to copy the “NA” down the rest of the column. There was no missing data in this dataset, so this step can be skipped. Create a Metadata Tab Metadata explains what each column represents in the dataset. Metadata is now a required component of data sharing, so it is best to initiate this process prior to data analysis. Ideally, this information is filled in by the scientist(s) who generated the data. Create a new tab (preferably as the first tab) and label it “XXXXX_METADATA” (ie., “Allostatic_METADATA”) Then relabel the original data tab as “XXXX_DATA” (ie., “Allostatic_DATA). Within the metadata tab, create three columns: the first, “Column Identifier”, contains each of the column names found in the data tab; the second, “Code”, contains the individual variable/ abbreviation for each column identifier; the third, “Description” contains additional information and definitions for abbreviations. Abbreviate and Capitalize Categorical Data Categorical data are easier to handle in programming languages when they are capitalized and abbreviated. It also helps reduce typos and potential typing mistakes within your script. For this dataset, the following variables were edited: Group “control” became “NS” for non-smoker “smoker” became “CS” for cigarette smoker Sex “f” became “F” for female “m” became “M” for male Race “AA” became “B” for Black “White” became “W” for White Excel Trick: To change cells that contain the same data simultaneously, navigate to “Edit”, click “Find”, and then “Replace”. Once the categorical data have been abbreviated, add those abbreviations to the metadata and describe what they symbolize. Alphabetize (Sort) the Data by the Categorical Variable of Interest For this dataset, we will sort by the column “Group”. This organizes the data and sets it up for the next step. Highlight all the column headers. Click on the “Sort &amp; Filter” button and click “Filter”. Click on the arrow on cell that contains the column name “Group” and click “Ascending”. Create a New Subject Number Column Analysis-specific subjects are created to give an ordinal subject number to each subject, which allows the scientist to easily identify the number of subjects. In addition, these new ordinal subject numbers will be used to create a subject identifier that combines both a subject’s group and subject number that is helpful for downstream visualization analyses. Relabel the subject number/identifier column as “Original_Subject_Number” and create an ordinal subject number column labeled “Subject_Number”. R reads in spaces between words as periods, therefore it’s common practice to replace spaces with underscores when doing data analysis in R. Avoid using dashes in column names or anywhere else in the dataset. Remove Special Symbols and Dashes Programming languages, in general, do not operate well with special symbols and dashes, particularly when included in column identifiers. For this reason, it is best to remove these while cleaning up your data, prior to importing it into R or your preferred programming software. In this case, this dataset contains dashes and Greek letters within some of the column header identifiers. Here, it is beneficial to remove these dashes (e.g., change IL-10 to IL10) and replace the Greek letters with first letter of the word in English (e.g., change TNF-\\(\\alpha\\) to TNFa). Bold all Column Names and Center all Data These data will likely be shared with collaborators, uploaded onto data deposition websites, and used as supporting information in published manuscripts. For these purposes, it is nice to format data in Excel such that it is visually appealing and easy to digest. For example, here, it is nice to bold column identifiers and center the data, as shown below: Create a Subject Identifier Column The subject identifier column labeled, “Group_Subject_No”, combines the subject number with the variable of interest (ie. Group for this dataset). This is useful for analyses to identify outliers by the subject number and the group. Insert 2 additional columns where the current “Sex” column is. To combine values from two different columns, type “=CONCAT(D1,” _ “,C1)” in the first cell in the first column inserted. Double click the right corner of the cell for the formula to be copied to last row in the dataset. Copy the entire column and paste only the values in the second column by navigating to the drop down arrow next to “Paste” and click “Paste Values”. Label the second column “Group_Subject_No” and delete the first column. Separate Subject Demographic Data from Experimental Measurements This example dataset is very small, so the demographic data (e.g., sex, race, age) was kept within the same file as the experimentally measured molecules. Though in larger datasets (e.g., genome-wide data, exposomic data, etc), it is often beneficial to separate the demographic data into one file that can be labeled according to the following format: “XXX_Subject_Info_061622” (ie. “Allostatic_Subject_Info_061622”). This step was not completed for this current data, since it had a smaller size and the downstream analyses were simple. Convert Data from Wide to Long Format A wide format contains values that DO NOT repeat the subject identifier column. For this dataset, each subject has one row containing all of its data, therefore the subject identifier occurs once in the dataset. Wide Format A long format contains values that DO repeat the subject identifier column. For this dataset, that means a new column was created entitled “Variable” containing all the mediator names and a column entitled “Value” containing all their corresponding values. In the screenshot, an additional column, “Category”, was added to help with the categorization of mediators in R analyses. Long Format The reason a long format is preferred is because it makes visualizations and statistical analyses more efficient in R. In the long format, we were able to add a column entitled “Category” to categorize the mediators into “AL Biomarker” or “Cytokine” allowing us to more easily subset the mediators in R. Read more about wide and long formats here. To convert the data from a wide to long format, follow the steps below: Pivoting Data from a Wide to Long Format To do this, a power query in Excel will be used. Note: If you are working on a Mac, you will need to have at least Excel 2016 installed to follow this tutorial, as Power Query is not avaialble for earlier versions. Add-ins are available for Windows users. See this link for more details. Start by copying all of the data, including the column titles. (Hint: Try using the keyboard shortcut mentioned above.) Click the tab at the top that says “Data”. Then click “Get Data (Power Query)” at the far left. It will ask you to choose a data source. Click “Blank table” in the bottom row. Paste the data into the table. (Hint: Use the shortcut Ctrl + “v”). At this point, your screen should look like the screenshot below. Click “Use first row as headers” and then click “Next” in the bottom right hand corner. Select all the columns with biomarker names. That should be the column “Cortisol” through the end. Click the “Transform” button in the upper left hand corner. Then click “Unpivot columns” in the middle of the pane. The final result should look like the sceenshot below with all the biomarkers now in one column entitled “Attribute” and their corresponding values in another column entitled “Value”. To save this, go back to the “Home” tab and click “Close &amp; load”. You should see something similar to the screenshot below. In the upper right with all the shaded tables (within the “Table” tab), click the arrow to the left of the green table until you see one with no shading. Then click the table with no colors. Click “Convert to Range” within the “Table” tab. This removes the power query capabilities, so that the data is a regular excel sheet. Now the “Category” column can be created to identify the types of biomarkers in the dataset. The allostatic load (AL) biomarkers denoted in the “Category” column include the variables Cortisol, CRP, Fibrinogen, Hba1c, HDL, and Noradrenaline. The rest of the variables were labeled as cytokines. Additionally, we can make this data more closely resemble the final long format screenshot by bolding the headers, centering all the data, etc. We have successfully wrangled our data and the final dataset now looks like this: Generating Summary-Level Statistics with Pivot Tables A PivotTable is a tool in Excel used to summarize numerical data. It’s called a pivot table, because it pivots or changes how the data is displayed to make statistical inferences. This can be useful for generating initial summary-level statistics to guage the distribution of data. To create a PivotTable, start by selecting all of the data. (Hint: Try using the keyboard shortcut mentioned above.) Click “Insert” tab on the upper left-hand side, click “PivotTable”, and click “OK”. The new PivotTable should be available in a new sheet as seen in the screenshot below. A PivotTable will be constructed based on the column headers that can be dragged into the PivotTable fields located on the right-hand side. For example, what if we were interested in determining if there were differences in average expression between non-smokers and cigarette smokers in each category of biomarkers? As seen below, drag the “Group” variable under the “Rows” field and drag the “Value” variable under the “Values” field. Notice that it automatically calculates the sum of the expression values for each group. To change the function to average, click the “i” icon and select “Average”. The output should mirror what’s below with non-smokers having an average expression that’s more than double that of cigarette smokers. Excel vs. R: Which Should You Use? For the most part, it’s better to perform final analyses in R (or another programming language) rather than Excel for the following reasons… R clearly shows the code (instructions), which makes editing, interpretability, and sharing easier. This makes analyses more reproducible and can save time. R has packages that makes more complex analyses possible (i.e., machine learning and heatmaps) that aren’t available in Excel. R can handle larger data sets. R can compute and process data faster. However, Excel is still a software that has many benefits for running analyses including… Excel is user-friendly and most people have experience in navigating the software at a basic level. Excel can be faster for rudimentary statistical analyses and visualizations. Depending on each scientist’s skill-level and the complexity of the analysis, Excel or R could be beneficial. Concluding Remarks In summary, this training module highlights the importance of data wrangling and how to do so in Microsoft Excel for downstream analyses. Concepts discussed include helpful Excel features like power queries and pivot tables and when to use Microsoft Excel vs. R. Additional Resources Data wrangling in Excel can be expedited with knowledge of useful features and functions to format data. Check out the resources below for additional information on Excel tricks. Data Analysis in Excel Excel Spreesheet Hacks Excel for Beginners Test Your Knowledge Try wrangling the “Module1_4_TYKInput.xlsx” to mimic the cleaned versions of the data found in “Module1_4_TYKSolution.xlsx”. This dataset includes sterol and cytokine concentration levels extracted from induced sputum samples collected after ozone exposure. After wrangling, you should end up with a sheet for subject information and a sheet for experimental data. Using the a PivotTable on the cleaned dataset, find the standard deviation of each cytokine variable stratified by the disease status. "],["downloading-and-programming-in-r.html", "Downloading and Programming in R Introduction to Training Module General Introduction and Installation of R and RStudio Introduction to R Packages Scripting Basics Code Troubleshooting Concluding Remarks", " Downloading and Programming in R This training module was developed by Kyle Roell, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module In this training module, we will provide a brief introduction of: R R Studio Packages in R Scripting basics Code troubleshooting General Introduction and Installation of R and RStudio What is R? R is a programming language. Computer script (lines of code) can be used to increase data analysis reproducibility, transparency, and methods sharing, and is becoming increasingly incorporated into exposure science, toxicology, and environmental health research. One of the most commonly used coding languages in the field of environmental health science is the R language. Some advantages of using R include the following: Free, open-source programming language that is licensed under the Free Software Foundation’s GNU General Public License Can be run across all major platforms and operating systems, including Unix, Windows, and MacOS Publicly available packages help you carry out analyses efficiently (without you having to code for everything yourself) Large, diverse collection of packages Comprehensive documentation When code is efficiently tracked during development/execution, it promotes reproducible analyses Because of these advantages, R has emerged as an avenue for world-wide collaboration in data science. Other commonly implemented scripting languages in the field of environmental health research include Python and SAS, among others; and these training tutorials focus on R as an important introductory-level example that also houses many relevant packages and example datasets as further described throughout TAME. Downloading and Installing R To download R, first navigate to https://cran.rstudio.com/ and download the .pkg file for your operating system. Install this file according to your computer’s typical program installation steps. What is RStudio? RStudio is an Integrated Development Environment (IDE) for R, which makes it more ‘user friendly’ when developing and using R script. It is a desktop application that can be downloaded for free, online. Downloading and Installing RStudio To download RStudio: Navigate to: https://posit.co/download/rstudio-desktop/ Scroll down and select “Download RStudio” Install according to your computer’s typical program installation steps RStudio Orientation Here is a screenshot demonstrating what the RStudio desktop app looks like: The default RStudio layout has four main panes (numbered above in the blue boxes): Source Editor: allows you to open and edit script files and view data. Console: where you can type code that will execute immediately when you press enter/return. This is also where code from script files will appear when you run the code. Environment: shows you the objects in your environment. Viewer: has a number of useful tabs, including: Files: a file manager that allows you to navigate similar to Finder or File Explorer Plots: where plots you generate by executing code will appear Packages: shows you packages that are loaded (checked) and those that can be loaded (unchecked) Help: where help pages will appear for packages and functions (see below for further instructions on the help option) Under “Tools” → “Global Options,” RStudio panes can be customized to appear in different configurations or with different color themes. A number of other options can also be changed. For example, you can choose to have colors highlighted the color they appear or rainbow colored parentheses that can help you visualize nested code. Introduction to R Packages One of the major benefits to coding in the R language is access to the continually expanding resource of thousands of user-developed packages. Packages represent compilations of code and functions fitted for a specialized focus or purpose. These are often written by R users and submitted to the CRAN, or another host such as BioConductor or Github. Packages aid in improved data analyses and methods sharing. Packages have varying utilities, spanning basic organization and manipulation of data, visualizing data, and more advanced approaches to parse and analyze data, with examples included in all of the proceeding training modules. Examples of some common packages that we’ll be using throughout these training modules include the following: tidyverse: A collection of open source R packages that share an underlying design philosophy, grammar, and data structures of tidy data. For more information on the tidyverse package, see its associated CRAN webpage, primary webpage, and peer-reviewed article released in 2018. ggplot2: A system for creating graphics. Users provide the data and tell R what type of graph to use, how to map variables to aesthetics (elements of the graph), and additional stylistic elements to include in the graph. For more information on the ggplot2 package, see its associated CRAN webpage and R Documentation. More information on these packages, as well as many others, is included throughout TAME training modules. Downloading/Installing R Packages R packages often do not need to be downloaded from a website. Instead, you can install packages and load them through running script in R. Note that you only need to install packages one time, but packages must be loaded each time you start a new R session. # Install the package install.packages(“tidyverse”) # Load the package for use library(tidyverse) Many packages also exist as part of the baseline configuration of an R working environment, and do not require manual loading each time you launch R. These include the following packages: datasets graphics methods stats utils You can learn more about a function by typing one question mark before the name of the function, which will bring up documentation in the Help tab of the Viewer window. Importantly, this documentation includes a description of the different arguments that can be passed to the function and examples for how to use the function. ?install.packages You can learn more about a package by typing two question marks before the name of the package. This will bring up vingettes and help pages associated with that package. ??tidyverse Scripting Basics Data Types Before writing any script, let’s first review different data types in R. Data types are what they imply – the type of data you are handling. It is important to understand data types because functions often require a specific data type as input. R has 5 basic data types: Logical (e.g., TRUE or FALSE) Integer (e.g., 1, 2, 3) Numeric (real or decimal) Character (e.g., ”apple”) Complex (e.g., 1 + 0i) Numeric variables are often stored as “double” values (sometimes shown as &lt; dbl &gt;), or a decimal type with at least two decimal places. Character variables can also be stored as factors, which are data structures that are implemented to store categorical data in a specific order (also known as levels). Data are stored in data structures. There are many different data structures in R. Some packages even implement unique data structures. The most common data structures are: Vectors: also known as an atomic vector, can contain characters, logical values, integers, or numeric values (but all elements must be the same data type). Matrices: a vector with multiple dimensions. Elements must still be all the same data type. Data frames: similar to a matrix but can contain different data types and additional attributes such as row names (and is one of the most common data structures in environmental health research). Tibbles are a stricter type of data frame implemented in the tidyverse package. Lists: a special type of vector that acts as a container – other data structures can be stored within the list, and lists can contain other lists. Lists can contain elements that are different data structures. Figure 1: Created with BioRender.com Writing Script R code is written line by line. It may take just one line or many lines of code for one step to be executed, depending on the number of arguments to the function you are using. R code is executed (run) by selecting the line(s) of code to run and pressing return/enter (or a keyboard shortcut), or by clicking “Run” in the upper right corner of the script. A very simple example of running code is as follows: 3 + 4 ## [1] 7 We can see that when we ran our code, the answer was returned. But what if we want to store that answer? We can assign that number to a variable named x using the assignment operator &lt;-: x &lt;- 3 + 4 Then, if we run a line of code with our variable, we will get that value: x ## [1] 7 The assignment operator can also be used to assign values to any of the data structures discussed above, such as vectors and data frames, as shown here: # Creating a vector of values called my_values my_values &lt;- c(7, 3, 8, 9) # Viewing the vector my_values ## [1] 7 3 8 9 # Creating a data frame of values corresponding to colors my_df &lt;- data.frame(values = my_values, color = c(&quot;Blue&quot;, &quot;Red&quot;, &quot;Yellow&quot;, &quot;Purple&quot;)) # Viewing the data frame my_df ## values color ## 1 7 Blue ## 2 3 Red ## 3 8 Yellow ## 4 9 Purple Comments You may have noticed in the code chunks above that there were # followed by phrases describing the code. R allows for scripts to contain non-code elements, called comments, that will not be run or interpreted. Comments are useful to help make code more interpretable for others or to add reminders of what and why parts of code may have been written. To make a comment, simply use a # followed by the comment. A # only comments out a single line of code. In other words, only that line will be commented and therefore not be run, but lines directly above/below it will still be run: # This is an R comment! For more on comments, see TAME 2.0 Module 2.2 Coding Best Practices. Autofilling RStudio will autofill function names and object names as you type, which can save a lot of time. When you are typing a variable or function name, you can press tab while typing. RStudio will look for variables or functions that match the first few letters you’ve typed. If multiple matches are found, RStudio will provide you with a drop down list to select from, which may be useful when searching through newly installed packages or trying to quickly type variable names in an R script. For example, let’s say we instead named our example data frame something much longer, and we had two data frames with similar names. If we start typing in my_ and pause our typing, all of the objects that start with that name will appear as options in a list. To select which one to autofill, navigate down the list and click return/enter. my_df_with_really_long_name &lt;- data.frame(values = my_values, color = c(&quot;Blue&quot;, &quot;Red&quot;, &quot;Yellow&quot;, &quot;Purple&quot;)) my_df_with_really_long_name_2 &lt;- data.frame(values = my_values, color = c(&quot;Green&quot;, &quot;Teal&quot;, &quot;Magenta&quot;, &quot;Orange&quot;)) Finding and Setting Your Working Directory Another step that is commonly done at the very beginning of your code is setting your working direction. This tells your computer where to look for files that you want to import and where to deposit output files produced during your scripted activities. To view your current working directory, run the following: getwd() To set or change the location of your working directory, run the following: setwd(&quot;/file path to where your input files are&quot;) Note that macOS file paths use / to separate folders, whereas PC file paths use \\. You can easily find the file path to your desired working directory by navigating to “Session”, then “Set Working Directory”, and “Choose Directory”: In the popup box, navigate to the folder you want to set as your working directory and click “Open.” Look in the R console, which will now contain a line of code with setwd() containing your file path. You can copy this line of code to the top of your script for future use. Alternatively, you can navigate to the folder you want in Finder or File Explorer and right click to see the file path. Within your working directory, you can make sub-folders to keep your analyses organized. Here is an example folder hierarchy: How you set up your folder hierarchy is highly dependent on your specific analysis and coding style. However, we recommend that you: Name your script something concise, but descriptive (no acronyms) Consider using dates when appropriate Separate your analysis into logical sections so that script doesn’t get too long or hard to follow Revisit and adapt your organization as the project evolves! Archive old code so you can revisit it A Quick Note About Projects Creating projects allows you to store your progress (open script, global environment) for one project in an R Project File. This facilitates quick transitions between multiple projects. Find detailed information about how to set up projects here. Importing Files After setting the working directory, you can import and export files using various functions based on the type of file being imported or exported. Often, it is easiest to import data into R that are in a comma separated value / comma delimited file (.csv) or tab / text delimited file (.txt). Other datatypes such as SAS data files or large .csv files may require different functions to be more efficiently read in, and some of these file formats will be discussed in future modules. Files can also be imported and exported from Excel using the openxlsx package. Below, we will demonstrate how to read in .csv and .txt files: # Read in the .csv data that&#39;s located in our working directory csv.dataset &lt;- read.csv(&quot;Module2_1_Input/Module2_1_InputData1.csv&quot;) # Read in the .txt data txt.dataset &lt;- read.table(&quot;Module2_1_Input/Module2_1_InputData1.txt&quot;) These datasets now appear as saved dataframes (“csv.dataset” and “txt.dataset”) in our working environment. Viewing Data After data have been loaded into R, or created within R, you will likely want to view what these datasets look like. Datasets can be viewed in their entirety, or datasets can be subsetted to quickly look at part of the data. Here’s some example script to view just the beginnings of a dataframe using the head() function: head(csv.dataset) ## Sample Var1 Var2 Var3 ## 1 sample1 1 2 1 ## 2 sample2 2 4 4 ## 3 sample3 3 6 9 ## 4 sample4 4 8 16 ## 5 sample5 5 10 25 Here, you can see that this automatically brings up a view of the first five rows of the dataframe. Another way to view the first five rows of a dataframe is to run the following: csv.dataset[1:5,] ## Sample Var1 Var2 Var3 ## 1 sample1 1 2 1 ## 2 sample2 2 4 4 ## 3 sample3 3 6 9 ## 4 sample4 4 8 16 ## 5 sample5 5 10 25 This brings us to an important concept - indexing! Brackets are used in R to index. Within the bracket, the first argument represents the row numbers, and the second argument represents the column numbers. A colon between two numbers means to select all of the columns in between the left and right numbers. The above line of code told R to select rows 1 to 5, and, by leaving the column argument blank, all of the columns. Expanding on this, to view the first 5 rows and 2 columns, we can run the following: csv.dataset[1:5, 1:2] ## Sample Var1 ## 1 sample1 1 ## 2 sample2 2 ## 3 sample3 3 ## 4 sample4 4 ## 5 sample5 5 For another example: What if we want to only view the first and third row, and first and fourth column? We can use a vector within the index to do this: csv.dataset[c(1, 3), c(1, 4)] ## Sample Var3 ## 1 sample1 1 ## 3 sample3 9 To view the entire dataset, use the View() function: View(csv.dataset) Another way to view a dataset is to just click on the name of the data in the environment pane. The view window will pop up in the same way that it did with the View() function. Determining Data Structures and Data Types As discussed above, there are a number of different data structures and types that can be used in R. Here, we will demonstrate functions that can be used to identify data structures and types within R objects. The glimpse() function, which is part of the tidyverse package, is helpful because it allows us to see an overview of our column names and the types of data contained within those columns. # Load tidyverse package library(tidyverse) glimpse(csv.dataset) ## Rows: 5 ## Columns: 4 ## $ Sample &lt;chr&gt; &quot;sample1&quot;, &quot;sample2&quot;, &quot;sample3&quot;, &quot;sample4&quot;, &quot;sample5&quot; ## $ Var1 &lt;int&gt; 1, 2, 3, 4, 5 ## $ Var2 &lt;int&gt; 2, 4, 6, 8, 10 ## $ Var3 &lt;int&gt; 1, 4, 9, 16, 25 Here, we see that our Sample column is a character column, while the rest are integers. The class() function is also helpful for understanding objects in our global environment: # What class (data structure) is our object? class(csv.dataset) ## [1] &quot;data.frame&quot; # What class (data type) is a specific column in our data? class(csv.dataset$Sample) ## [1] &quot;character&quot; These functions are particularly helpful when introducing new functions or troubleshooting code because functions often require input data to be a specific structure or data type. Exporting Data Now that we have these datasets saved as dataframes, we can use these as examples to export data files from the R environment back into our local directory. There are many ways to export data in R. Data can be written out into a .csv file, tab delimited .txt file, or RData file, for example. There are also many functions within packages that write out specific datasets generated by that package. To write out to a .csv file: write.csv(csv.dataset, &quot;Module2_1_SameCSVFileNowOut.csv&quot;) To write out a .txt tab delimited file: write.table(txt.dataset, &quot;Module2_1_SameTXTFileNowOut.txt&quot;) R also allows objects to be saved in RData files. These files can be read into R, as well, and will load the object into the current workspace. Entire workspaces are also able to be saved in RData files, such that when you open an RData file, your script and Global Environment will be just as you saved them. Below includes example code to carry out these tasks, and though these files are not provided, they are just example code for future reference. # Read in saved single R data object r.obj = readRDS(&quot;data.rds&quot;) # Write single R object to file saveRDS(object, &quot;single_object.rds&quot;) # Read in multiple saved R objects load(&quot;multiple_data.RData&quot;) # Save multiple R objects save(object1, object2, &quot;multiple_objects.RData&quot;) # Save entire workspace save.image(&quot;entire_workspace.RData&quot;) # Load entire workspace load(&quot;entire_workspace.RData&quot;) Code Troubleshooting Learning how to code is an iterative, exploratory process. The secret to coding is to… Make sure to include “R” and the package and/or function name in your search. Don’t be afraid to try out different solutions until you find one that works for you, but also know when it is time to ask for help. For example, when you have tried solutions available on forums, but they aren’t working for you, or you know a colleague has already spent a significant amount of time developing code for this specific task. Note that when reading question/answer forums, make sure to look at how recent a post is, as packages are updated frequently, and old answers may or may not work. Some common reasons that code doesn’t work and potential solutions to these problems include: Two packages are loaded that have functions with the same name, and the default function is not the one you are intending to run. Solutions: specify the package that you want the function to be called from each time you use it (e.g., dplyr::select()) or re-assign that function at the beginning of your script (e.g., select &lt;- dplyr::select) Your data object is the wrong input type (is a data frame and needs to be a matrix, is character but needs to be numeric) Solution: double check the documentation (?functionname) for the input/variable type needed You accidentally wrote over your data frame or variable with another section of code Solution: re-run your code from the beginning, checking that your input is in the correct format There is a bug in the function/package you are trying to use (this is most common after packages are updated or after you update your version of R) Solution: post an issue on GitHub for that package (or StackOverflow if there is not a GitHub) using a reproducible example There are a number of forums that can be extremely helpful when troubleshooting your code, such as: Stack Overflow: one of the most common forums to post questions related to coding and will often be the first few links in a Google search about any code troubleshooting. It is free to make an account, which allows you to post and answer questions. Cross Validated: a forum focused on statistics, including machine learning, data analysis, data mining, and data visualization, and is best for conceptual questions related to how statistical tests are carried out, when to use specific tests, and how to interpret tests (rather than code execution questions, which are more appropriate to post on Stack Overflow). BioConductor Forum: provides a platform for specific coding and conceptual questions about BioConductor packages. GitHub: can also be used to create posts about specific issues/bugs for functions within that package. Before you post a question, make sure you have thoroughly explored answers to existing similar questions and are able to explain in your question why those haven’t worked for you. You will also need to provide a reproducible example of your error or question, meaning that you provide all information (input data, packages, code) needed such that others can reproduce your exact issues. While demonstrating a reproducible example is beyond the scope of this module, see the below links and packages for help getting started: Detailed step-by-step guides for how to make reproducible examples: How to Reprex by Ariel Muldoon What’s a reproducible example (reprex) and how do I create one? Helpful packages: reprex: part of tidyverse, useful for preparing reproducible code for posting to forums. datapasta: useful for creating code you can copy and paste that creates a new data frame as a subset of your original data. Concluding Remarks Together, this training module provides introductory level information on installing and loading packages in R, scripting basics, importing and exporting data, and code troubleshooting. Additional Resources Coursera Stack Overflow How to Learn R R for Data Science Test Your Knowledge Install R and RStudio on your computer. Launch RStudio and explore installing packages (e.g., tidyverse) and understanding data types using the built-in datasets in R. Make a vector of the letters A-E. Make a data frame of the letters A-E in one column and their corresponding number in the alphabet order in the second column (e.g., A corresponds with 1). "],["coding-best-practices.html", "Coding “Best” Practices Introduction to Training Module Scripting File Types Script Headers and Annotation Coding Style Script Organization Concluding Remarks", " Coding “Best” Practices This training module was developed by Kyle Roell, Alexis Payton, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module In this training module, we will be going over coding “best” practices. The reason we put “best” in quotes is because these practices are what we currently consider best or better, though everyone has different coding styles, annotation styles, etc that also change over time. Here, we hope to give you a sense of what we do when coding, why we do it, and why we think it is important. We will also be pointing out other guides to style, annotations, and best practices that we suggest implementing into your own coding. Some of the questions we hope to answer in this module are: What type of scripting file should I use? What should I name my script? What should I put at the top of every script and why is it important? How should I annotate my code? Why are annotations important? How do I implement these coding practices into my own code? Where can I find other resources to help with coding best practices? In the following sections, we will be addressing these questions. Keep in mind that the advice and suggestions in this section are just that: advice and suggestions. So please take them into consideration and integrate them into your own coding style as appropriate. Scripting File Types Two of the most common scripting file types applicable to the R language are .R (normal R files) and .Rmd (R Markdown). Normal R files appear as plain text and can be used for running any normal R code. R Markdown files are used for more intensive documentation of code and allow for a combination of code, non-code text explaining the code, and viewing of code output, tables, and figures that are rendered together into an output file (typically .html, although other formats such as .pdf are also offered). For example, TAME is coded using R Markdown, which allows us to include blocks of non-code text, hyperlinks, annotated code, schematics, and output figures all in one place. We highly encourage the use of R Markdown as the default scripting file type for R-based projects because it produces a polished final document that is easy for others to follow, whereas .R files are more appropriate for short, one-off analyses and writing in-depth functions and packages. However, code executed in normal .R files and R Markdown will produce the same results, and ultimately, which file type to use is personal preference. See below for screenshots that demonstrate some of the stylistic differences between .R, .Rmd, and .Rmd knitted to HTML format: If you are interested in learning more about the basic features of R Markdown and how to use them, see the following resources: RStudio introduction to R Markdown R Markdown Cheat Sheet Bookdown R Markdown guide Including external images in R Markdown with knitr Interactive plots with plotly Interactive data tables with DT Naming the Script File The first thing we need to talk about, which is sometimes overlooked in the discussion of coding practices, is script file naming conventions and high level descriptive headers within a script. It is important to remember to name your code something concise, but descriptive. You want to be able to easily recognize what the script is for and does without a cumbersome, lengthy title. Some tips for naming conventions: Be concise, but descriptive Use dates when appropriate Avoid special characters Use full words if possible, avoiding non-standard acronyms Keep in mind that each script should have a clear purpose within a given project. And, it is sometimes necessary, and often common, to have multiple scripts within one project that all pertain to different parts of the analysis. For example, it may be appropriate to have one script for data cleaning and pre-processing and another script for analyzing data. When scripting an analysis with multiple sub-analyses, some prefer to keep code for each sub-analysis separate (e.g., one file for an ANOVA and one file for a k-means analysis on the same data input), while others prefer to have longer code files with more subsections. Whichever method you choose, we recommend maintaining clear documentation that indicates locations for input and output files for each sub-analysis (e.g., whether global environment objects or output files from a previous script are needed to run the current script). Script Headers and Annotation Script Header Once your script is created and named, it is generally recommended to include a header at the top of the script. The script header can be used for describing: Title of Script - This can be a longer or more readable name than script file name. Author(s) - Who wrote the script? Date - When was the script developed? Description - Provides a more detailed description of the purpose of the script and any notes or special considerations for this particular script. In R, it is common to include multiple #, the comment operator, or a # followed by another special character, to start and end a block of coding annotation or the script header. An example of this in an .R file is shown below: ######################################################################## ######################################################################## ### Script Longer Title ### ### Description of what this script does! ### Also can include special notes or anything else here. ### ### Created by: Kyle Roell and Julia Rager ### Last updated: 01 May 2023 ######################################################################## ######################################################################## This block of comment operators is common in .R but not .Rmd files because .Rmd files have their own specific type of header, known as the YAML, which contains the title, author, date, and formatting outputs for the .Rmd file: We will now review how annotations within the script itself can make a huge difference in understanding the code within. Annotations Before we review coding style considerations, it is important to address code annotating. So, what are annotations and why are they important? Annotations are notes embedded within your code as comments that will not be run. The beauty of annotating your code is that not only others, but future you, will be able to read through and better understand what a particular piece of code does. We suggest annotating your code while you write it and incorporate a lot of description. While not every single line needs an annotation, or a very detailed one, it is helpful to provide comments and annotation as much as you can while maintaining feasibility. General annotation style In general, annotations will be short sentences that describe what your code does or why you are executing that specific code. This can be helpful when you are defining a covariate a specific way, performing a specific analytical technique, or just generally explaining why you are doing what you’re doing. # Performing logistic regression to assess association between xyz and abc # Regression confounders: V1, V2, V3 ... xyz.regression.output = glm(xyz ~ abc + V1 + V2 + V3, family=binomial(), data=example.data) Mid-script headings Another common approach to annotations is to use mid-script type headings to separate out the script into various sections. For example, you might want to create distinct sections for “Loading Packages, Data, and Setup”, “Covariate Definition”, “Correlation Analysis”, “Regression Analysis”, etc. This can help you, and others, reading your script, to navigate the script more easily. It also can be more visually pleasing to see the script split up into multiple sections as opposed to one giant chunk of code interspersed with comments. Similar to above, the following example is specific to .R files. For .Rmd files, sub headers can be created by increasing the number of # before the header. ########################################################################### ########################################################################### ### ### Regression Analyses ### ### You can even add some descriptions or notes here about this section! ### ########################################################################### # Performing logistic regression to assess association between xyz and abc # Regression confounders: V1, V2, V3 ... xyz.regression.output = glm(xyz ~ abc + V1 + V2 + V3, family=binomial(), data=example.data) General tips for annotations: Make comments that are useful and meaningful You don’t need to comment every single line In general, you probably won’t over-comment your script, so more is generally better That being said, don’t write super long paragraphs every few lines Split up your script into various sections using mid-script headings when appropriate Quick, short comments and annotations While it is important to provide descriptive annotations, not every one needs to be a sentence or longer. As stated previously, it is not necessary to comment every single line. Here is an example of very brief commenting: # Loading necessary packages library(ggplot2) # Plotting package In the example above, we can see that these short comments clearly convey what the script does – load the necessary package and indicate what the package is needed for. Short, one line annotations can also be placed after lines to clarify that specific line or within the larger mid-script headings to split up these larger sections of code. Coding Style Coding style is often a contentious topic! There are MANY styles of coding, and no two coders have the same exact style, even if they are following the same reference. Here, we will provide some guides to coding style and go over some of the basic, general tips for making your code readable and efficient. Here is an example showing how you can use spacing to align variable assignment: # Example of using spacing for alignment of variable assignment Longer_variable_name_x = 1 Short_name_y = 2 Note that guides will suggest you use &lt;- as the assignment operator. However, for most situations, &lt;- and = will do the same thing. For spacing around certain symbols and operators: Include a space after if, before parenthesis Include a space on either side of symbols such as &lt; The first (opening) curly brace should not be on its own line, but the second (closing) should # Example of poor style if(Longer_variable_name_x &lt;Short_name_y) {Short_name_y = 0} # Example of better style if (Longer_variable_name_x &lt; Short_name_y) { Short_name_y = 0 } Summary of general tips for coding style: Variable names Make them intuitive, short, but descriptive Use the same convention throughout (ex: separating words with . or _) Data names Use _datatype (e.g., cytokines_df for a dataframe data type) following a short, concise name for your data so that readers know what type of input is being used AND/OR Clearly define important data frames or other input in your code comments (e.g., “cytokines” refers to a data frame containing all of the cytokine data from this experiment) Separate long lines onto two or more lines (typically for loops or functions) Use &lt;- for assignment operator Using spacing appropriately for readability Alignment of lines After certain keywords when appropriate Be consistent throughout Example: if you use Tidyverse conventions, continue to use it throughout your script Try to make your code as readable as possible Common style guides: Google’s Style Guide Tidyverse Style Guide R Blogger’s Best Coding Practices Script Organization Lastly, it is important to note that organizing your script efficiently can help with readability as well. In general, as stated before, the beginning of your script should contain some sort of header lines to describe the script. The basic ordering we suggest for most scripts is: Header section Loading libraries and data Function definitions (if any user defined functions exist) Data and variables manipulation Analyses While following this exact organization isn’t absolute, using this structure or something similar can greatly improve the clarity of your analyses and make them easier for others to follow. Concluding Remarks In this module, we demonstrate basic coding style and best practices. Please reference additional style guides (above) and create your own style from best practices that work for you. Aim to make your code understandable and readable, and consider what a future reader, including yourself, will need to understand why you wrote your code the way you did and how to apply your code to new analyses. Test Your Knowledge Using the input file provided (“Module2_2_TYKInput.R”): Convert the script and annotations into R Markdown format. Improve the organization, comments, and scripting to follow the coding best practices described in this module. List the changes you made at the bottom of the new R Markdown file. Notes on the starting code: This starting code uses dummy data to demonstrate how to make a graph in R that includes bars representing the mean, with standard deviation error bars overlaid. You don’t need to understand every step in the code to be able to improve the existing coding style! You can run each step of the code if needed to understand better what it does. "],["data-manipulation-and-reshaping.html", "Data Manipulation and Reshaping Introduction to Training Module Data Manipulation Using Base R Introduction to Tidyverse Concluding Remarks", " Data Manipulation and Reshaping This training module was developed by Kyle Roell, Alexis Payton, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Data within the fields of exposure science, toxicology, and public health are very rarely prepared and ready for downstream statistical analyses and visualization code. The beginning of almost any scripted analysis includes important formatting steps that make the data easier to read and work with. This can be done in several ways, including: Base R operations and functions, or A collection of packages (and philosophy) known as The Tidyverse. In this training tutorial we will review some of the most common ways you can organize and manipulate data, including: Merging data Filtering and subsetting data Pivoting data wider and longer (also known as casting and melting) These approaches will first be demonstrated using the functions available in base R. Then, the exact same approaches will be demonstrated using the functions and syntax that are part of the Tidyverse package. We will demonstrate these data manipulation and organization methods using an environmentally relevant example data set from a human cohort. This dataset was generated by creating data distributions randomly pulled from our previously published cohorts, resulting in a unique data set for these training purposes. The dataset contains environmental exposure metrics from metal levels obtained using sources of drinking water and human urine samples and associated demographic data. Training Module’s Environmental Health Question This training module was specifically developed to answer the following environmental health question using data manipulation and reshaping approaches: What is the average urinary chromium concentration across different maternal education levels? We’ll use base R and Tidydverse to answer this question, but let’s start with Base R. Workspace Preparation and Data Import Set your working directory In preparation, first let’s set our working directory to the folder path that contains our input files: setwd(&quot;/file path to where your input files are&quot;) Note that macOS file paths use / as folder separators, and PC file paths use \\. Importing example datasets Next, let’s read in our example data sets: demographic_data &lt;- read.csv(&quot;Module2_3_Input/Module2_3_InputData1.csv&quot;) chemical_data &lt;- read.csv(&quot;Module2_3_Input/Module2_3_InputData2.csv&quot;) Viewing example datasets Let’s see what these datasets look like: dim(demographic_data) ## [1] 200 6 dim(chemical_data) ## [1] 200 7 The demographic data set includes 200 rows x 7 columns, while the chemical measurement data set includes 200 rows x 7 columns. We can preview the demographic data frame by using the head() function, which displays all the columns and the first 6 rows of a data frame: head(demographic_data) ## ID BMI MAge MEdu BW GA ## 1 1 27.7 22.99928 3 3180.058 34 ## 2 2 26.8 30.05142 3 3210.823 43 ## 3 3 33.2 28.04660 3 3311.551 40 ## 4 4 30.1 34.81796 3 3266.844 32 ## 5 5 37.4 42.68440 3 3664.088 35 ## 6 6 33.3 24.94960 3 3328.988 40 These demographic data are organized according to subject ID (first column) followed by the following subject information: ID: subject number BMI: body mass index MAge: maternal age in years MEdu: maternal education level; 1 = “less than high school”, 2 = “high school or some college”, 3 = “college or greater” BW: body weight in grams GA: gestational age in weeks We can also preview the chemical dataframe: head(chemical_data) ## ID DWAs DWCd DWCr UAs UCd UCr ## 1 1 6.426464 1.292941 51.67987 10.192695 0.7537104 42.60187 ## 2 2 7.832384 1.798535 50.10409 11.815088 0.9789506 41.30757 ## 3 3 7.516569 1.288461 48.74001 10.079057 0.1903262 36.47716 ## 4 4 5.906656 2.075259 50.92745 8.719123 0.9364825 42.47987 ## 5 5 7.181873 2.762643 55.16882 9.436559 1.4977829 47.78528 ## 6 6 9.723429 3.054057 51.14812 11.589403 1.6645837 38.26386 These chemical data are organized according to subject ID (first column), followed by measures of: DWAs: drinking water arsenic levels in µg/L DWCd: drinking water cadmium levels in µg/L DWCr: drinking water chromium levels in µg/L UAs: urinary arsenic levels in µg/L UCd: urinary cadmium levels in µg/L UCr: urinary chromium levels in µg/L Data Manipulation Using Base R Merging Data Using Base R Syntax Merging datasets represents the joining together of two or more datasets, using a common identifier (generally some sort of ID) to connect the rows. This is useful if you have multiple datasets describing different aspects of the study, different variables, or different measures across the same samples. Samples could correspond to the same study participants, animals, cell culture samples, environmental media samples, etc, depending on the study design. In the current example, we will be joining human demographic data and environmental metals exposure data collected from drinking water and human urine samples. Let’s start by merging the example demographic data with the chemical measurement data using the base R function merge(). To learn more about this function, you can type ?merge, which brings up helpful information in the R console. To merge these datasets with the merge function, use the following code. The by = argument specifies the column used to match the rows of data. full.data &lt;- merge(demographic_data, chemical_data, by = &quot;ID&quot;) dim(full.data) ## [1] 200 12 This merged dataframe contains 200 rows x 12 columns. Viewing this merged dataframe, we can see that the merge() function retained the first column in each original dataframe (ID), though did not replicate it since it was used as the identifier for merging. All other columns include their original data, just merged together by the IDs in the first column. head(full.data) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 These datasets were actually quite easy to merge, since they had the same exact column identifier and number of rows. You can edit your script to include more specifics in instances when these may differ across datasets that you would like to merge. This option allows you to edit the name of the column that is used in each dataframe. Here, these are still the same “ID”, but you can see that adding the by.x and by.y arguments allows you to specify instances when different column names are used in the two datasets. full.data &lt;- merge(demographic_data, chemical_data, by.x = &quot;ID&quot;, by.y = &quot;ID&quot;) # Viewing data head(full.data) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 Note that after merging datasets, it is always helpful to check that the merging was done properly before proceeding with your data analysis. Helpful checks could include viewing the merged dataset, checking the numbers of rows and columns to make sure chunks of data are not missing, and searching for values (or strings) that exist in one dataset but not the other, among other mechanisms of QA/QC. Filtering and Subsetting Data Using Base R Syntax Filtering and subsetting data are useful tools when you need to focus on specific parts of your dataset for downstream analyses. These could represent, for example, specific samples or participants that meet certain criteria that you are interested in evaluating. It is also useful for removing unneeded variables or samples from dataframes as you are working through your script. Note that in the examples that follow, we will create new dataframes that are distinguished from our original dataframe by adding sequential numbers to the end of the dataframe name (e.g., subset.data1, subset.data2, subset.data3). This style of dataframe naming is useful for the simple examples we are demonstrating, but in a full scripted analysis, we encourage the use of more descriptive dataframe names. For example, if you are subsetting your data to include only the first 100 rows, you could name that dataframe “data.first100.” For this example, let’s first define a vector of columns that we want to keep in our analysis, then subset the data by keeping only the columns specified in our vector: # Defining a vector of columns to keep in the analysis subset.columns &lt;- c(&quot;BMI&quot;, &quot;MAge&quot;, &quot;MEdu&quot;) # Subsetting the data by selecting the columns represented in the defined &#39;subset.columns&#39; vector subset.data1 &lt;- full.data[,subset.columns] # Viewing the top of this subsetted dataframe head(subset.data1) ## BMI MAge MEdu ## 1 27.7 22.99928 3 ## 2 26.8 30.05142 3 ## 3 33.2 28.04660 3 ## 4 30.1 34.81796 3 ## 5 37.4 42.68440 3 ## 6 33.3 24.94960 3 We can also easily subset data based on row numbers. For example, to keep only the first 100 rows: subset.data2 &lt;- full.data[1:100,] # Viewing the dimensions of this new dataframe dim(subset.data2) ## [1] 100 12 To remove the first 100 rows, we use the same code as above, but include a - sign before our vector to indicate that these rows should be removed: subset.data3 &lt;- full.data[-c(1:100),] # Viewing the dimensions of this new dataframe dim(subset.data3) ## [1] 100 12 Conditional statements are also written to filter and subset data. A conditional statement is written to execute one block of code if the statement is true and a different block of code if the statement is false. A conditional statement requires a Boolean or true/false statement that will be either TRUE or FALSE. A couple of the more commonly used functions used to create conditional statements include… if(){} or an if statement means “execute R code when the condition is met”. if(){} else{} or an if/else statement means “execute R code when condition 1 is met, if not execute R code for condition 2”. ifelse() is a function that executes the same logic as an if/else statement. The first argument specifies a condition to be met. If that condition is met, R code in the second argument is executed, and if that condition is not met, R code in the third argument is executed. There are six comparison operators that are used to created these Boolean values: == means “equals”. != means “not equal”. &lt; means “less than”. &gt; means “greater than”. &lt;= means “less than or equal to”. &gt;= mean “greater than or equal to”. There are also three logical operators that are used to create these Boolean values: &amp; means “and”. | means “or”. ! means “not”. We can filter data based on conditions using the subset() function. For example, the following code filters for subjects whose BMI is greater than 25 and who have a college education: subset.data4 &lt;- subset(full.data, BMI &gt; 25 &amp; MEdu == 3) Additionally, we can subset and select specific columns we would like to keep, using the select argument within the subset() function: # Filtering for subjects whose BMI is less than 22 or greater than 27 # Also selecting the BMI, maternal age, and maternal education columns subset.data5 &lt;- subset(full.data, BMI &lt; 22 | BMI &gt; 27, select = subset.columns) For more information on the subset() function, see its associated documentation. Melting and Casting Data using Base R Syntax Melting and casting refers to the conversion of data to “long” or “wide” form as discussed previously in TAME 2.0 Module 1.4 Data Wrangling in Excel. You will often see data within the environmental health field in wide format, though long format is necessary for some procedures, such as plotting with ggplot2 and performing certain analyses. Here, we’ll illustrate some example script to melt and cast data using the reshape2 package. Let’s first install and load the reshape2 package: if (!requireNamespace(&quot;reshape2&quot;)) install.packages(&quot;reshape2&quot;); library(reshape2) ## ## Attaching package: &#39;reshape2&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## smiths Using the fully merged dataframe, let’s remind ourselves what these data look like in the current dataframe format: head(full.data) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 These data are represented by single subject identifiers listed as unique IDs per row, with associated environmental measures and demographic data organized across the columns. Thus, this dataframe is currently in wide (also known as casted) format. Let’s convert this dataframe to long (also known as melted) format. Here, will will specify that we want a row for each unique sample ID + variable measure pair by using id = \"ID\": full.melted &lt;- melt(full.data, id = &quot;ID&quot;) # Viewing this new dataframe head(full.melted) ## ID variable value ## 1 1 BMI 27.7 ## 2 2 BMI 26.8 ## 3 3 BMI 33.2 ## 4 4 BMI 30.1 ## 5 5 BMI 37.4 ## 6 6 BMI 33.3 You can see here that each measure that was originally contained as a unique column has been reoriented, such that the original column header is now listed throughout the second column labeled variable. Then, the third column contains the value of this variable. Let’s see an example view of the middle of this new dataframe: full.melted[1100:1110,1:3] ## ID variable value ## 1100 100 DWAs 7.928885 ## 1101 101 DWAs 8.677403 ## 1102 102 DWAs 8.115183 ## 1103 103 DWAs 7.134189 ## 1104 104 DWAs 8.816142 ## 1105 105 DWAs 7.487227 ## 1106 106 DWAs 7.541973 ## 1107 107 DWAs 6.313516 ## 1108 108 DWAs 6.654474 ## 1109 109 DWAs 7.564429 ## 1110 110 DWAs 7.357122 Here, we can see a different variable (DWAs) now being listed. This continues throughout the entire dataframe, which has the following dimensions: dim(full.melted) ## [1] 2200 3 Let’s now re-cast this dataframe back into wide format using the dcast() function. Here, we are telling the dcast() function to give us a sample (ID) for every variable in the column labeled variable. The column names from the variable column and corresponding values from the value column are then used to fill in the dataset: full.cast &lt;- dcast(full.melted, ID ~ variable) head(full.cast) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 Here, we can see that this dataframe is back in its original casted (or wide) format. Now that we’re familiar with some base R functions to reshape our data, let’s answer our original question: What is the average urinary chromium concentration for each maternal education level? Although it is not necessary to calculate the average, we could first subset our data frame to only include the two columns we are interested in (MEdu and UCr): subset.data6 &lt;- full.data[,c(&quot;MEdu&quot;, &quot;UCr&quot;)] head(subset.data6) ## MEdu UCr ## 1 3 42.60187 ## 2 3 41.30757 ## 3 3 36.47716 ## 4 3 42.47987 ## 5 3 47.78528 ## 6 3 38.26386 Next, we will make a new data frame for each maternal education level: # Creating new data frames based on maternal education category data.matedu.1 &lt;- subset(subset.data6, MEdu == 1) data.matedu.2 &lt;- subset(subset.data6, MEdu == 2) data.matedu.3 &lt;- subset(subset.data6, MEdu == 3) # Previewing the first data frame to make sure our function is working as specified head(data.matedu.1) ## MEdu UCr ## 14 1 38.59349 ## 18 1 47.77878 ## 37 1 35.33980 ## 63 1 34.72255 ## 66 1 34.13982 ## 76 1 31.38145 Last, we can calculate the average urinary chromium concentration using each of our data frames: mean(data.matedu.1$UCr) ## [1] 39.88055 mean(data.matedu.2$UCr) ## [1] 40.61807 mean(data.matedu.3$UCr) ## [1] 40.41556 With this, we can answer our Environmental Health Question: What is the average urinary chromium concentration across different maternal education levels? Answer: The average urinary Chromium concentrations are 39.9 µg/L for participants with less than high school education, 40.6 µg/L for participants with high school or some college education, and 40.4 µg/L for participants with college education or greater. Introduction to Tidyverse Tidyverse is a collection of packages that are commonly used to more efficiently organize and manipulate datasets in R. This collection of packages has its own specific type of syntax and formatting that differ slightly from base R functions. There are eight core tidyverse packages: For data visualization and exploration: ggplot2 For data wrangling and transformation: dplyr tidyr stringr forcats For data import and management: tibble readr For functional programming: purr Here, we will carry out all the of the same data organization exercises demonstrated above using packages that are part of The Tidyverse, specifically using functions that are part of the dplyr and tidyr packages. Downloading and Loading the Tidyverse Package If you don’t have tidyverse already installed, you will need to install it using: if(!require(tidyverse)) install.packages(&quot;tidyverse&quot;) And then load the tidyverse package using: library(tidyverse) Note that by loading the tidyverse package, you are also loading all of the packages included within The Tidyverse and do not need to separately load these packages. Merging Data Using Tidyverse Syntax To merge the same example dataframes using tidyverse, you can run the following script: full.data.tidy &lt;- inner_join(demographic_data, chemical_data, by = &quot;ID&quot;) head(full.data.tidy) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 Note that you can still merge dataframes that have different ID column names with the argument by = c(\"ID.x\", \"ID.y\"). tidyverse also has other join, functions, shown in the graphic below (source): inner_join keeps only rows that have matching ID variables in both datasets full_join keeps the rows in both datasets left_join matches rows based on the ID variables in the first dataset (and omits any rows from the second dataset that do not have matching ID variables in the first dataset) right_join matches rows based on ID variables in the second dataset (and omits any rows from the first dataset that do not have matching ID variables in the second dataset) anti_join(x,y) keeps the rows that are unique to the first dataset anti_join(y,x) keeps the rows that are unique to the second dataset The Pipe Operator One of the most important elements of Tidyverse syntax is use of the pipe operator (%&gt;%). The pipe operator can be used to chain multiple functions together. It takes the object (typically a dataframe) to the left of the pipe operator and passes it to the function to the right of the pipe operator. Multiple pipes can be used in chain to execute multiple data cleaning steps without the need for intermediate dataframes. The pipe operator can be used to pass data to functions within all of the Tidyverse universe packages, not just the functions demonstrated here. Below, we can see the same code executed above, but this time with the pipe operator. The demographic_data dataframe is passed to inner_join() as the first argument to that function, with the following arguments remaining the same. full.data.tidy2 &lt;- demographic_data %&gt;% inner_join(chemical_data, by = &quot;ID&quot;) head(full.data.tidy2) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 Because the pipe operator is often used in a chain, it is best practice is to start a new line after each pipe operator, with the new lines of code indented. This makes code with multiple piped steps easier to follow. However, if just one function is being executed, the pipe operator can be used on the same line as the input and function or omitted altogether (as shown in the previous two code chunks). Here is an example of placing the function to the right of the pipe operator on a new line, with placeholder functions shown as additional steps: full.data.tidy3 &lt;- demographic_data %&gt;% inner_join(chemical_data, by = &quot;ID&quot;) %&gt;% additional_function_1() %&gt;% additional_function_2() Filtering and Subsetting Data Using Tidyverse Syntax Column-wise functions The select() function is used to subset columns in Tidyverse. Here, we can use our previously defined vector subset.columns in the select() function to keep only the columns in our subset.columns vector. The all_of() function tells the select() to keep all of the columns that match elements of the subset.columns vector. subset.tidy1 &lt;- full.data.tidy %&gt;% select(all_of(subset.columns)) head(subset.tidy1) ## BMI MAge MEdu ## 1 27.7 22.99928 3 ## 2 26.8 30.05142 3 ## 3 33.2 28.04660 3 ## 4 30.1 34.81796 3 ## 5 37.4 42.68440 3 ## 6 33.3 24.94960 3 There are many different ways that select() can be used. See below for some examples using dummy variable names: # Select specific ranges in the dataframe data &lt;- data %&gt;% select(start_column_1:end_column_1) data &lt;- data %&gt;% select(c(start_column_1:end_column_1, start_column_2:end_column_2)) # Select columns that match the elements in a character vector an an additional range of columns data &lt;- data %&gt;% select(c(all_of(character_vector), start_column_1:end_column_1)) To select columns that have names that contain specific strings, you can use functions such as starts_with(), ends_with(), and contains(). These functions allow you to ignore the case of the strings with ignore.case = TRUE. These arguments can be combined with specific column names and other selection ranges. data &lt;- data %&gt;% select(starts_with(&quot;starting_string&quot;)) data &lt;- data %&gt;% select(other_column_to_keep, starts_with(&quot;starting_string&quot;)) To remove columns using tidyverse, you can use similar code, but include a - sign before the argument defining the columns. # Removing columns subset.tidy2 &lt;- full.data.tidy %&gt;% select(-all_of(subset.columns)) # Viewing this new dataframe head(subset.tidy2) ## ID BW GA DWAs DWCd DWCr UAs UCd UCr ## 1 1 3180.058 34 6.426464 1.292941 51.67987 10.192695 0.7537104 42.60187 ## 2 2 3210.823 43 7.832384 1.798535 50.10409 11.815088 0.9789506 41.30757 ## 3 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 0.1903262 36.47716 ## 4 4 3266.844 32 5.906656 2.075259 50.92745 8.719123 0.9364825 42.47987 ## 5 5 3664.088 35 7.181873 2.762643 55.16882 9.436559 1.4977829 47.78528 ## 6 6 3328.988 40 9.723429 3.054057 51.14812 11.589403 1.6645837 38.26386 Row-wise functions The slice() function can be used to keep or remove a certain number of rows based on their position within the dataframe. For example, we can retain only the first 100 rows using the following code: subset.tidy3 &lt;- full.data.tidy %&gt;% slice(1:100) dim(subset.tidy3) ## [1] 100 12 Or, we can remove the first 100 rows: subset.tidy4 &lt;- full.data.tidy %&gt;% slice(-c(1:100)) dim(subset.tidy4) ## [1] 100 12 The related functions slice_min() and slice_max() can be used to select rows with the smallest or largest values of a variable. The filter() function can be used to keep or remove specific rows based on conditional statements. For example, we can keep only rows where BMI is greater than 25 and age is greater than 31: subset.tidy5 &lt;- full.data.tidy %&gt;% filter(BMI &gt; 25 &amp; MAge &gt; 31) dim(subset.tidy5) ## [1] 49 12 Combining column and row-wise functions Now, we can see how Tidyverse makes it easy to chain together multiple data manipulation steps. Here, we first filter rows based on values for BMI and age, then we select our columns of interest: subset.tidy6 &lt;- full.data.tidy %&gt;% filter(BMI &gt; 25 &amp; MAge &gt; 31) %&gt;% select(BMI, MAge, MEdu) head(subset.tidy6) ## BMI MAge MEdu ## 1 30.1 34.81796 3 ## 2 37.4 42.68440 3 ## 3 36.9 33.58589 3 ## 4 33.7 33.82961 3 ## 5 25.7 37.08028 3 ## 6 28.4 47.85761 3 Melting and Casting Data Using Tidyverse Syntax To melt and cast data in Tidyverse, you can use the pivot functions (i.e., pivot_longer() or pivot_wider()). The first argument in the pivot_longer() function specifies which columns should be pivoted. This can be specified with either positive or negative selection - i.e., naming columns to pivot with a vector or range or naming columns not to pivot with a - sign. Here, we are telling the function to pivot all of the columns except the ID column, which we need to keep to be able to trace back which values came from which subject. The names_to = argument allows you to set what you want to name the column that stores the variable names (the column names in wide format). The values_to = argument allows you to set what you want to name the column that stores the values. We almost always call these columns “var” and “value”, respectively, but you can name them anything that makes sense for your dataset. full.pivotlong &lt;- full.data.tidy %&gt;% pivot_longer(-ID, names_to = &quot;var&quot;, values_to = &quot;value&quot;) head(full.pivotlong, 15) ## # A tibble: 15 × 3 ## ID var value ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 BMI 27.7 ## 2 1 MAge 23.0 ## 3 1 MEdu 3 ## 4 1 BW 3180. ## 5 1 GA 34 ## 6 1 DWAs 6.43 ## 7 1 DWCd 1.29 ## 8 1 DWCr 51.7 ## 9 1 UAs 10.2 ## 10 1 UCd 0.754 ## 11 1 UCr 42.6 ## 12 2 BMI 26.8 ## 13 2 MAge 30.1 ## 14 2 MEdu 3 ## 15 2 BW 3211. To pivot our data back to wide format, we can use pivot_wider(), which will pull the column names from the column specified in the names_from = argument and the corresponding values from the column specified in the values_from = argument. full.pivotwide &lt;- full.pivotlong %&gt;% pivot_wider(names_from = &quot;var&quot;, values_from = &quot;value&quot;) head(full.pivotwide) ## # A tibble: 6 × 12 ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs UCd UCr ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 27.7 23.0 3 3180. 34 6.43 1.29 51.7 10.2 0.754 42.6 ## 2 2 26.8 30.1 3 3211. 43 7.83 1.80 50.1 11.8 0.979 41.3 ## 3 3 33.2 28.0 3 3312. 40 7.52 1.29 48.7 10.1 0.190 36.5 ## 4 4 30.1 34.8 3 3267. 32 5.91 2.08 50.9 8.72 0.936 42.5 ## 5 5 37.4 42.7 3 3664. 35 7.18 2.76 55.2 9.44 1.50 47.8 ## 6 6 33.3 24.9 3 3329. 40 9.72 3.05 51.1 11.6 1.66 38.3 Now that we’re familiar with some tidyverse functions to reshape our data, let’s answer our original question: What is the average urinary Chromium concentration for each maternal education level? We can use the group_by() function to group our dataset by education class, then the summarize function to calculate the mean of our variable of interest within each class. Note how much shorter and more efficient this code is than the code we used to calculate the same values using base R! full.data %&gt;% group_by(MEdu) %&gt;% summarize(Avg_UCr = mean(UCr)) ## # A tibble: 3 × 2 ## MEdu Avg_UCr ## &lt;int&gt; &lt;dbl&gt; ## 1 1 39.9 ## 2 2 40.6 ## 3 3 40.4 For more detailed and advanced examples of pivoting in Tidyverse, see the Tidyverse Pivoting Vignette. Concluding Remarks This training module provides an introductory level overview of data organization and manipulation basics in base R and Tidyverse, including merging, filtering, subsetting, melting, and casting, and demonstrates these methods with an environmentally relevant dataset. These methods are used regularly in scripted analyses and are important preparation steps for almost all downstream analyses and visualizations. Test Your Knowledge What subjects, arranged from highest to lowest drinking water cadmium levels, had babies at at least 35 weeks and had urinary cadmium levels of at least 1.5 µg/L? Hint: Try using the arrange() function from the tidyverse package. "],["improving-coding-efficiencies.html", "Improving Coding Efficiencies Introduction to Training Module Loops Functions List operations Concluding Remarks Additional Resources", " Improving Coding Efficiencies This training module was developed by Elise Hickman, Alexis Payton, Kyle Roell, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module In this module, we’ll explore how to improve coding efficiency. Coding efficiency involves performing a task in as few lines as possible and can… Shorten code by eliminating redundancies Reduce the number of typos Help other coders understand script better Specific approaches that we will discuss in this module include loops, functions, and list operations, which can all be used to make code more succinct. A loop is employed when we want to perform a repetitive task, while a function contains a block of code organized together to perform one specific task. List operations, in which the same function is applied to a list of dataframes, can also be used to code more efficiently. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Are there statistically significant differences in drinking water arsenic, cadmium, and chromium between normal weight (BMI &lt; 25) and overweight (BMI \\(\\geq\\) 25) subjects? Are there statistically significant differences in drinking water arsenic, cadmium, and chromium between underweight (BMI &lt; 18.5) and non-underweight (BMI \\(\\geq\\) 18.5) subjects? Are there statistically significant difference in drinking water arsenic, cadmium, and chromium between non-obese (BMI &lt; 29.9) and obese (BMI \\(\\geq\\) 29.9) subjects? We will demonstrate how this analysis can be approached using for loops, functions, or list operations. We will introduce the syntax and structure of each approach first, followed by application of the approach to our data. First, let’s prepare the workspace and familiarize ourselves with the dataset we are going to use. Data Import and Workspace Preparation Installing required packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you. We will be using the tidyverse package for data manipulation steps and the rstatix package for statistical tests, as it provides pipe friendly adaptations of the base R statistical tests and returns results in a dataframe rather than a list format, making results easier to access. This brings up an important aspect of coding efficiency - sometimes, there is already a package that has been designed with functions to help you execute your desired analysis in an efficient way, so you don’t need to write custom functions yourself! So, don’t forget to explore packages relevant to your analysis before spending a lot of time developing custom solutions (although, sometimes this is necessary). if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;) if (!requireNamespace(&quot;rstatix&quot;)) install.packages(&quot;rstatix&quot;) Loading required packages library(tidyverse) library(rstatix) Setting your working directory setwd(&quot;/file path to where your input files are&quot;) Importing example dataset The first example dataset contains subject demographic data, and the second dataset contains corresponding chemical data. Familiarize yourself with these data used previously in TAME 2.0 Module 2.3 Data Manipulation and Reshaping. # Load the demographic data demographic_data &lt;- read.csv(&quot;Module2_4_Input/Module2_4_InputData1.csv&quot;) # View the top of the demographic dataset head(demographic_data) ## ID BMI MAge MEdu BW GA ## 1 1 27.7 22.99928 3 3180.058 34 ## 2 2 26.8 30.05142 3 3210.823 43 ## 3 3 33.2 28.04660 3 3311.551 40 ## 4 4 30.1 34.81796 3 3266.844 32 ## 5 5 37.4 42.68440 3 3664.088 35 ## 6 6 33.3 24.94960 3 3328.988 40 # Load the chemical data chemical_data &lt;- read.csv(&quot;Module2_4_Input/Module2_4_InputData2.csv&quot;) # View the top of the chemical dataset head(chemical_data) ## ID DWAs DWCd DWCr UAs UCd UCr ## 1 1 6.426464 1.292941 51.67987 10.192695 0.7537104 42.60187 ## 2 2 7.832384 1.798535 50.10409 11.815088 0.9789506 41.30757 ## 3 3 7.516569 1.288461 48.74001 10.079057 0.1903262 36.47716 ## 4 4 5.906656 2.075259 50.92745 8.719123 0.9364825 42.47987 ## 5 5 7.181873 2.762643 55.16882 9.436559 1.4977829 47.78528 ## 6 6 9.723429 3.054057 51.14812 11.589403 1.6645837 38.26386 Preparing the example dataset For ease of analysis, we will merge these two datasets before proceeding. # Merging data full_data &lt;- inner_join(demographic_data, chemical_data, by = &quot;ID&quot;) # Previewing new data head(full_data) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 Continuous demographic variables, like BMI, are often dichotomized (or converted to a categorical variable with two categories representing higher vs. lower values) to increase statistical power in analyses. This is particularly important for clinical data that tend to have smaller sample sizes. In our initial dataframe, BMI is a continuous or numeric variable; however, our questions require us to dichotomize BMI. We can use the following code, which relies on if/else logic (see TAME 2.0 Module 2.3 Data Manipulation and Reshaping for more information) to generate a new column representing our dichotomized BMI variable for our first environmental health question. # Adding dichotomized BMI column full_data &lt;- full_data %&gt;% mutate(Dichotomized_BMI = ifelse(BMI &lt; 25, &quot;Normal&quot;, &quot;Overweight&quot;)) # Previewing new data head(full_data) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr Dichotomized_BMI ## 1 0.7537104 42.60187 Overweight ## 2 0.9789506 41.30757 Overweight ## 3 0.1903262 36.47716 Overweight ## 4 0.9364825 42.47987 Overweight ## 5 1.4977829 47.78528 Overweight ## 6 1.6645837 38.26386 Overweight We can see that we now have created a new column entitled Dichotomized_BMI that we can use to perform a statistical test to assess if there are differences between drinking water metals between normal and overweight subjects. Loops We will start with loops. There are three main types of loops in R: for, while, and repeat. We will focus on for loops in this module, but for more in-depth information on loops, including the additional types of loops, see here. Before applying loops to our data, let’s discuss how for loops work. The basic structure of a for loop is shown here: # Basic structure of a for loop for (i in 1:4){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 for loops always start with for followed by a statement in parentheses. The argument in the parentheses tells R how to iterate (or repeat) through the code in the curly brackets. Here, we are telling R to iterate through the code in curly brackets 4 times. Each time we told R to print the value of our iterator, or i, which has a value of 1, 2, 3, and then 4. Loops can also iterate through columns in a dataset. For example, we can use a for loop to print the ages of each subject: # Creating a smaller dataframe for our loop example full_data_subset &lt;- full_data[1:6, ] # Finding the total number of rows or subjects in the dataset number_of_rows &lt;- length(full_data_subset$MAge) # Creating a for loop to iterate from 1 to the last row for (i in 1:number_of_rows){ # Printing each subject age # Need to put `[i]` to index the correct value corresponding to the row we are evaluating print(full_data_subset$MAge[i]) } ## [1] 22.99928 ## [1] 30.05142 ## [1] 28.0466 ## [1] 34.81796 ## [1] 42.6844 ## [1] 24.9496 Now that we know how a for loop works, how can we apply this approach to determine whether there are statistically significant differences in drinking water arsenic, cadmium, and chromium between normal weight (BMI &lt; 25) and overweight (BMI \\(\\geq\\) 25) subjects. Because our data are normally distributed and there are two groups that we are comparing, we will use a t-test applied to each metal measured in drinking water. Testing for assumptions is outside the scope of this module, but see TAME 2.0 Module 3.3 Normality Tests and Data Transformation for more information on this topic. Running a t-test in R is very simple, which we can demonstrate by running a t-test on the drinking water arsenic data: # Running t-test and storing results in t_test_res t_test_res &lt;- full_data %&gt;% t_test(DWAs ~ Dichotomized_BMI) # Viewing results t_test_res ## # A tibble: 1 × 8 ## .y. group1 group2 n1 n2 statistic df p ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 DWAs Normal Overweight 96 104 -0.728 192. 0.468 We can see that our p-value is 0.468. Because this is greater than 0.05, we cannot reject the null hypothesis that normal weight and overweight subjects are exposed to the same drinking water arsenic concentrations. Although this was a very simple line of code to run, what if we have many columns we want to run the same t-test on? We can use a for loop to iterate through these columns. Let’s break down the steps of our for loop before executing the code. First, we will define the variables (columns) we want to run our t-test on. This is different from our approach above, because in those code chunks, we were using numbers to indicate the number of iterations through the loop. Here, we are naming the specific variables instead, and R will iterate though each of these variables. Note that we could omit this step and instead use the numeric column index of our variables of interest [7:9]. However, naming the specific columns makes this approach more robust because if additional data are added to or removed from our dataframe, the numeric column index of our variables could change. Which approach you choose really depends on the purpose of your loop! Second, we will create an empty dataframe where we will store the results generated by our for loop. Third, we will actually run our for loop. This will tell R: for each variable in our vars_of_interest vector, run a t-test with that variable (and store the results in a temporary dataframe called “res”), then add those results to our final results dataframe. A row will be added to the results dataframe each time R iterates through a new variable, resulting in a dataframe that stores the results of all of our t-tests. # Defining variables (columns) we want to run a t-test on vars_of_interest &lt;- c(&quot;DWAs&quot;, &quot;DWCd&quot;, &quot;DWCr&quot;) # Creating an empty dataframe to store results t_test_res_DW &lt;- data.frame() # Running for loop for (i in vars_of_interest) { # Storing the results of each iteration of the loop in a temporary results dataframe res &lt;- full_data %&gt;% # Writing the formula needed for each iteration of the loop t_test(as.formula(paste(i, &quot;~ Dichotomized_BMI&quot;, sep = &quot;&quot;))) # Adding a row to the results dataframe each time the loop is iterated t_test_res_DW &lt;- bind_rows(t_test_res_DW, res) } # Viewing our results t_test_res_DW ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Normal Overweight 96 104 -0.7279621 192.3363 0.468 ## 2 DWCd Normal Overweight 96 104 -0.5894360 196.1147 0.556 ## 3 DWCr Normal Overweight 96 104 0.1102933 197.9870 0.912 With this, we can answer Environmental Health Question #1: Are there statistically significant differences in drinking water arsenic, cadmium, and chromium between normal weight (BMI &lt; 25) and overweight (BMI \\(\\geq\\) 25) subjects? Answer: No, there are not any statistically significant differences in drinking water metals between normal weight and overweight subjects. Formulas and Pasting Note the use of the code as.formula(paste0(i, \"~ Dichotomized_BMI\")). Let’s take a quick detour to discuss the use of the as.formula() and paste() functions, as these are important functions often used in loops and user-defined functions. Many statistical test functions and regression functions require one argument to be a formula, which is typically formatted as y ~ x, where y is the dependent variable of interest and x is an independent variable. For some functions, additional variables can be included on the right side of the formula to represent covariates (additional variables of interest). The function as.formula() returns the argument in parentheses in formula format so that it can be correctly passed to other functions. We can demonstrate that here by assigning a dummy variable j the character string var1: # Assigning variable j &lt;- &quot;var1&quot; # Demonstrating output of as.formula() as.formula(paste(j, &quot; ~ Dichotomized_BMI&quot;, sep = &quot;&quot;)) ## var1 ~ Dichotomized_BMI We can use the paste() function to combine strings of characters. The paste function takes each argument (as many arguments as is needed) and pastes them together into one character string, with the separator between arguments set by the sep = argument. When our y variable is changing with each iteration of our for loop, we can use the paste() function to write our formula correctly by telling the function to paste the variable i, followed by the rest of our formula, which stays the same for each iteration of the loop. Let’s examine the output of just the paste() part of our code: paste(j, &quot; ~ Dichotomized_BMI&quot;, sep = &quot;&quot;) ## [1] &quot;var1 ~ Dichotomized_BMI&quot; The paste() function is very flexible and can be useful in many other settings when you need to create one character string from arguments from different sources! Notice that the output looks different from the output of as.formula(). There is a returned index ([1]), and there are quotes around the character string. The last function we will highlight here is the noquote() function, which can be helpful if you’d like a string without quotes: noquote(paste(j, &quot; ~ Dichotomized_BMI&quot;, sep = &quot;&quot;)) ## [1] var1 ~ Dichotomized_BMI However, this still returns an indexed number, so there are times when it will not allow code to execute properly (for example, when we need a formula format). Next, we will learn about functions and apply them to our dataset to answer our additional environmental health questions. Functions Functions are useful when you want to execute a block of code organized together to perform one specific task, and you want to be able to change parameters for that task easily rather than having to copy and paste code over and over that largely stays the same but might have small modifications in certain arguments. The basic structure of a function is as follows: function_name &lt;- function(parameter_1, parameter_2...){ # Function body (where the code goes) insert_code_here # What the function returns return() } A function requires you to name it as we did with function_name. In parentheses, the function requires you to specify the arguments or parameters. Parameters (i.e., parameter_1) act as placeholders in the body of the function. This allows us to change the values of the parameters each time a function is called, while the majority of the code remains the same. Lastly, we have a return() statement, which specifies what object (i.e., vector, dataframe, etc.) we want to retrieve from a function. Although a function can display the last expression from the function body in the absence of a return() statement, it’s a good habit to include it as the last expression. It is important to note that, although functions can take many input parameters and execute large code chunks, they can only return one item, whether that is a value, vector, dataframe, plot, code output, or list. When writing your own functions, it is important to describe the purpose of the function, its input, its parameters, and its output so that others can understand what your functions does and how to use it. This can be defined either in text above a code chunk if you are using R Markdown or as comments within the code itself. We’ll start with a simple function. Let’s say we want to convert temperatures from Fahrenheit to Celsius. We can write a function that takes the temperature in Fahrenheit and converts it to Celsius. Note that we have given our parameters descriptive names (fahrenheit_temperature, celsius_temperature), which makes our code more readable than if we assigned them dummy names such as x and y. # Function to convert temperatures in Fahrenheit to Celsius ## Parameters: temperature in Fahrenheit (input) ## Output: temperature in Celsius fahrenheit_to_celsius &lt;- function(fahrenheit_temperature){ celsius_temperature &lt;- (fahrenheit_temperature - 32) * (5/9) return(celsius_temperature) } Notice that the above code block was run, but there isn’t an output. Rather, running the code assigns the function code to that function. When you run code defining a function, that function will appear in your Global Environment under the “Functions” section. We can see the output of the function by providing an input value. Let’s start by converting 41 degrees Fahrenheit to Celsius: # Calling the function # Here, 41 is the `fahrenheit_temperature` in the function fahrenheit_to_celsius(41) ## [1] 5 41 degrees Fahrenheit is equivalent to 5 degrees Celsius. We can also have the function convert a vector of values. # Defining vector of temperatures vector_of_temperatures &lt;- c(81,74,23,65) # Calling the function fahrenheit_to_celsius(vector_of_temperatures) ## [1] 27.22222 23.33333 -5.00000 18.33333 Before getting back to answer our environmental health related questions, let’s look at one more example of a function. This time we’ll create a function that can calculate the circumference of a circle based on its radius in inches. Here you can also see a different style of commenting to describe the function’s purpose, inputs, and outputs. circle_circumference &lt;- function(radius){ # Calculating a circle&#39;s circumference based on the radius inches # :parameters: radius # :output: circumference and radius # Calculating diameter first diameter &lt;- 2 * radius # Calculating circumference circumference &lt;- pi * diameter return(circumference) } # Calling function circle_circumference(3) ## [1] 18.84956 So, if a circle had a radius of 3 inches, its circumference would be ~19 inches. What if we were interested in seeing the diameter to double check our code? diameter ## Error in eval(expr, envir, enclos): object &#39;diameter&#39; not found R throws an error, because the variable diameter was created inside the function and the function only returned the circumference variable. This is actually one of the ways that functions can improve coding efficiency - by not needing to store intermediate variables that aren’t of interest to the main goal of the code or analysis. However, there are two ways we can still see the diameter variable: Put print statements in the body of the function (print(diameter)). Have the function return a different variable or list of variables (c(circumference, diameter)). See the below section on List Operation for more on this topic. We can now move on to using a more complicated function to answer all three of our environmental health questions without repeating our earlier code three times. The main difference between each of our first three environmental health questions is the BMI cutoff used to dichotomize the BMI variable, so we can use that as one of the parameters for our function. We can also use arguments in our function to name our groups. We can adapt our previous for loop code into a function that will take different BMI cutoffs and return statistical results by including parameters to define the parts of the analysis that will change with each unique question. For example: Changing the BMI cutoff from a number (in our previous code) to our parameter name that specifies the cutoff Changing the group names for assigning category (in our previous code) to our parameter names # Function to dichotomize BMI into different categories and return results of t-test on drinking water metals between dichotomized groups ## Parameters: ### input_data: dataframe containing BMI and drinking water metals levels ### bmi_cutoff: numeric value specifying the cut point for dichotomizing BMI ### lower_group_name: name for the group of subjects with BMIs lower than the cutoff ### upper_group_name: name for the group of subjects with BMIs higher than the cutoff ### variables: vector of variable names that statistical test should be run on ## Output: dataframe with statistical results for each variable in the variables vector bmi_DW_ttest &lt;- function(input_data, bmi_cutoff, lower_group_name, upper_group_name, variables){ # Creating dichotomized variable dichotomized_data &lt;- input_data %&gt;% mutate(Dichotomized_BMI = ifelse(BMI &lt; bmi_cutoff, lower_group_name, upper_group_name)) # Creating an empty dataframe to store results t_test_res_DW &lt;- data.frame() # Running for loop for (i in variables) { # Storing the results of each iteration of the loop in a temporary results dataframe res &lt;- dichotomized_data %&gt;% # Writing the formula needed for each iteration of the loop t_test(as.formula(paste(i, &quot;~ Dichotomized_BMI&quot;, sep = &quot;&quot;))) # Adding a row to the results dataframe each time the loop is iterated t_test_res_DW &lt;- bind_rows(t_test_res_DW, res) } # Return results return(t_test_res_DW) } For the first example of using the function, we have included the name of each argument for clarity, but this isn’t necessary if you pass in the arguments in the order they were defined when writing the function. # Defining variables (columns) we want to run a t-test on vars_of_interest &lt;- c(&quot;DWAs&quot;, &quot;DWCd&quot;, &quot;DWCr&quot;) # Apply function for normal vs. overweight (bmi_cutoff = 25) bmi_DW_ttest(input_data = full_data, bmi_cutoff = 25, lower_group_name = &quot;Normal&quot;, upper_group_name = &quot;Overweight&quot;, variables = vars_of_interest) ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Normal Overweight 96 104 -0.7279621 192.3363 0.468 ## 2 DWCd Normal Overweight 96 104 -0.5894360 196.1147 0.556 ## 3 DWCr Normal Overweight 96 104 0.1102933 197.9870 0.912 Here, we can see the same results as above in the Loops section. We can next apply the function to answer our additional environmental health questions: # Apply function for underweight vs. non-underweight (bmi_cutoff = 18.5) bmi_DW_ttest(full_data, 18.5, &quot;Underweight&quot;, &quot;Non-Underweight&quot;, vars_of_interest) ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Non-Underweight Underweight 166 34 -0.86947835 53.57143 0.388 ## 2 DWCd Non-Underweight Underweight 166 34 -0.97359810 55.45450 0.334 ## 3 DWCr Non-Underweight Underweight 166 34 0.04305105 56.08814 0.966 # Apply function for non-obese vs. obese (bmi_cutoff = 29.9) bmi_DW_ttest(full_data, 29.9, &quot;Non-Obese&quot;, &quot;Obese&quot;, vars_of_interest) ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Non-Obese Obese 144 56 -1.9312097 86.80253 0.0567 ## 2 DWCd Non-Obese Obese 144 56 0.3431076 94.52209 0.7320 ## 3 DWCr Non-Obese Obese 144 56 -0.6878311 89.61818 0.4930 With this, we can answer Environmental Health Questions #2 &amp; #3: Are there statistically significant differences in drinking water arsenic, cadmium, and chromium between underweight (BMI &lt; 18.5) and non-underweight (BMI \\(\\geq\\) 18.5) subjects or between non-obese (BMI &lt; 29.9) and obese (BMI \\(\\geq\\) 29.9) subjects? Answer: No, there are not any statistically significant differences in drinking water metals between underweight and non-underweight subjects or between non-obese and obese subjects. Here, we were able to answer all three of our environmental health questions within relatively few lines of code by using a function to efficiently assess different variations on our analysis. In the last section of this module, we will demonstrate how to use list operations to improve coding efficiency. List operations Lists are a data type in R that can store other data types (including lists, to make nested lists). This allows you to store multiple dataframes in one object and apply the same functions to each dataframe in the list. Lists can also be helpful for storing the results of a function if you would like to be able to access multiple outputs. For example, if we return to our example of a function that calculates the circumference of a circle, we can store both the diameter and circumference as list objects. The function will then return a list containing both of these values when called. # Adding list element to our function circle_circumference_4 &lt;- function(radius){ # Calculating a circle&#39;s circumference and diameter based on the radius in inches # :parameters: radius # :output: list that contains diameter [1] and circumference [2] # Calculating diameter first diameter &lt;- 2 * radius # Calculating circumference circumference &lt;- pi * diameter # Storing results in a named list results &lt;- list(&quot;diameter&quot; = diameter, &quot;circumference&quot; = circumference) # Return results results } # Calling function circle_circumference_4(10) ## $diameter ## [1] 20 ## ## $circumference ## [1] 62.83185 We can also call the results individually using the following code: # Storing results of function circle_10 &lt;- circle_circumference_4(10) # Viewing only diameter ## Method 1 circle_10$diameter ## [1] 20 ## Method 2 circle_10[1] ## $diameter ## [1] 20 # Viewing only circumference ## Method 1 circle_10$circumference ## [1] 62.83185 ## Method 2 circle_10[2] ## $circumference ## [1] 62.83185 In the context of our dataset, we can use list operations to clean up and combine our results from all three BMI stratification approaches. This is often necessary to prepare data to share with collaborators or for supplementary tables in a manuscript. Let’s revisit our code for producing our statistical results, this time assigning our results to a dataframe rather than viewing them. # Defining variables (columns) we want to run a t-test on vars_of_interest &lt;- c(&quot;DWAs&quot;, &quot;DWCd&quot;, &quot;DWCr&quot;) # Normal vs. overweight (bmi_cutoff = 25) norm_vs_overweight &lt;- bmi_DW_ttest(input_data = full_data, bmi_cutoff = 25, lower_group_name = &quot;Normal&quot;, upper_group_name = &quot;Overweight&quot;, variables = vars_of_interest) # Underweight vs. non-underweight (bmi_cutoff = 18.5) under_vs_nonunderweight &lt;- bmi_DW_ttest(full_data, 18.5, &quot;Underweight&quot;, &quot;Non-Underweight&quot;, vars_of_interest) # Non-obese vs. obese (bmi_cutoff = 29.9) nonobese_vs_obese &lt;- bmi_DW_ttest(full_data, 29.9, &quot;Non-Obese&quot;, &quot;Obese&quot;, vars_of_interest) # Viewing one results dataframe as an example norm_vs_overweight ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Normal Overweight 96 104 -0.7279621 192.3363 0.468 ## 2 DWCd Normal Overweight 96 104 -0.5894360 196.1147 0.556 ## 3 DWCr Normal Overweight 96 104 0.1102933 197.9870 0.912 For publication purposes, let’s say we want to make the following formatting changes: Keep only the comparison of interest (for example Normal vs. Overweight) and the associated p-value, removing columns that are not as useful for interpreting or sharing the results Rename the .y. column so that its contents are clearer Collapse all of our data into one final dataframe We can first write a function to execute these cleaning steps: # Function to clean results dataframes ## Parameters: ### input_data: dataframe containing results of t-test ## Output: cleaned dataframe data_cleaning &lt;- function(input_data) { data &lt;- input_data %&gt;% # Rename .y. column rename(&quot;Variable&quot; = &quot;.y.&quot;) %&gt;% # Merge group1 and group2 unite(Comparison, group1, group2, sep = &quot; vs. &quot;) %&gt;% # Keep only columns of interest select(c(Variable, Comparison, p)) return(data) } Then, we can make a list of our dataframes to clean and apply: # Making list of dataframes t_test_res_list &lt;- list(norm_vs_overweight, under_vs_nonunderweight, nonobese_vs_obese) # Viewing list of dataframes head(t_test_res_list) ## [[1]] ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Normal Overweight 96 104 -0.7279621 192.3363 0.468 ## 2 DWCd Normal Overweight 96 104 -0.5894360 196.1147 0.556 ## 3 DWCr Normal Overweight 96 104 0.1102933 197.9870 0.912 ## ## [[2]] ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Non-Underweight Underweight 166 34 -0.86947835 53.57143 0.388 ## 2 DWCd Non-Underweight Underweight 166 34 -0.97359810 55.45450 0.334 ## 3 DWCr Non-Underweight Underweight 166 34 0.04305105 56.08814 0.966 ## ## [[3]] ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Non-Obese Obese 144 56 -1.9312097 86.80253 0.0567 ## 2 DWCd Non-Obese Obese 144 56 0.3431076 94.52209 0.7320 ## 3 DWCr Non-Obese Obese 144 56 -0.6878311 89.61818 0.4930 And we can apply the cleaning function to each of the dataframes using the lapply() function, which takes a list as the first argument and the function to apply to each list element as the second argument: # Applying cleaning function t_test_res_list_cleaned &lt;- lapply(t_test_res_list, data_cleaning) # Vieweing cleaned dataframes head(t_test_res_list_cleaned) ## [[1]] ## Variable Comparison p ## 1 DWAs Normal vs. Overweight 0.468 ## 2 DWCd Normal vs. Overweight 0.556 ## 3 DWCr Normal vs. Overweight 0.912 ## ## [[2]] ## Variable Comparison p ## 1 DWAs Non-Underweight vs. Underweight 0.388 ## 2 DWCd Non-Underweight vs. Underweight 0.334 ## 3 DWCr Non-Underweight vs. Underweight 0.966 ## ## [[3]] ## Variable Comparison p ## 1 DWAs Non-Obese vs. Obese 0.0567 ## 2 DWCd Non-Obese vs. Obese 0.7320 ## 3 DWCr Non-Obese vs. Obese 0.4930 Last, we can collapse our list down into one dataframe using the do.call() and rbind.data.frame() functions, which together, take the elements of the list and collapse them into a dataframe by binding the rows together: t_test_res_cleaned &lt;- do.call(rbind.data.frame, t_test_res_list_cleaned) # Viewing final dataframe t_test_res_cleaned ## Variable Comparison p ## 1 DWAs Normal vs. Overweight 0.4680 ## 2 DWCd Normal vs. Overweight 0.5560 ## 3 DWCr Normal vs. Overweight 0.9120 ## 4 DWAs Non-Underweight vs. Underweight 0.3880 ## 5 DWCd Non-Underweight vs. Underweight 0.3340 ## 6 DWCr Non-Underweight vs. Underweight 0.9660 ## 7 DWAs Non-Obese vs. Obese 0.0567 ## 8 DWCd Non-Obese vs. Obese 0.7320 ## 9 DWCr Non-Obese vs. Obese 0.4930 The above example is just that - an example to demonstrate the mechanics of using list operations. However, there are actually a couple of even more efficient ways to execute the above cleaning steps: Build cleaning steps into the analysis function if you know you will not need to access the raw results dataframe. Bind all three dataframes together, then execute the cleaning steps. We will demonstrate #2 below: # Start by binding the rows of each of the results dataframes t_test_res_cleaned_2 &lt;- bind_rows(norm_vs_overweight, under_vs_nonunderweight, nonobese_vs_obese) %&gt;% # Rename .y. column rename(&quot;Variable&quot; = &quot;.y.&quot;) %&gt;% # Merge group1 and group2 unite(Comparison, group1, group2, sep = &quot; vs. &quot;) %&gt;% # Keep only columns of interest select(c(Variable, Comparison, p)) # Viewing results t_test_res_cleaned_2 ## Variable Comparison p ## 1 DWAs Normal vs. Overweight 0.4680 ## 2 DWCd Normal vs. Overweight 0.5560 ## 3 DWCr Normal vs. Overweight 0.9120 ## 4 DWAs Non-Underweight vs. Underweight 0.3880 ## 5 DWCd Non-Underweight vs. Underweight 0.3340 ## 6 DWCr Non-Underweight vs. Underweight 0.9660 ## 7 DWAs Non-Obese vs. Obese 0.0567 ## 8 DWCd Non-Obese vs. Obese 0.7320 ## 9 DWCr Non-Obese vs. Obese 0.4930 As you can see, this dataframe is the same as the one we produced using list operations. It was produced using fewer lines of code and without the need for a user-defined function! For our purposes, this was a more efficient approach. However, we felt it was important to demonstrate the mechanics of list operations because there may be times where you do need to keep dataframes separate during specific analyses. Concluding Remarks This module provided an introduction to loops, functions, and list operations and demonstrated how to use them to efficiently analyze an environmentally relevant dataset. When and how you implement these approaches depends on your coding style and the goals of your analysis. Although here we were focused on statistical tests and data cleaning, these flexible approaches can be used in a variety of data analysis steps. We encourage you to implement loops, functions, and list operations in your analyses when you find the need to iterate through statistical tests, visualizations, data cleaning, or other common workflow elements! Additional Resources Intro2r Loops Intro2r Functions in R Hadley Wickham Advanced R - Functionals Test Your Knowledge Use the same input data we used in this module to answer the following questions and produce a cleaned, publication-ready data table of results. Note that these data are normally distributed, so you can use a t-test. Are there statistically significant differences in urine metal concentrations (ie. arsenic levels, cadmium levels, etc.) between younger (MAge &lt; 40) and older (MAge \\(\\geq\\) 40) mothers? Are there statistically significant differences in urine metal concentrations (ie. arsenic levels, cadmium levels, etc.) between between normal weight (BMI &lt; 25) and overweight (BMI \\(\\geq\\) 25) subjects? "],["data-visualizations.html", "Data Visualizations Introduction to Data Visualizations Introduction to Training Module Density Plot Visualization Boxplot Visualization Correlation Visualizations Heatmap Visualization Concluding Remarks", " Data Visualizations This training module was developed by Alexis Payton, Kyle Roell, Lauren E. Koval, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Data Visualizations Selecting an approach to visualize data is an important consideration when presenting scientific research, given that figures have the capability to summarize large amounts of data efficiently and effectively. (At least that’s the goal!) This module will focus on basic data visualizations that we view to be most commonly used, both in and outside of the field of environmental health research, many of which you have likely seen before. This module is not meant to be an exhaustive representation of all figure types, rather it serves as an introduction to some types of figures and how to approach choosing the one that most optimally displays your data and primary findings. When selecting a data visualization approach, here are some helpful questions you should first ask yourself: What message am I trying to convey with this figure? How does this figure highlight major findings from the paper? Who is the audience? What type of data am I working with? A Guide To Getting Data Visualization Right is a great resource for determining which figure is best suited for various types of data. More complex methodology-specific charts are presented in succeeding TAME modules. These include visualizations for: Two Group Comparisons (e.g.,boxplots and logistic regression) in Module 3.4 Introduction to Statistical Tests and Module 4.4 Two Group Comparisons and Visualizations Multi-Group Comparisons (e.g.,boxplots) in Module 3.4 Introduction to Statistical Tests and Module 4.5 Mult-Group Comparisons and Visualizations Supervised Machine Learning (e.g.,decision boundary plots, variable importance plots) in Module 5.3 Supervised ML Model Interpretation Unsupervised Machine Learning Principal Component Analysis (PCA) plots and heatmaps in Module 5.4 Unsupervised Machine Learning I: K-Means Clustering &amp; PCA Dendrograms, clustering visualizations, heatmaps, and variable contribution plots in Module 5.5 Unsupervised Machine Learning II: Additional Clustering Applications -Omics Expression (e.g.,MA plots and volcano plots) in Module 6.2 -Omics and Systems Biology: Transcriptomic Applications Mixtures Methods Forest Plots in Module 6.3 Mixtures I: Overview and Quantile G-Computation Application Trace Plots in Module 6.4 Mixtures II: BKMR Application Sufficient Similarity (e.g.,heatmaps, clustering) in Module 6.5 Mixtures III: Sufficient Similarity Toxicokinetic Modeling (e.g.,line graph, dose response) in Module 6.6 Toxicokinetic Modeling Introduction to Training Module Visualizing data is an important step in any data analysis, including those carried out in environmental health research. Often, visualizations allow scientists to better understand trends and patterns within a particular dataset under evaluation. Even after statistical analysis of a dataset, it is important to then communicate these findings to a wide variety of target audiences. Visualizations are a vital part of communicating complex data and results to target audiences. In this module, we highlight some figures that can be used to visualize larger, more high-dimensional datasets using figures that are more simple (but still relevant!) than methods presented later on in TAME. This training module specifically reviews the formatting of data in preparation of generating visualizations, scaling datasets, and then guides users through the generation of the following example data visualizations: Density plots Boxplots Correlation plots Heatmaps These visualization approaches are demonstrated using a large environmental chemistry dataset. This example dataset was generated through chemical speciation analysis of smoke samples collected during lab-based simulations of wildfire events. Specifically, different biomass materials (eucalyptus, peat, pine, pine needles, and red oak) were burned under two combustion conditions of flaming and smoldering, resulting in the generation of 12 different smoke samples. These data have been previously published in the following environmental health research studies, with data made publicly available: Rager JE, Clark J, Eaves LA, Avula V, Niehoff NM, Kim YH, Jaspers I, Gilmour MI. Mixtures modeling identifies chemical inducers versus repressors of toxicity associated with wildfire smoke. Sci Total Environ. 2021 Jun 25;775:145759. doi: 10.1016/j.scitotenv.2021.145759. Epub 2021 Feb 10. PMID: 33611182. Kim YH, Warren SH, Krantz QT, King C, Jaskot R, Preston WT, George BJ, Hays MD, Landis MS, Higuchi M, DeMarini DM, Gilmour MI. Mutagenicity and Lung Toxicity of Smoldering vs. Flaming Emissions from Various Biomass Fuels: Implications for Health Effects from Wildland Fires. Environ Health Perspect. 2018 Jan 24;126(1):017011. doi: 10.1289/EHP2200. PMID: 29373863. GGplot2 ggplot2 is a powerful package used to create graphics in R. It was designed based on the philosophy that every figure can be built using a dataset, a coordinate system, and a geom that specifies the type of plot. As a result, it is fairly straightforward to create highly customizable figures and is typically preferred over using base R to generate graphics. We’ll generate all of the figures in this module using ggplot2. For additional resources on ggplot2 see ggplot2 Posit Documentation and Data Visualization with ggplot2. Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;GGally&quot;)) install.packages(&quot;GGally&quot;); if (!requireNamespace(&quot;corrplot&quot;)) install.packages(&quot;corrplot&quot;); if (!requireNamespace(&quot;pheatmap&quot;)) install.packages(&quot;pheatmap&quot;); Loading R packages required for this session library(tidyverse) library(GGally) library(corrplot) library(reshape2) library(pheatmap) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example dataset Then let’s read in our example dataset. As mentioned in the introduction, this example dataset represents chemical measurements across 12 different biomass burn scenarios representing potential wildfire events. Let’s upload and view these data: # Load the data smoke_data &lt;- read.csv(&quot;Module3_1_Input/Module3_1_InputData.csv&quot;) # View the top of the dataset head(smoke_data) ## Chemical.Category Chemical CASRN Eucalyptus_Smoldering ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 0.06 ## 2 n-Alkanes 3-Methylnonadecane 6418-45-7 0.04 ## 3 n-Alkanes Docosane 629-97-0 0.21 ## 4 n-Alkanes Dodecylcyclohexane 1795-17-1 0.04 ## 5 n-Alkanes Eicosane 112-95-8 0.11 ## 6 n-Alkanes Heneicosane 629-94-7 0.13 ## Eucalyptus_Flaming Peat_Smoldering Peat_Flaming Pine_Smoldering Pine_Flaming ## 1 0.06 1.36 0.06 0.06 0.06 ## 2 0.04 1.13 0.90 0.47 0.04 ## 3 0.25 9.46 0.57 0.16 0.48 ## 4 0.04 0.25 0.04 0.04 0.04 ## 5 0.25 7.55 0.54 0.17 0.29 ## 6 0.28 6.77 0.34 0.13 0.42 ## Pine_Needles_Smoldering Pine_Needles_Flaming Red_Oak_Smoldering ## 1 0.06 0.06 0.06 ## 2 0.04 0.72 0.04 ## 3 0.32 0.18 0.16 ## 4 0.12 0.04 0.04 ## 5 0.28 0.16 0.15 ## 6 0.30 0.13 0.13 ## Red_Oak_Flaming Units ## 1 0.13 ng_per_uL ## 2 0.77 ng_per_uL ## 3 0.36 ng_per_uL ## 4 0.04 ng_per_uL ## 5 0.38 ng_per_uL ## 6 0.69 ng_per_uL Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: How do the distributions of the chemical concentration data differ based on each biomass burn scenario? Are there correlations between biomass burn conditions based on the chemical concentration data? Under which biomass burn conditions are concentrations of certain chemical categories the highest? We can create a density plot to answer the first question. Similar to a histogram, density plots are an effective way to show overall distributions of data and can be useful to compare across various test conditions or other stratifications of the data. In this example of a density plot, we’ll visualize the distributions of chemical concentration data on the x axis. A density plot automatically displays where values are concentrated on the y axis. Additionally, we’ll want to have multiple density plots within the same figure for each biomass burn condition. Before the data can be visualized, it needs to be converted from a wide to long format. This is because we need to have variable or column names entitled Chemical_Concentration and Biomass_Burn_Condition that can be placed into ggplot(). For review on converting between long and wide formats and using other tidyverse tools, see TAME 2.0 Module 2.3 Data Manipulation &amp; Reshaping. longer_smoke_data = pivot_longer(smoke_data, cols = 4:13, names_to = &quot;Biomass_Burn_Condition&quot;, values_to = &quot;Chemical_Concentration&quot;) head(longer_smoke_data) ## # A tibble: 6 × 6 ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Flaming ## # ℹ 1 more variable: Chemical_Concentration &lt;dbl&gt; Scaling dataframes for downstream data visualizations A data preparation method that is commonly used to convert values into those that can be used to better illustrate overall data trends is data scaling. Scaling can be achieved through data transformations or normalization procedures, depending on the specific dataset and goal of analysis/visualization. Scaling is often carried out using data vectors or columns of a dataframe. For this example, we will normalize the chemical concentration dataset using a basic scaling and centering procedure using the base R function, scale(). This algorithm results in the normalization of a dataset using the mean value and standard deviation. This scaling step will convert chemical concentration values in our dataset into normalized values across samples, such that each chemical’s concentration distributions are more easily comparable between the different biomass burn conditions. For more information on the scale() function, see its associated RDocumentation and helpful tutorial on Implementing the scale() function in R. scaled_longer_smoke_data = longer_smoke_data %&gt;% # scaling within each chemical group_by(Chemical) %&gt;% mutate(Scaled_Chemical_Concentration = scale(Chemical_Concentration)) %&gt;% ungroup() head(scaled_longer_smoke_data) # see the new scaled values now in the last column (column 7) ## # A tibble: 6 × 7 ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Flaming ## # ℹ 2 more variables: Chemical_Concentration &lt;dbl&gt;, ## # Scaled_Chemical_Concentration &lt;dbl[,1]&gt; We can see that in the Scaled_Chemical_Concentration column, each chemical is scaled based on a normal distribution centered around 0, with values now less than or greater than zero. Now that we have our dataset formatted, let’s plot it. Density Plot Visualization The following code can be used to generate a density plot: ggplot(scaled_longer_smoke_data, aes(x = Scaled_Chemical_Concentration, color = Biomass_Burn_Condition)) + geom_density() Answer to Environmental Health Question 1, Method I With this method, we can answer Environmental Health Question #1: How do the distributions of the chemical concentration data differ based on each biomass burn scenario? Answer: In general, there are a high number of chemicals that were measured at relatively lower abundances across all smoke samples (hence, the peak in occurrence density occurring towards the left, before 0). The three conditions of smoldering peat, flaming peat, and flaming pine contained the most chemicals at the highest relative concentrations (hence, these lines are the top three lines towards the right). Boxplot Visualization A boxplot can also be used to answer our first environmental health question: How do the distributions of the chemical concentration data differ based on each biomass burn scenario?. A boxplot also displays a data’s distribution, but it incorporates a visualization of a five number summary (i.e., minimum, first quartile, median, third quartile, and maximum). Any outliers are displayed as dots. For this example, let’s have Scaled_Chemical_Concentration on the x axis and Biomass_Burn_Condition on the y axis. The scaled_longer_smoke_data dataframe is the format we need, so we’ll use that for plotting. ggplot(scaled_longer_smoke_data, aes(x = Scaled_Chemical_Concentration, y = Biomass_Burn_Condition, color = Biomass_Burn_Condition)) + geom_boxplot() Answer to Environmental Health Question 1, Method II With this alternative method, we can answer, in a different way, Environmental Health Question #1: How do the distributions of the chemical concentration data differ based on each biomass burn scenario? Answer, Method II: The median chemical concentration is fairly low (less than 0) for all biomass burn conditions. Overall, there isn’t much variation in chemical concentrations with the exception of smoldering peat, flaming peat, and flaming eucalyptus. Correlation Visualizations Let’s turn our attention to the second environmental health question: Are there correlations between biomass burn conditions based on the chemical concentration data? We’ll use two different correlation visualizations to answer this question using the GGally package. GGally is a package that serves as an extension of ggplot2, the baseline R plotting system based on the grammar of graphics. GGally is very useful for creating plots that compare groups or features within a dataset, among many other utilities. Here we will demonstrate the ggpairs() function within GGally using the scaled chemistry dataset. This function will produce an image that shows correlation values between biomass burn sample pairs and also illustrates the overall distributions of values in the samples. For more information on GGally, see its associated RDocumentation and example helpful tutorial. GGally requires a wide dataframe with ids (i.e.,Chemical) as the rows and the variables that will be compared to each other (i.e.,Biomass_Burn_Condition) as the columns. Let’s create that dataframe. # first selecting the chemical, biomass burn condition, and # the scaled chemical concentration columns wide_scaled_data = scaled_longer_smoke_data %&gt;% pivot_wider(id_cols = Chemical, names_from = &quot;Biomass_Burn_Condition&quot;, values_from = &quot;Scaled_Chemical_Concentration&quot;) %&gt;% # converting the chemical names to row names column_to_rownames(var = &quot;Chemical&quot;) head(wide_scaled_data) ## Eucalyptus_Smoldering Eucalyptus_Flaming Peat_Smoldering ## 2-Methylnonadecane -0.3347765 -0.3347765 2.841935 ## 3-Methylnonadecane -0.8794448 -0.8794448 1.649829 ## Docosane -0.3465132 -0.3327216 2.842787 ## Dodecylcyclohexane -0.4240624 -0.4240624 2.646734 ## Eicosane -0.3802202 -0.3195928 2.841691 ## Heneicosane -0.3895328 -0.3166775 2.835527 ## Peat_Flaming Pine_Smoldering Pine_Flaming ## 2-Methylnonadecane -0.3347765 -0.3347765 -0.3347765 ## 3-Methylnonadecane 1.1161291 0.1183422 -0.8794448 ## Docosane -0.2223890 -0.3637526 -0.2534201 ## Dodecylcyclohexane -0.4240624 -0.4240624 -0.4240624 ## Eicosane -0.1940076 -0.3542370 -0.3022707 ## Heneicosane -0.2875354 -0.3895328 -0.2486793 ## Pine_Needles_Smoldering Pine_Needles_Flaming ## 2-Methylnonadecane -0.3347765 -0.3347765 ## 3-Methylnonadecane -0.8794448 0.6984509 ## Docosane -0.3085863 -0.3568568 ## Dodecylcyclohexane 0.7457649 -0.4240624 ## Eicosane -0.3066012 -0.3585675 ## Heneicosane -0.3069635 -0.3895328 ## Red_Oak_Smoldering Red_Oak_Flaming ## 2-Methylnonadecane -0.3347765 -0.1637228 ## 3-Methylnonadecane -0.8794448 0.8144726 ## Docosane -0.3637526 -0.2947948 ## Dodecylcyclohexane -0.4240624 -0.4240624 ## Eicosane -0.3628981 -0.2632960 ## Heneicosane -0.3895328 -0.1175398 By default, ggpairs() displays Pearson’s correlations. To show Spearman’s correlations takes more nuance, but can be done using the code that has been commented out below. # ggpairs with Pearson&#39;s correlations wide_scaled_data = data.frame(as.matrix(wide_scaled_data)) ggpairs(wide_scaled_data) # ggpairs with Spearman&#39;s correlations # pearson_correlations = cor(wide_scaled_data, method = &quot;spearman&quot;) # ggpairs(wide_scaled_data, upper = list(continuous = wrap(ggally_cor, method = &quot;spearman&quot;))) Many of these biomass burn conditions have significant correlations denoted by the asterisks. ’*’: p value &lt; 0.1 ’**’: p value &lt; 0.05 ’***’: p value &lt; 0.01 The upper right portion displays the correlation values, where a value less than 0 indicates negative correlation and a value greater than 0 signifies positive correlation. The diagonal shows the density plots for each variable. The lower left portion visualizes the values of the two variables compared using a scatterplot. Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: Are there correlations between biomass burn conditions based on the chemical concentration data? Answer: There is low correlation between many of the variables (-0.5 &lt; correlation value &lt; 0.5). Eucalyptus flaming and pine flaming are significantly positively correlated along with peat flaming and pine needles flaming (correlation value ~0.7 and p value &lt; 0.001). We can visualize correlations another way using the other function from GGally, ggcorr(), which visualizes each correlation as a square. Note that this function calculates Pearson’s correlations by default. However, this can be changed using the method parameter shown in the code commented out below. # Pearson&#39;s correlations ggcorr(wide_scaled_data) # Spearman&#39;s correlations # ggcorr(wide_scaled_data, method = &quot;spearman&quot;) We’ll visualize correlations between each of the groups using one more figure using the corrplot() function from the corrplot package. # Need to supply corrplot with a correlation matrix, here, using the &#39;cor&#39; function corrplot(cor(wide_scaled_data)) Each of these correlation figures displays the same information, but the one you choose to use is a matter of personal preference. Click on the following resources for additional information on ggpairs() and corrplot(). Heatmap Visualization Last, we’ll turn our attention to answering the final environmental health question: Under which biomass burn conditions are concentrations of certain chemical categories the highest? This can be addressed with the help of a heatmap. Heatmaps are a highly effective method of viewing an entire dataset at once. Heatmaps can appear similar to correlation plots, but typically illustrate other values (e.g., concentrations, expression levels, presence/absence, etc) besides correlation values. They are used to draw patterns between two variables of highest interest (that comprise the x and y axis, though additional bars can be added to display other layers of information). In this instance, we’ll use a heatmap to determine whether there are patterns apparent between chemical categories and biomass burn condition on chemical concentrations. For this example, we can plot Biomass_Burn_Condition and Chemical.Category on the axes and fill in the values with Scaled_Chemical_Concentration. When generating heatmaps, scaled values are often used to better distinguish patterns between groups/samples. In this example, we also plan to display the median scaled concentration value within the heatmap as an additional layer of helpful information to aid in interpretation. To do so, we’ll need to take the median chemical concentration for each biomass burn condition within each chemical category. However, since we want ggplot() to visualize the median scaled values with the color of the tiles this step was already necessary. # We&#39;ll find the median value and add that data to the dataframe as an additional column heatmap_df = scaled_longer_smoke_data %&gt;% group_by(Biomass_Burn_Condition, Chemical.Category) %&gt;% mutate(Median_Scaled_Concentration = median(Scaled_Chemical_Concentration)) head(heatmap_df) ## # A tibble: 6 × 8 ## # Groups: Biomass_Burn_Condition, Chemical.Category [6] ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Flaming ## # ℹ 3 more variables: Chemical_Concentration &lt;dbl&gt;, ## # Scaled_Chemical_Concentration &lt;dbl[,1]&gt;, Median_Scaled_Concentration &lt;dbl&gt; Now we can plot the data and add the Median_Scaled_Concentration to the figure using geom_text(). Note that specifying the original Scaled_Chemical_Concentration in the fill parameter will NOT give you the same heatmap as specifying the median values in ggplot(). ggplot(data = heatmap_df, aes(x = Chemical.Category, y = Biomass_Burn_Condition, fill = Median_Scaled_Concentration)) + geom_tile() + # function used to specify a heatmap for ggplot geom_text(aes(label = round(Median_Scaled_Concentration, 2))) # adding concentration values as text, rounding to two values after the decimal Answer to Environmental Health Question 3 With this, we can answer Environmental Health Question #3: Under which biomass burn conditions are concentrations of certain chemical categories the highest? Answer: Peat flaming has the highest concentrations of inorganics and ions. Eucalyptus smoldering has the highest concentrations of levoglucosans. Pine smoldering has the highest concentrations of methoxyphenols. Peat smoldering has the highest concentrations of n-alkanes. Pine needles smoldering has highest concentrations of PAHs. This same heatmap can be achieved another way using the pheatmap() function from the pheatmap package. Using this function requires us to use a wide dataset, which we need to create. It will contain Chemical.Category, Biomass_Burn_Condition and Scaled_Chemical_Concentration. heatmap_df2 = scaled_longer_smoke_data %&gt;% group_by(Biomass_Burn_Condition, Chemical.Category) %&gt;% # using the summarize function instead of mutate function as was done previously since we only need the median values now summarize(Median_Scaled_Concentration = median(Scaled_Chemical_Concentration)) %&gt;% # transforming the data to a wide format pivot_wider(id_cols = Biomass_Burn_Condition, names_from = &quot;Chemical.Category&quot;, values_from = &quot;Median_Scaled_Concentration&quot;) %&gt;% # converting the chemical names to row names column_to_rownames(var = &quot;Biomass_Burn_Condition&quot;) head(heatmap_df2) ## Inorganics Ions Levoglucosan Methoxyphenols ## Eucalyptus_Flaming 0.05405359 0.05273246 0.4208870 -0.44781893 ## Eucalyptus_Smoldering -0.68595076 -0.80160192 1.7772753 -0.06449444 ## Peat_Flaming 2.24332901 1.77515899 -0.9383328 -0.51488738 ## Peat_Smoldering -0.51860591 -0.36146158 -0.8041211 0.05720971 ## Pine_Flaming -0.02063532 -0.05999543 -0.1992054 -0.50269422 ## Pine_Needles_Flaming 0.36405527 0.82229035 -0.8570130 -0.46331332 ## PAHs n-Alkanes ## Eucalyptus_Flaming 1.2885776 -0.3790357 ## Eucalyptus_Smoldering -0.4724635 -0.3465132 ## Peat_Flaming -0.5369746 -0.3093608 ## Peat_Smoldering -0.3162278 2.8238921 ## Pine_Flaming 1.7825403 -0.3347765 ## Pine_Needles_Flaming -0.4179505 -0.3850613 Now let’s generate the same heatmap this time using the pheatmap() function: pheatmap(heatmap_df2, # removing the clustering option from both rows and columns cluster_rows = FALSE, cluster_cols = FALSE, # adding the values for each cell, making those values black, and changing the font size display_numbers = TRUE, number_color = &quot;black&quot;, fontsize = 12) Notice that the pheatmap() function does not include axes or legend titles as with ggplot(), however those can be added to the figure after exporting from R in MS Powerpoint or Adobe. Additional parameters, including cluster_rows, for the pheatmap() function are discussed further in TAME 2.0 Module 5.4 Unsupervised Machine Learning. For basic heatmaps like the ones shown here, ggplot() or pheatmap() can both be used however, both have their pros and cons. For example, ggplot() figures tend to be more customizable and easily combined with other figures, while pheatmap() has additional parameters built into the function that can make plotting certain features advantageous like clustering. Concluding Remarks In conclusion, this training module provided example code to create highly customizable data visualizations using ggplot2 pertinent to environmental health research. Test Your Knowledge Replicate the figure below! The heatmap still visualizes the median chemical concentrations, but this time we’re separating the burn conditions, allowing us to determine if the concentrations of chemicals released are contingent upon the burn condition. For additional figures available and to view aspects of figures that can be changed in GGplot2, check out this GGPlot2 Cheat Sheet. You might need it to make this figure! Hint 1: Use the separate() function from tidyverse to split Biomass_Burn_Condition into Biomass and Burn_Condition. Hint 2: Use the function facet_wrap() within ggplot() to separate the heatmaps by Burn_Condition. "],["improving-data-visualizations.html", "Improving Data Visualizations Introduction to Data Visulization Conventions Introduction to Training Module Creating an Improved Boxplot Visualization Creating an Improved Heatmap Visualization Creating Multi-Plot Figures Concluding Remarks", " Improving Data Visualizations This training module was developed by Alexis Payton, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Data Visulization Conventions Data visualizations are used to convey key takeaways from a research study’s findings in a clear, succinct manner to highlight data trends, patterns, and/or relationships. In environmental health research, this is of particular importance for high-dimensional datasets that can typically be parsed using multiple methods, potentially resulting in many different approaches to visualize data. As a consequence, researchers are often faced with an overwhelming amount of options when deciding which visualization scheme(s) most optimally translate their results for effective dissemination. Effective data visualization approaches are vital to a researcher’s success for many reasons. For instance, manuscript readers or peer reviewers often scroll through a study’s text and focus on the quality and novelty of study figures before deciding whether to read/review the paper. Therefore, the importance of data visualizations cannot be understated in any research field. As a high-level introduction, it is important that we first communicate some traits that we think are imperative towards ensuring a successful data visualization approach as described in more detail below. Keys to successful data visualizations: Consider your audience, data type, and research question prior to selecting a figure to visualize your data For example, if more computationally complex methods are used in a manuscript that is intended for a journal with an audience that doesn’t have that same level of expertise, consider spending time focusing on how those results are presented in an approachable way for that audience. For a review of how to choose a rudimentary chart based on the data type, check out How to Choose the Right Data Visualization. Some of these basic charts will be presented in this module, while more complex analysis-specific visualizations, especially ones developed for high-dimensional data will be presented in later modules. Take the legibility of the figure into account This includes avoiding abbreviations when possible. (If they can’t be avoided explain them in the caption.) All titles should be capitalized, including titles for the legend(s) and axes. Underscores and periods between words should be replaced with spaces. Consider the legibility of the figure if printed in black and white. (However, that’s not as important these days.) Lastly, feel free to describe your plot in further detail in the caption to aid the reader in understanding the results presented. Minimize text Main titles aren’t necessary for single paneled figures (like the examples below), because in a publication the title of the figure is right underneath each figure. It’s good practice to remove this kind of extraneous text, which can make the figure seem more cluttered. Titles can be helpful in multi-panel figures, especially if there are multiple panels with the same figure type that present slightly different results. For example, in the Test Your Knowledge section, you’ll need to create two heatmaps, but one displays data under smoldering conditions and the other displays data under flaming conditions. In general, try to reduce the amount of extraneous text in a plot to keep a reader focused on the most important elements and takeaways in the plot. Use the minimal number of figures you need to support your narrative It is important to include an optimal number of figures within manuscripts and scientific reports. Too many figures might overwhelm the overall narrative, while too few might not provide enough substance to support your main findings. It can be helpful to also consider placing some figures in supplemental material to aid in the overall flow of your scientific writing. Select an appropriate color palette Packages have been developed to offer color palettes including MetBrewer and RColorBrewer. In addition, ggsci is a package that offers a collection of color palettes used in various scientific journals. For more information, check out MetBrewer, see its associated RDocumentation and example tutorial. For more information on RColorBrewer, see its associated RDocumentation and example tutorial. For more information on ggsci, see its associated RDocumentation. In general, it’s better to avoid bright and flashy colors that can be difficult to read. It’s advisable to use colors for manuscript figures that are color-blind friendly. Check out these Stack overflow answers about color blind-safe color palettes and packages. Popular packages for generating colorblind-friendly palettes include viridis and rcartocolor. Use color strategically Color can be used to visualize a variable. There are three ways to categorize color schemes - sequential, diverging, and qualitative. Below, definitions are provided for each along with example figures that we’ve previously published that illustrate each color scheme. In addition, figure titles and captions are also provided for context. Note that some of these figures have been simplified from what was originally published to show more streamlined examples for TAME. Sequential: intended for ordered categorical data (i.e., disease severity, likert scale, quintiles). The choropleth map below is from Winker, Payton et. al. Figure 2: Figure 1. Geospatial distribution of the risk of future wildfire events across North Carolina. Census tracts in North Carolina were binned into quintiles based on Wildfire Hazard Potential (WHP) with 1 (pale orange) having the lowest risk and 5 (dark red) having the highest risk. Figure regenerated here in alignment with its published Creative Commons Attribution 4.0 International License Diverging: intended to emphasize continuous data at extremes of the data range (typically using darker colors) and mid-range values (typically using lighter colors). This color scheme is ideal for charts like heatmaps. The heatmap below is from Payton, Perryman et. al. Figure 3: Figure 6. Individual cytokine expression levels across all subjects. Cytokine concentrations were derived from nasal lavage fluid samples. On the x axis, subjects were ordered first according to tobacco use status, starting with non-smokers then cigarette smokers and e-cigarette users. Within tobacco use groups, subjects are ordered from lowest to highest average cytokine concentration from left to right. Within each cluster shown on the y axis, cytokines are ordered from lowest to highest average cytokine concentration from bottom to top. Figure regenerated here in alignment with its published Creative Commons Attribution 4.0 International License Qualitative: intended for nominal categorical data to visualize clear differences between groups (i.e., soil types and exposure groups). The dendrogram below is from Koval et. al. Figure 4: Figure 2. Translating chemical use inventory data to inform human exposure patterning. Groups A-I illustrate the identified clusters of exposure source categories. Figure regenerated here in alignment with its published Creative Commons Attribution 4.0 International License Consider ordering axes to reveal patterns relevant to the research questions Ordering the axes can reveal potential patterns that may not be clear in the visualization otherwise. In the cytokine expression heatmap above, there are not clear differences in cytokine expression across the tobacco use groups. However, e-cigarette users seem to have slightly more muted responses compared to non-smokers and cigarette smokers in clusters B and C, which was corroborated in subsequent statistical analyses. It is also evident that Cluster A had the lowest cytokine concentrations, followed by Cluster B, and then Cluster C with the greatest concentrations. What makes these figures so compelling is how the aspects introduced above were thoughtfully incorporated. In the next section, we’ll put those principles into practice using data that were described and referenced previously in TAME 2.0 Module 3.1 Data Visualizations. Introduction to Training Module In this module, ggplot2, R’s data visualization package will be used to walk through ways to improve data visualizations. We’ll recreate two figures (i.e., the boxplot and heatmap) constructed previously in TAME 2.0 Module 3.1 Data Visualizations and improve them so they are publication-ready. Additionally, we’ll write figure titles and captions to contextualize the results presented for each visualization. When writing figure titles and captions, it is helpful to address the research question or overall concept that the figure seeks to capture rather than getting into the weeds of specific methods the plot is based on. This is especially important when visualizing more complex methods that your audience might not have as much knowledge on. Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;MetBrewer&quot;)) install.packages(&quot;MetBrewer&quot;); if (!requireNamespace(&quot;RColorBrewer&quot;)) install.packages(&quot;RColorBrewer&quot;); if (!requireNamespace(&quot;pheatmap&quot;)) install.packages(&quot;pheatmap&quot;); if (!requireNamespace(&quot;cowplot&quot;)) install.packages(&quot;cowplot&quot;); Loading required R packages library(tidyverse) library(MetBrewer) library(RColorBrewer) library(pheatmap) library(cowplot) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example dataset Let’s now read in our example dataset. As mentioned in the introduction, this example dataset represents chemical measurements across 12 different biomass burn scenarios, representing chemicals emitted during potential wildfire events. Let’s upload and view these data: # Load the data smoke_data &lt;- read.csv(&quot;Module3_2_Input/Module3_2_InputData.csv&quot;) # View the top of the dataset head(smoke_data) ## Chemical.Category Chemical CASRN Eucalyptus_Smoldering ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 0.06 ## 2 n-Alkanes 3-Methylnonadecane 6418-45-7 0.04 ## 3 n-Alkanes Docosane 629-97-0 0.21 ## 4 n-Alkanes Dodecylcyclohexane 1795-17-1 0.04 ## 5 n-Alkanes Eicosane 112-95-8 0.11 ## 6 n-Alkanes Heneicosane 629-94-7 0.13 ## Eucalyptus_Flaming Peat_Smoldering Peat_Flaming Pine_Smoldering Pine_Flaming ## 1 0.06 1.36 0.06 0.06 0.06 ## 2 0.04 1.13 0.90 0.47 0.04 ## 3 0.25 9.46 0.57 0.16 0.48 ## 4 0.04 0.25 0.04 0.04 0.04 ## 5 0.25 7.55 0.54 0.17 0.29 ## 6 0.28 6.77 0.34 0.13 0.42 ## Pine_Needles_Smoldering Pine_Needles_Flaming Red_Oak_Smoldering ## 1 0.06 0.06 0.06 ## 2 0.04 0.72 0.04 ## 3 0.32 0.18 0.16 ## 4 0.12 0.04 0.04 ## 5 0.28 0.16 0.15 ## 6 0.30 0.13 0.13 ## Red_Oak_Flaming Units ## 1 0.13 ng_per_uL ## 2 0.77 ng_per_uL ## 3 0.36 ng_per_uL ## 4 0.04 ng_per_uL ## 5 0.38 ng_per_uL ## 6 0.69 ng_per_uL Now that we’ve been able to view the dataset, let’s come up with questions that can be answered with our boxplot and heatmap figure. This will inform how we format the dataframe for visualization. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Boxplot: How do the distributions of the chemical concentration data differ based on each biomass burn scenario? Heatmap: Which classes of chemicals show the highest concentrations across the evaluated biomass burn conditions? How can these figures be combined into a single plot that can be then be exported from R? Formatting dataframes for downstream visualization code First, format the dataframe by changing it from a wide to long format and normalizing the chemical concentration data. For more details on this data reshaping visit TAME 2.0 Module 2.3 Data Manipulation &amp; Reshaping. scaled_longer_smoke_data = pivot_longer(smoke_data, cols = 4:13, names_to = &quot;Biomass_Burn_Condition&quot;, values_to = &quot;Chemical_Concentration&quot;) %&gt;% # scaling within each chemical group_by(Chemical) %&gt;% mutate(Scaled_Chemical_Concentration = scale(Chemical_Concentration)) %&gt;% ungroup() head(scaled_longer_smoke_data) ## # A tibble: 6 × 7 ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Flaming ## # ℹ 2 more variables: Chemical_Concentration &lt;dbl&gt;, ## # Scaled_Chemical_Concentration &lt;dbl[,1]&gt; Creating an Improved Boxplot Visualization As we did in the previous module, a boxplot will be constructed to answer the first environmental heath question: How do the distributions of the chemical concentration data differ based on each biomass burn scenario?. Let’s remind ourselves of the original figure from the previous module. Based on the figure above, peat smoldering has the highest median scaled chemical concentration. However, this was difficult to determine given that the burn conditions aren’t labeled on the x axis and a sequential color palette was used, making it difficult to identify the correct boxplot with its burn condition in the legend. If you look closely, the colors in the legend are in a reverse order of the colors assigned to the boxplots. Let’s identify some elements of this graph that can be modified to make it easier to answer our research question. There are four main aspects we can adjust on this figure: 1. The legibility of the text in the legend and axes. Creating spaces between the text or exchanging the underscores for spaces improves the legibility of the figure. 2. The order of the boxplots. Ordering the biomass burn conditions from highest to lowest based on their median scaled chemical concentration allows the reader to easily determine the biomass burn condition that had the greatest or least chemical concentrations relative to each other. In R, this can be done by putting the Biomass_Burn_Condition variable into a factor. 3. Use of color. Variables can be visualized using color, text, size, etc. In this figure, it is redundant to have the biomass burn condition encoded in the legend and the color. Instead this variable can be put on the y axis and the legend will be removed to be more concise. The shades of the colors will also be changed, but to keep each burn condition distinct from each other, colors will be chosen that are distinct from one another. Therefore, we will choose a qualitative color scheme. 4. Show all data points when possible. Many journals now require that authors report every single value when making data visualizations, particularly for small n studies using bar graphs and boxplots to show results. Instead of just displaying the mean/median and surrounding data range, it is advised to show how every replicate landed in the study range when possible. Note that this requirement is not feasible for studies with larger sample sizes though should be considered for smaller in vitro and animal model studies. Let’s start with addressing #1: Legibility of Axis Text. The legend title and axis titles can easily be changed with ggplot(), so that will be done later. To remove the underscore from the Biomass_Burn_Condition column, we can use the function gsub(), which will replace all of the underscores with spaces, resulting in a cleaner-looking graph. # First adding spaces between the biomass burn conditions scaled_longer_smoke_data = scaled_longer_smoke_data %&gt;% mutate(Biomass_Burn_Condition = gsub(&quot;_&quot;, &quot; &quot;, Biomass_Burn_Condition)) # Viewing dataframe head(scaled_longer_smoke_data) ## # A tibble: 6 × 7 ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine Flaming ## # ℹ 2 more variables: Chemical_Concentration &lt;dbl&gt;, ## # Scaled_Chemical_Concentration &lt;dbl[,1]&gt; #2. Reordering the boxplots based on the median scaled chemical concentration. After calculating the median scaled chemical concentration for each biomass burn condition, the new dataframe will be arranged from lowest to highest median scaled concentration from the top of the dataframe to the bottom. This order will be saved in a vector, median_biomass_order. Although the biomass burn conditions are saved from lowest to highest concentration, ggplot() will plot them in reverse order with the highest concentration at the top and the lowest at the bottom of the y axis. Axis reordering can also be accomplished using reorder within the ggplot() function as described here and here. median_biomass = scaled_longer_smoke_data %&gt;% group_by(Biomass_Burn_Condition) %&gt;% summarize(Median_Concentration = median(Scaled_Chemical_Concentration)) %&gt;% # arranges dataframe from lowest to highest from top to bottom arrange(Median_Concentration) head(median_biomass) ## # A tibble: 6 × 2 ## Biomass_Burn_Condition Median_Concentration ## &lt;chr&gt; &lt;dbl&gt; ## 1 Red Oak Smoldering -0.459 ## 2 Eucalyptus Smoldering -0.451 ## 3 Pine Smoldering -0.424 ## 4 Pine Needles Smoldering -0.370 ## 5 Pine Needles Flaming -0.350 ## 6 Red Oak Flaming -0.337 # Saving that order median_biomass_order = median_biomass$Biomass_Burn_Condition # Putting into factor to organize the burn conditions scaled_longer_smoke_data$Biomass_Burn_Condition = factor(scaled_longer_smoke_data$Biomass_Burn_Condition, levels = median_biomass_order) # Final dataframe to be used for plotting head(scaled_longer_smoke_data) ## # A tibble: 6 × 7 ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine Flaming ## # ℹ 2 more variables: Chemical_Concentration &lt;dbl&gt;, ## # Scaled_Chemical_Concentration &lt;dbl[,1]&gt; Now that the dataframe has been finalized, we can plot the new boxplot. The final revision, #3: Making Use of Color, will be addressed with ggplot(). However, a palette can be chosen from the MetBrewer package. # Choosing the &quot;Jurarez&quot; palette from the `MetBrewer` package # `n = 12`, since there are 12 biomass burn conditions juarez_colors = met.brewer(name = &quot;Juarez&quot;, n = 12)[1:12] #4. Show all data points when possible will also be addressed with ggplot() by simply using geom_point(). FigureX1 = ggplot(scaled_longer_smoke_data, aes(x = Scaled_Chemical_Concentration, y = Biomass_Burn_Condition, color = Biomass_Burn_Condition)) + geom_boxplot() + # jittering the points, so they&#39;re not all on top of each other and adding transparency geom_point(position = position_jitter(h = 0.1), alpha = 0.7) + theme_light() + # changing the theme theme(axis.text = element_text(size = 9), # changing size of axis labels axis.title = element_text(face = &quot;bold&quot;, size = rel(1.5)), # changes axis titles legend.position = &quot;none&quot;) + # removes legend xlab(&#39;Scaled Chemical Concentration (pg/uL)&#39;) + ylab(&#39;Biomass Burn Condition&#39;) + # changing axis labels scale_color_manual(values = c(juarez_colors)) # changing the colors FigureX1 An appropriate title for this figure could be: “Figure X. Chemical concentration distributions of biomass burn conditions. The boxplots are based on the scaled chemical concentration values, which used the raw chemical concentrations values scaled within each chemical. The individual dots represent the concentrations of each chemical. The biomass burn conditions on the y axis are ordered from greatest (top) to least (bottom) based on median scaled chemical concentration.” Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question #1: Which biomass burn condition has the highest total chemical concentration? Answer: Smoldering peat has the highest median chemical concentration, however the median concentrations are comparable across all biomass burn conditions. All the flaming conditions have the highest median chemical concentrations and more overall variation than their respective smoldering conditions with the exception of smoldering peat. You may notice that the scaled chemical concentration was put on the x axis and burn condition was put on the y axis and not vice versa. When names are longer in length, they are more legible if placed on the y axis. Other aspects of the figure were changed in the latest version, but those are minor compared to changing the order of the boxplots, revamping the text, and changing the usage of color. For example, the background was changed from gray to white. Figure backgrounds are generally white, because the figure is easier to read if the paper is printed in black and white. A plot’s background can easily be changed to white in R using theme_light(), theme_minimal(), or theme_bw(). Posit provides a very helpful GGplot2 cheat sheet for changing a figure’s parameters. Creating an Improved Heatmap Visualization We’ll use a heatmap to answer the second environmental health question: Which classes of chemicals show the highest concentrations across the evaluated biomass burn conditions? Let’s view the original heatmap from the previous module and find aspects of it that can be improved. # Changing the biomass condition variable back to a character from a factor scaled_longer_smoke_data$Biomass_Burn_Condition = as.character(scaled_longer_smoke_data$Biomass_Burn_Condition) # Calculating the median value within each biomass burn condition and category scaled_longer_smoke_data = scaled_longer_smoke_data %&gt;% group_by(Biomass_Burn_Condition, Chemical.Category) %&gt;% mutate(Median_Scaled_Concentration = median(Scaled_Chemical_Concentration)) # Plotting ggplot(data = scaled_longer_smoke_data, aes(x = Chemical.Category, y = Biomass_Burn_Condition, fill = Median_Scaled_Concentration)) + geom_tile() + geom_text(aes(label = round(Median_Scaled_Concentration, 2))) # adding concentration values as text, rounding to two values after the decimal From the figure above, it’s clear that certain biomass burn conditions are associated with higher chemical concentrations for some of the chemical categories. For example, peat flaming exposure was associated with higher levels of inorganics and ions, while pine smoldering exposure was associated with higher levels of methoxyphenols. Although these are important findings, it is still difficult to determine if there are greater similarities in chemical profiles based on the biomass or the incineration temperature. Therefore, let’s identify some elements of this chart that can be modified to make it easier to answer our research question. There are three main aspects we can adjust on this figure: 1. The legibility of the text in the legend and axes. Similar to what we did previously, we’ll replace underscores and periods with spaces in the axis labels and titles. 2. The order of the axis labels. Ordering the biomass burn condition and chemical category from highest to lowest based on their median scaled chemical concentration allows the reader to easily determine the biomass burn condition that had the greatest or least total chemical concentrations relative to each other. From the previous boxplot figure, biomass burn condition is already in this order, however we need to order the chemical category by putting the variable into a factor. 3. Use of color. Notice that in the boxplot we used a qualitative palette, which is best for creating visual differences between different classes or groups. In this heatmap, we’ll choose a diverging color palette that uses two or more contrasting colors. A diverging color palette is able to highlight mid range with a lighter color and values at either extreme with a darker color or vice versa. #1: Legibility of Text can be addressed in ggplot() and so can #2: Reordering the heatmap. Biomass_Burn_Condition has already been reordered and put into a factor, but we need to do the same with Chemical.Category. Similar to before, median scaled chemical concentration for each chemical category will be calculated. However, this time the new dataframe will be arranged from highest to lowest median scaled concentration from the top of the dataframe to the bottom. ggplot() will plot them in the SAME order with the highest concentration on the left side and the lowest on the right side of the figure. # Order the chemical category by the median scaled chemical concentration median_chemical = scaled_longer_smoke_data %&gt;% group_by(Chemical.Category) %&gt;% summarize(Median_Concentration = median(Scaled_Chemical_Concentration)) %&gt;% arrange(-Median_Concentration) head(median_chemical) ## # A tibble: 6 × 2 ## Chemical.Category Median_Concentration ## &lt;chr&gt; &lt;dbl&gt; ## 1 Inorganics -0.265 ## 2 n-Alkanes -0.335 ## 3 Ions -0.359 ## 4 Levoglucosan -0.417 ## 5 Methoxyphenols -0.434 ## 6 PAHs -0.459 # Saving that order median_chemical_order = median_chemical$Chemical.Category # Putting into factor to organize the chemical categories scaled_longer_smoke_data$Chemical.Category = factor(scaled_longer_smoke_data$Chemical.Category, levels = median_chemical_order) # Putting burn conditons back into a factor to organize them scaled_longer_smoke_data$Biomass_Burn_Condition = factor(scaled_longer_smoke_data$Biomass_Burn_Condition, levels = median_biomass_order) # Viewing the dataframe to be plotted head(scaled_longer_smoke_data) ## # A tibble: 6 × 8 ## # Groups: Biomass_Burn_Condition, Chemical.Category [6] ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine Flaming ## # ℹ 3 more variables: Chemical_Concentration &lt;dbl&gt;, ## # Scaled_Chemical_Concentration &lt;dbl[,1]&gt;, Median_Scaled_Concentration &lt;dbl&gt; Now that the dataframe has been finalized, we can plot the new boxplot. The final revision, #3: Making Use of Color, will be addressed with ggplot(). Here a palette is chosen from the RColorBrewer package. # Only needed to choose 2 colors for &#39;low&#39; and &#39;high&#39; in the heatmap # `n = 8` in the code to generate more colors that can be chosen from rcolorbrewer_colors = brewer.pal(n = 8, name = &#39;Accent&#39;) FigureX2 = ggplot(data = scaled_longer_smoke_data, aes(x = Chemical.Category, y = Biomass_Burn_Condition, fill = Median_Scaled_Concentration)) + geom_tile(color = &#39;white&#39;) + # adds white space between the tiles geom_text(aes(label = round(Median_Scaled_Concentration, 2))) + # adding concentration values as text theme_minimal() + # changing the theme theme(axis.text = element_text(size = 9), # changing size of axis labels axis.title = element_text(face = &quot;bold&quot;, size = rel(1.5)), # changes axis titles legend.title = element_text(face = &#39;bold&#39;, size = 10), # changes legend title legend.text = element_text(size = 9)) + # changes legend text labs(x = &#39;Chemical Category&#39;, y = &#39;Biomass Burn Condition&#39;, fill = &quot;Scaled Chemical\\nConcentration (pg/mL)&quot;) + # changing axis labels scale_fill_gradient(low = rcolorbrewer_colors[5], high = rcolorbrewer_colors[6]) # changing the colors FigureX2 An appropriate title for this figure could be: “Figure X. Chemical category concentrations across biomass burn conditions. Scaled chemical concentration values are based on the raw chemical concentration values scaled within each chemical. Chemical category on the x axis is ordered from highest to lowest median concentration from left to right. Biomass burn condition on the y axis is ordered from the highest to lowest median concentration from top to bottom. The values in each tile represent the median scaled chemical concentration.” Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: Which classes of chemicals show the highest concentrations across the evaluated biomass burn conditions? Answer: Ordering the axes from highest to lowest concentration didn’t help organize the data as much as we would’ve liked given some of the variance of chemical concentrations across the chemical categories. Nevertheless, it’s still clear that peat flaming produces the highest concentration of inorganics and ions, peat smoldering with n-Alkanes, eucalyptus smoldering with Levoglucosan, pine smoldering with methoxyphenols, and pine flaming with PAHs. In addition, flaming conditions seem to have higher levels of inorganics and ions while smoldering conditions seem to have higher levels of levoglucosan and PAHs. It would be helpful if there was a way to group these chemical profiles based on similarity and that’s where the pheatmap() function can be helpful when it can be difficult to spot those patterns using visual inspection alone. Just for fun, let’s briefly visualize a hierarchical clustering heatmap, which will be used to group both the biomass burn conditions and chemical categories based on their chemical concentrations. In this module, we’ll focus only on the pheatmap() visualization, but more information on hierarchical clustering can be found in Module 5.5 Unsupervised Machine Learning II: Additional Clustering Applications. As we showed in the previous module, this function requires a wide dataframe which we’ll need to create. It will contain Chemical.Category, Biomass_Burn_Condition and Scaled_Chemical_Concentration. heatmap_df2 = scaled_longer_smoke_data %&gt;% group_by(Biomass_Burn_Condition, Chemical.Category) %&gt;% # using the summarize function instead of mutate function as was done previously since we only need the median values now summarize(Median_Scaled_Concentration = median(Scaled_Chemical_Concentration)) %&gt;% # transforming the data to a wide format pivot_wider(id_cols = Biomass_Burn_Condition, names_from = &quot;Chemical.Category&quot;, values_from = &quot;Median_Scaled_Concentration&quot;) %&gt;% # converting the chemical names to row names column_to_rownames(var = &quot;Biomass_Burn_Condition&quot;) head(heatmap_df2) ## Inorganics n-Alkanes Ions Levoglucosan ## Red Oak Smoldering -0.6236516 -0.3895328 -0.8011155 1.1042911 ## Eucalyptus Smoldering -0.6859508 -0.3465132 -0.8016019 1.7772753 ## Pine Smoldering -0.8012749 -0.3637526 -0.8016019 0.9961855 ## Pine Needles Smoldering -0.8436790 -0.3076779 -0.4611222 -0.6357051 ## Pine Needles Flaming 0.3640553 -0.3850613 0.8222903 -0.8570130 ## Red Oak Flaming 0.4858796 -0.3016948 0.6793662 -0.8642615 ## Methoxyphenols PAHs ## Red Oak Smoldering 0.04674277 -0.5179641 ## Eucalyptus Smoldering -0.06449444 -0.4724635 ## Pine Smoldering 2.74452541 -0.4580922 ## Pine Needles Smoldering -0.22835290 -0.3162278 ## Pine Needles Flaming -0.46331332 -0.4179505 ## Red Oak Flaming -0.51488738 -0.5318127 Now let’s generate the same heatmap this time using the pheatmap() function: # creating a color palette blue_pink_palette = colorRampPalette(c(rcolorbrewer_colors[5], rcolorbrewer_colors[6])) pheatmap(heatmap_df2, # changing the color scheme color = blue_pink_palette(40), # hierarchical clustering of the biomass burn conditions cluster_rows = TRUE, # creating white space between the two largest clusters cutree_row = 2, # adding the values for each cell and making those values black display_numbers = TRUE, number_color = &quot;black&quot;, # changing the font size and the angle of the column names fontsize = 12, angle_col = 45) By using incorporating the dendrogram into the visualization, it’s easier to see that the chemical profiles have greater similarities within incineration temperatures rather than biomasses (with the exception of pine needles smoldering). Creating Multi-Plot Figures We can combine figures using the plot_grid() function from the cowplot package. For additional information on the plot_grid() function and parameters that can be changed see Arranging Plots in a Grid. Other packages that have figure combining capabilities include the patchwork package and the grid_arrange() function from the gridExtra package. Figures can also be combined after they’re exported from R using other applications like MS powerpoint and Adobe pdf. FigureX = plot_grid(FigureX1, FigureX2, # Adding labels, changing size their size and position labels = &quot;AUTO&quot;, label_size = 15, label_x = 0.04, rel_widths = c(1, 1.5)) FigureX An appropriate title for this figure could be: “Figure X. Chemical concentration distributions across biomass burn conditions. (A) The boxplots are based on the scaled chemical concentration values, which used the raw chemical concentrations values scaled within each chemical. The individual dots represent the concentrations of each chemical. The biomass burn conditions on the y axis are ordered from greatest (top) to least (bottom) based on median scaled chemical concentration. (B) The heatmap visualizes concentrations across chemical categories. Chemical category on the x axis is ordered from highest to lowest median concentration from left to right. Biomass burn condition on the y axis is ordered from the highest to lowest median concentration from top to bottom. The values in each tile represent the median scaled chemical concentration. By putting these two figures side by side, it’s now easier to compare the distributions of each biomass burn condition in figure A alongside the median chemical category concentrations in figure B that are responsible for the variation seen on the left. Concluding Remarks In conclusion, this training module provided information and example code for improving, streamlining, and making ggplot2 figures publication ready. Keep in mind that concepts and ideas presented in this module can be subjective and might need to be amended given the situation, dataset, and visualization. Additional Resources Beginner’s Guide to Data Visualizations and Improving Data Visualizations in R Generating Colors for Visualizations Additional Hands on Training Brewer, Cynthia A. 1994. Color use guidelines for mapping and visualization. Chapter 7 (pp. 123-147) in Visualization in Modern Cartography Hattab, G., Rhyne, T.-M., &amp; Heider, D. (2020). Ten simple rules to colorize biological data visualization. PLOS Computational Biology, 16(10), e1008259. PMID: 33057327 Lastly, for researchers who are newer to R programming, ggpubr is a package specifically designed to create publication-ready graphs similar to ggplot2 with more concise syntax. This package is particularly useful for statistically relevant visualizations, which are further explored in later modules including, TAME 2.0 Module 3.4 Introduction to Statistical Tests, TAME 2.0 Module 4.4 Two Group Comparisons and Visualizations, and TAME 2.0 Module 4.5 Multigroup Comparisons and Visualizations. Test Your Knowledge Replicate the figure below! The heatmap is the same as the “Test Your Knowledge” figure from TAME 2.0 Module 3.1 Data Visualizations. This time we’ll focus on making the figure look more publication ready by cleaning up the titles, cleaning up the labels, and changing the colors. The heatmap still visualizes the median chemical concentrations, but this time we’re separating the burn conditions, allowing us to determine if the concentrations of chemicals released are contingent upon the burn condition. Hint: To view additional aspects of figures that can be changed in ggplot2 check out this GGPlot2 Cheat Sheet. It might come in handy! "],["normality-tests-and-data-transformations.html", "Normality Tests and Data Transformations Introduction to Training Module What is a Normal Distribution? Qualitative Assessment of Normality Quantitative Normality Assessment Data Transformation Additional Considerations Regarding Normality Concluding Remarks", " Normality Tests and Data Transformations This training module was developed by Elise Hickman, Alexis Payton, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module When selecting the appropriate statistical tests to evaluate potential trends in your data, selection often relies upon whether or not underlying data are normally distributed. Many statistical tests and methods that are commonly implemented in exposure science, toxicology, and environmental health research rely on assumptions of normality. Applying a statistical test intended for data with a specific distribution when your data do not fit within that distribution can generate unreliable results, with the potential for false positive and false negative findings. Thus, one of the most common statistical tests to perform at the beginning of an analysis is a test for normality. In this training module, we will: Review the normal distribution and why it is important Demonstrate how to test whether your variable distributions are normal… Qualitatively, with histograms and Q-Q plots Quantitatively, with the Shapiro-Wilk test Discuss data transformation approaches Demonstrate log2 data transformation for non-normal data Discuss additional considerations related to normality We will demonstrate normality assessment using example data derived from a study in which chemical exposure profiles were collected across study participants through silicone wristbands. This exposure monitoring technique has been described through previous publications, including the following examples: O’Connell SG, Kincl LD, Anderson KA. Silicone wristbands as personal passive samplers. Environ Sci Technol. 2014 Mar 18;48(6):3327-35. doi: 10.1021/es405022f. Epub 2014 Feb 26. PMID: 24548134; PMCID: PMC3962070. Kile ML, Scott RP, O’Connell SG, Lipscomb S, MacDonald M, McClelland M, Anderson KA. Using silicone wristbands to evaluate preschool children’s exposure to flame retardants. Environ Res. 2016 May;147:365-72. doi: 10.1016/j.envres.2016.02.034. Epub 2016 Mar 3. PMID: 26945619; PMCID: PMC4821754. Hammel SC, Hoffman K, Phillips AL, Levasseur JL, Lorenzo AM, Webster TF, Stapleton HM. Comparing the Use of Silicone Wristbands, Hand Wipes, And Dust to Evaluate Children’s Exposure to Flame Retardants and Plasticizers. Environ Sci Technol. 2020 Apr 7;54(7):4484-4494. doi: 10.1021/acs.est.9b07909. Epub 2020 Mar 11. PMID: 32122123; PMCID: PMC7430043. Levasseur JL, Hammel SC, Hoffman K, Phillips AL, Zhang S, Ye X, Calafat AM, Webster TF, Stapleton HM. Young children’s exposure to phenols in the home: Associations between house dust, hand wipes, silicone wristbands, and urinary biomarkers. Environ Int. 2021 Feb;147:106317. doi: 10.1016/j.envint.2020.106317. Epub 2020 Dec 17. PMID: 33341585; PMCID: PMC7856225. In the current example dataset, chemical exposure profiles were obtained from the analysis of silicone wristbands worn by 97 participants for one week. Chemical concentrations on the wristbands were measured with gas chromatography mass spectrometry. The subset of chemical data used in this training module are all phthalates, a group of chemicals used primarily in plastic products to increase flexibility and durability. Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;openxlsx&quot;)) install.packages(&quot;openxlsx&quot;); if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); if (!requireNamespace(&quot;ggpubr&quot;)) install.packages(&quot;ggpubr&quot;); Loading R packages required for this session library(openxlsx) # for importing data library(tidyverse) # for manipulating and plotting data library(ggpubr) # for making Q-Q plots with ggplot Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example dataset # Import data wrist_data &lt;- read.xlsx(&quot;Module3_3_Input/Module3_3_InputData.xlsx&quot;) # Viewing the data head(wrist_data) ## S_ID Age DEP DBP BBP DEHA DEHP DEHT ## 1 1 24.76986 335.58857 574.5443 40.67286 755.8157 10621.7029 30420.68 ## 2 2 25.39452 56.38286 1075.7114 243.48857 2716.7314 3036.5757 23991.82 ## 3 3 34.55068 515.65429 121.1657 205.86857 3286.5886 3056.2743 46188.06 ## 4 4 23.83562 1009.00714 373.4957 66.97571 3966.5371 729.7971 17900.74 ## 5 5 39.29315 33.74143 104.0629 77.17286 1654.3317 2599.7129 13597.44 ## 6 6 36.15616 168.79714 503.8300 61.98429 398.6314 1492.6143 29875.76 ## DINP TOTM ## 1 26534.290 1447.86000 ## 2 10073.704 39.46143 ## 3 1842.949 112.67714 ## 4 78779.567 92.31000 ## 5 3682.956 161.84571 ## 6 23845.493 182.56429 Our example dataset contains subject IDs (S_ID), subject ages, and measurements of 8 different phthalates from silicone wristbands: DEP: Diethyl phthalate DBP : Dibutyl phthalate BBP : Butyl benzyl phthalate DEHA : Di(2-ethylhexyl) adipate DEHP : Di(2-ethylhexyl) phthalate DEHT: Di(2-ethylhexyl) terephthalate DINP : Diisononyl phthalate TOTM : Trioctyltrimellitate The units for the chemical data are nanogram of chemical per gram of silicone wristband (ng/g) per day the participant wore the wristband. One of the primary questions in this study was whether there were significant differences in chemical exposure between subjects with different levels of social stress or between subjects with differing demographic characteristics. However, before we can analyze the data for significant differences between groups, we first need to assess whether our numeric variables are normally distributed. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Are these data normally distributed? How does the distribution of data influence the statistical tests performed on the data? Before answering these questions, let’s define normality and how to test for it in R. What is a Normal Distribution? A normal distribution is a distribution of data in which values are distributed roughly symmetrically out from the mean such that 68.3% of values fall within one standard deviation of the mean, 95.4% of values fall within 2 standard deviations of the mean, and 99.7% of values fall within three standard deviations of the mean. Figure 5: Figure Credit: D Wells, CC BY-SA 4.0 https://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons Common parametric statistical tests, such as t-tests, one-way ANOVAs, and Pearson correlations, rely on the assumption that data fall within the normal distribution for calculation of z-scores and p-values. Non-parametric tests, such as the Wilcoxon Rank Sum test, Kruskal-Wallis test, and Spearman Rank correlation, do not rely on assumptions about data distribution. Some of the aforementioned between-group comparisons were introduced in TAME 2.0 Module 3.4 Introduction to Statistical Tests. They, along with non-parametric tests, are explored further in later modules including TAME 2.0 Module 4.4 Two-Group Comparisons &amp; Visualizations and TAME 2.0 Module 4.5 Multi-group Comparisons &amp; Visualizations. Qualitative Assessment of Normality We can begin by assessing the normality of our data through plots. For example, plotting data using histograms, densities, or Q-Q plots can graphically help inform if a variable’s values appear to be normally distributed or not. We will start with visualizing our data distributions with histograms. Histograms Let’s start with visualizing the distribution of the participant’s ages using the hist() function that is part of base R. hist(wrist_data$Age) We can edit some of the parameters to improve this basic histogram visualization. For example, we can decrease the size of each bin using the breaks parameter: hist(wrist_data$Age, breaks = 10) The hist() function is useful for plotting single distributions, but what if we have many variables that need normality assessment? We can leverage ggplot2’s powerful and flexible graphics functions such as geom_histogram() and facet_wrap() to inspect histograms of all of our variables in one figure panel. For more information on data manipulation in general, see TAME 2.0 Module 2.3 Data Manipulation &amp; Reshaping and for more on ggplot2 including the use of facet_wrap(), see TAME 2.0 Module 3.2 Improving Data Visualizations. First, we’ll pivot our data to longer to prepare for plotting. Then, we’ll make our plot. We can use the theme_set() function to set a default graphing theme for the rest of the script. A graphing theme represents a set of default formatting parameters (mostly colors) that ggplot will use to make your graphs. theme_bw() is a basic theme that includes a white background for the plot and dark grey axis text and minor axis lines. The theme that you use is a matter of personal preference. For more on the different themes available through ggplot2, see here. # Pivot data longer to prepare for plotting wrist_data_long &lt;- wrist_data %&gt;% pivot_longer(!S_ID, names_to = &quot;variable&quot;, values_to = &quot;value&quot;) # Set theme for graphing theme_set(theme_bw()) # Make figure panel of histograms ggplot(wrist_data_long, aes(value)) + geom_histogram(fill = &quot;gray40&quot;, color = &quot;black&quot;, binwidth = function(x) {(max(x) - min(x))/25}) + facet_wrap(~ variable, scales = &quot;free&quot;) + labs(y = &quot;# of Observations&quot;, x = &quot;Value&quot;) From these histograms, we can see that our chemical variables do not appear to be normally distributed. Q-Q Plots Q-Q (quantile-quantile) plots are another way to visually assess normality. Similar to the histogram above, we can create a single Q-Q plot for the age variable using base R functions. Normal Q-Q plots (Q-Q plots where the theoretical quantiles are based on a normal distribution) have theoretical quantiles on the x-axis and sample quantiles, representing the distribution of the variable of interest from the dataset, on the y-axis. If the variable of interest is normally distributed, the points on the graph will fall along the reference line. # Plot points qqnorm(wrist_data$Age) # Add a reference line for theoretically normally distributed data qqline(wrist_data$Age) Small variations from the reference line, as seen above, are to be expected for the most extreme values. Overall, we can see that the age data are relatively normally distributed, as the points fall along the reference line. To make a figure panel with Q-Q plots for all of our variables of interest, we can use the ggqqplot() function within the ggpubr package. This function generates Q-Q plots and has arguments that are similar to ggplot2. ggqqplot(wrist_data_long, x = &quot;value&quot;, facet.by = &quot;variable&quot;, ggtheme = theme_bw(), scales = &quot;free&quot;) With this figure panel, we can see that the chemical data have very noticeable deviations from the reference, suggesting non-normal distributions. To answer our first environmental health question, age is the only variable that appears to be normally distributed in our dataset. This is based on our histograms and Q-Q plots with data centered in the middle and spreading with a distribution on both the lower and upper sides that follow typical normal data distributions. However, chemical concentrations appear to be non-normally distributed. Next, we will implement a quantitative approach to assessing normality, based on a statistical test for normality. Quantitative Normality Assessment Single Variable Normality Assessment We will use the Shapiro-Wilk test to quantitatively assess whether our data distribution is normal, again looking at the age data. This test can be carried out simply using the shapiro.test() function from the base R stats package. When using this test and interpreting its results, it is important to remember that the null hypothesis is that the sample distribution is normal, and a significant p-value means the distribution is non-normal. shapiro.test(wrist_data$Age) ## ## Shapiro-Wilk normality test ## ## data: wrist_data$Age ## W = 0.9917, p-value = 0.8143 This test resulted in a p-value of 0.8143, so we cannot reject the null hypothesis (that data are normally distributed). This means that we can assume that age is normally distributed, which is consistent with our visualizations above. Multiple Variable Normality Assessment With a large dataset containing many variables of interest (e.g., our example data with multiple chemicals), it is more efficient to test each column for normality and then store those results in a dataframe. We can use the base R function apply() to apply the Shapiro Wilk test over all of the numeric columns of our dataframe. This function generates a list of results, with a list element for each variable tested. There are also other ways that you could iterate through each of your columns, such as a for loop or a function as discussed in TAME 2.0 Module 2.4 Improving Coding Efficiencies. # Apply Shapiro Wilk test shapiro_res &lt;- apply(wrist_data %&gt;% select(-S_ID), 2, shapiro.test) # View first three list elements glimpse(shapiro_res[1:3]) ## List of 3 ## $ Age:List of 4 ## ..$ statistic: Named num 0.992 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;W&quot; ## ..$ p.value : num 0.814 ## ..$ method : chr &quot;Shapiro-Wilk normality test&quot; ## ..$ data.name: chr &quot;newX[, i]&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;htest&quot; ## $ DEP:List of 4 ## ..$ statistic: Named num 0.225 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;W&quot; ## ..$ p.value : num 2.74e-20 ## ..$ method : chr &quot;Shapiro-Wilk normality test&quot; ## ..$ data.name: chr &quot;newX[, i]&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;htest&quot; ## $ DBP:List of 4 ## ..$ statistic: Named num 0.658 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;W&quot; ## ..$ p.value : num 1.08e-13 ## ..$ method : chr &quot;Shapiro-Wilk normality test&quot; ## ..$ data.name: chr &quot;newX[, i]&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;htest&quot; We can then convert those list results into a dataframe. Each variable is now in a row, with columns describing outputs of the statistical test. # Create results dataframe shapiro_res &lt;- do.call(rbind.data.frame, shapiro_res) # View results dataframe shapiro_res ## statistic p.value method data.name ## Age 0.9917029 8.143367e-01 Shapiro-Wilk normality test newX[, i] ## DEP 0.2248611 2.736536e-20 Shapiro-Wilk normality test newX[, i] ## DBP 0.6584967 1.076529e-13 Shapiro-Wilk normality test newX[, i] ## BBP 0.2367689 3.757059e-20 Shapiro-Wilk normality test newX[, i] ## DEHA 0.6646692 1.454576e-13 Shapiro-Wilk normality test newX[, i] ## DEHP 0.6163531 1.519572e-14 Shapiro-Wilk normality test newX[, i] ## DEHT 0.8072684 6.315917e-10 Shapiro-Wilk normality test newX[, i] ## DINP 0.5741864 2.486638e-15 Shapiro-Wilk normality test newX[, i] ## TOTM 0.3397424 6.901903e-19 Shapiro-Wilk normality test newX[, i] Finally, we can clean up our results dataframe and add a column that will quickly tell us whether our variables are normally or non-normally distributed based on the Shapiro-Wilk normality test results. # Clean dataframe shapiro_res &lt;- shapiro_res %&gt;% # Add normality conclusion mutate(normal = ifelse(p.value &lt; 0.05, F, T)) %&gt;% # Remove columns that do not contain informative data select(c(p.value, normal)) # View cleaned up dataframe shapiro_res ## p.value normal ## Age 8.143367e-01 TRUE ## DEP 2.736536e-20 FALSE ## DBP 1.076529e-13 FALSE ## BBP 3.757059e-20 FALSE ## DEHA 1.454576e-13 FALSE ## DEHP 1.519572e-14 FALSE ## DEHT 6.315917e-10 FALSE ## DINP 2.486638e-15 FALSE ## TOTM 6.901903e-19 FALSE The results from the Shapiro-Wilk test demonstrate that age data are normally distributed, while the chemical concentration data are non-normally distributed. These results support the conclusions we made based on our qualitative assessment above with histograms and Q-Q plots. Answer to Environmental Health Question 1 With this, we can now answer Environmental Health Question #1: Are these data normally distributed? Answer: Age is normally distributed, while chemical concentrates are non-normally distributed. Answer to Environmental Health Question 2 We can also answer Environmental Health Question #2: How does the distribution of data influence the statistical tests performed on the data? Answer: Parametric statistical tests should be used when analyzing the age data, and non-parametric tests should be used when analyzing the chemical concentration data Data Transformation There are a number of approaches that can be used to change the range and/or distribution of values within each variable. Typically, the purpose for applying these changes is to reduce bias in a dataset, remove known sources of variation, or prepare data for specific downstream analyses. The following are general definitions for common terms used when discussing these changes: Transformation refers to any process used to change data into other, related values. Normalization and standardization are types of data transformation. Transformation can also refer to performing the same mathematical operation on every value in your dataframe. For example, taking the log2 or log10 of every value is referred to as log transformation. Normalization is the process of transforming variables so that they are on a similar scale and therefore are comparable. This can be important when variables in a dataset contain a mixture of data types that are represented by vastly different numeric magnitudes or when there are known sources of variability across samples. Normalization methods are highly dependent on the type of input data. One example of normalization is min-max scaling, which results in a range for each variable of 0 to 1. Although normalization in computational methodologies typically refers to min-max scaling or other similar methods where the variable’s range is bounded by specific values, wet-bench approaches also employ normalization - for example, using a reference gene for RT-qPCR assays or dividing a total protein amount for each sample by the volume of each sample to obtain a concentration. Standardization, also known as Z-score normalization, is a specific type of normalization that involves subtracting each value from the mean of that variable and dividing by that variable’s standard deviation. The standardized values for each variable will have a mean of 0 and a standard deviation of 1. The scale() function in R performs standardization by default when the data are centered (argument center = TRUE is included within the scale function). Transformation of example data When data are non-normally distributed, such as with the chemical concentrations in our example dataset, it may be desirable to transform the data so that the distribution becomes closer to a normal distribution, particularly if there are only parametric tests available to test your hypothesis. A common transformation used in environmental health research is log2 transformation, in which data are transformed by taking the log2 of each value in the dataframe. Let’s log2 transform our chemical data and examine the resulting histograms and Q-Q plots to qualitatively assess whether data appear more normal following transformation. We will apply a pseudo-log2 transformation, where we will add 1 to each value before log2 transforming so that all resulting values are positive and any zeroes in the dataframe do not return -Inf. # Apply psuedo log2 (pslog2) transformation to chemical data wrist_data_pslog2 &lt;- wrist_data %&gt;% mutate(across(DEP:TOTM, ~ log2(.x + 1))) # Pivot data longer wrist_data_pslog2_long &lt;- wrist_data_pslog2 %&gt;% pivot_longer(!S_ID, names_to = &quot;variable&quot;, values_to = &quot;value&quot;) # Make figure panel of histograms ggplot(wrist_data_pslog2_long, aes(value)) + geom_histogram(fill = &quot;gray40&quot;, color = &quot;black&quot;, binwidth = function(x) {(max(x) - min(x))/25}) + facet_wrap(~ variable, scales = &quot;free&quot;) + labs(y = &quot;# of Observations&quot;, x = &quot;Value&quot;) # Make a figure panel of Q-Q plots ggqqplot(wrist_data_pslog2_long, x = &quot;value&quot;, facet.by = &quot;variable&quot;, ggtheme = theme_bw(), scales = &quot;free&quot;) Both the histograms and the Q-Q plots demonstrate that our log2 transformed data are more normally distributed than the raw data graphed above. Let’s apply the Shapiro-Wilk test to our log2 transformed data to determine if the chemical distributions are normally distributed. # Apply Shapiro Wilk test shapiro_res_pslog2 &lt;- apply(wrist_data_pslog2 %&gt;% select(-S_ID), 2, shapiro.test) # Create results dataframe shapiro_res_pslog2 &lt;- do.call(rbind.data.frame, shapiro_res_pslog2) # Clean dataframe shapiro_res_pslog2 &lt;- shapiro_res_pslog2 %&gt;% ## Add normality conclusion mutate(normal = ifelse(p.value &lt; 0.05, F, T)) %&gt;% ## Remove columns that do not contain informative data select(c(p.value, normal)) # View cleaned up dataframe shapiro_res_pslog2 ## p.value normal ## Age 0.814336705 TRUE ## DEP 0.001335217 FALSE ## DBP 0.368954224 TRUE ## BBP 0.052805523 TRUE ## DEHA 0.979072298 TRUE ## DEHP 0.304963678 TRUE ## DEHT 0.770066136 TRUE ## DINP 0.883662530 TRUE ## TOTM 0.004399442 FALSE The results from the Shapiro-Wilk test demonstrate that the the log2 chemical concentration data are more normally distributed than the raw data. Overall, the p-values, even for the chemicals that are still non-normally distributed, are much higher, and only 2 out of the 8 chemicals are non-normally distributed by the Shapiro-Wilk test. We can also calculate average p-values across all variables for our raw and log2 transformed data to further demonstrate this point. # Calculate the mean Shapiro-Wilk p-value for the raw chemical data mean(shapiro_res$p.value) ## [1] 0.09048186 # Calculate the mean Shapiro-Wilk p-value for the pslog2 transformed chemical data mean(shapiro_res_pslog2$p.value) ## [1] 0.4643995 Therefore, the log2 chemical data would be most appropriate to use if researchers are wanting to perform parametric statistical testing (and particularly if there is not a non-parametric statistical test for a given experimental design). It is important to note that if you proceed to statistical testing using log2 or other transformed data, graphs you make of significant results should use the transformed values on the y-axis, and findings should be interpreted in the context of the transformed values. Additional Considerations Regarding Normality The following sections detail additional considerations regarding normality. Similar to other advice in TAME, appropriate methods for handling normality assessment and normal versus non-normal data can be dependent on your field, lab, endpoints of interest, and downstream analyses. We encourage you to take those elements of your study into account, alongside the guidance provided here, when assessing normality. Regardless of the specific steps you take, be sure to report normality assessment steps and the data transformation or statistical test decisions you make based on them in your final report or manuscript. Determining which data should go through normality testing: Values for all samples (rows) that will be going into statistical testing should be tested for normality. If you are only going to be statistically testing a subset of your data, perform the normality test on that subset. Another way to think of this is that data points that are on the same graph together and/or that have been used as input for a statistical test should be tested for normality together. Analyzing datasets with a mixture of normally and non-normally distributed variables: There are a couple of different routes you can pursue if you have a mixture of normally and non-normally distributed variables in your dataframe: Perform parametric statistical tests on the normally distributed variables and non-parametric tests on the non-normally distributed variable. Perform the statistical test across all variables that fits with the majority of the variable distributions in your dataset. Our preference is to perform one test across all variables of the same data type/endpoint (e.g., all chemical concentrations, all cytokine concentrations). Aim to choose an approach that fits best rather than perfectly. Improving efficiency for normality assessment: If you find yourself frequently performing the same normality assessment workflow, consider writing a function that will execute each normality testing step (making a histogram, making a Q-Q plot, determining Shapiro-Wilk normality variable by variable, and determining the average Shapiro-Wilk p-value across all variables) and store the results in a list for easy inspection. Concluding Remarks In conclusion, this training module serves as an introduction to and step by step tutorial for normality assessment and data transformations. Approaches described in this training module include visualizations to qualitatively assess normality, statistical tests to quantitatively assess normality, data transformation, and other distribution considerations relating to normality. These methods are an important step in data characterization and exploration prior to downstream analyses and statistical testing, and they can be applied to nearly all studies carried out in environmental health research. Additional Resources Descriptive Statistics and Normality Tests for Statistical Data STHDA Normality Test in R Normalization vs. Standardization Test Your Knowledge Use the input file provided (“Module3_3_TYKInput.xlsx”), which represents a similar dataset to the one used in the module, to answer the following questions: Are any variables normally distributed in the raw data? Does psuedo log2 transforming the values make the distributions overall more or less normally distributed? What are the average Shapiro-Wilk p-values for the raw and psuedo log2 transformed data? "],["intoduction-to-statistical-tests.html", "Intoduction to Statistical Tests Introduction to Training Module Assessing Normality &amp; Homogeneity of Variance Two-Group Visualizations and Statistical Comparisons using the T-Test Three-Group Visualizations and Statistical Comparisons using an ANOVA Regression Modeling and Visualization: Linear and Logistic Regressions Statistical Evaluations of Categorical Data using the Chi-Squared Test and Fisher’s Exact Test Concluding Remarks", " Intoduction to Statistical Tests This training module was developed by Alexis Payton, Kyle Roell, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module This training module provides a brief introduction to some of the most commonly implemented statistics and associated visualizations used in exposure science, toxicology, and environmental health studies. This module first uploads an example dataset that is similar to the data used in TAME 2.0 Module 2.3 Data Manipulation &amp; Reshaping], though it includes some expanded subject information data to allow for more example statistical tests. Then, methods to evaluate data normality are presented, including visualization-based and statistical-based approaches. Basic statistical tests discussed in this module include: T test Analysis of Variance (ANOVA) with a Tukey’s Post-Hoc test Regression Modeling (Linear and Logistic) Chi-squared test Fisher’s exact test These statistical tests are very simple, with more extensive examples and associated descriptions of statistical models in the proceeding applications-based training modules in: TAME 2.0 Module 4.4 Two-Group Comparisons &amp; Visualizations TAME 2.0 Module 4.5 Multi-Group Comparisons &amp; Visualizations TAME 2.0 Module 4.6 Advanced Multi-Group Comparisons &amp; Visualizations Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); if (!requireNamespace(&quot;car&quot;)) install.packages(&quot;car&quot;); if (!requireNamespace(&quot;ggpubr&quot;)) install.packages(&quot;ggpubr&quot;); if(!requireNamespace(&quot;effects&quot;)) install.packages(&quot;effects&quot;); Loading R packages required for this session library(tidyverse) # all tidyverse packages, including dplyr and ggplot2 library(car) # package for statistical tests library(ggpubr) # ggplot2 based plots library(effects) # for linear modeling Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example datasets Let’s read in our example dataset. Note that these data are similar to those used previously, except that demographic and chemical measurement data were previously merged, and a few additional columns of subject information/demographics were added to serve as more thorough examples of data for use in this training module. # Loading data full.data &lt;- read.csv(&quot;Module3_4_Input/Module3_4_InputData.csv&quot;) Let’s view the top of the first 9 columns of data in this dataframe: full.data[1:10,1:9] ## ID BMI BMIcat MAge MEdu BW GA Smoker Smoker3 ## 1 1 27.7 Overweight 22.99928 College_Degree 3180.058 34 NS Never ## 2 2 26.8 Overweight 30.05142 College_Degree 3210.823 43 S Never ## 3 3 33.2 Overweight 28.04660 College_Degree 3311.551 40 NS Never ## 4 4 30.1 Overweight 34.81796 College_Degree 3266.844 32 S Never ## 5 5 37.4 Overweight 42.68440 College_Degree 3664.088 35 NS Never ## 6 6 33.3 Overweight 24.94960 College_Degree 3328.988 40 NS Never ## 7 7 24.8 Overweight 29.54798 College_Degree 3061.949 30 NS Never ## 8 8 16.9 Underweight 24.94954 College_Degree 3332.539 38 NS Current ## 9 9 36.9 Overweight 33.58589 College_Degree 3260.482 39 NS Never ## 10 10 21.7 Normal 39.29018 College_Degree 3141.723 35 NS Current These represent the subject information/demographic data, which include the following columns: ID: subject number BMI: body mass index BMIcat: BMI &lt;= 18.5 binned as “Underweight”, 18.5 &lt; BMI &lt;= 24.5 binned as “Normal”, BMI &gt; 24.5 binned as “Overweight” MAge: maternal age in years MEdu: maternal education level; “No_HS_Degree” = “less than high school”, “No_College_Degree” = “high school or some college”, “College_Degree” = “college or greater” BW: body weight in grams GA: gestational age in weeks Smoker: “NS” = non-smoker, “S” = smoker Smoker3: “Never”, “Former”, or “Current” smoking status Let’s now view the remaining columns (columns 10-15) in this dataframe: full.data[1:10,10:15] ## DWAs DWCd DWCr UAs UCd UCr ## 1 6.426464 1.292941 51.67987 10.192695 0.7537104 42.60187 ## 2 7.832384 1.798535 50.10409 11.815088 0.9789506 41.30757 ## 3 7.516569 1.288461 48.74001 10.079057 0.1903262 36.47716 ## 4 5.906656 2.075259 50.92745 8.719123 0.9364825 42.47987 ## 5 7.181873 2.762643 55.16882 9.436559 1.4977829 47.78528 ## 6 9.723429 3.054057 51.14812 11.589403 1.6645837 38.26386 ## 7 6.268547 1.218410 52.08578 8.887948 0.6347667 39.45535 ## 8 6.718448 1.414975 54.96740 9.304968 0.6658849 45.09987 ## 9 9.074928 2.727755 55.72826 10.818153 1.6585757 42.58577 ## 10 5.771691 2.410993 47.06552 8.747217 1.7354305 34.80661 These columns represent the environmental exposure measures, including: DWAs: drinking water arsenic levels in µg/L DWCd: drinking water cadmium levels in µg/L DWCr: drinking water chromium levels in µg/L UAs: urinary arsenic levels in µg/L UCd: urinary cadmium levels in µg/L UCr: urinary chromium levels in µg/L Now that the script is prepared and the data are uploaded, we can start by asking some initial questions about the data that can be answered by running some basic statistical tests and visualizations. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Are there statistically significant differences in BMI between non-smokers and smokers? Are there statistically significant differences in BMI between current, former, and never smokers? Is there a relationship between maternal BMI and birth weight? Are maternal age and gestational age considered potential covariates in the relationship between maternal BMI and birth weight? Are there statistically significant differences in gestational age based on whether a subject is a non-smoker or a smoker? Is there a relationship between smoking status and BMI? Assessing Normality &amp; Homogeneity of Variance Statistical test selection often relies upon whether or not the underlying data are normally distributed and that variance across the groups is the same (homogeneity of variances). Many statistical tests and methods that are commonly implemented in exposure science, toxicology, and environmental health research rely on assumptions of normality. Thus, one of the most common statistical tests to perform at the beginning of an analysis is a test for normality. As discussed in the previous module, there are a few ways to evaluate the normality of a dataset: First, you can visually gauge whether a dataset appears to be normally distributed through plots. For example, plotting data using histograms, densities, or Q-Q plots can graphically help inform if a variable’s values appear to be normally distributed or not. Second, you can evaluate normality using statistical tests, such as the Kolmogorov-Smirnov (K-S) test and Shapiro-Wilk test. When using these tests and interpreting their results, it is important to remember that the null hypothesis is that the sample distribution is normal, and a significant p-value means the distribution is non-normal. Let’s start with the first approach based on data visualizations. In this module, we’ll primarily be generating figures using the ggubr package which is specifically designed to generate ggplot2-based figures using more streamlined coding syntax. In addition, this package has statistical parameters for plotting that are useful for basic statistical analysis, especially for people with introductory experience to plotting in R. For further documentation on ggubr, click here. Let’s begin with a histogram to view the distribution of BMI data using the gghistogram() function from the ggubr package: gghistogram(data = full.data, x = &quot;BMI&quot;, bins = 20) Let’s also view the Q–Q (quantile-quantile) plot using the ggqqplot() function also from the ggubr package: ggqqplot(full.data$BMI, ylab = &quot;BMI&quot;) From these visualizations, the BMI variable appears to be normally distributed, with data centered in the middle and spreading with a distribution on both the lower and upper sides that follow typical normal data distributions. Let’s now implement the second approach based on statistical tests for normality. Here, let’s use the Shapiro-Wilk test as an example, again looking at the BMI data. shapiro.test(full.data$BMI) ## ## Shapiro-Wilk normality test ## ## data: full.data$BMI ## W = 0.99232, p-value = 0.3773 This test resulted in a p-value of 0.3773, so we cannot reject the null hypothesis (that the BMI data are normally distributed). These findings support the assumption that these data are normally distributed. Next, we’ll assess homogeneity of variance using the Levene’s test. This will be done using the leveneTest()function from the car package: # First convert the smoker variable to a factor full.data$Smoker = factor(full.data$Smoker, levels = c(&quot;NS&quot;, &quot;S&quot;)) leveneTest(BMI ~ Smoker, data = full.data) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 0.2631 0.6086 ## 198 The p value, (Pr&gt;F), is 0.6086 indicating that variance in BMI across the smoking groups is the same. Therefore, the assumptions of a t-test, including normality and homogeneity of variance, have been met. Two-Group Visualizations and Statistical Comparisons using the T-Test T-tests are commonly used to test for a significant difference between the means of two groups in normally distributed data. In this example, we will be answering Environmental Health Question 1: Are there statistically significant differences in BMI between non-smokers and smokers? We will specifically implement a two sample t-test (or independent samples t-test). Let’s first visualize the BMI data across these two groups using boxplots: ggboxplot(data = full.data, x = &quot;Smoker&quot;, y = &quot;BMI&quot;) From this plot, it looks like non-smokers (labeled “NS”) may have significantly higher BMI than smokers (labeled “S”), though we need statistical evaluation of these data to more thoroughly evaluate this potential data trend. It is easy to perform a t-test on these data using the t.test() function from the base R stats package: t.test(data = full.data, BMI ~ Smoker) ## ## Welch Two Sample t-test ## ## data: BMI by Smoker ## t = 2.5372, df = 80.362, p-value = 0.01311 ## alternative hypothesis: true difference in means between group NS and group S is not equal to 0 ## 95 percent confidence interval: ## 0.583061 4.823447 ## sample estimates: ## mean in group NS mean in group S ## 26.11176 23.40851 Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question #1: Are there statistically significant differences in BMI between non-smokers and smokers? Answer: From this statistical output, we can see that the overall mean BMI in non-smokers (group “NS”) is 26.1, and the overall mean BMI in smokers (group “S”) is 23.4. We can also see that the resulting p-value comparison between the means of these two groups is, indeed, significant (p-value = 0.013), meaning that the means between these groups are significantly different (i.e., are not equal). It’s also helpful to save these results into a variable within the R global environment, which then allows us to access specific output values and extract them more easily for our records. For example, we can run the following to specifically extract the resulting p-value from this test: ttest.res &lt;- t.test(data = full.data, BMI ~ Smoker) # making a list in the R global environment with the statistical results signif(ttest.res$p.value, 2) # pulling the p-value and using the `signif` function to round to 2 significant figures ## [1] 0.013 Three-Group Visualizations and Statistical Comparisons using an ANOVA Analysis of Variance (ANOVA) is a statistical method that can be used to compare means across three or more groups in normally distributed data. To demonstrate an ANOVA test on this dataset, let’s answer Environmental Health Question 2: Are there statistically significant differences in BMI between current, former, and never smokers? To do this we’ll use the Smoker3 variable from our dataset. Let’s again start by viewing these data distributions using a boxplot: ggboxplot(data = full.data, x = &quot;Smoker3&quot;, y = &quot;BMI&quot;) From this cursory review of the data, it looks like the current smokers likely demonstrate significantly different BMI measures than the former and never smokers, though we need statistical tests to verify this potential trend. We also require statistical tests to evaluate potential differences (or lack of differences) between former and never smokers. Let’s now run the ANOVA to compare BMI between smoking groups, using the aov() function to fit an ANOVA model: smoker_anova = aov(data = full.data, BMI ~ Smoker3) smoker_anova ## Call: ## aov(formula = BMI ~ Smoker3, data = full.data) ## ## Terms: ## Smoker3 Residuals ## Sum of Squares 2046.713 6817.786 ## Deg. of Freedom 2 197 ## ## Residual standard error: 5.882861 ## Estimated effects may be unbalanced We need to extract the typical ANOVA results table using either the summary() or anova() function on the resulting fitted object: anova(smoker_anova) ## Analysis of Variance Table ## ## Response: BMI ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Smoker3 2 2046.7 1023.36 29.57 5.888e-12 *** ## Residuals 197 6817.8 34.61 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This table outputs a lot of information, including the F value referring to the resulting F-statistic, Pr(&gt;F) referring to the p-value of the F-statistic, and other values that are described in detail through other available resources including this helpful video through PennState’s statistics online resources. Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: Are there statistically significant differences in BMI between current, former, never smokers? Answer: From this ANOVA output table, we can conclude that the group means across all three groups are not equal given that the p value, written as Pr(&gt;F) is significant (p value = 5.88 x 10-12). However, it doesn’t tell us which groups differ from each other and that’s where post hoc tests like Tukey’s are useful. Let’s run a Tukey’s post hoc test using the TukeyHSD() function in base R to determine which of the current, former, and never smokers have significant differences in BMI: smoker_tukey = TukeyHSD(smoker_anova) smoker_tukey ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = BMI ~ Smoker3, data = full.data) ## ## $Smoker3 ## diff lwr upr p adj ## Former-Current 7.4436765 4.203006 10.684347 0.0000005 ## Never-Current 8.1167857 5.595470 10.638102 0.0000000 ## Never-Former 0.6731092 -2.011764 3.357982 0.8245448 Although the above Tukey object contains a column p adj, those are the raw unadjusted p values. It is common practice to adjust p values from multiple comparisons to prevent the reporting of false positives or reporting of a significant difference that doesn’t actually exist (Feise, 2002). There are a couple of different methods that are used to adjust p values including the Bonferroni and the Benjamini &amp; Hochberg approaches. For this example, we’ll use the p.adjust() function to obtain the Benjamini &amp; Hochberg adjusted p values. Check out the associated RDocumentation to discover other methods that can be used to adjust p values using the p.adjust() function: # First converting the Tukey object into a dataframe smoker_tukey_df = data.frame(smoker_tukey$Smoker3) %&gt;% # renaming the `p adj` to `P Value` for clarity rename(`P Value` = p.adj) # Adding a column with the adjusted p values smoker_tukey_df$`P Adj` = p.adjust(smoker_tukey_df$`P Value`, method = &quot;fdr&quot;) smoker_tukey_df ## diff lwr upr P Value P Adj ## Former-Current 7.4436765 4.203006 10.684347 5.064863e-07 7.597295e-07 ## Never-Current 8.1167857 5.595470 10.638102 3.432921e-12 1.029876e-11 ## Never-Former 0.6731092 -2.011764 3.357982 8.245448e-01 8.245448e-01 Answer to Environmental Health Question 2 We can use this additional information to further answer Environmental Health Question #2: Are there statistically significant differences in BMI between current, former, and never smokers? Answer: Current smokers have significantly lower BMIs than people who have never smoked and people who have formerly smoked. This is made evident by the 95% confidence intervals (lwr and upr) that don’t cross 0 and the p values that are less than 0.05 even after adjusting. Regression Modeling and Visualization: Linear and Logistic Regressions Regression modeling aims to find a relationship between a dependent variable (or outcome, response, y) and an independent variable (or predictor, explanatory variable, x). There are many forms of regression analysis, but here we will focus on two: linear regression and logistic regression. In brief, linear regression is generally used when you have a continuous dependent variable and there is assumed to be some sort of linear relationship between the dependent and independent variables. Conversely, logistic regression is often used when the dependent variable is dichotomous. Let’s first run through an example linear regression model to answer Environmental Health Question 3: Is there a relationship between maternal BMI and birth weight? Linear Regression We will first visualize the data and a run simple correlation analysis to evaluate whether these data are generally correlated. Then, we will run a linear regression to evaluate the relationship between these variables in more detail. Plotting the variables against one another and adding a linear regression line using the function ggscatter() from the ggubr package: ggscatter(full.data, x = &quot;BMI&quot;, y = &quot;BW&quot;, # Adding a linear line with 95% condfidence intervals as the shaded region add = &quot;reg.line&quot;, conf.int = TRUE, # Customize reg. line add.params = list(color = &quot;blue&quot;, fill = &quot;lightgray&quot;), # Adding Pearson&#39;s correlation coefficient cor.coef = TRUE, cor.method = &quot;pearson&quot;, cor.coeff.args = list(label.sep = &quot;\\n&quot;)) We can also run a basic correlation analysis between these two variables using the cor.test() function. This function uses the Pearson’s correlation test as default, which we can implement here due to the previously discussed assumption of normality for this dataset. Note that other tests are needed in instances when data are not normally distributed (e.g., Spearman Rank). This function is used here to extract the Pearson’s correlation coefficient and p-value (which also appear above in the upper left corner of the graph): cor.res &lt;- cor.test(full.data$BW, full.data$BMI) signif(cor.res$estimate, 2) ## cor ## 0.25 signif(cor.res$p.value, 2) ## [1] 0.00039 Together, it looks like there may be an association between BW and BMI, based on these correlation results, demonstrating a significant p-value of 0.0004. To test this further, let’s run a linear regression analysis using the lm() function, using BMI (X) as the independent variable and BW as the dependent variable (Y): crude_lm &lt;- lm(data = full.data, BW ~ BMI) summary(crude_lm) # viewing the results summary ## ## Call: ## lm(formula = BW ~ BMI, data = full.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -598.39 -116.72 8.11 136.54 490.11 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3069.201 52.576 58.38 &lt; 2e-16 *** ## BMI 7.208 1.997 3.61 0.000388 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 188 on 198 degrees of freedom ## Multiple R-squared: 0.06176, Adjusted R-squared: 0.05702 ## F-statistic: 13.03 on 1 and 198 DF, p-value: 0.0003876 Answer to Environmental Health Question 3 With this, we can answer Environmental Health Question #3: Is there a relationship between maternal BMI and birth weight? Answer: Not only is there a slight positive correlation between maternal BMI and BW as indicated by ~0.25 correlation coefficient, this linear relationship is significant due to the p-value being ~0.0004. Additionally, we can derive confidence intervals for the BMI estimate using: confint(crude_lm)[&quot;BMI&quot;,] ## 2.5 % 97.5 % ## 3.270873 11.145740 Notice that the r-squared (R2) value in regression output is the squared value of the previously calculated correlation coefficient (R). signif(sqrt(summary(crude_lm)$r.squared), 2) ## [1] 0.25 In epidemiological studies, the potential influence of confounders is considered by including important covariates within the final regression model. Let’s go ahead and investigate Environmental Health Question 4: Are maternal age and gestational age considered potential covariates in the relationship between maternal BMI and birth weight? We can do that by adding those variables to the linear model. adjusted_lm = lm(data = full.data, BW ~ BMI + MAge + GA) summary(adjusted_lm) ## ## Call: ## lm(formula = BW ~ BMI + MAge + GA, data = full.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -454.04 -111.24 5.79 116.46 488.41 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2247.995 117.946 19.060 &lt; 2e-16 *** ## BMI 6.237 1.774 3.515 0.000547 *** ## MAge 4.269 1.887 2.263 0.024752 * ## GA 19.612 2.656 7.385 4.28e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 165.5 on 196 degrees of freedom ## Multiple R-squared: 0.2805, Adjusted R-squared: 0.2695 ## F-statistic: 25.47 on 3 and 196 DF, p-value: 5.884e-14 Let’s further visualize these regression modeling results by adding a regression line to the original scatterplot. Before doing so, we’ll use the effect() function from the effects package to make estimated predictions of birth weight values for the crude and adjusted linear models. The crude model only has BMI as the dependent variable, while the adjusted model includes BMI, maternal age, and gestational age as dependent variables. This function creates a table that contains 5 columns: fitted values for BMI (BMI), predictor values (fit), standard errors of the predictions (se), lower confidence limits (lower), and upper confidence limits (upper). An additional column, Model, was added to specify whether the values correspond to the crude or adjusted model. For additional information on visualizing adjusted linear models, see Plotting Adjusted Associations in R. crude_lm_predtable = data.frame(effect(term = &quot;BMI&quot;, mod = crude_lm), Model = &quot;Crude&quot;) adjusted_lm_predtable = data.frame(effect(term = &quot;BMI&quot;, mod = adjusted_lm), Model = &quot;Adjusted&quot;) # Viewing one of the tables crude_lm_predtable ## BMI fit se lower upper Model ## 1 10 3141.284 33.63898 3074.948 3207.621 Crude ## 2 19 3206.159 18.54497 3169.588 3242.730 Crude ## 3 28 3271.034 14.21563 3243.000 3299.067 Crude ## 4 36 3328.700 24.86346 3279.669 3377.732 Crude ## 5 45 3393.575 41.18575 3312.356 3474.794 Crude Now we can plot each linear model and their corresponding 95% confidence intervals (CI). It’s easier to visualize this using ggplot2 instead of ggubr so that’s what we’ll use: options(repr.plot.width=9, repr.plot.height=6) # changing dimensions of the entire figure ggplot(full.data, aes(x = BMI, y = BW)) + geom_point() + # Crude line geom_line(data = crude_lm_predtable, mapping = aes(x = BMI, y = fit, color = Model)) + # Adjusted line geom_line(data = adjusted_lm_predtable, mapping = aes(x = BMI, y = fit, color = Model)) + # Crude 95% CI geom_ribbon(data = crude_lm_predtable, mapping = aes(x = BMI, y = fit, ymin = lower, ymax = upper, fill = Model), alpha = 0.25) + # Adjusted 95% CI geom_ribbon(data = adjusted_lm_predtable, mapping = aes(x = BMI, y = fit, ymin = lower, ymax = upper, fill = Model), alpha = 0.25) Answer to Environmental Health Question 4 With this, we can answer Environmental Health Question #4: Are maternal age and gestational age considered potential covariates in the relationship between maternal BMI and birth weight? Answer: BMI is still significantly associated with BW and the included covariates are also shown to be significantly related to birth weight in this model. However, the addition of gestational age and maternal age did not have much of an impact on modifying the relationship between BMI and birth weight. Logistic Regression To carry out a logistic regression, we need to evaluate one continuous variable (here, we select gestational age, using the GA variable) and one dichotomous variable (here, we select smoking status, using the Smoker variable) to evaluate Environmental Health Question 5: Are there statistically significant differences in gestational age based on whether a subject is a non-smoker or a smoker? Because smoking status is a dichotomous variable, we will use logistic regression to look at this relationship. Let’s first visualize these data using a stacked bar plot for the dichotomous smoker dataset: ggboxplot(data = full.data, x = &quot;Smoker&quot;, y = &quot;GA&quot;) With this visualization, it’s difficult to tell whether or not there are significant differences in maternal education based on smoking status. Let’s now run the statistical analysis, using logistic regression modeling: # Before running the model, &quot;Smoker&quot;, needs to be binarized to 0&#39;s or 1&#39;s for the glm function glm_data = full.data %&gt;% mutate(Smoker = ifelse(Smoker == &quot;NS&quot;, 0,1)) # Use GLM (generalized linear model) and specify the family as binomial # This tells GLM to run a logistic regression log.res = glm(Smoker ~ GA, family = &quot;binomial&quot;, data = glm_data) summary(log.res) # viewing the results ## ## Call: ## glm(formula = Smoker ~ GA, family = &quot;binomial&quot;, data = glm_data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.26669 1.37042 0.924 0.3553 ## GA -0.06764 0.03796 -1.782 0.0747 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 218.10 on 199 degrees of freedom ## Residual deviance: 214.89 on 198 degrees of freedom ## AIC: 218.89 ## ## Number of Fisher Scoring iterations: 4 Similar to the regression modeling analysis, we can also derive confidence intervals: confint(log.res)[&quot;GA&quot;,] ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## -0.14301103 0.00640128 Answer to Environmental Health Question 5 With this, we can answer Environmental Health Question #5: Are there statistically significant differences in maternal education level based on whether they are a non-smoker or a smoker? Answer: Collectively, these results show a non-significant p-value relating gestational age to smoking status. The confidence intervals also overlap across zero. Therefore, these data do not demonstrate a significant association between gestational age and smoking status. Statistical Evaluations of Categorical Data using the Chi-Squared Test and Fisher’s Exact Test Chi-squared test and Fisher’s exact tests are used primarily when evaluating data distributions between two categorical variables. The difference between a Chi-squared test and the Fisher’s exact test surrounds the specific procedure being run. The Chi-squared test is an approximation and is run with larger sample sizes to determine whether there is a statistically significant difference between the expected vs. observed frequencies in one or more categories of a contingency table. The Fisher’s exact test is similar, though is an exact measure that can be run on any sample size, including smaller sample sizes. The number of samples or subjects (n) considered to be sufficiently large enough is subjective, contingent upon the research question being asked, and the experimental design. However, smaller sample sizes can be more permissible if the sample is normally distributed, but generally speaking having n &gt; 30 is a common convention in statistics (Alexander, 2022). For this example, we are interested in evaluating the potential relationship between two categorical variables: smoking status (using the Smoker variable) and categorical BMI group (using the BMIcat variable) to address Environmental Health Question 6: Is there a relationship between smoking status and BMI? To run these categorical statistical tests, let’s first create and view a 2-way contingency table describing the frequencies of observations across the categorical BMI and smoking groups: ContingencyTable &lt;- with(full.data, table(BMIcat, Smoker)) ContingencyTable ## Smoker ## BMIcat NS S ## Normal 43 14 ## Overweight 87 22 ## Underweight 23 11 Now let’s run the Chi-squared test on this table: chisq.test(ContingencyTable) ## ## Pearson&#39;s Chi-squared test ## ## data: ContingencyTable ## X-squared = 2.1849, df = 2, p-value = 0.3354 Note that we can also run the Chi-squared test using the following code, without having to generate the contingency table: chisq.test(full.data$BMIcat, full.data$Smoker) ## ## Pearson&#39;s Chi-squared test ## ## data: full.data$BMIcat and full.data$Smoker ## X-squared = 2.1849, df = 2, p-value = 0.3354 Or: with(full.data, chisq.test(BMIcat, Smoker)) ## ## Pearson&#39;s Chi-squared test ## ## data: BMIcat and Smoker ## X-squared = 2.1849, df = 2, p-value = 0.3354 Answer to Environmental Health Question 6 Note that these all produce the same results. With this, we can answer Environmental Health Question #6: Is there a relationship between smoking status and BMI? Answer: This results in a p-value = 0.34, demonstrating that there is no significant relationship between BMI categories and smoking status. We can also run a Fisher’s Exact Test when considering sample sizes. We won’t run this here due to computing time, but here is some example code for your records: #With small sample sizes, can use Fisher&#39;s Exact Test #fisher.test(full.data$BMI, full.data$Smoker) Concluding Remarks In conclusion, this training module serves as a high-level introduction to basic statistics and visualization methods. Statistical approaches described in this training module include tests for normality, t-test, analysis of variance, regression modeling, chi-squared test, and Fisher’s exact test. Visualization approaches include boxplots, histograms, scatterplots, and regression lines. These methods serve as an important foundation for nearly all studies carried out in environmental health research. Test Your Knowledge If we’re interested in investigating if there are significant differences in birth weight based on maternal education level, which statistical test should you use? Is that relationship considered to be statistically significant and how can we visualize the distributions of these groups? "],["overview-of-experimental-design-and-example-data.html", "Overview of Experimental Design and Example Data Introduction to Training Module Replicates Orientation to Example Data for Chapter 4 Concluding Remarks", " Overview of Experimental Design and Example Data This training module was developed by Elise Hickman, Sarah Miller, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Converting wet lab experimentation data into dry lab analyses facilitates reproducibility and transparency in data analysis. This is helpful for consistency across members of the same research group, review of analyses by collaborators or reviewers, and implementation of similar future analyses. In comparison with analysis workflows that use subscription- or license-based applications, such as Prism or SAS, analysis workflows that leverage open-source programming languages such as R also increase accessibility of analyses. Additionally, scripted analyses minimize the risk for copy-paste error, which can occur when cleaning experimental data, transferring it to an analysis application, and exporting and formatting analysis results. Some of the barriers in converting wet lab experimentation into dry lab analyses include data cleaning, selection and implementation of appropriate statistical tests, and reporting results. This chapter will provide introductory material guiding wet-bench scientists in R analyses, bridging the gap between commonly available R tutorials (which, while helpful, may not provide sufficient level of detail or relevant examples) and intensive data science workflows (which may be too detailed). In this module, we will provide an overview of key experimental design features and terms that will be used throughout this chapter, and we will provide a detailed overview of the example data. In the subsequent modules, we will dive into analyzing the example data. Replicates One of the most important components of selecting an appropriate analysis is first understanding how data should be compared between samples, which often means addressing experimental replicates. There are two main types of replicates that are used in environmental health research: biological replicates and technical replicates. Biological Replicates Biological replicates are the preferred unit of statistical comparison because they represent biologically distinct samples, demonstrating biological variation in the system. What is considered to be a biological replicate can depend on what model system is being used. For example, in studies with human clinical samples or cells from different human donors, the different humans are considered the biological replicates. In studies using animals as model organisms, individual animals are typically considered biological replicates, although this can vary depending on the experimental design. In studies that use cell lines, which are derived from one human or animal and are modified to continuously grow in culture, a biological replicate could be either cells from different passages (different thawed aliquots) grown in completely separate flasks, all experimented with on the same day, or repeating an experiment on the same set of cells (one thawed aliquot) but on separate experimental days, so the cells have grown/replicated between experiments. The final “N” that you report should reflect your biological replicates, or independent experiments. What constitutes an independent experiment or biological replicate is highly field-, lab-, organism-, and endpoint-dependent, so make sure to discuss this within your research group in the experiment planning phase and again before your analysis begins. No matter what you choose, ensure that when you report your results, you are transparent about what your biological replicates are. For example, the below diagram (adapted from BitesizeBio) illustrates different ways of defining replicates in experiments with cell lines: Figure 6: Created with BioRender.com N = 3 cells could be considered technical replicates if the endpoint of interest is very low throughput, such as single cell imaging or analyses. N = 3 cell culture wells is a more common approach to technical replicates and is typically used when one sample is collected from each well, such as in the case of media or cell lysate collection. Note that each well within the Week 1 biological replicate would be considered a technical replicate for Week 1’s experiment. Similarly, each well within the Week 2 biological replicate would be considered a technical replicate for Week 2’s experiment. For more on technical replicates, see the next section. Although N = 3 cell lines is a less common approach to biological replicates, some argue for this approach because each cell line is typically derived from one biological source. In this scenario, each of the cell lines would be unique but would represent the same cell type or lineage (e.g., for respiratory epithelium, A549, 16HBE, and BEAS-2B cell lines). Also note that to perform statistical analyses, an N of at least 3 biological replicates is needed, and an even higher N may be needed for a sufficiently powered study. Although power calculations are outside the scope of this module, we encourage you to use power calculation resources, such as G*Power to assist in selecting an appropriate N for your study. Technical Replicates Technical replicates are repeated measurements on the same sample or biological source, demonstrating the variation underlying protocols, equipment, and sample handling. In environmental health research, there can be technical replicates separately related to either the experimental design or the downstream analyses. Technical replicates related to experimental design refer to the chemical exposure for cell-based (in vitro) experiments, where there may be multiple wells of cells from the same passage or human/mouse exposed to the same treatment. Technical replicates related to downstream analyses refer to the endpoints that are measured after chemical exposure in each sample. To illustrate this, consider an experiment where cells from four unique human donors (D1-D4) are grown in cell culture plates, and then three wells of cells from each donor are exposed to a chemical treatment (Tx) or a vehicle control (Ctrl). The plate layout might look something like this, with technical replicates related to experimental design, i.e. chemical exposure, in the same color: Figure 7: Created with BioRender.com For this experiment, we have four biological replicates (the four donors) and three technical exposure replicates per dose (because three wells from each donor were exposed to each condition). The technical replicates here capture potential unintended variation between wells in cell growth and chemical exposure. Following the exposure of the cells to a chemical of interest, the media is collected from each well and assayed using a plate reader assay for concentrations of a marker of inflammation. For each sample collected (from each well), there are three technical replicates used to measure the concentration of the inflammatory marker. The purpose of these technical replicates is to capture potential unintended well-to-well variation in the plate reader assay. The plate layout might look something like this, with the letter and number in each well of the plate layout representing the well in the exposure plate layout that the media sample being assayed came from: Figure 8: Created with BioRender.com Technical replicates should typically be averaged before performing any statistical analysis. For the experiment described above, we would: Average the technical replicates for the plate reader assay to obtain one value per original cell culture well for inflammatory marker concentration. Then, average the technical replicates for the chemical exposure to obtain one value per biological replicate (donor). This would result in a dataset with eight values (four control and four treatment) for statistical analysis. Number and inclusion of technical replicates The above example is just one approach to experimental design. As mentioned above in the biological replicates section, selection of appropriate biological and technical replicates can vary greatly depending on your model organism, experimental design, assay, and standards in the field. For example, there may be cases where well-to-well variation for certain assays is minimal compared with variation between biological replicates, or when including technical replicates for each donor is experimentally or financially unfeasible, resulting in a lack of technical replicates. Matched Experimental Design Matching (also known as paired or repeated measures) in an experimental design is also a very important concept when selecting the appropriate statistical analysis. In experiments with matched design, multiple measurements are collected from the same biological replicate. This typically provides increased statistical power because changes are observed within each biological replicate relative to its starting point. In environmental health research, this can include study designs such as: Samples were collected from the same individuals, animals, or cell culture wells pre- and post-exposure. Cells from the same biological replicate were exposed to different doses of a chemical. The experimental design described above represents a matched design because cells from the same donor are exposed to both the treatment and the vehicle control. Orientation to Example Data for Chapter 4 In this chapter, we will be using an example dataset derived from an in vitro, or cell culture, experiment. Before diving into analysis of these data in the subsequent modules, we will provide an overview of where these data came from and preview what the input data frames look like. Experimental Design In this experiment, primary human bronchial epithelial cells (HBECs) from sixteen different donors were exposed to the gas acrolein, which is emitted from the combustion of fossil fuels, tobacco, wood, and plastic. Inhalation exposure to acrolein is associated with airway inhalation, and this study aimed to understand how exposure to acrolein changes secretion of markers of inflammation. Prior to experimentation, the HBECs were grown on a permeable membrane support for 24 days with air on one side and liquid media on the other side, allowing them to differentiate into a form that is very similar to what is found in the human body. The cells were then exposed for 2 hours to 0 (filtered air), 0.6, 1, 2, or 4 ppm acrolein, with two technical replicate wells from each donor per dose. Twenty-four hours later, the media was collected, and concentrations of inflammatory markers were measured using an enzyme-linked immunosorbent assay (ELISA). Figure 9: Created with BioRender.com Note that this is a matched experimental design because cells from every donor were exposed to every concentration of acrolein, rather than cells from different donors being exposed to each of the different doses. Starting Data Next, let’s familiarize ourselves with the data that resulted from this experiment. There are two input data files, one that contains cytokine concentration data and one that contains demographic information about the donors: The cytokine data contains information about the cytokine measurements for each of the six proteins measured in the basolateral media for each sample (units = pg/mL), which can be identified by the donor, dose, and replicate columns. The demographic data contains information about the age and sex of each donor. In the subsequent modules, we’ll be using these data to assess whether exposure to acrolein significantly changes secretion of inflammatory markers and whether donor characteristics, such as sex and age, modify these responses. Concluding Remarks This module reviewed important components of experimental design, such as replicates and matching, which are critical for data pre-processing and selecting appropriate statistical tests. Test Your Knowledge Read the following experimental design descriptions. For each description, determine the number of biological replicates (per group), the number of technical replicates, and whether the experimental design is matched. One hundred participants are recruited to a study aiming to determine whether people who use e-cigarettes have different concentrations of inflammatory markers in their airways. Fifty participants are non e-cigarette users and 50 participants are e-cigarette users. After the airway samples are collected, each sample is analyzed with an ELISA, with three measurements taken per sample. Twenty mice are used in a study aiming to understand the effects of particulate matter on cardiovascular health. The mice are randomized such that half of the mice are exposed to filtered air and half are exposed to particulate matter. During the exposures, the mice are continuously monitored for endpoints such as heart rate and heart function. One month later, the mice that were exposed to particulate matter are exposed to filtered air, and the mice that were exposed to filtered air are exposed to particulate matter, with the same cardiovascular endpoints collected. "],["data-import-processing-and-summary-statistics.html", "Data Import, Processing, and Summary Statistics Introduction to Training Module Data Import Handling Missing Values Averaging Replicates Descriptive Statistics Normality Assessment and Data Transformation Concluding Remarks", " Data Import, Processing, and Summary Statistics This training module was developed by Elise Hickman, Alexis Payton, Sarah Miller, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module The first steps in any scripted analysis of wet-bench data include importing the data, cleaning the data to prepare for analyses, and conducting preliminary data exploration steps, such as addressing missing values, calculating summary statistics, and assessing normality. Although less exciting than diving right into the statistical analysis, these steps are crucial in guiding downstream analyses and ensuring accurate results. In this module, we will discuss each of these steps and work through them using an example dataset (introduced in TAME 2.0 Module 4.1 Overview of Experimental Design and Example Data of inflammatory markers secreted by airway epithelial cells after exposure to different concentrations of acrolein. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: What is the mean concentration of each inflammatory biomarker by acrolein concentration? Are our data normally distributed? Data Import First, we need to import our data. Data can be imported into R from many different file formats, including .csv (as demonstrated in previous chapters), .txt, .xlsx, and .pdf. Often, data are formatted in Excel prior to import, and the openxlsx package provides helpful functions that allow the user to import data from Excel, create workbooks for storing results generated in R, and export data from R to Excel workbooks. Below, we will use the read.xlsx() function to import our data directly from Excel. Other useful packages include pdftools (PDF import), tm (text mining of PDFs), and plater (plate reader formatted data import). Workspace Preparation and Data Import Set working directory In preparation, first let’s set our working directory to the folder path that contains our input files: setwd(&quot;/filepath to where your input files are&quot;) Load required packages And load required packages: library(openxlsx) # for importing Excel files library(DT) # for easier viewing of data tables library(tidyverse) # for data cleaning and graphing library(imputeLCMD) # for data imputation with QRILC library(table1) # for summary table library(vtable) # for summary table library(ggpubr) # for making Q-Q plots with ggplot Import example datasets Next, let’s read in our example datasets: biomarker_data &lt;- read.xlsx(&quot;Module4_2_Input/Module4_2_InputData1.xlsx&quot;) demographic_data &lt;- read.xlsx(&quot;Module4_2_Input/Module4_2_InputData2.xlsx&quot;) View example datasets First, let’s preview our example data. Using the datatable() function from the DT package allows us to interactively scroll through our biomarker data. datatable(biomarker_data) We can see that our biomarker data are arranged with samples in rows and sample information and biomarker measurements in the columns. datatable(demographic_data) Our demographic data provide information about the donors that our cells came from, matching to the Donor column in our biomarker data. Handling Missing Values Next, we will investigate whether we have missing values and which variables and donors have missing values. # Calculate the total number of NAs per variable biomarker_data %&gt;% summarise(across(IL1B:VEGF, ~sum(is.na(.)))) ## IL1B IL6 IL8 IL10 TNFa VEGF ## 1 0 0 6 5 0 0 # Calculate the number of missing values per subject biomarker_data %&gt;% group_by(Donor) %&gt;% summarise(across(IL1B:VEGF, ~sum(is.na(.)))) ## # A tibble: 16 × 7 ## Donor IL1B IL6 IL8 IL10 TNFa VEGF ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 D1 0 0 0 0 0 0 ## 2 D10 0 0 0 2 0 0 ## 3 D11 0 0 0 0 0 0 ## 4 D12 0 0 2 0 0 0 ## 5 D13 0 0 0 3 0 0 ## 6 D14 0 0 0 0 0 0 ## 7 D15 0 0 0 0 0 0 ## 8 D16 0 0 0 0 0 0 ## 9 D2 0 0 2 0 0 0 ## 10 D3 0 0 0 0 0 0 ## 11 D4 0 0 0 0 0 0 ## 12 D5 0 0 0 0 0 0 ## 13 D6 0 0 0 0 0 0 ## 14 D7 0 0 0 0 0 0 ## 15 D8 0 0 0 0 0 0 ## 16 D9 0 0 2 0 0 0 Here, we can see that we do have a few missing values. What should we do with these values? Missing Values and Data Imputation Missing values Before deciding what to do about our missing values, it’s important to understand why they are missing. There are a few different types of missing values that could be present in a dataset: Missing completely at random (MCAR): has nothing to do with the experimental unit being studied (e.g., a sample is damaged or lost in the lab) Missing at random (MAR): there may be a systematic difference between missing and measured values, but they can be explained by observed differences in the data or experimental unit Missing not at random (MNAR): data are missing due to factors that are not observed/measured (e.g., measurement for a specific endpoint is below the limit of detection (LOD) of an assay) We know from the researchers who generated this dataset that the values are missing because these specific proteins were below the limit of detection for the assay for certain samples; therefore, our data are missing not at random. This can help us with our choice of imputation method, described below. Imputation Imputation is the assignment of a value to a missing data point by inferring that value from other properties of the dataset or externally defined limits. Whether or not you should impute your data is not a one-size-fits-all approach and may vary depending on your field, experimental design, the type of data, and the type of missing values in your dataset. Two questions you can ask yourself when deciding whether or not to impute data are: Is imputation needed for downstream analyses? Some analyses are not permissive to including NAs or 0s; others are. Will imputing values bias my analyses unnecessarily? If so, consider analyzing subsets of the data that are complete separately. There are many different imputation methods (too many to cover them all in this module); here, we will introduce a few that we use most often. We encourage you to explore these in more depth and to understand typical imputation workflows for your lab, data type, and/or discipline. For variables where imputed values are expected to be generally bound by the existing range of data (e.g., MCAR): missForest For variables with samples below the limit of detection for the assay, such as for mass spectrometry or ELISAs (e.g., MNAR) Replace non-detects with the limit of detection divided by the square root of 2 Quantile Regression Imputation of Left-Censored Data (QRILC) GSimp (can also be used to impute values above a specific threshold) If you do impute missing values, make sure to include both your raw and imputed data, along with detailed information about the imputation method, within your manuscript, supplemental information, and/or GitHub. You can even present summary statistics for both raw and imputed data for additional transparency. Imputation of Our Data Before imputing our data, it is a good idea to implement a background filter that checks to see if a certain percentage of values for each variable are missing. For variables with a very high percentage of missing values, imputation can be unreliable because there is not enough information for the imputation algorithm to reference. The threshold for what this percentage should be can vary by study design and the extent to which your data are subset into groups that may have differing biomarker profiles; however, a common threshold we frequently use is to remove variables with missing data for 25% or more of samples. We can use the following code to calculate the percentage values missing for each endpoint: biomarker_data %&gt;% summarise(across(IL1B:VEGF, ~sum(is.na(.))/nrow(biomarker_data)*100)) ## IL1B IL6 IL8 IL10 TNFa VEGF ## 1 0 0 3.75 3.125 0 0 Here, we can see that only about 3-4% of values are missing for our variables with missing data, so we will proceed to imputation with our dataset as-is. We will impute values using QRILC, which pulls from the left side of the data distribution (the lower values) to impute missing values. We will write a function that will apply QRILC imputation to our dataframe. This function takes a data frame with missing values as input and returns a dataframe with QRILC imputed values in place of NAs as output. QRILC_imputation = function(df){ # Normalize data before applying QRILC per QRILC documentation ## Select only numeric columns, log2 transform, and convert to a matrix ### 4 comes from there being 3 metadata columns before the numeric data starts QRILC_prep = df[,4:dim(df)[2]] %&gt;% mutate_all(., function(x) log2(x + 1)) %&gt;% as.matrix() imputed_QRILC_object = impute.QRILC(QRILC_prep, tune.sigma = 0.1) QRILC_log2_df = data.frame(imputed_QRILC_object[1]) # Converting back the original scale QRILC_df = QRILC_log2_df %&gt;% mutate_all(., function(x) 2^x - 1) # Adding back in metadata columns QRILC_df = cbind(Donor = df$Donor, Dose = df$Dose, Replicate = df$Replicate, QRILC_df) return(QRILC_df) } Now we can apply the QRILC function to our dataframe. We use the function set.seed() to ensure that the QRILC function generates the same numbers each time we run the script. For more on setting seeds, see here. # Set random seed to ensure reproducibility in results set.seed(1104) # Apply function biomarker_data_imp &lt;- QRILC_imputation(biomarker_data) Averaging Replicates The last step we need to take before our data are ready for analysis is averaging the two technical replicates for each donor and dose. We will do this by creating an ID column that represents the donor and dose together and using that column to group and average the data. This results in a dataframe where our rows contain data representing each biological replicate exposed to each of the five concentrations of acrolein. biomarker_data_imp_avg &lt;- biomarker_data_imp %&gt;% # Create an ID column that represents the donor and dose unite(Donor_Dose, Donor, Dose, sep = &quot;_&quot;) %&gt;% # Average replicates with each unique Donor_Dose group_by(Donor_Dose) %&gt;% summarize(across(IL1B:VEGF, mean)) %&gt;% # Round results to the same number of significant figures as the original data mutate(across(IL1B:VEGF, \\(x) round(x, 2))) %&gt;% # Separate back out the Donor_Dose column separate(Donor_Dose, into = c(&quot;Donor&quot;, &quot;Dose&quot;), sep = &quot;_&quot;) # View new data frame datatable(biomarker_data_imp_avg) Descriptive Statistics Generating descriptive statistics (e.g., mean, median, mode, range, standard deviation) can be helpful for understanding the general distribution of your data and for reporting results either in the main body of a manuscript/report (for small datasets) or in the supplementary material (for larger datasets). There are a number of different approaches that can be used to calculate summary statistics, including functions that are part of base R and that are part of packages. Here, we will demonstrate a few different ways to efficiently calculate descriptive statistics across our dataset. Method #1 - Tidyverse and Basic Functions The mean, or average of data points, is one of the most commonly reported summary statistics and is often reported as mean ± standard deviation to demonstrate the spread in the data. Here, we will make a table of mean ± standard deviation for each of our biomarkers across each of the dose groups using tidyverse functions. # Calculate means biomarker_group_means &lt;- biomarker_data_imp_avg %&gt;% group_by(Dose) %&gt;% summarise(across(IL1B:VEGF, \\(x) mean(x))) # View data datatable(biomarker_group_means) You’ll notice that there are a lot of decimal places in our calculated means, while in our original data, there are only two decimal places. We can add a step to round the data to our above code chunk to produce cleaner results. # Calculate means biomarker_group_means &lt;- biomarker_data_imp_avg %&gt;% group_by(Dose) %&gt;% summarise(across(IL1B:VEGF, \\(x) mean(x))) %&gt;% mutate(across(IL1B:VEGF, \\(x) round(x, 2))) # View data datatable(biomarker_group_means) With this, we can answer Environmental Health Question 1: What is the mean concentration of each inflammatory biomarker by acrolein concentration? Answer: With the above table, we can see the mean concentrations for each of our inflammatory biomarkers by acrolein dose. IL-8 overall has the highest concentrations, followed by VEGF and IL-6. For IL1B, IL8, TNFa, and VEGF, it appears that the concentration of the biomarker goes up with increasing dose. We can use very similar code to calculate our standard deviations: # Calculate means biomarker_group_sds &lt;- biomarker_data_imp_avg %&gt;% group_by(Dose) %&gt;% summarise(across(IL1B:VEGF, \\(x) sd(x))) %&gt;% mutate(across(IL1B:VEGF, \\(x) round(x, 1))) # View data datatable(biomarker_group_sds) Now we’ve calculated both the means and standard deviations! However, these are typically presented as mean ± standard deviation. We can merge these dataframes by executing the following steps: Pivot each dataframe to a long format, with each row containing the value for one biomarker at one dose. Create a variable that represents each unique row (combination of Dose and variable). Join the data frames by row. Unite the two columns with mean and standard deviation, with ± in between them. Pivot the dataframe wider so that the dataframe resembles what we started with for the means and standard deviations. First, we’ll pivot each dataframe to a long format and create a variable that represents each unique row. # Pivot dataframes longer and create variable column for each row biomarker_group_means_long &lt;- pivot_longer(biomarker_group_means, !Dose, names_to = &quot;variable&quot;, values_to = &quot;mean&quot;) %&gt;% unite(Dose_variable, Dose, variable, remove = FALSE) biomarker_group_sds_long &lt;- pivot_longer(biomarker_group_means, !Dose, names_to = &quot;variable&quot;, values_to = &quot;sd&quot;) %&gt;% unite(Dose_variable, Dose, variable, remove = FALSE) # Preview what dataframe looks like datatable(biomarker_group_means_long) Next, we will join the mean and standard deviation datasets. Notice that we are only joining the Dose_variable and sd columns from the standard deviation data frame to prevent duplicate columns (Dose, variable) from being included. # Merge the dataframes by row biomarker_group_summstats &lt;- left_join(biomarker_group_means_long, biomarker_group_sds_long %&gt;% select(c(Dose_variable, sd)), by = &quot;Dose_variable&quot;) # Preview the new dataframe datatable(biomarker_group_summstats) Then, we can unite the mean and standard deviation columns and add the ± symbol between them by storing that character as a variable and pasting that variable in our paste() function. # Store plus/minus character plusminus &lt;-&quot;\\u00b1&quot; Encoding(plusminus)&lt;-&quot;UTF-8&quot; # Create new column with mean +/- standard deviation biomarker_group_summstats &lt;- biomarker_group_summstats %&gt;% mutate(mean_sd = paste(mean, plusminus, sd, sep = &quot; &quot;)) # Preview the new dataframe datatable(biomarker_group_summstats) Last, we can pivot the dataframe wider to revert it to its original layout, which is more digestible to read. # Pivot dataframe wider biomarker_group_summstats &lt;- biomarker_group_summstats %&gt;% # Remove columns we don&#39;t need any more select(-c(Dose_variable, mean, sd)) %&gt;% # Pivot wider pivot_wider(id_cols = Dose, names_from = &quot;variable&quot;, values_from = &quot;mean_sd&quot;) # View final dataframe datatable(biomarker_group_summstats) These data are now in a publication-ready format that can be exported to a .txt, .csv., or .xlsx file for easy sharing. Method #2 - Applying a List of Functions Calculating our mean and standard deviation separately using tidyverse wasn’t too difficult, but what if we want to calculate other descriptive statistics, such as minimum, median, and maximum? We could use the above approach, but we would need to make a separate dataframe for each and then merge them all together. Instead, we can use the map_dfr() function from the purrr package, which is also part of tidyverse. This function takes a list of functions you want to apply to your data and applies these functions over specified columns in the data. Let’s see how it works: # Define summary functions summary_functs &lt;- lst(min, median, mean, max, sd) # Apply functions to data, grouping by dose # .id = &quot;statistic&quot; tells the function to create a column describing which statistic that row is reporting biomarker_descriptive_stats_all &lt;- map_dfr(summary_functs, ~ summarize(biomarker_data_imp_avg %&gt;% group_by(Dose), across(IL1B:VEGF, .x)), .id = &quot;statistic&quot;) # View data datatable(biomarker_descriptive_stats_all) Depending on your final goal, descriptive statistics data can then be extracted from this dataframe and cleaned up or reformatted as needed to create a publication-ready table! Other Methods There are also packages that have been developed for specifically making summary tables, such as table1 and vtable. These packages can create summary tables in HTML format, which appear nicely in R Markdown and can be copied and pasted into Word. Here, we will briefly demonstrate how these packages work, and we encourage you to explore more using the package vignettes! Table1 The table1 package makes summary tables using the function table1(), which takes the columns that you want in the rows of the table on the left side of the first argument, followed by | and then the grouping variable. The output table can be customized in a number of ways, including what summary statistics are output and whether or not statistical comparisons are run between groups (see package vignette for more details). # Get names of all of the columns to include in the table paste(names(biomarker_data_imp_avg %&gt;% select(IL1B:VEGF)), collapse=&quot; + &quot;) ## [1] &quot;IL1B + IL6 + IL8 + IL10 + TNFa + VEGF&quot; # Make the table table1(~ IL1B + IL6 + IL8 + IL10 + TNFa + VEGF | Dose, data = biomarker_data_imp_avg) Vtable The vtable package includes the function st(), which can also be used to make HTML tables (and other output formats; see out argument). For example: # HTML output st(biomarker_data_imp_avg, group = &#39;Dose&#39;) Table 1: Summary Statistics Dose 0 0.6 1 2 4 Variable N Mean SD N Mean SD N Mean SD N Mean SD N Mean SD IL1B 16 9.8 1.9 16 10 1.9 16 11 1.5 16 11 1.6 16 12 2 IL6 16 597 658 16 670 810 16 565 652 16 536 474 16 288 297 IL8 16 21987 7570 16 24703 10768 16 22607 10648 16 41687 17939 16 94439 33535 IL10 16 1.4 1.3 16 0.44 0.56 16 1.2 1.2 16 0.88 0.43 16 1.2 0.78 TNFa 16 3.2 1.8 16 3.1 1.7 16 4.1 2.4 16 5.2 2 16 6.7 2.7 VEGF 16 971 417 16 1023 486 16 1196 621 16 1754 749 16 1617 836 # Dataframe output st(biomarker_data_imp_avg, group = &#39;Dose&#39;, out = &#39;return&#39;) ## Variable N Mean SD N Mean SD N Mean SD N Mean SD N Mean ## 1 Dose 0 0.6 1 2 4 ## 2 IL1B 16 9.8 1.9 16 10 1.9 16 11 1.5 16 11 1.6 16 12 ## 3 IL6 16 597 658 16 670 810 16 565 652 16 536 474 16 288 ## 4 IL8 16 21987 7570 16 24703 10768 16 22607 10648 16 41687 17939 16 94439 ## 5 IL10 16 1.4 1.3 16 0.44 0.56 16 1.2 1.2 16 0.88 0.43 16 1.2 ## 6 TNFa 16 3.2 1.8 16 3.1 1.7 16 4.1 2.4 16 5.2 2 16 6.7 ## 7 VEGF 16 971 417 16 1023 486 16 1196 621 16 1754 749 16 1617 ## SD ## 1 ## 2 2 ## 3 297 ## 4 33535 ## 5 0.78 ## 6 2.7 ## 7 836 Similar to table1, see the package vignette for detailed information about how to customize tables using this package. Normality Assessment and Data Transformation The last step we will take before beginning to test our data for statistical differences between groups (in the next module) is to understand our data’s distribution through normality assessment. This will inform which statistical tests we will perform on our data. For more detail on normality testing, including detailed explanations of each type of normality assessment and explanations of the code underlying the following graphs and tables, see TAME Module 3.3, Normality Tests and Data Transformations. We’ll start by looking at histograms of our data for qualitative normality assessment: # Set theme theme_set(theme_bw()) # Pivot data longer to prepare for plotting biomarker_data_imp_avg_long &lt;- biomarker_data_imp_avg %&gt;% pivot_longer(-c(Donor, Dose), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) # Make figure panel of histograms ggplot(biomarker_data_imp_avg_long, aes(value)) + geom_histogram(fill = &quot;gray40&quot;, color = &quot;black&quot;, binwidth = function(x) {(max(x) - min(x))/25}) + facet_wrap(~ variable, scales = &quot;free&quot;, nrow = 2) + labs(y = &quot;# of Observations&quot;, x = &quot;Value&quot;) From these histograms, we can see that IL-1B appears to be normally distributed, while the other endpoints do not appear to be normally distributed. We can also use Q-Q plots to assess normality qualitatively: ggqqplot(biomarker_data_imp_avg_long, x = &quot;value&quot;, facet.by = &quot;variable&quot;, ggtheme = theme_bw(), scales = &quot;free&quot;) With this figure panel, we can see that most of the variables have very noticeable deviations from the reference, suggesting non-normal distributions. To assess normality quantitatively, we can use the Shapiro-Wilk test. Note that the null hypothesis is that the sample distribution is normal, and a significant p-value means the distribution is non-normal. # Apply Shapiro Wilk test to dataframe shapiro_res &lt;- apply(biomarker_data_imp_avg %&gt;% select(IL1B:VEGF), 2, shapiro.test) # Create results data frame shapiro_res &lt;- do.call(rbind.data.frame, shapiro_res) # Clean data frame shapiro_res &lt;- shapiro_res %&gt;% ## Add normality conclusion mutate(normal = ifelse(p.value &lt; 0.05, F, T)) %&gt;% ## Remove columns that do not contain informative data select(c(p.value, normal)) # View cleaned up data frame datatable(shapiro_res) With this, we can answer Environmental Health Question 2: Are our data normally distributed? Answer: The results from the Shapiro-Wilk test demonstrate that the IL1B data are normally distributed, while the other variables are non-normally distributed. These results support the conclusions we made based on our qualitative assessment above with histograms and Q-Q plots. Log2 Transforming and Re-Assessing Normality Log2 transformation is a common transformation used in environmental health research and can move data closer to a normal distribution. For more on data transformation, see TAME Module 3.3, Normality Tests and Data Transformations. We will pseudo-log2 transform our data, which adds a 1 to each value before log2 transformation and ensures that resulting values are positive real numbers. Let’s see if the log2 data are more normally distributed than the raw data. # Apply log2 transformation to data biomarker_data_imp_avg_log2 &lt;- biomarker_data_imp_avg %&gt;% mutate(across(IL1B:VEGF, ~ log2(.x + 1))) Make histogram panel: # Pivot data longer and make figure panel of histograms biomarker_data_imp_avg_log2_long &lt;- biomarker_data_imp_avg_log2 %&gt;% pivot_longer(-c(Donor, Dose), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) # Make histogram panel ggplot(biomarker_data_imp_avg_log2_long, aes(value)) + geom_histogram(fill = &quot;gray40&quot;, color = &quot;black&quot;, binwidth = function(x) {(max(x) - min(x))/25}) + facet_wrap(~ variable, scales = &quot;free&quot;) + labs(y = &quot;# of Observations&quot;, x = &quot;Value&quot;) Make Q-Q plot panel: ggqqplot(biomarker_data_imp_avg_log2_long, x = &quot;value&quot;, facet.by = &quot;variable&quot;, ggtheme = theme_bw(), scales = &quot;free&quot;) Run Shapiro-Wilk test: # Apply Shapiro Wilk test shapiro_res_log2 &lt;- apply(biomarker_data_imp_avg_log2 %&gt;% select(IL1B:VEGF), 2, shapiro.test) # Create results data frame shapiro_res_log2 &lt;- do.call(rbind.data.frame, shapiro_res_log2) # Clean data frame shapiro_res_log2 &lt;- shapiro_res_log2 %&gt;% ## Add normality conclusion mutate(normal = ifelse(p.value &lt; 0.05, F, T)) %&gt;% ## Remove columns that do not contain informative data select(c(p.value, normal)) # View cleaned up data frame shapiro_res_log2 ## p.value normal ## IL1B 0.250821677 TRUE ## IL6 0.001445717 FALSE ## IL8 0.017386740 FALSE ## IL10 0.018935719 FALSE ## TNFa 0.194774045 TRUE ## VEGF 0.047231367 FALSE The histograms and Q-Q plots demonstrate that the log2 data are more normally distributed than the raw data. The results from the Shapiro-Wilk test also demonstrate that the the log2 data are more normally distributed as a whole than the raw data. Overall, the p-values, even for the variables that are still non-normally distributed, are much higher. So, should we proceed with the raw data or the log2 data? This depends on what analyses we plan to do. In general, it is best to keep the data in as close to its raw format as possible, so if all of our analyses are available with a non-parametric test, we could use our raw data. However, some statistical tests do not have a non-parametric equivalent, in which case it would likely be best to use the log2-transformed data. For subsequent modules, we will proceed with the log2 data for consistency; however, choices regarding normality assessment can vary, so be sure to discuss these choices within your research group before proceeding with your analysis. For more on decisions regarding normality, see TAME Module 3.3 Normality Tests and Data Transformations. For more on parametric vs. non-parametric tests, see TAME Module 4.4 Two Group Comparisons and Visualizations and TAME Module 4.5 Multi-Group Comparisons and Visualizations. Concluding Remarks Taken together, this module demonstrates important data processing steps necessary before proceeding with between-group statistical testing, including data import, handling missing values, averaging replicates, generating descriptive statistics tables, and assessing normality. Careful consideration and description of these steps in the methods section of a manuscript or report increases reproducibility of analyses and helps to improve the accuracy and statistical validity of subsequent statistical results. Test Your Knowledge Functional endpoints from these cultures were also measured. These endpoints were: 1) Membrane Permeability (MemPerm), 2) Trans-Epithelial Electrical Resistance (TEER), 3) Ciliary Beat Frequency (CBF), and 4) Expression of Mucin (MUC5AC). Work through the same processes demonstrated in this module using the provided data (“Module4_2_TYKInput.xlsx”) to answer the following questions: How many technical replicates are there for each dose? Are there any missing values? What are the average values for each endpoint by dose? Are the raw data normally distributed? "],["data-import-from-pdf-sources.html", "Data Import from PDF Sources Introduction to Training Module Importing Data from Many Single PDFs with the Same Formatting Importing Data Stored in PDF Tables Concluding Remarks", " Data Import from PDF Sources This training module was developed by Elise Hickman, Alexis Payton, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Most tutorials for R rely on importing .csv, .xlsx, or .txt files, but there are numerous other file formats that can store data, and these file formats can be more difficult to import into R. PDFs can be particularly difficult to interface with in R because they are not formatted with defined rows/columns/cells as is done in Excel or .csv/.txt formatting. In this module, we will demonstrate how to import data from from PDFs into R and format it such that it is amenable for downstream analyses or export as a table. Familiarity with tidyverse, for loops, and functions will make this module much more approachable, so be sure to review TAME 2.0 Modules 2.3 Data Manipulation and Reshaping and 2.4 Improving Coding Efficiencies if you need a refresher. Overview of Example Data To demonstrate import of data from PDFs, we will be leveraging two example datasets, described in more detail in their respective sections later on in the module. PDFs generated by Nanoparticle Tracking Analysis (NTA), a technique used to quantify the size and distribution of particles (such as extracellular vesicles) in a sample. We will be extracting data from an experiment in which epithelial cells were exposed to four different environmental chemicals or a vehicle control, and secreted particles were isolated and characterized using NTA. A PDF containing information about variables collected as part of a study whose samples are part of NIH’s BioLINCC Repository. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Which chemical(s) increase and decrease the concentration of particles secreted by epithelial cells? How many variables total are available to us to request from the study whose data are store in the repository, and what are these variables? Importing Data from Many Single PDFs with the Same Formatting Getting Familiar with the Example Dataset The following example is based on extracting data from PDFs generated by Nanoparticle Tracking Analysis (NTA), a technique used to quantify the size and distribution of particles in a sample. Each PDF file is associated with one sample, and each PDF contains multiple values that we want to extract. Although this is a very specific type of data, keep in mind that this general approach can be applied to any data stored in PDF format - you will just need to make modifications based on the layout of your PDF file! For this example, we will be extracting data from 5 PDFs that are identically formatted but contain information unique to each sample. The samples represent particles isolated from epithelial cell media following an experiment where cells were exposed to four different environmental chemicals (labeled “A”, “B”, “C”, and “D”) or a vehicle control (labeled “Ctrl”). Here is what a full view of one of the PDFs looks like, with values we want to extract highlighted in yellow: Our goal is to extract these values and end up with a dataframe that looks like this, with each sample in a row and each variable in a column: If your files are not already named in a way that reflects unique sample information, such as the date of the experiment or sample ID, update your file names to contain this information before proceeding with the script. Here are the names for the example PDF files: Workspace Preparation and Data Import Installing and loading required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you. We will be using the pdftools and tm packages to extract text from the PDF. And instead of using head() to preview dataframes, we will be using the function datatable() from the DT package. This function produces interactive tables and generates better formatting for viewing dataframes that have long character strings (like the ones we will be viewing in this section). if (!requireNamespace(&quot;pdftools&quot;)) install.packages(&quot;pdftools&quot;) if (!requireNamespace(&quot;tm&quot;)) install.packages(&quot;tm&quot;) if (!requireNamespace(&quot;DT&quot;)) install.packages(&quot;DT&quot;) if (!requireNamespace(&quot;janitor&quot;)) install.packages(&quot;janitor&quot;) Next, load the packages. library(tidyverse) library(pdftools) library(tm) library(DT) library(janitor) Initial data import from PDF files The following code stores the file names of all of the files in your directory that end in .pdf. To ensure that only PDFs of interest are imported, consider making a subfolder within your directory containing only the PDF extraction script file and the PDFs you want to extract data from. pdf_list &lt;- list.files(path = &quot;./Module4_3_Input&quot;, pattern = &quot;488.pdf$&quot;) We can see that each of our file names are now contained in the list. head(pdf_list) ## [1] &quot;20230214_0002_Expt1_A_size_488.pdf&quot; ## [2] &quot;20230214_0006_Expt1_Ctrl_size_488.pdf&quot; ## [3] &quot;20230214_0014_Expt1_C_size_488.pdf&quot; ## [4] &quot;20230214_0023_Expt1_D_size_488.pdf&quot; ## [5] &quot;20230214_0024_Expt1_B_size_488.pdf&quot; Next, we need to make a dataframe to store the extracted data. The PDF Identifier column will store the file name, and the Text column will store extracted text from the PDF. pdf_raw &lt;- data.frame(&quot;PDF Identifier&quot; = c(), &quot;Text&quot; = c()) The following code uses a for loop to loop through each file (as stored in the pdf_list vector) and extract the text from the PDF. Sometimes this code generates duplicates, so we will also remove the duplicates with distinct(). for (i in 1:length(pdf_list)){ # Iterating through each pdf file and separating each line of text document_text = pdf_text(paste(&quot;./Module4_3_Input/&quot;, pdf_list[i], sep = &quot;&quot;)) %&gt;% strsplit(&quot;\\n&quot;) # Saving the name of each PDF file and its text document = data.frame(&quot;PDF Identifier&quot; = gsub(x = pdf_list[i], pattern = &quot;.pdf&quot;, replacement = &quot;&quot;), &quot;Text&quot; = document_text, stringsAsFactors = FALSE) colnames(document) &lt;- c(&quot;PDF Identifier&quot;, &quot;Text&quot;) # Appending the new text data to the dataframe pdf_raw &lt;- rbind(pdf_raw, document) } pdf_raw &lt;- pdf_raw %&gt;% distinct() The new dataframe contains the data from all of the PDFs, with the PDF Identifier column containing the name of the input PDF file that corresponds to the text in the column next to it. datatable(pdf_raw) Extracting Variables of Interest Specific variables of interest can be extracted from the pdf_raw dataframe by filtering the dataframe for rows that contain a specific character string. This character string could be the variable of interest (if that word or set of words is unique and only occurs in that one place in the document) or a character string that occurs in the same line of the PDF as your variable of interest. Examples of both of these approaches are shown below. It is important to note that there can be different numbers of spaces in each row and after each semicolon, which will change the sep argument for each variable. For example, there are a different number of spaces after the semicolon for “Dilution Factor” than there are for “Concentration” (see above PDF screen shot for reference). We will work through an example for the first variable of interest, dilution factor, in detail. First, we can see what the dataframe looks like when we just filter rows based on keeping only rows that contain the string “Dilution Factor” in the text column using the grepl() function. dilution_factor_df &lt;- pdf_raw %&gt;% filter(grepl(&quot;Dilution Factor&quot;, Text)) datatable(dilution_factor_df) The value we are trying to extract is at the end of a long character string. We will want to use the tidyverse function separate() to isolate those values, but we need to know what part of the character string will separate the dilution factor values from the rest of the text. To determine this, we can call just one of the data cells and copy the semicolon and following spaces for use in the separate() function. # Return the value in the first row and second column. dilution_factor_df[1,2] ## [1] &quot; Temperature: 24.64 °C sensed Dilution Factor: 200&quot; Building on top of the previous code, we can now separate the dilution factor value from the rest of the text in the string. The separate() function takes an input data column and separates it into two or more columns based on the character passed to the separation argument. Here, everything before the separation string is discarded by setting the first new column to NA. Everything after the separation string will be stored in a new column called Dilution Factor, The starting Text column is removed by default. dilution_factor_df &lt;- pdf_raw %&gt;% filter(grepl(&quot;Dilution Factor&quot;, Text)) %&gt;% separate(Text, into = c(NA, &quot;Dilution Factor&quot;), sep = &quot;: &quot;) datatable(dilution_factor_df) For the “Original Concentration” variable, we filter rows by the string “pH” because the word concentration is found in multiple locations in the document. concentration_df = pdf_raw %&gt;% filter(grepl(&quot;pH&quot;, Text)) %&gt;% separate(Text, c(NA, &quot;Concentration&quot;), sep = &quot;: &quot;) datatable(concentration_df) With the dilution factor variable, there were no additional characters after the value of interest, but here, “Particles / mL” remains and needs to be removed so that the data can be used in downstream analyses. We can add an additional cleaning step to remove “Particles / mL” from the data and add the units to the column title. sep = \" P\" refers to the space before and first letter of the string to be removed. concentration_df = pdf_raw %&gt;% filter(grepl(&quot;pH&quot;, Text)) %&gt;% separate(Text, c(NA, &quot;Concentration&quot;), sep = &quot;: &quot;) %&gt;% separate(Concentration, c(&quot;Concentration (Particles/ mL)&quot;, NA), sep = &quot; P&quot;) datatable(concentration_df) Next, we want to extract size distribution data from the lower table. Note that the space in the first separate() function comes from the space between the “Number” and “Concentration” column in the string, and the space in the second separate() function comes from the space between the variable name and the number of interest. We can also convert values to numeric since they are currently stored as characters. size_distribution_df = pdf_raw %&gt;% filter(grepl(&quot;X10&quot;, Text)| grepl(&quot;X50 &quot;, Text)| grepl(&quot;X90&quot;, Text) | grepl(&quot;Mean&quot;, Text)| grepl(&quot;StdDev&quot;, Text)) %&gt;% separate(Text, c(&quot;Text&quot;, NA), sep = &quot; &quot;) %&gt;% separate(Text, c(&quot;Text&quot;, &quot;Size&quot;), sep = &quot; &quot;) %&gt;% mutate(Size = as.numeric(Size)) %&gt;% pivot_wider(names_from = Text, values_from = Size) datatable(size_distribution_df) Creating the final dataframe Now that we have created dataframes for all of the variables that we are interested in, we can join them together into one final dataframe. # Make list of all dataframes to include all_variables &lt;- list(dilution_factor_df, concentration_df, size_distribution_df) # Combine dataframes using reduce function. Sometimes, duplicate rows are generated by full_join. full_df = all_variables %&gt;% reduce(full_join, by = &quot;PDF Identifier&quot;) %&gt;% distinct() # View new dataframe datatable(full_df) For easier downstream analysis, the last step is to separate the PDF Identifier column into an informative sample ID that matches up with other experimental data. final_df &lt;- full_df %&gt;% separate(&#39;PDF Identifier&#39;, # Split sample identifier column into new columns, retaining the original column into = c(&quot;Date&quot;, &quot;FileNumber&quot;, &quot;Experiment Number&quot;, &quot;Sample_ID&quot;, &quot;Size&quot;, &quot;Wavelength&quot;), sep = &quot;_&quot;, remove = FALSE) %&gt;% select(-c(FileNumber, Size)) %&gt;% # Remove uninformative columns mutate(across(&#39;Dilution Factor&#39;:&#39;StdDev&#39;, as.numeric)) # Change variables to numeric where appropriate datatable(final_df) Let’s make a graph to help us answer Environmental Health Question 1. theme_set(theme_bw()) data_for_graphing &lt;- final_df %&gt;% clean_names() data_for_graphing$sample_id &lt;- factor(data_for_graphing$sample_id, levels = c(&quot;Ctrl&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;)) ggplot(data_for_graphing, aes(x = sample_id, y = concentration_particles_m_l)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;gray70&quot;, color = &quot;black&quot;) + ylab(&quot;Particle Concentration (Particles/mL)&quot;) + xlab(&quot;Exposure&quot;) With this, we can answer Environmental Health Question #1: Which chemical(s) increase and decrease the concentration of particles secreted by epithelial cells? Answer: Chemicals B and C appear to increase the concentration of secreted particles. However, additional replicates of this experiment are needed to assess statistical significance. Importing Data Stored in PDF Tables The above workflow is useful if you just want to extract a few specific values from PDFs, but isn’t as useful if data are already in a table format in a PDF. The tabulapdf package provides helpful functions for extracting dataframes from tables in PDF format. Getting Familiar with the Example Dataset The following example is based on extracting dataframes from a long PDF containing many individual data tables. This particular PDF came from the NIH’s BioLINCC Repository and details variables that researchers can request from the repository. Variables are part of larger datasets that contain many variables, with each dataset in a separate table. All of the tables are stored in one PDF file, and some of the tables are longer than one page (this will become relevant later on!). Similar to the first PDF workflow, remember that this is a specific example intended to demonstrate how to work through extracting data from PDFs. Modifications will need to be made for differently formatted PDFs. Here is what the first three pages of our 75-page starting PDF look like: If we zoom in a bit more on the first page, we can see that the dataset name is defined in bold above each table. This formatting is consistent throughout the PDF. The zoomed in view also allows us to see the columns and their contents more clearly. Some are more informative than others. The columns we are most interested in are listed below along with a description to guide you through the contents. Num: The number assigned to each variable in the dataset. This numbering restarts with 1 for each table. Variable: The variable name. Type: The type (or class) of the variable, either numeric or character. Label: A description of the variable and values associated with the variable. After extracting the data, we want to end up with a dataframe that contains all of the variables, their corresponding columns, and a column that indicates which dataset the variable is associated with: Workspace Preparation and Data Import Installing and loading required R packages Similar to previous sections, we need to install and load a few packages before proceeding. The tabulapdf package needs to be installed in a specific way as shown below and can sometimes be difficult to install on Macs. If errors are produced, follow the troubleshooting tips outlined in this Stack Overflow solution. # To install all of the packages except for tabulapdf if (!requireNamespace(&quot;stringr&quot;)) install.packages(&quot;stringr&quot;) if (!requireNamespace(&quot;pdftools&quot;)) install.packages(&quot;pdftools&quot;) if (!requireNamespace(&quot;rJava&quot;)) install.packages(&quot;rJava&quot;) # To install tabulapdf if (!require(&quot;remotes&quot;)) { install.packages(&quot;remotes&quot;) } library(remotes) remotes::install_github(c(&quot;ropensci/tabulizerjars&quot;, &quot;ropensci/tabulapdf&quot;), force=TRUE, INSTALL_opts = &quot;--no-multiarch&quot;) ## ## ── R CMD build ───────────────────────────────────────────────────────────────── ## * checking for file ‘/private/var/folders/kc/3hph74j116v2dqn0nbnkbgtr0000gp/T/RtmpXUIRoN/remotes1807f527819eb/ropensci-tabulizerjars-d1924e0/DESCRIPTION’ ... OK ## * preparing ‘tabulizerjars’: ## * checking DESCRIPTION meta-information ... OK ## * checking for LF line-endings in source and make files and shell scripts ## * checking for empty or unneeded directories ## * building ‘tabulizerjars_1.0.1.tar.gz’ ## ## ## ── R CMD build ───────────────────────────────────────────────────────────────── ## * checking for file ‘/private/var/folders/kc/3hph74j116v2dqn0nbnkbgtr0000gp/T/RtmpXUIRoN/remotes1807f569da79c/ropensci-tabulapdf-1ed0728/DESCRIPTION’ ... OK ## * preparing ‘tabulapdf’: ## * checking DESCRIPTION meta-information ... OK ## * checking for LF line-endings in source and make files and shell scripts ## * checking for empty or unneeded directories ## Removed empty directory ‘tabulapdf/docs’ ## * building ‘tabulapdf_1.0.5-3.tar.gz’ Load packages: library(tabulapdf) library(tidyverse) library(janitor) library(pdftools) library(stringr) Initial data import from PDF file The extract_tables() function automatically extracts tables from PDFs and stores them as tibbles (a specific tidyverse data structure similar to a dataframe) within a list. One table is extracted per page, even if the table spans multiple pages. This line of code can take a few seconds to run depending on the length of your PDF. tables &lt;- extract_tables(&quot;Module4_3_Input/Module4_3_InputData4.pdf&quot;, output = &quot;tibble&quot;) Glimpsing the first three elements in the tables list, we can see that each list element is a dataframe containing the columns from the PDF tables. glimpse(tables[1:3]) ## List of 3 ## $ : spc_tbl_ [30 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## ..$ Num : num [1:30] 1 NA NA NA 2 NA NA 3 NA NA ... ## ..$ Variable: chr [1:30] &quot;AAQ_1000&quot; NA NA NA ... ## ..$ Type : chr [1:30] &quot;Num&quot; NA NA NA ... ## ..$ Len : num [1:30] 8 NA NA NA 8 NA NA 8 NA NA ... ## ..$ Format : chr [1:30] &quot;2.&quot; NA NA NA ... ## ..$ Informat: chr [1:30] &quot;2.&quot; NA NA NA ... ## ..$ Label : chr [1:30] &quot;In the past 3 days, how much of the time did your asthma keep you from&quot; &quot;doing your usual activities at work, school, or at home? 0=None of the&quot; &quot;time, 1=A little of the time, 2=Some of the time, 3=Most of the time, 4=All&quot; &quot;of the time&quot; ... ## ..- attr(*, &quot;spec&quot;)= ## .. .. cols( ## .. .. Num = col_double(), ## .. .. Variable = col_character(), ## .. .. Type = col_character(), ## .. .. Len = col_double(), ## .. .. Format = col_character(), ## .. .. Informat = col_character(), ## .. .. Label = col_character() ## .. .. ) ## ..- attr(*, &quot;problems&quot;)=&lt;externalptr&gt; ## $ : spc_tbl_ [46 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## ..$ Num : num [1:46] 1 2 3 4 5 6 NA NA 7 NA ... ## ..$ Variable: chr [1:46] &quot;ABP_1000&quot; &quot;ABP_1010&quot; &quot;ABP_1020&quot; &quot;ABP_1030&quot; ... ## ..$ Type : chr [1:46] &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; ... ## ..$ Len : num [1:46] 8 8 8 8 8 8 NA NA 8 NA ... ## ..$ Format : num [1:46] 2 2 2 2 2 2 NA NA 2 NA ... ## ..$ Informat: num [1:46] 2 2 2 2 2 2 NA NA 2 NA ... ## ..$ Label : chr [1:46] &quot;Are you currently retired? 1=Yes,0=No&quot; &quot;Are you retired because of asthma? 1=Yes,0=No&quot; &quot;Are you currently unemployed? 1=Yes,0=No&quot; &quot;Are you unemployed because of asthma? 1=Yes,0=No&quot; ... ## ..- attr(*, &quot;spec&quot;)= ## .. .. cols( ## .. .. Num = col_double(), ## .. .. Variable = col_character(), ## .. .. Type = col_character(), ## .. .. Len = col_double(), ## .. .. Format = col_double(), ## .. .. Informat = col_double(), ## .. .. Label = col_character() ## .. .. ) ## ..- attr(*, &quot;problems&quot;)=&lt;externalptr&gt; ## $ : spc_tbl_ [21 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## ..$ Num : num [1:21] 20 NA NA 21 NA NA 22 NA NA 23 ... ## ..$ Variable: chr [1:21] &quot;ABP_1190&quot; NA NA &quot;ABP_1200&quot; ... ## ..$ Type : chr [1:21] &quot;Num&quot; NA NA &quot;Num&quot; ... ## ..$ Len : num [1:21] 8 NA NA 8 NA NA 8 NA NA 8 ... ## ..$ Format : chr [1:21] &quot;2.&quot; NA NA &quot;2.&quot; ... ## ..$ Informat: chr [1:21] &quot;2.&quot; NA NA &quot;2.&quot; ... ## ..$ Label : chr [1:21] &quot;How much bother is the worry that you will have an asthma attack when&quot; &quot;visiting a new place? 0=No bother at all, 1=Minor irritation, 2=Slight&quot; &quot;bother, 3=Moderate bother, 4=A lot of bother, 5=Makes my life a misery&quot; &quot;How much bother is the worry that you will catch a cold? 0=No bother at&quot; ... ## ..- attr(*, &quot;spec&quot;)= ## .. .. cols( ## .. .. Num = col_double(), ## .. .. Variable = col_character(), ## .. .. Type = col_character(), ## .. .. Len = col_double(), ## .. .. Format = col_character(), ## .. .. Informat = col_character(), ## .. .. Label = col_character() ## .. .. ) ## ..- attr(*, &quot;problems&quot;)=&lt;externalptr&gt; Exploring further, here is how each dataframe is formatted: datatable(tables[[1]]) Notice that, although the dataframe format mirrors the PDF table format, the label column is stored across multiple rows with NAs in the other columns of that row because the text was across multiple lines. In our final dataframe, we will want the entire block of text in one cell. We can also remove the “Len”, “Format”, and “Informat” columns because they are not informative and they are not found in every table. Next, we will walk through how to clean up this table using a series of steps in tidyverse. Cleaning dataframes First, we will select the columns we are interested in and use the fill() function to change the NAs in the “Num” column so that each line of text in the “Label” column has the correct “Num” value in the same row. cleaned_table1 &lt;- data.frame(tables[[1]]) %&gt;% # Extract the first table in the list # Select only the columns of interest select(c(Num, Variable, Type, Label)) %&gt;% # Change the &quot;Num&quot; column to numeric, which is required for the fill function mutate(Num = as.numeric(Num)) %&gt;% # Fill in the NAs in the &quot;Num&quot; column down the column fill(Num, .direction = &quot;down&quot;) datatable(cleaned_table1) We still need to move all of the Label text for each variable into one cell in one row instead of across multiple rows. For this, we can use the unlist() function. Here is a demonstration of how the unlist() function works using just the first variable: cleaned_table1_var1 &lt;- cleaned_table1 %&gt;% # Filter dataframe to just contain rows associated with the first variable filter(Num == 1) %&gt;% # Paste all character strings in the Label column with a space in between them into a new column called &quot;new_label&quot; mutate(new_label = paste(unlist(Label), collapse = &quot; &quot;)) datatable(cleaned_table1_var1) We now have all of the text we want in one cell, but we have duplicate rows that we don’t need. We can get rid of these rows by assigning blank values “NA” and then omitting rows that contain NAs. cleaned_table1_var1 &lt;- cleaned_table1_var1 %&gt;% mutate(across(Variable, na_if, &quot;&quot;)) %&gt;% na.omit() datatable(cleaned_table1_var1) We need to apply this code to the whole dataframe and not just one variable, so we can add group_by(Num) to our cleaning workflow, followed by the code we just applied to our filtered dataframe. cleaned_table1 &lt;- data.frame(tables[[1]]) %&gt;% # Extract the first table in the list # Select only the columns of interest select(c(Num, Variable, Type, Label)) %&gt;% # Change the &quot;Num&quot; column to numeric, which is required for the fill function mutate(Num = as.numeric(Num)) %&gt;% # Fill in the NAs in the &quot;Num&quot; column down the column fill(Num, .direction = &quot;down&quot;) %&gt;% # Group by variable number group_by(Num) %&gt;% # Unlist the text replace the text in the &quot;Label&quot; column with the unlisted text mutate(Label = paste(unlist(Label), collapse =&quot; &quot;)) %&gt;% # Make blanks in the &quot;Variable&quot; column into NAs mutate(across(Variable, na_if, &quot;&quot;)) %&gt;% # Remove rows with NAs na.omit() datatable(cleaned_table1) Ultimately, we need to clean up each dataframe in the list the same way, and we need all of the dataframes to be in one dataframe, instead of in a list. There are a couple of different ways to do this. Both rely on the code shown above for cleaning up each dataframe. Option #1 uses a for loop, while Option #2 uses application of a function on the list of dataframes. Both result in the same ending dataframe! Option #1 # Create a dataframe for storing variables variables &lt;- data.frame() # Make a for loop to format each dataframe and add it to the variables for (i in 1:length(tables)) { table &lt;- data.frame(tables[[i]]) %&gt;% select(c(Num, Variable, Type, Label)) %&gt;% mutate(Num = as.numeric(Num)) %&gt;% fill(Num, .direction = &quot;down&quot;) %&gt;% group_by(Num) %&gt;% mutate(Label = paste(unlist(Label), collapse =&quot; &quot;)) %&gt;% mutate(across(Variable, na_if, &quot;&quot;)) %&gt;% na.omit() variables &lt;- bind_rows(variables, table) } # View resulting dataframe datatable(variables) Option #2 # Write a function that applies all of the cleaning steps to an dataframe (output = cleaned dataframe) clean_tables &lt;- function(data) { data &lt;- data %&gt;% select(c(Num, Variable, Type, Label)) %&gt;% mutate(Num = as.numeric(Num)) %&gt;% fill(Num, .direction = &quot;down&quot;) %&gt;% group_by(Num) %&gt;% mutate(Label = paste(unlist(Label), collapse =&quot; &quot;)) %&gt;% mutate(across(Variable, na_if, &quot;&quot;)) %&gt;% na.omit() return(data) } # Apply the function over each table in the list of tables tables_clean &lt;- lapply(X = tables, FUN = clean_tables) # Unlist the dataframes and combine them into one dataframe tables_clean_unlisted &lt;- do.call(rbind, tables_clean) # View resulting dataframe datatable(tables_clean_unlisted) Adding Dataset Names We now have a dataframe with all of the information from the PDFs contained in one long table. However, now we need to add back in the label on top of each table. We can’t do this with the tabulapdf package because the name isn’t stored in the table. But we can use the pdftools package for this! First, we will read in the pdf using the PDF tools package. This results in a vector containing a long character string for each page of the PDF. Notice a few features of these character strings: Each line is separated by \\n Elements [1] and [2] of the vector contain the text “dataset Name:”, while element [3] does not because the third page was a continuation of the table from the second page and therefore did not have a table title. table_names &lt;- pdf_text(&quot;Module4_3_Input/Module4_3_InputData4.pdf&quot;) head(table_names[1:3]) ## [1] &quot; 10:13 Tuesday, February 11, 2020 1\\n\\n\\nData Set Name: aaaq.sas7bdat\\n\\nNum Variable Type Len Format Informat Label\\n 1 AAQ_1000 Num 8 2. 2. In the past 3 days, how much of the time did your asthma keep you from\\n doing your usual activities at work, school, or at home? 0=None of the\\n time, 1=A little of the time, 2=Some of the time, 3=Most of the time, 4=All\\n of the time\\n 2 AAQ_1010 Num 8 2. 2. During the past 3 days, how often have you had asthma symptoms? 0=Not\\n at all, 1=Once per day, 2=2-3 times per day, 3=4-5 times per day, 4=6 or\\n more times per day\\n 3 AAQ_1020 Num 8 2. 2. During the past 3 days, how often have you used your rescue inhaler or\\n nebulizer medication ? 0=Not at all, 1=Once per day, 2=2-3 times per day,\\n 3=4-5 times per day, 4=6 or more times per day\\n 4 AAQ_1030 Num 8 2. 2. During the past 3 days, how many total times did your asthma symptoms\\n wake you up from sleep? 0=Not at all, 1=1 time in the last 3 days, 2=2-3\\n times in the last 3 days, 3=4-5 times in the last 3 days, 4=&gt;=6 times in the\\n last 3 days\\n 5 AAQ_1040 Num 8 2. 2. How would you rate the amount of impairment you have experienced due\\n to your asthma in the past 3 days? 0=No impairment, 1=Mild impairment,\\n 2=Moderate impairment, 3=Severe impairment, 4=Very severe impairment\\n 6 AAQ_1050 Num 8 2. 2. How stressed or frightened were you by your asthma symptoms in the past\\n 3 days? 0=Not at all, 1=Mildly, 2=Moderately, 3=Severely, 4=Very\\n severely\\n 7 AAQ_1060 Num 8 2. 2. Why do you think your asthma was worse in the past 3 days compared to\\n what is normal for you? 0=I have not been worse over the past 3 days. My\\n asthma symptoms have been usual., 1=Common cold, 2=Allergies,\\n 3=Pollution or chemical irritant, 4=Too little asth\\n 8 VNUM_C Char 3 $3. $3. Visit Number (character)\\n 9 VNUM Num 8 Visit Number (numeric)\\n 10 VDATE Num 8 Number of days from Visit 1 to this visit\\n 11 RAND_ID Char 6 Randomized Master ID\\n 12 ENROLL_TYPE Char 15 Enrollment Type (Screen Fail, Randomized, Healthy Control)\\n 13 ENROLL_ORDER Num 8 Enrollment Order Number\\n&quot; ## [2] &quot; 10:13 Tuesday, February 11, 2020 2\\n\\n\\n\\nData Set Name: abp.sas7bdat\\n\\nNum Variable Type Len Format Informat Label\\n 1 ABP_1000 Num 8 2. 2. Are you currently retired? 1=Yes,0=No\\n 2 ABP_1010 Num 8 2. 2. Are you retired because of asthma? 1=Yes,0=No\\n 3 ABP_1020 Num 8 2. 2. Are you currently unemployed? 1=Yes,0=No\\n 4 ABP_1030 Num 8 2. 2. Are you unemployed because of asthma? 1=Yes,0=No\\n 5 ABP_1040 Num 8 2. 2. Do you get paid to do work? 1=Yes,0=No\\n 6 ABP_1050 Num 8 2. 2. How much does your asthma bother you at your paid work? 0=No bother\\n at all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother, 4=A lot of\\n bother, 5=Makes my life a misery\\n 7 ABP_1060 Num 8 2. 2. Overall, how much does your asthma bother you when you do jobs\\n around the house? 0=No bother at all, 1=Minor irritation, 2=Slight bother,\\n 3=Moderate bother, 4=A lot of bother, 5=Makes my life a misery,\\n 0=None of these really apply to me\\n 8 ABP_1070 Num 8 2. 2. Overall, how much does your asthma bother your social life? 0=No bother\\n at all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother, 4=A lot of\\n bother, 5=Makes my life a misery\\n 9 ABP_1080 Num 8 2. 2. Overall, how much does your asthma bother your personal life? 0=No\\n bother at all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother,\\n 4=A lot of bother, 5=Makes my life a misery, 0=None of these really\\n apply to me\\n 10 ABP_1090 Num 8 2. 2. Are you involved in leisure activities, such as: walking for pleasure,\\n sports, exercise, travelling, taking vacations? 1=Yes,0=No\\n 11 ABP_1100 Num 8 2. 2. When involved in leisure activities, how much does your asthma bother\\n you? 0=No bother at all, 1=Minor irritation, 2=Slight bother, 3=Moderate\\n bother, 4=A lot of bother, 5=Makes my life a misery\\n 12 ABP_1110 Num 8 2. 2. Would you say that you can&#39;t do some of these sorts of things because of\\n asthma? 1=Yes,0=No\\n 13 ABP_1120 Num 8 2. 2. How much does your asthma bother you when you sleep? 0=No bother at\\n all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother, 4=A lot of\\n bother, 5=Makes my life a misery\\n 14 ABP_1130 Num 8 2. 2. How much does the cost of your asthma medicines bother you? 0=No\\n bother at all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother,\\n 4=A lot of bother, 5=Makes my life a misery\\n 15 ABP_1140 Num 8 2. 2. Do you get free prescriptions? 1=Yes,0=No\\n 16 ABP_1150 Num 8 2. 2. How much does the inconvenience or embarrassment of taking your\\n asthma medicines bother you? 0=No bother at all, 1=Minor irritation,\\n 2=Slight bother, 3=Moderate bother, 4=A lot of bother, 5=Makes my life\\n a misery\\n 17 ABP_1160 Num 8 2. 2. How much do coughs and colds bother you? 0=No bother at all, 1=Minor\\n irritation, 2=Slight bother, 3=Moderate bother, 4=A lot of bother,\\n 5=Makes my life a misery, 0=None of these really apply to me\\n 18 ABP_1170 Num 8 2. 2. Feeling upset is also a bother. Does your asthma make you feel anxious,\\n depressed, tired, or helpless? 1=Yes,0=No\\n 19 ABP_1180 Num 8 2. 2. Feeling upset is also a bother. Does your asthma make you feel anxious,\\n depressed, tired, or helpless? 0=No bother at all, 1=Minor irritation,\\n 2=Slight bother, 3=Moderate bother, 4=A lot of bother, 5=Makes my life\\n a misery\\n&quot; ## [3] &quot; 10:13 Tuesday, February 11, 2020 3\\n\\n\\nNum Variable Type Len Format Informat Label\\n 20 ABP_1190 Num 8 2. 2. How much bother is the worry that you will have an asthma attack when\\n visiting a new place? 0=No bother at all, 1=Minor irritation, 2=Slight\\n bother, 3=Moderate bother, 4=A lot of bother, 5=Makes my life a misery\\n 21 ABP_1200 Num 8 2. 2. How much bother is the worry that you will catch a cold? 0=No bother at\\n all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother, 4=A lot of\\n bother, 5=Makes my life a misery\\n 22 ABP_1210 Num 8 2. 2. How much bother is the worry that you will let others down? 0=No bother\\n at all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother, 4=A lot of\\n bother, 5=Makes my life a misery\\n 23 ABP_1220 Num 8 2. 2. How much bother is the worry that your health may get worse in the\\n future? 0=No bother at all, 1=Minor irritation, 2=Slight bother,\\n 3=Moderate bother, 4=A lot of bother, 5=Makes my life a misery\\n 24 ABP_1230 Num 8 2. 2. How much bother is the worry that you won&#39;t be able to cope with an\\n asthma attack? 0=No bother at all, 1=Minor irritation, 2=Slight bother,\\n 3=Moderate bother, 4=A lot of bother, 5=Makes my life a misery\\n 25 VNUM_C Char 3 $3. $3. Visit Number (character)\\n 26 VNUM Num 8 Visit Number (numeric)\\n 27 VDATE Num 8 Number of days from Visit 1 to this visit\\n 28 RAND_ID Char 6 Randomized Master ID\\n 29 ENROLL_TYPE Char 15 Enrollment Type (Screen Fail, Randomized, Healthy Control)\\n 30 ENROLL_ORDER Num 8 Enrollment Order Number\\n&quot; Similar to the table cleaning section, we will work through an example of extracting the text of interest from one of these character vectors, then apply the same code to all of the character vectors. First, we will select just the first element in the vector and make it into a dataframe. # Create dataframe dataset_name_df_var1 &lt;- data.frame(strsplit(table_names[1], &quot;\\n&quot;)) # Clean column name colnames(dataset_name_df_var1) &lt;- c(&quot;Text&quot;) # View dataframe datatable(dataset_name_df_var1) Next, we will extract the dataset name using the same approach used in extracting values from the nanoparticle tracking example above and assign the name to a variable. We filter by the string “Data Set Name” because this is the start of the text string in the row where our dataset name is stored and is the same across all of our datasets. # Create dataframe dataset_name_df_var1 &lt;- dataset_name_df_var1 %&gt;% filter(grepl(&quot;Data Set Name&quot;, dataset_name_df_var1$Text)) %&gt;% separate(Text, into = c(NA, &quot;dataset&quot;), sep = &quot;Data Set Name: &quot;) # Assign variable dataset_name_var1 &lt;- dataset_name_df_var1[1,1] # View variable name dataset_name_var1 ## [1] &quot;aaaq.sas7bdat&quot; Now that we have the dataset name stored as a variable, we can create a dataframe that will correspond to the rows in our variables dataframe. The challenge is that each dataset contains a different number of variables! We can determine how many rows each dataset contains by returning to our variables dataframe and calculating the number of rows associated with each dataset. The following code splits the variables dataframe into a list of dataframes by each occurrence of 1 in the “Num” column (when the numbering restarts for a new dataset). # Calculate the number of rows associated with each dataset for reference dataset_list &lt;- split(variables, cumsum(variables$Num == 1)) glimpse(dataset_list[1:3]) ## List of 3 ## $ 1:&#39;data.frame&#39;: 13 obs. of 4 variables: ## ..$ Num : num [1:13] 1 2 3 4 5 6 7 8 9 10 ... ## ..$ Variable: chr [1:13] &quot;AAQ_1000&quot; &quot;AAQ_1010&quot; &quot;AAQ_1020&quot; &quot;AAQ_1030&quot; ... ## ..$ Type : chr [1:13] &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; ... ## ..$ Label : chr [1:13] &quot;In the past 3 days, how much of the time did your asthma keep you from doing your usual activities at work, sch&quot;| __truncated__ &quot;During the past 3 days, how often have you had asthma symptoms? 0=Not at all, 1=Once per day, 2=2-3 times per d&quot;| __truncated__ &quot;During the past 3 days, how often have you used your rescue inhaler or nebulizer medication ? 0=Not at all, 1=O&quot;| __truncated__ &quot;During the past 3 days, how many total times did your asthma symptoms wake you up from sleep? 0=Not at all, 1=1&quot;| __truncated__ ... ## $ 2:&#39;data.frame&#39;: 30 obs. of 4 variables: ## ..$ Num : num [1:30] 1 2 3 4 5 6 7 8 9 10 ... ## ..$ Variable: chr [1:30] &quot;ABP_1000&quot; &quot;ABP_1010&quot; &quot;ABP_1020&quot; &quot;ABP_1030&quot; ... ## ..$ Type : chr [1:30] &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; ... ## ..$ Label : chr [1:30] &quot;Are you currently retired? 1=Yes,0=No&quot; &quot;Are you retired because of asthma? 1=Yes,0=No&quot; &quot;Are you currently unemployed? 1=Yes,0=No&quot; &quot;Are you unemployed because of asthma? 1=Yes,0=No&quot; ... ## $ 3:&#39;data.frame&#39;: 11 obs. of 4 variables: ## ..$ Num : num [1:11] 1 2 3 4 5 6 7 8 9 10 ... ## ..$ Variable: chr [1:11] &quot;ACT_1&quot; &quot;ACT_2&quot; &quot;ACT_3&quot; &quot;ACT_4&quot; ... ## ..$ Type : chr [1:11] &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; ... ## ..$ Label : chr [1:11] &quot;In the past 4 weeks, how much of the time did your asthma keep you from getting as much done at work, school or&quot;| __truncated__ &quot;During the past 4 weeks, how often have your had shortness of breath? 1=All of the time, 2=Most of the time, 3=&quot;| __truncated__ &quot;During the past 4 weeks, how often did your asthma symptoms wake you up at night or earlier than usual in the m&quot;| __truncated__ &quot;During the past 4 weeks, how often have you used your rescue inhaler or nebulizer medication? 1=All of the time&quot;| __truncated__ ... The number of rows in each list is the number of variables in that dataset. We can use this value in creating our dataframe of dataset names. # Store the number of rows in a variable n_rows = nrow(data.frame(dataset_list[1])) # Repeat the dataset name for the number of variables there are dataset_name_var1 = data.frame(&quot;dataset_name&quot; = rep(dataset_name_var1, times = n_rows)) # View data farme datatable(dataset_name_var1) We now have a dataframe that can be joined with our variables dataframe for the first table. We can apply this approach to each table in our original PDF using a for loop. # Make dataframe to store dataset names dataset_names &lt;- data.frame() # Create list of datasets dataset_list &lt;- split(variables, cumsum(variables$Num == 1)) # Remove elements from the table_names vector that do not contain the string &quot;Data Set Name&quot; table_names_filtered &lt;- stringr::str_subset(table_names, &#39;Data Set Name&#39;) # Populate dataset_names dataframe for (i in 1:length(table_names_filtered)) { # Get dataset name dataset_name_df &lt;- data.frame(strsplit(table_names_filtered[i], &quot;\\n&quot;)) base::colnames(dataset_name_df) &lt;- c(&quot;Text&quot;) dataset_name_df &lt;- dataset_name_df %&gt;% filter(grepl(&quot;Data Set Name&quot;, dataset_name_df$Text)) %&gt;% separate(Text, into = c(NA, &quot;dataset&quot;), sep = &quot;Data Set Name: &quot;) dataset_name &lt;- dataset_name_df[1,1] # Determine number of variables in that dataset data_set &lt;- data.frame(dataset_list[i]) n_rows = nrow(data_set) # Repeat the dataset name for the number of variables there are dataset_name = data.frame(&quot;Data Set Name&quot; = rep(dataset_name, times = n_rows)) # Bind to dataframe dataset_names &lt;- bind_rows(dataset_names, dataset_name) } # Rename column colnames(dataset_names) &lt;- c(&quot;Data Set Name&quot;) # View datatable(dataset_names) Combining Dataset Names and Variable Information Last, we will merge together the dataframe containing dataset names and variable information. # Merge together final_variable_df &lt;- cbind(dataset_names, variables) %&gt;% rename(&quot;Variable Description&quot; = &quot;Label&quot;, &quot;Variable Number Within Dataset&quot; = &quot;Num&quot;) %&gt;% clean_names() datatable(final_variable_df) We can also determine how many total variables we have, all of which are accessible via the table we just generated. # Total number of variables nrow(final_variable_df) ## [1] 1190 # Total number of variables With this, we can answer Environmental Health Question #2: How many variables total are available to us to request from the study whose data are stored in the repository, and what are these variables? Answer: There are 1190 variable available to us. We can browse through the variables, including the sub-table they were from, the type of variable they are, and how they were derived using the table we generated. Concluding Remarks This training module provides example case studies demonstrating how to import PDF data into R and clean it so that it is more useful and accessible for analyses. The approaches demonstrated in this module, though specific to our specific example data, can be adapted to many different types of PDF data. Test Your Knowledge Using the same input files that we used in part 1, “Importing Data from Many Single PDFs with the Same Formatting”, found in the Module4_3_TYKInput folder, extract the remaining variables of interest (Original Concentration and Positions Removed) from the PDFs and summarize them in one dataframe. "],["two-group-comparisons-and-visualizations.html", "Two-Group Comparisons and Visualizations Introduction to Training Module Overview of Two Group Statistical Tests Statistical vs Biological Significance Unpaired Test Example Paired Test Example Visualizing Results Concluding Remarks", " Two-Group Comparisons and Visualizations This training module was developed by Elise Hickman, Alexis Payton, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Two group statistical comparisons, in which we want to know whether the means between two different groups are significantly different, are some of the most common statistical tests in environmental health research and even biomedical research as a field. In this training module, we will demonstrate how to run two group statistical comparisons and how to present publication-quality figures and tables of these results. We will continue to use the same example dataset as used in this chapter’s previous modules, which represents concentrations of inflammatory biomarkers secreted by airway epithelial cells after exposure to different concentrations of acrolein. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Are there significant differences in inflammatory biomarker concentrations between cells from male and female donors at baseline? Are there significant differences in inflammatory biomarker concentrations between cells exposed to 0 and 4 ppm acrolein? Workspace Preparation and Data Import Here, we will import the processed data that we generated at the end of TAME 2.0 Module 4.2 Data Import, Processing, and Summary Statistics. These data, along with the associated demographic data, were introduced in TAME 2.0 Module 4.1 Overview of Experimental Design and Example Data. These data represent log2 concentrations of inflammatory biomarkers secreted by airway epithelial cells after exposure to four different concentrations of acrolein (plus filtered air as a control). We will also load packages that will be needed for the analysis, including previously introduced packages such as openxlsx, tidyverse, DT, and ggpubr, and additional packages relevant to statistical analysis and graphing that will be discussed in greater detail below. # Load packages library(openxlsx) library(tidyverse) library(DT) library(rstatix) library(ggpubr) # Import data biomarker_data &lt;- read.xlsx(&quot;Module4_4_Input/Module4_4_InputData1.xlsx&quot;) demographic_data &lt;- read.xlsx(&quot;Module4_4_Input/Module4_4_InputData2.xlsx&quot;) # View data datatable(biomarker_data) datatable(demographic_data) Overview of Two Group Statistical Tests Before applying statistical tests to our data, let’s first review common two-group statistical tests, their underlying assumptions, and variations on these tests. Common Tests The two most common two-group statistical tests are the… t-test (also known as the student’s t-test) and the Wilcoxon test (also known as the Wilcox test, Wilcoxon test, or Mann Whitney test) Both of these tests are testing the null hypothesis that the means of the two populations (groups) are the same; the alternative hypothesis is that they are not the same. A significant p-value means that we can reject the null hypothesis that the means of the two groups are the same. Whether or not a p-value meets criteria for significance is experiment-specific, though commonly implemented p-value filters for significance include p&lt;0.05 and p&lt;0.01. P-values can also be called alpha values, and they indicate the probability of a type I error, or false positive, where the null hypothesis is rejected despite it actually being true. On the other hand, a type II error, or false negative, occurs when the null hypothesis is not rejected when it actually should have been. Assumptions The main difference between these two tests is in the assumption about the underlying distribution of the data. T-tests assume that the data are pulled from a normal distribution, while Wilcoxon tests do not assume that the data are pulled from a normal distribution. Therefore, it is most appropriate to use a t-test when data are, in general, normally distributed and a Wilcoxon test when data are not normally distributed. Additional assumptions underlying t-tests and Wilcoxon test are: The dependent variable is continuous or ordinal (discrete, ordered values). The data is collected from a representative, random sample. T-tests also assume that: The standard deviations of the two groups are approximately equal (also called homogeneity of variance). When to Use a Parametric vs Non-Parametric Test? Deciding whether to use a parametric or non-parametric test isn’t a one size fits all approach, and the decision should be made holistically for each dataset. Typically, parametric tests should be used when the data are normally distributed, continuous, random sampled, without extreme outliers, and representative of independent samples or participants. A non-parametric test can be used when the sample size (n) is small, outliers are present in the dataset, and/or the data are not normally distributed. This decision matters more when dealing with smaller sample sizes (n&lt;10) as smaller sample sizes are more prone to being skewed, and parametric tests are more sensitive to outliers. Therefore, when dealing with a smaller n, it might be best to perform a data transformation as discussed in TAME 2.0 Module 3.3 Normality Testing &amp; Data Transformations and then perform a parametric test if more parametric assumptions are able to be met, or to use non-parametric tests. For larger sample sizes (n&gt;50), outliers can potentially be removed and the dataset can be retested for assumptions. Lastly, what’s considered “small” or “large” in regards to sample size can be subjective and should be taken into consideration within the context of the experiment. Variations Unequal Variance: When the assumption of homogeneity of variance is not met, a Welch’s t-test is generally preferred over a student’s t-test. This can be implemented easily by setting var.equal = FALSE as an argument to the function executing the t-test (e.g., t.test(), t_test()). For more on testing homogeneity of variance in R, see here. Paired vs Unpaired: Variations on the t-test and Wilcoxon test are used when the experimental design is paired (also called repeated measures or matching). This occurs when there are different treatments, exposures, or time points collected from the same biological/experimental unit. For example, cells from the same donor or passage number exposed to different concentrations of a chemical represents a paired design. Matched/paired experiments have increased power to detect significant differences because samples can be compared back to their own controls. One vs Two-Sided: A one-sided test evaluates the hypothesis that the mean of the treatment group significantly differs in a specific direction from the control. A two-sided test evaluates the hypothesis that the mean of the treatment group significantly differs from the control but does not specify a direction for that change. A two-sided test is the preferred approach and the default in R because, typically, either direction of change is possible and represents an informative finding. However, one-sided tests may be appropriate if an effect can only possibly occur in one direction. This can be implemented by setting alternative = \"one.sided\" within the statistical testing function. Which test should I choose? We provide the following flowchart to help guide your choice of statistical test to compare two groups: Figure 10: Created with BioRender.com Statistical vs Biological Significance Another important topic to discuss before proceeding to statistical testing is the true meaning of statistical significance. Statistical significance simply means that it is unlikely that the patterns being observed are due to random chance. However, just because an effect is statistically significant does not mean that it is biologically significant (i.e., has notable biological consequences). Often, there also needs to be a sufficient magnitude of effect (also called effect size) for the effects on a system to be meaningful. Although a p-value &lt; 0.05 is often considered the threshold for significance, this is just a standard threshold set to a generally “acceptable” amount of error (5%). What about a p-value of 0.058 with a very large biological effect? Accounting for effect size is also why filters such as log2 fold change are often applied alongside p-value filters in -omics based analysis. In discussions of effect size, the population size is also a consideration - a small percentage increase in a very large population can represent tens of thousands of individuals (or more). Another consideration is that we frequently do not know what magnitude of biological effect should be considered “significant.” These discussions can get complicated very quickly, and here we do not propose to have a solution to these thought experiments; rather, we recommend considering both statistical and biological significance when interpreting data. And, as stated in other sections of TAME, transparent reporting of statistical results will aid the audience in interpreting the data through their preferred perspectives. Unpaired Test Example We will start by performing a statistical test to determine whether there are significant differences in biomarker concentrations between male and female donors at baseline (0 ppm exposure). Previously we determined that the majority of our data was non-normally distributed (see TAME 2.0 Module 4.2 Data Import, Processing, and Summary Statistics), so we’ll skip testing for that assumption in this module. Based on those results, we will use the Wilcoxon test to determine if there are significant differences between groups. The Wilcoxon test does not assume homogeneity of variance, so we do not need to test for that prior to applying the test. This is an unpaired analysis because samples collected from the cells derived from male and female donor cells are different sets of cells (i.e., independent from each other). Thus, the specific statistical test applied will be the Wilcoxon Rank Sum test. First, we will filter our dataframe to only data representing the control (0 ppm) exposure: biomarker_data_malevsfemale &lt;- biomarker_data %&gt;% filter(Dose == &quot;0&quot;) Next, we need to add the demographic data to our dataframe: biomarker_data_malevsfemale &lt;- biomarker_data_malevsfemale %&gt;% left_join(demographic_data %&gt;% select(Donor, Sex), by = &quot;Donor&quot;) Here is what our data look like now: datatable(biomarker_data_malevsfemale) We can demonstrate the basic anatomy of the Wilcoxon test function wilcox.test() by running the function on just one variable. wilcox.test(IL1B ~ Sex, data = biomarker_data_malevsfemale) ## ## Wilcoxon rank sum exact test ## ## data: IL1B by Sex ## W = 29, p-value = 0.8371 ## alternative hypothesis: true location shift is not equal to 0 The p-value of 0.8371 indicates that males and females do not have significantly different concentrations of IL-1\\(\\beta\\). The wilcox.test() function is part of the pre-loaded package stats. The package rstatix provides identical statistical tests to stats but in a pipe-friendly (tidyverse-friendly) format, and these functions output results as dataframes rather than the text displayed above. biomarker_data_malevsfemale %&gt;% wilcox_test(IL1B ~ Sex) ## # A tibble: 1 × 7 ## .y. group1 group2 n1 n2 statistic p ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 IL1B F M 7 9 29 0.837 Here, we can see the exact same results as with the wilcox.test() function. For the rest of this module, we’ll proceed with using the rstatix version of statistical testing functions. Although it is simple to run the Wilcoxon test with the code above, it’s impractical for a large number of endpoints and doesn’t store the results in an organized way. Instead, we can run the Wilcoxon test over every variable of interest using a for loop. There are also other ways you could approach this, such as a function applied over a list. This for loop runs the Wilcoxon test on each endpoint, stores the results in a dataframe, and then binds together the results dataframes for each variable of interest. Note that you could easily change wilcox_test() to t_test() and add additional arguments to modify the way the statistical test is run. # Create a vector with the names of the variables you want to run the test on endpoints &lt;- colnames(biomarker_data_malevsfemale %&gt;% select(IL1B:VEGF)) # Create dataframe to store results sex_wilcoxres &lt;- data.frame() # Run for loop for (i in 1:length(endpoints)) { # Assign a name to the endpoint variable. endpoint &lt;- endpoints[i] # Run wilcox test and store in results dataframe. res_df &lt;- biomarker_data_malevsfemale %&gt;% wilcox_test(as.formula(paste0(endpoint, &quot;~ Sex&quot;, sep = &quot;&quot;))) # Bind results from this test with other tests in this loop sex_wilcoxres &lt;- rbind(sex_wilcoxres, res_df) } # View results sex_wilcoxres ## # A tibble: 6 × 7 ## .y. group1 group2 n1 n2 statistic p ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 IL1B F M 7 9 29 0.837 ## 2 IL6 F M 7 9 28.5 0.791 ## 3 IL8 F M 7 9 34 0.832 ## 4 IL10 F M 7 9 43 0.252 ## 5 TNFa F M 7 9 39 0.458 ## 6 VEGF F M 7 9 30 0.916 With this, we can answer Environmental Health Question #1: Are there significant differences in inflammatory biomarker concentrations between cells from male and female donors at baseline? Answer: There are not any significant differences in concentrations of any of our biomarkers between male and female donors at baseline. Adjusting for Multiple Hypothesis Testing Above, we compared concentrations between males and females for six different endpoints or variables. Each time we run a comparison (with a p-value threshold of &lt; 0.05), we are accepting that there is a 5% chance that a significant result will actually be due to random chance and that we are rejecting the null hypothesis when it is actually true (type I error). Since we are testing six different hypotheses simultaneously, what is the probability then of observing at least one significant result due just to chance? \\[\\mathbb{P}({\\rm At Least One Significant Result}) = 1 - \\mathbb{P}({\\rm NoSignificantResults}) = 1 - (1 - 0.05)^{6} = 0.26\\] Here, we can see that we have a 26% chance of observing at least one significant result, even if all the tests are actually not significant. This chance increases as our number of endpoints increases; therefore, adjusting for multiple hypothesis testing becomes even more important with larger datasets. Many methods exist for adjusting for multiple hypothesis testing, with some of the most popular including Bonferroni, False Discovery Rate (FDR), and Benjamini-Hochberg (BH). However, opinions about when and how to adjust for multiple hypothesis testing can vary and also depend on the question you are trying to answer. For example, when there are a low number of variables (e.g., &lt; 10), it’s often not necessary to adjust for multiple hypothesis testing, and when there are many variables (e.g., 100s to 1000s), it is necessary, but what about for an intermediate number of comparisons? Whether or not to apply multiple hypothesis test correction also depends on whether each endpoint is of interest on its own or whether the analysis seeks to make general statements about all of the endpoints together and on whether reducing type I or type II error is most important in the analysis. For this analysis, we will not adjust for multiple hypothesis testing due to our relatively low number of variables. For more on multiple hypothesis testing, check out the following publications: Mohieddin J; Naser AP. “Why, When and How to Adjust Your P Values?”. Cell Journal (Yakhteh), 20, 4, 2018, 604-607. doi: 10.22074/cellj.2019.5992 PUBMID: 30124010 Feise, R.J. Do multiple outcome measures require p-value adjustment?. BMC Med Res Methodol 2, 8 (2002). https://doi.org/10.1186/1471-2288-2-8 PUBMID: 12069695 Paired Test Example To demonstrate an example of a paired two-group test, we can also determine whether exposure to 4 ppm acrolein significantly changes biomarker concentrations. This is now a paired design because each donor’s cells were exposed to both 0 and 4 ppm acrolein. To prepare the data, we will filter the dataframe to only include 0 and 4 ppm: biomarker_data_0vs4 &lt;- biomarker_data %&gt;% filter(Dose == &quot;0&quot; | Dose == &quot;4&quot;) Let’s view the dataframe. Note how the measurements for each donor are next to each other - this an important element of the default handling of the paired analysis in R. The dataframe should have the donors in the same order for the 0 and 4 ppm data. datatable(biomarker_data_0vs4) We can now run the same type of loop that we ran before, changing the independent variable in the formula to ~ Dose and adding paired = TRUE to the wilcox_test() function. # Create a vector with the names of the variables you want to run the test on endpoints &lt;- colnames(biomarker_data_0vs4 %&gt;% select(IL1B:VEGF)) # Create dataframe to store results dose_wilcoxres &lt;- data.frame() # Run for loop for (i in 1:length(endpoints)) { # Assign a name to the endpoint variable. endpoint &lt;- endpoints[i] # Run wilcox test and store in results dataframe. res_df &lt;- biomarker_data_0vs4 %&gt;% wilcox_test(as.formula(paste0(endpoint, &quot;~ Dose&quot;, sep = &quot;&quot;)), paired = TRUE) # Bind results from this test with other tests in this loop dose_wilcoxres &lt;- rbind(dose_wilcoxres, res_df) } # View results dose_wilcoxres ## # A tibble: 6 × 7 ## .y. group1 group2 n1 n2 statistic p ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 IL1B 0 4 16 16 18 0.00763 ## 2 IL6 0 4 16 16 114 0.0155 ## 3 IL8 0 4 16 16 0 0.0000305 ## 4 IL10 0 4 16 16 65 0.9 ## 5 TNFa 0 4 16 16 5 0.000305 ## 6 VEGF 0 4 16 16 28 0.0386 Although this dataframe contains useful information about our statistical test, such as the groups being compared, the sample size (n) of each group, and the test statistic, what we really want (and what would likely be shared in supplemental material), is a more simplified version of these results in table format and more detailed information (n, specific statistical test, groups being compared) in the table legend. We can clean up the results using the following code to make clearer column names and ensure that the p-values are formatted consistently. dose_wilcoxres &lt;- dose_wilcoxres %&gt;% select(c(.y., p)) %&gt;% mutate(p = format(p, digits = 3, scientific = TRUE)) %&gt;% rename(&quot;Variable&quot; = &quot;.y.&quot;, &quot;P-Value&quot; = &quot;p&quot;) datatable(dose_wilcoxres) With this, we can answer Environmental Health Question #2: Are there significant differences in inflammatory biomarker concentrations between cells exposed to 0 and 4 ppm acrolein? Answer: Yes, there are significant differences in IL-1\\(\\beta\\), IL-6, IL-8, TNF-\\(\\alpha\\), and VEGF concentrations between cells exposed to 0 and 4 ppm acrolein. Visualizing Results Now, let’s visualize our results using ggplot2. For an introduction to ggplot2 visualizations, see TAME 2.0 Modules 3.1 Data Visualizations and 3.2 Improving Data Visualizations, as well as the extensive online documentation available for ggplot2. Single Plots We will start by making a very basic box and whisker plot of the IL-1\\(\\beta\\) data with individual data points overlaid. It is best practice to show all data points, allowing the reader to view the whole spread of the data, which can be obscured by plots such as bar plots with mean and standard error. # Setting theme for plot theme_set(theme_bw()) # Making plot ggplot(biomarker_data_0vs4, aes(x = Dose, y = IL1B)) + geom_boxplot() + geom_jitter(position = position_jitter(0.15)) We could add statistical markings to denote significance to this graph manually in PowerPoint or Adobe Illustrator, but there are actually R packages that act as extensions to ggplot2 and will do this for you! Two of our favorites are ggpubr and ggsignif. Here is an example using ggpubr: ggplot(biomarker_data_0vs4, aes(x = Dose, y = IL1B)) + geom_boxplot() + geom_jitter(position = position_jitter(0.15)) + # Adding a p value from a paired Wilcoxon test stat_compare_means(method = &quot;wilcox.test&quot;, paired = TRUE) We can further clean up our figure by modifying elements of the plot’s theme, including the font sizes, axis range, colors, and the way that the statistical results are presented. Perfecting figures can be time consuming but ultimately worth it, because clear figures aid greatly in presenting a coherent story that is understandable to readers/listeners. ggplot(biomarker_data_0vs4, aes(x = Dose, y = IL1B)) + # outlier.shape = NA removes outliers geom_boxplot(aes(fill = Dose), outlier.shape = NA) + # Changing box plot colors scale_fill_manual(values = c(&quot;#BFBFBF&quot;, &quot;#EE2B2B&quot;)) + geom_jitter(size = 3, position = position_jitter(0.15)) + # Adding a p value from a paired Wilcoxon test stat_compare_means(method = &quot;wilcox.test&quot;, paired = TRUE, # Changing the value to asterisks and moving to the middle of the plot label = &quot;p.signif&quot;, label.x = 1.5, label.y = 4.5, size = 12) + ylim(2.5, 5) + # Changing y axis label labs(y = &quot;Log2(IL-1\\u03B2 (pg/mL))&quot;) + # Removing legend theme(legend.position = &quot;none&quot;, axis.title = element_text(color = &quot;black&quot;, size = 15), axis.title.x = element_text(vjust = -0.75), axis.title.y = element_text(vjust = 2), axis.text = element_text(color = &quot;black&quot;, size = 12)) Multiple plots Making one plot was relatively straightforward, but to graph all of our endpoints, we would either need to repeat that code chunk for each individual biomarker or write a function to create similar plots given a specific biomarker as input. Then, we would need to stitch together the individual plots in external software or using a package such as patchwork (which is a great package if you need to combine individual figures from different sources or different size ratios!). While these are workable solutions and would get us to the same place, ggplot2 actually contains a function - facet_wrap() - that can be used to graph multiple endpoints from the same groups in one figure panel, which takes care of a lot of the work for us! To prepare our data for facet plotting, first we will pivot it longer: biomarker_data_0vs4_long &lt;- biomarker_data_0vs4 %&gt;% pivot_longer(-c(Donor, Dose), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) datatable(biomarker_data_0vs4_long) Then, we can use similar code to what we used to make our single graph, with a few modifications to plot multiple panels simultaneously and adjust the style of the plot. Although it is beyond the scope of this module to explain the mechanics of each line of code, here are a few specific things to note about the code below that may be helpful when constructing similar plots: To create the plot with all six endpoints instead of just one, we: Changed input dataframe from wide to long format Changed y = from one specific endpoint to value Added the facet_wrap() argument ~ variable tells the function to make an individual plot for each variable nrow = 2 tells the function to put the plots into two rows scales = \"free_y\" tells the function to allow each individual graph to have a unique y-scale that best shows all of the data on that graph labeller feeds the edited (more stylistically correct) names for each panel to the function To ensure that the statistical results appear cleanly, within stat_compare_means(), we: Added hide.ns = TRUE so that only significant results are shown Added label.x.npc = \"center\" and hjust = 0.5 to ensure that asterisks are centered on the plot and that the text is center justified To add padding along the y axis, allowing space for significance asterisks, we added scale_y_continuous(expand = expansion(mult = c(0.1, 0.4))) # Create clean labels for the graph titles new_labels &lt;- c(&quot;IL10&quot; = &quot;IL-10&quot;, &quot;IL1B&quot; = &quot;IL-1\\u03B2 &quot;, &quot;IL6&quot; = &quot;IL-6&quot;, &quot;IL8&quot; = &quot;IL-8&quot;, &quot;TNFa&quot; = &quot;TNF-\\u03b1&quot;, &quot;VEGF&quot; = &quot;VEGF&quot;) # Make graph ggplot(biomarker_data_0vs4_long, aes(x = Dose, y = value)) + # outlier.shape = NA removes outliers geom_boxplot(aes(fill = Dose), outlier.shape = NA) + # Changing box plot colors scale_fill_manual(values = c(&quot;#BFBFBF&quot;, &quot;#EE2B2B&quot;)) + geom_jitter(size = 1.5, position = position_jitter(0.15)) + # Adding a p value from a paired Wilcoxon test stat_compare_means(method = &quot;wilcox.test&quot;, paired = TRUE, # Changing the value to asterisks and moving to the middle of the plot label = &quot;p.signif&quot;, size = 10, hide.ns = TRUE, label.x.npc = &quot;center&quot;, hjust = 0.5) + # Adding padding y axis scale_y_continuous(expand = expansion(mult = c(0.1, 0.4))) + # Changing y axis label ylab(expression(Log[2]*&quot;(Concentration (pg/ml))&quot;)) + # Faceting by each biomarker facet_wrap(~ variable, nrow = 2, scales = &quot;free_y&quot;, labeller = labeller(variable = new_labels)) + # Removing legend theme(legend.position = &quot;none&quot;, axis.title = element_text(color = &quot;black&quot;, size = 12), axis.title.x = element_text(vjust = -0.75), axis.title.y = element_text(vjust = 2), axis.text = element_text(color = &quot;black&quot;, size = 10), strip.text = element_text(size = 12, face = &quot;bold&quot;)) An appropriate title for this figure could be: “Figure X. Exposure to 4 ppm acrolein increases inflammatory biomarker secretion in primary human bronchial epithelial cells. Groups were compared using the Wilcoxon signed rank test. * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001, **** p &lt; 0.0001, n = 16 per group (paired).” Concluding Remarks In this module, we introduced two-group statistical tests, which are some of the most common statistical tests applied in biomedical research. We applied these tests to our example dataset and demonstrated how to produce publication-quality tables and figures of our results. Implementing a workflow such as this enables efficient analysis of wet-bench generated data and customization of output figures and tables suited to your personal preferences. Test Your Knowledge Functional endpoints from these cultures were also measured. These endpoints were: 1) Membrane Permeability (MemPerm), 2) Trans-Epithelial Electrical Resistance (TEER), 3) Ciliary Beat Frequency (CBF), and 4) Expression of Mucin (MUC5AC). These data were already processed and tested for normality (see Test Your Knowledge for TAME 2.0 Module 4.2 Data Import, Processing, and Summary Statistics), with results indicating that two of the endpoints are normally distributed and two non-normally distributed. Due to the relatively low n of this dataset, we therefore recommend using non-parametric statistical tests. Use the same processes demonstrated in this module and the provided data (“Module4_4_TYKInput1.xlsx” (functional data) and “Module4_4_TYKInput2.xlsx” (demographic data)), run analyses and make publication-quality figures and tables to answer the following questions to determine: Are there significant differences in functional endpoints between cells from male and female donors at baseline? Are there significant differences in functional endpoints between cells exposed to 0 and 4 ppm acrolein? Go ahead and use non-parametric tests for these analyses. "],["multi-group-and-multi-variable-comparisons-and-visualizations.html", "Multi-Group and Multi-Variable Comparisons and Visualizations Introduction to Training Module Overview of Multi-Group Statistical Tests Multi-Group Analysis Example Visualization of Multi-Group Statistical Results Concluding Remarks", " Multi-Group and Multi-Variable Comparisons and Visualizations This training module was developed by Elise Hickman, Alexis Payton, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module In the previous module, we covered how to apply two-group statistical testing, one of the most basic types of statistical tests. In this module, we will build on the concepts introduced previously to apply statistical testing to datasets with more than two groups, which are also very common in environmental health research. We will review common multi-group overall effects tests and post-hoc tests, and we will demonstrate how to apply these tests and how to graph the results using the same example dataset as in previous modules in this chapter, which represents concentrations of inflammatory biomarkers secreted by airway epithelial cells after exposure to different concentrations of acrolein. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Are there significant differences in inflammatory biomarker concentrations between different doses of acrolein? Do TNF-\\(\\alpha\\) concentrations significantly increase with increasing dose of acrolein? Workspace Preparation and Data Import Here, we will import the processed data that we generated at the end of TAME 2.0 Module 4.2, introduced in TAME 2.0 Module 4.1 Overview of Experimental Design and Example Data and the associated demographic data. These data represent log2 concentrations of inflammatory biomarkers secreted by airway epithelial cells after exposure to four different concentrations of acrolein (plus filtered air as a control). We will also load packages that will be needed for the analysis, including previously introduced packages such as openxlsx, tidyverse, DT, ggpubr, and rstatix. # Load packages library(openxlsx) library(tidyverse) library(DT) library(rstatix) library(ggpubr) # Import data biomarker_data &lt;- read.xlsx(&quot;Module4_5_Input/Module4_5_InputData1.xlsx&quot;) demographic_data &lt;- read.xlsx(&quot;Module4_5_Input/Module4_5_InputData2.xlsx&quot;) # View data datatable(biomarker_data) datatable(demographic_data) Overview of Multi-Group Statistical Tests Before applying statistical tests to our data, let’s first review the mechanics of multi-group statistical tests, including overall effects tests and post-hoc tests. Figure 11: Created with BioRender.com Overall Effects Tests The first step for multi-group statistical testing is to run an overall effects test. The null hypothesis for the overall effects test is that there are no differences among group means. A significant p-value rejects the null hypothesis that the groups are drawn from populations with the same mean and indicates that at least one group differs significantly from the overall mean. Similar to two-group statistical testing, choice of the specific overall statistical test to run depends on whether the data are normally or non-normally distributed and whether the experimental design is paired: Figure 12: Created with BioRender.com Importantly, overall effects tests return one p-value regardless of the number of groups being compared. To determine which pairwise comparisons are significant, post-hoc testing is needed. Post-Hoc Testing If significance is obtained with an overall effects test, we can use post-hoc testing to determine which specific pairs of groups are significantly different from each other. Just as with two group statistical tests and overall effects multi-group statistical tests, choosing the appropriate post-hoc test depends on the data’s normality and whether the experimental design is paired: Figure 13: Created with BioRender.com Note that the above diagram represents commonly selected post-hoc tests; others may also be appropriate depending on your specific experimental design. As with other aspects of the analysis, be sure to report which post-hoc test(s) you performed! Correcting for Multiple Hypothesis Testing Correcting for multiple hypothesis testing is important for both the overall effects test (if you are running it over many endpoints) and post-hoc tests; however, it is particularly important for post-hoc tests. This is because even an analysis of a relatively small number of experimental groups results in quite a few pairwise comparisons. Comparing each of our five dose groups to each other in our example data, there are 10 separate statistical tests being performed! Therefore, it is generally advisable to adjust pairwise post-hoc testing p-values. The Tukey’s HSD function within rstatix does this automatically, while pairwise t-tests, pairwise Wilcoxon tests, and Dunn’s test do not. P-value adjustment can be added to their respective rstatix functions using the p.adjust.method = argument. When applying a post-hoc test, you may choose to compare every group to every other group, or you may only be interested in significant differences between specific groups (e.g., treatment groups vs. a control). This choice will be governed by your hypothesis. Statistical testing functions will typically default to comparing all groups to each other, but the comparisons can be defined using the comparisons = argument if you want to restrict the test to specific comparisons. It is important to decide at the beginning of your analysis which comparisons are relevant to your hypothesis because the number of pairwise tests performed in the post-hoc analysis will influence how much the resulting p-values will be adjusted for multiple hypothesis testing. Which test should I choose? Use the following flowchart to help guide your choice of statistical test to compare multiple groups: Figure 14: Created with BioRender.com Multi-Group Analysis Example To determine whether there are significant differences across all of our doses, the Friedman test is the most appropriate due to our matched experimental design and non-normally distributed data. The friedman_test() function is part of the rstatix package. This package also has many other helpful functions for statistical tests that are pipe/tidyverse friendly. To demonstrate how this test works, we will first perform the test on one variable: biomarker_data %&gt;% friedman_test(IL1B ~ Dose | Donor) ## # A tibble: 1 × 6 ## .y. n statistic df p method ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 IL1B 16 12.5 4 0.0140 Friedman test A p-value of 0.01 indicates that we can reject the null hypothesis that all of our data are drawn from groups that have equivalent means. Now, we can run a for loop similar to our two-group comparisons in TAME 2.0 Module 4.4, Two Group Comparisons and Visualizations to determine the overall p-value for each endpoint: # Create a vector with the names of the variables you want to run the test on endpoints &lt;- colnames(biomarker_data %&gt;% select(IL1B:VEGF)) # Create data frame to store results dose_friedmanres &lt;- data.frame() # Run for loop for (i in 1:length(endpoints)) { # Assign a name to the endpoint variable. endpoint &lt;- endpoints[i] # Run wilcox test and store in results data frame. res &lt;- biomarker_data %&gt;% friedman_test(as.formula(paste0(endpoint, &quot;~ Dose | Donor&quot;, sep = &quot;&quot;))) %&gt;% select(c(.y., p)) dose_friedmanres &lt;- rbind(dose_friedmanres, res) } # View results datatable(dose_friedmanres) These results demonstrate that all of our endpoints have significant overall differences across doses (p &lt; 0.05). To determine which pairwise comparisons are significant, we next need to apply a post-hoc test. We will apply a pairwise, paired Wilcoxon test due to our experimental design and data distribution, with the Benjamini-Hochberg (BH) correction for multiple testing: dose_wilcox_posthoc_IL1B &lt;- biomarker_data %&gt;% pairwise_wilcox_test(IL1B ~ Dose, paired = TRUE, p.adjust.method = &quot;BH&quot;) dose_wilcox_posthoc_IL1B ## # A tibble: 10 × 9 ## .y. group1 group2 n1 n2 statistic p p.adj p.adj.signif ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 IL1B 0 0.6 16 16 17 0.006 0.025 * ## 2 IL1B 0 1 16 16 11 0.002 0.017 * ## 3 IL1B 0 2 16 16 21 0.013 0.033 * ## 4 IL1B 0 4 16 16 18 0.008 0.025 * ## 5 IL1B 0.6 1 16 16 38 0.13 0.186 ns ## 6 IL1B 0.6 2 16 16 31 0.058 0.096 ns ## 7 IL1B 0.6 4 16 16 25 0.025 0.05 * ## 8 IL1B 1 2 16 16 43 0.211 0.234 ns ## 9 IL1B 1 4 16 16 40 0.159 0.199 ns ## 10 IL1B 2 4 16 16 62 0.782 0.782 ns Here, we can now see whether there are statistically significant differences in IL-1\\(\\beta\\) secretion between each of our doses. To generate pairwise comparison results for each of our inflammatory biomarkers, we can run a for loop similar to the one we ran for our overall test: # Create a vector with the names of the variables you want to run the test on endpoints &lt;- colnames(biomarker_data %&gt;% select(IL1B:VEGF)) # Create data frame to store results dose_wilcox_posthoc &lt;- data.frame() # Run for loop for (i in 1:length(endpoints)) { # Assign a name to the endpoint variable. endpoint &lt;- endpoints[i] # Run wilcox test and store in results data frame. res &lt;- biomarker_data %&gt;% pairwise_wilcox_test(as.formula(paste0(endpoint, &quot;~ Dose&quot;, sep = &quot;&quot;)), paired = TRUE, p.adjust.method = &quot;BH&quot;) dose_wilcox_posthoc &lt;- rbind(dose_wilcox_posthoc, res) } # View results datatable(dose_wilcox_posthoc) We now have a dataframe storing all of our pairwise comparison results. However, this is a lot to scroll through, making it hard to interpret. We can generate a publication-quality table by manipulating the table and joining it with the overall test data. dose_results_cleaned &lt;- dose_wilcox_posthoc %&gt;% unite(comparison, group1, group2, sep = &quot; vs. &quot;) %&gt;% select(c(.y., comparison, p.adj)) %&gt;% pivot_wider(id_cols = &quot;.y.&quot;, names_from = &quot;comparison&quot;, values_from = &quot;p.adj&quot;) %&gt;% left_join(dose_friedmanres, by = &quot;.y.&quot;) %&gt;% relocate(p, .after = &quot;.y.&quot;) %&gt;% rename(&quot;Variable&quot; = &quot;.y.&quot;, &quot;Overall&quot; = &quot;p&quot;) %&gt;% mutate(across(&#39;Overall&#39;:&#39;2 vs. 4&#39;, \\(x) format(x, scientific = TRUE, digits = 3))) datatable(dose_results_cleaned) To more easily see overall significance patterns, we could also make the same table but with significance stars instead of p-values by keeping the p.adjust.signif column instead of the p.adj column in our post-hoc test results dataframe: dose_results_cleaned_2 &lt;- dose_wilcox_posthoc %&gt;% unite(comparison, group1, group2, sep = &quot; vs. &quot;) %&gt;% select(c(.y., comparison, p.adj.signif)) %&gt;% pivot_wider(id_cols = &quot;.y.&quot;, names_from = &quot;comparison&quot;, values_from = &quot;p.adj.signif&quot;) %&gt;% left_join(dose_friedmanres, by = &quot;.y.&quot;) %&gt;% relocate(p, .after = &quot;.y.&quot;) %&gt;% rename(&quot;Variable&quot; = &quot;.y.&quot;, &quot;Overall&quot; = &quot;p&quot;) %&gt;% mutate(across(&#39;Overall&#39;:&#39;2 vs. 4&#39;, \\(x) format(x, scientific = TRUE, digits = 3))) datatable(dose_results_cleaned_2) Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question #1 : Are there significant differences in inflammatory biomarker concentrations between different doses of acrolein? Answer: Yes, there are significant differences in inflammatory biomarker concentrations between different doses of acrolein. The overall p-values for all biomarkers are significant. Within each biomarker, at least one pairwise comparison was significant between doses, with a majority of these significant comparisons being with the highest dose (4 ppm). Visualization of Multi-Group Statistical Results The statistical results we generated are a lot to digest in table format, so it can be helpful to graph the results. As our statistical testing becomes more complicated, so does the code used to generate results. The ggpubr package can perform statistical testing and overlay the results onto graphs for a specific set of tests, such as overall effects tests and unpaired t-tests or Wilcoxon tests. However, for tests that aren’t available by default, the package also contains the helpful stat_pvalue_manual() function that can be added to plots. This is what we will need to use to add the results of our pairwise, paired Wilcoxon test with BH correction, as there is no option for BH correction within the default function we might otherwise use (stat_compare_means()). We will first work through an example of this using one of our endpoints, and then we will demonstrate how to apply it to facet plotting. Single Plot We first need to format our existing statistical results so that they match the format that the function needs as input. Specifically, the dataframe needs to contain the following columns: group1 and group2: the groups being compared A column containing the results you want displayed (p, p.adj, or p.adj.signif typically) y.position, which tells the function where to plot the significance markers Our results dataframe for IL-1\\(\\beta\\) already contains our groups and p-values: datatable(dose_wilcox_posthoc_IL1B) We can add the position columns using the function add_xy_position(): dose_wilcox_posthoc_IL1B &lt;- dose_wilcox_posthoc_IL1B %&gt;% add_xy_position(x = &quot;Dose&quot;, step.increase = 2) datatable(dose_wilcox_posthoc_IL1B) Now, we are ready to make a graph of our results. We will use stat_friedman_test() to add our overall p-value and stat_pvalue_manual() to add our pairwise values. # Set graphing theme theme_set(theme_bw()) # Make plot ggplot(biomarker_data, aes(x = Dose, y = IL1B)) + geom_boxplot(aes(fill = Dose), outlier.shape = NA) + scale_fill_manual(values = c(&quot;#BFBFBF&quot;, &quot;#D5A298&quot;, &quot;#E38273&quot;, &quot;#EB5F4E&quot;, &quot;#EE2B2B&quot;)) + geom_jitter(size = 3, position = position_jitter(0.15)) + stat_friedman_test(wid = &quot;Donor&quot;, p.adjust.method = &quot;none&quot;, label = &quot;p = {p.format}&quot;, label.x.npc = &quot;left&quot;, label.y = 9.5, hjust = 0.5, size = 6) + stat_pvalue_manual(dose_wilcox_posthoc_IL1B, label = &quot;p.adj.signif&quot;, size = 12, hide.ns = TRUE) + ylim(2.5, 10) + labs(y = &quot;Log2(IL-1\\u03B2 (pg/mL))&quot;, x = &quot;Acrolein (ppm)&quot;) + theme(legend.position = &quot;none&quot;, axis.title = element_text(color = &quot;black&quot;, size = 15), axis.title.x = element_text(vjust = -0.75), axis.title.y = element_text(vjust = 2), axis.text = element_text(color = &quot;black&quot;, size = 12)) However, to make room for all of our annotations, our data become compressed, and it makes it difficult to see our data. Although presentation of statistical results is largely a matter of personal preference, we could clean up this plot by making our annotations appear on top of the bars, with indication in the figure legend that the comparison is with a specific dose. We will do this by: Filtering our results to those that are significant. Changing the symbol for comparisons that are not to the 0 dose. Layering this text onto the plot with geom_text() rather than stat_pvalue_manual(). First, let’s filter our results to significant results and change the symbol for comparisons that are not to the 0 dose to a caret (^) instead of stars. We can do this by creating a new column called label that keeps the existing label if group1 is 0, and if not, changes the label to a caret of the same length. We then use the summarize function to paste the labels for each of the groups together, resulting in a final dataframe containing our annotations for our plot. dose_wilcox_posthoc_IL1B_2 &lt;- dose_wilcox_posthoc_IL1B %&gt;% # Filter results to those that are significant filter(p.adj &lt;= 0.05) %&gt;% # Make new symbol mutate(label = ifelse(group1 == &quot;0&quot;, p.adj.signif, strrep(&quot;^&quot;, nchar(p.adj.signif)))) %&gt;% # Select only the columns we need select(c(group1, group2, label)) %&gt;% # Combine symbols for the same group group_by(group2) %&gt;% summarise(label = paste(label, collapse=&quot; &quot;)) %&gt;% # Remove duplicate row distinct(group2, .keep_all = TRUE) %&gt;% # Rename group2 to dose rename(&quot;Dose&quot; = &quot;group2&quot;) dose_wilcox_posthoc_IL1B_2 ## # A tibble: 4 × 2 ## Dose label ## &lt;chr&gt; &lt;chr&gt; ## 1 0.6 * ## 2 1 * ## 3 2 * ## 4 4 * ^ Then, we can use the same code as for our previous plot, but instead of using stat_pvalue_manual(), we will use geom_text() in combination with the dataframe we just created. ggplot(biomarker_data, aes(x = Dose, y = IL1B)) + geom_boxplot(aes(fill = Dose), outlier.shape = NA) + scale_fill_manual(values = c(&quot;#BFBFBF&quot;, &quot;#D5A298&quot;, &quot;#E38273&quot;, &quot;#EB5F4E&quot;, &quot;#EE2B2B&quot;)) + geom_jitter(size = 3, position = position_jitter(0.15)) + stat_friedman_test(wid = &quot;Donor&quot;, p.adjust.method = &quot;none&quot;, label = &quot;p = {p.format}&quot;, label.x.npc = &quot;left&quot;, label.y = 4.85, hjust = 0.5, size = 6) + geom_text(data = dose_wilcox_posthoc_IL1B_2, aes(x = Dose, y = 4.5, label = paste0(label)), size = 10, hjust = 0.5) + ylim(2.5, 5) + labs(y = &quot;Log2(IL-1\\u03B2 (pg/mL))&quot;, x = &quot;Acrolein (ppm)&quot;) + theme(legend.position = &quot;none&quot;, axis.title = element_text(color = &quot;black&quot;, size = 15), axis.title.x = element_text(vjust = -0.75), axis.title.y = element_text(vjust = 2), axis.text = element_text(color = &quot;black&quot;, size = 12)) An appropriate title for this figure could be: “Figure X. Exposure to 0.6-4 ppm acrolein increases IL-1\\(\\beta\\) secretion in primary human bronchial epithelial cells. Groups were compared using the Friedman test to obtain overall p-value and Wilcoxon signed rank test for post-hoc testing. * p &lt; 0.05 in comparison with 0 ppm, ^ p &lt; 0.05 in comparison with 0.6 ppm, n = 16 per group (paired).” Faceted Plot Ideally, we would extend this sort of graphical approach to our faceted plot showing all of our endpoints. However, there are quite a few statistically significant comparisons to graph, including comparisons that are significant between different pairs of doses (not just back to the control). While we could attempt to graph all of them, ultimately, this will lead to a cluttered figure panel. When thinking about how to simplify our plots, some options are: Instead of using the number of symbols to represent p-values, we could use a single symbol to represent any comparison with a p-value with at least p &lt; 0.05, and that symbol could be different depending on which group the significance is in comparison to. Symbols can be difficult to parse in R, so we could use letters or even the group names above the column of interest. For example, if the concentration of an endpoint at 2 ppm was significant in comparison with both 0 and 0.6 ppm, we could annotate “0, 0.6” above the 2 ppm column, or we could choose a letter (“a, b”) or symbol (“*, ^“) to convey these results. If the pattern is the same across many of the endpoints measured, we could graph a subset of the endpoints with the most notable data trends or the most biological meaning for the main body of the manuscript, with data for additional endpoints referred to in the text and shown in the supplemental figures or tables. If most of the significant comparisons are back to the control group, we could choose to only show comparisons with the control group, with textual description of the other significant comparisons and indication that those specific p-values can be viewed in the supplemental table of results. Which approach you decide to take (or maybe another approach altogether) is a matter of both personal preference and your specific study goals. You may also decide that it is important to you to show all significant comparisons, which will require more careful formatting of the plots to ensure that all text and annotations are legible. For this module, we will proceed with option #3 because many of our comparisons to the control dose (0) are significant, and we have enough groups that there likely will not be space to annotate all of them above our data. We will take similar steps here that we did when constructing our single endpoint graph, with a couple of small differences. Specifically, we need to: Create a dataframe of labels/annotations as we did above, but now filtered to only significant comparisons with the 0 group. Add to the label/annotation dataframe what we want the y position for each of the labels to be, which will be different for each endpoint. First, let’s create our annotations dataframe. We will start with the results dataframe from our posthoc testing: datatable(dose_wilcox_posthoc) dose_wilcox_posthoc_forgraph &lt;- dose_wilcox_posthoc %&gt;% filter(p.adj &lt;= 0.05) %&gt;% # Filter for only comparisons to 0 filter(group1 == &quot;0&quot;) %&gt;% # Rename columns rename(&quot;variable&quot; = &quot;.y.&quot;, &quot;Dose&quot; = &quot;group2&quot;) datatable(dose_wilcox_posthoc_forgraph) The Dose column will be used to tell ggplot2 where to place the annotations on the x axis, but we need to also specify where to add the annotations on the y axis. This will be different for each variable because each variable is on a different scale. We can approach this by computing the maximum value of each variable, then increasing that by 20% to add some space on top of the points. sig_labs_y &lt;- biomarker_data %&gt;% summarise(across(IL1B:VEGF, \\(x) max(x))) %&gt;% t() %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;variable&quot;) %&gt;% rename(&quot;y_pos&quot; = &quot;V1&quot;) %&gt;% mutate(y_pos = y_pos*1.2) sig_labs_y ## variable y_pos ## 1 IL1B 4.967953 ## 2 IL6 14.074658 ## 3 IL8 20.752105 ## 4 IL10 3.224473 ## 5 TNFa 4.478723 ## 6 VEGF 14.148027 Then, we can join these data to our labeling dataframe to complete what we need to make the annotations. dose_wilcox_posthoc_forgraph &lt;- dose_wilcox_posthoc_forgraph %&gt;% left_join(sig_labs_y, by = &quot;variable&quot;) Now, it’s time to graph! Keep in mind that although the plotting script can get long and unweildy, each line is just a new instruction to ggplot about a formatting element or an additional layer to add to the graph. # Pivot data longer biomarker_data_long &lt;- biomarker_data %&gt;% pivot_longer(-c(Donor, Dose), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) # Create clean labels for the graph titles new_labels &lt;- c(&quot;IL10&quot; = &quot;IL-10&quot;, &quot;IL1B&quot; = &quot;IL-1\\u03B2 &quot;, &quot;IL6&quot; = &quot;IL-6&quot;, &quot;IL8&quot; = &quot;IL-8&quot;, &quot;TNFa&quot; = &quot;TNF-\\u03b1&quot;, &quot;VEGF&quot; = &quot;VEGF&quot;) # Make graph ggplot(biomarker_data_long, aes(x = Dose, y = value)) + # outlier.shape = NA removes outliers geom_boxplot(aes(fill = Dose), outlier.shape = NA) + # Changing box plot colors scale_fill_manual(values = c(&quot;#BFBFBF&quot;, &quot;#D5A298&quot;, &quot;#E38273&quot;, &quot;#EB5F4E&quot;, &quot;#EE2B2B&quot;)) + geom_jitter(size = 1.5, position = position_jitter(0.15)) + # Adding a p value from Friedman test stat_friedman_test(wid = &quot;Donor&quot;, p.adjust.method = &quot;none&quot;, label = &quot;p = {p.format}&quot;, label.x.npc = &quot;left&quot;, vjust = -3.5, hjust = 0.1, size = 3.5) + # Add label geom_text(data = dose_wilcox_posthoc_forgraph, aes(x = Dose, y = y_pos, label = p.adj.signif, size = 5, hjust = 0.5)) + # Adding padding y axis scale_y_continuous(expand = expansion(mult = c(0.1, 0.6))) + # Changing y axis label ylab(expression(Log[2]*&quot;(Concentration (pg/ml))&quot;)) + # Changing x axis label xlab(&quot;Acrolein (ppm)&quot;) + # Faceting by each biomarker facet_wrap(~ variable, nrow = 2, scales = &quot;free_y&quot;, labeller = labeller(variable = new_labels)) + # Removing legend theme(legend.position = &quot;none&quot;, axis.title = element_text(color = &quot;black&quot;, size = 12), axis.title.x = element_text(vjust = -0.75), axis.title.y = element_text(vjust = 2), axis.text = element_text(color = &quot;black&quot;, size = 10), strip.text = element_text(size = 12, face = &quot;bold&quot;)) An appropriate title for this figure could be: “Figure X. Exposure to acrolein increases secretion of proinflammatory biomarkers in primary human bronchial epithelial cells. Groups were compared using the Friedman test to obtain overall p-value and Wilcoxon signed rank test for post-hoc testing. * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001, **** p &lt; 0.0001 for comparison with control. For additional significant comparisons, see Supplemental Table X. n = 16 per group (paired).” Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2 : Do TNF-\\(\\alpha\\) concentrations significantly increase with increasing dose of acrolein? Answer: Yes, TNF-\\(\\alpha\\) concentrations significantly increase with increasing dose of acrolein, which we were able to visualize, along with other mediators, in our facet plot. Concluding Remarks In this module, we introduced common multi-group statistical tests, including both overall effects tests and post-hoc testing. We applied these tests to our example dataset and demonstrated how to produce publication-quality tables and figures of our results. Implementing a workflow such as this enables efficient analysis of wet-bench generated data and customization of output figures and tables suited to your personal preferences. Additional Resources STHDA: How to Add P-Values and Significance Levels to ggplots using ggpubr Adding p-values with ggprism Overview of ggsignif Test Your Knowledge Functional endpoints from these cultures were also measured. These endpoints were: 1) Membrane Permeability (MemPerm), 2) Trans-Epithelial Electrical Resistance (TEER), 3) Ciliary Beat Frequency (CBF), and 4) Expression of Mucin (MUC5AC). These data were already processed and tested for normality (see Test Your Knowledge for TAME 2.0 Module 4.2 Data Import, Processing, and Summary Statistics), with results indicating that two of the endpoints are normally distributed and two non-normally distributed. Use the same processes demonstrated in this module and the provided data (“Module4_5_TYKInput.xlsx” (functional data)) to run analyses and make a publication-quality figure panel and table to answer the following question: Are there significant differences in functional endpoints between cells treated with different concentrations of acrolein? For an extra challenge, try also making your faceted plot in the style of option #1 above, with different symbols, letters, or group names above columns to indicate which group that column in significant in comparison with. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
