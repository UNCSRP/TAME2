# (PART\*) Chapter 7 Environmental Health Database Mining {-}

# 7.1 Comparative Toxicogenomics Database

This training module was developed by Lauren E. Koval, Kyle R. Roell, and Julia E. Rager. 

All input files (script, data, and figures) can be downloaded from the [UNC-SRP TAME2 GitHub website](https://github.com/UNCSRP/TAME2).

```{r , include=FALSE}
#set default values for R Markdown "knitting" to HTML, Word, or PDF
knitr::opts_chunk$set(echo = TRUE) #print code chunks
```


## Introduction to Training Module

The Comparative Toxicogenomics Database (CTD) is a publicly available, online database that provides manually curated information about chemical-gene/protein interactions, chemical-disease and gene-disease relationships. CTD also recently incorporated curation of exposure data and chemical-phenotype relationships.

CTD is located at: http://ctdbase.org/. Here is a screenshot of the CTD homepage (as of August 5, 2021):
```{r, echo=FALSE, fig.align='center'} 
#knitr::include_graphics("_book/TAME_Toolkit_files/figure-html/Module3_1_CTD_homepage.jpg")
knitr::include_graphics("Module7_1_Input/Module7_1_Image1.jpg")
```

In this module, we will be using CTD to access and download data to perform data organization and analysis as an applications-based example towards environmental health research. This activity represents a demonstration of basic data manipulation, filtering, and organization steps in R, while highlighting the utility of CTD to identify novel genomic/epigenomic relationships to environmental exposures. Example visualizations are also included in this training module's script, providing visualizations of gene list comparison results.



### Training Module's Environmental Health Questions
This training module was specifically developed to answer the following environmental health questions:

(1) Which genes show altered expression in response to arsenic exposure?
(2) Of the genes showing altered expression, which may be under epigenetic control?



### Script Preparations

#### Cleaning the global environment
```{r}
rm(list=ls())
```


#### Installing required R packages
If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you.
```{r, results=FALSE, message=FALSE}
if (!requireNamespace("tidyverse"))
  install.packages("tidyverse")
if (!requireNamespace("VennDiagram"))
install.packages("VennDiagram")
if (!requireNamespace("grid"))
install.packages("grid")
```


#### Loading R packages required for this session
```{r, results=FALSE, message=FALSE}
library(tidyverse)
library(VennDiagram)
library(grid)
```


#### Set your working directory
```{r, eval=FALSE, echo=TRUE}
setwd("/filepath to where your input files are")
```

 

## CTD Data in R

### Organizing Example Dataset from CTD

CTD requires manual querying of its database, outside of the R scripting environment. Because of this, let's first manually pull the data we need for this example analysis. We can answer both of the example questions by pulling all chemical-gene relationship data for arsenic, which we can do by following the below steps:

Navigate to the main CTD website: http://ctdbase.org/.

Select at the top, 'Search' -> 'Chemical-Gene Interactions'. 
<br>
```{r, echo=FALSE, fig.align='center'} 
knitr::include_graphics("Module7_1_Input/Module7_1_Image2.jpg")
```

<br>
  
Select to query all chemical-gene interaction data for arsenic.
<br>
```{r, echo=FALSE, fig.align='center'} 
knitr::include_graphics("Module7_1_Input//Module7_1_Image3.jpg")
```
<br>

  
Note that there are lots of results, represented by many many rows of data! Scroll to the bottom of the webpage and select to download as 'CSV'.
<br>
```{r, echo=FALSE, fig.align='center'} 
knitr::include_graphics("Module7_1_Input//Module7_1_Image4.jpg")
```
  
<br>
    
This is the file that we can now use to import into the R environment and analyze!
Note that the data pulled here represent data available on August 1, 2021



### Loading the Example CTD Dataset into R



Read in the csv file of the results from CTD query:
```{r, results=FALSE, message=FALSE}
ctd = read_csv("Module7_1_Input/Module7_1_InputData1.csv")
```



Let's first see how many rows and columns of data this file contains:
```{r}
dim(ctd)
```
This dataset includes 6280 observations (represented by rows) linking arsenic exposure to gene-level alterations
With information spanning across 9 columns



Let's also see what kind of data are organized within the columns:
```{r}
colnames(ctd)
```


```{r}
# Viewing the first five rows of data, across all 9 columns
ctd[1:9,1:5] 
```




#### Filtering data for genes with altered expression



To identify genes with altered expression in association with arsenic, we can leverage the results of our CTD query and filter this dataset to include only the rows that contain the term "expression" in the "Interaction Actions" column.
```{r}
exp_filt = ctd %>% filter(grepl("expression", `Interaction Actions`))
```

We now have 2586 observations, representing instances of arsenic exposure causing a changes in a target gene's expression levels.
```{r}
dim(exp_filt)
```



Let's see how many unique genes this represents:
```{r}
length(unique(exp_filt$`Gene Symbol`))
```
This reflects 1878 unique genes that show altered expression in association with arsenic.



Let's make a separate dataframe that includes only the unique genes, based on the "Gene Symbol" column.
```{r}
exp_genes = exp_filt %>% distinct(`Gene Symbol`, .keep_all=TRUE)

# Removing columns besides gene identifier
exp_genes = exp_genes[,4] 

# Viewing the first 10 genes listed
exp_genes[1:10,] 
```
This now provides us a list of 1878 genes showing altered expression in association with arsenic.


##### Technical notes on running the distinct function within tidyverse:
By default, the distinct function keeps the first instance of a duplicated value. This does have implications if the rest of the values in the rows differ. You will only retain the data associated with the first instance of the duplicated value (which is why we just retained the gene column here). It may be useful to first find the rows with the duplicate value and verify that results are as you would expect before removing observations. For example, in this dataset, expression levels can increase or decrease. If you were looking for just increases in expression, and there were genes that showed increased and decreased expression across different samples, using the distinct function just on "Gene Symbol" would not give you the results you wanted. If the first instance of the gene symbol noted decreased expression, that gene would not be returned in the results even though it might be one you would want. For this example case, we only care about expression change, regardless of direction, so this is not an issue. The distinct function can also take multiple columns to consider jointly as the value to check for duplicates if you are concerned about this.

<br>

### Answer to Environmental Health Question 1

:::question
*With this, we can answer **Environmental Health Question 1***:
Which genes show altered expression in response to arsenic exposure?
:::
:::answer
**Answer**: This list of 1878 genes have published evidence supporting their altered expression levels associated with arsenic exposure.
:::

<br>

## Identifying Genes Under Epigenetic Control


For this dataset, let's focus on gene-level methylation as a marker of epigenetic regulation. Let's return to our main dataframe, representing the results of the CTD query, and filter these results for only the rows that contain the term "methylation" in the "Interaction Actions" column.
```{r}
met_filt = ctd %>% filter(grepl("methylation",`Interaction Actions`))
```

We now have 3211 observations, representing instances of arsenic exposure causing a changes in a target gene's methylation levels.
```{r}
dim(met_filt)
```


Let's see how many unique genes this represents.
```{r}
length(unique(met_filt$`Gene Symbol`))
```
This reflects 3142 unique genes that show altered methylation in association with arsenic



Let's make a separate dataframe that includes only the unique genes, based on the "Gene Symbol" column.
```{r}
met_genes = met_filt %>% distinct(`Gene Symbol`, .keep_all=TRUE)

# Removing columns besides gene identifier
met_genes = met_genes[,4] 
```
This now provides us a list of 3142 genes showing altered methylation in association with arsenic.



With this list of genes with altered methylation, we can now compare it to previous list of genes with altered expression to yeild our final list of genes of interest. To achieve this last step, we present two different methods to carry out list comparisons below.



#### Method 1 for list comparisons: Merging



Merge the expression results with the methylation resuts on the Gene Symbol column found in both datasets.
```{r}
merge_df = merge(exp_genes, met_genes, by = "Gene Symbol")
```
We end up with 315 rows reflecting the 315 genes that show altered expression and altered methylation

Let's view these genes:
```{r}
merge_df[1:315,]
```



### Answer to Environmental Health Question 2

:::question
*With this, we can answer **Environmental Health Question 2***:
Of the genes showing altered expression, which may be under epigenetic control?
:::
:::answer
**Answer**:  We identified 315 genes with altered expression resulting from arsenic exposure, that also demonstrate epigenetic modifications from arsenic. These genes include many high interest molecules involved in regulating cell health, including several cyclin dependent kinases (e.g., CDK2, CDK4, CDK5, CDK6), molecules involved in oxidative stress (e.g., FOSB, NOS2), and cytokines involved in inflammatory response pathways (e.g., IFNG, IL10, IL16, IL1R1, IR1RAP, TGFB1, TGFB3).
:::



#### Method 2 for list comparisons: Intersection
For further training, shown here is another method for pulling this list of interest, through the use of the 'intersection' function.



Obtain a list of the overlapping genes in the overall expression results and the methylation results.
```{r}
inxn = intersect(exp_filt$`Gene Symbol`,met_filt$`Gene Symbol`)
```
Again, we end up with a list of 315 unique genes that show altered expression and altered methylation.



This list can be viewed on its own or converted to a dataframe (df).
```{r}
inxn_df = data.frame(genes=inxn)
```



This list can also be conveniently used to filter the original query results.
```{r}
inxn_df_all_data = ctd %>% filter(`Gene Symbol` %in% inxn)
```



Note that in this last case, the same 315 genes are present, but this time the results contain all records from the original query results, hence the 875 rows (875 records observations reflecting the 315 genes).
```{r}
summary(unique(sort(inxn_df_all_data$`Gene Symbol`))==sort(merge_df$`Gene Symbol`))
dim(inxn_df_all_data)
```


Visually we can represent this as a Venn diagram. Here, we use the ["VennDiagram"](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-35) R package. 

```{r venn, message=F, eval=F, fig.align = "center"}
# Use the data we previously used for intersection in the venn diagram function
venn.plt = venn.diagram(
  x = list(exp_filt$`Gene Symbol`, met_filt$`Gene Symbol`),
  category.names = c("Altered Expression" , "Altered Methylation"),
  filename = NULL,
  
  # Change font size, type, and position
  cat.cex = 1.15,
  cat.fontface = "bold",
  cat.default.pos = "outer",
  cat.pos = c(-27, 27),
    cat.dist = c(0.055, 0.055),
  
  # Change color of ovals
  col=c("#440154ff", '#21908dff'),
  fill = c(alpha("#440154ff",0.3), alpha('#21908dff',0.3)),
)

```

```{r print-venn, fig.width = 7, fig.height = 7, echo=F, message=F, fig.align = "center"}
# Use the data we previously used for intersection in the venn diagram function
venn.plt = venn.diagram(
  x = list(exp_filt$`Gene Symbol`, met_filt$`Gene Symbol`),
  category.names = c("Altered Expression" , "Altered Methylation"),
  filename = NULL,
  output=F,
  
  # Change font size, type, and position
  cat.cex = 1.15,
  cat.fontface = "bold",
  cat.default.pos = "outer",
  cat.pos = c(-27, 27),
  cat.dist = c(0.055, 0.055),
  
  # Change color of ovals
  col=c("#440154ff", '#21908dff'),
  fill = c(alpha("#440154ff",0.3), alpha('#21908dff',0.3)),
)

grid::grid.draw(venn.plt)
```


## Concluding Remarks
In conclusion, we identified 315 genes that show altered expression in response to arsenic exposure that may be under epigenetic control. These genes represent critical mediators of oxidative stress and inflammation, among other important cellular processes. Results yielded an important list of genes representing potential targets for further evaluation, to better understand mechanism of environmental exposure-induced disease. Together, this example highlights the utility of CTD to address environmental health research questions.

For more information, see the recently updated primary CTD publication:  

+ Davis AP, Grondin CJ, Johnson RJ, Sciaky D, Wiegers J, Wiegers TC, Mattingly CJ. Comparative Toxicogenomics Database (CTD): update 2021. Nucleic Acids Res. 2021 Jan 8;49(D1):D1138-D1143. PMID: [33068428](https://pubmed.ncbi.nlm.nih.gov/33068428/).

Additional case studies relevant to environmental health research include the following:

+ An example publication leveraging CTD findings to identify mechanisms of metals-induced birth defects: Ahir BK, Sanders AP, Rager JE, Fry RC. Systems biology and birth defects prevention: blockade of the glucocorticoid receptor prevents arsenic-induced birth defects. Environ Health Perspect. 2013 Mar;121(3):332-8. PMID: [23458687](https://pubmed.ncbi.nlm.nih.gov/23458687/).  

+ An example publication leveraging CTD to help fill data gaps on data poor chemicals, in combination with ToxCast/Tox21 data streams, to elucidate environmental influences on disease pathways: Kosnik MB, Planchart A, Marvel SW, Reif DM, Mattingly CJ. Integration of curated and high-throughput screening data to elucidate environmental influences on disease pathways. Comput Toxicol. 2019 Nov;12:100094. PMID: [31453412](https://pubmed.ncbi.nlm.nih.gov/31453412/).  

+ An example publication leveraging CTD to extract chemical-disease relationships used to derive new chemical risk values, with the goal of prioritizing connections between environmental factors, genetic variants, and human diseases: Kosnik MB, Reif DM. Determination of chemical-disease risk values to prioritize connections between environmental factors, genetic variants, and human diseases. Toxicol Appl Pharmacol. 2019 Sep 15;379:114674. [PMID: 31323264](https://pubmed.ncbi.nlm.nih.gov/31323264/).




<label class="tykfont">
Test Your Knowledge 
</label>

:::tyk

Using the same dataset from this module (available at the GitHub site and as Module7_1_TYKInput.csv): 

1. Filter the data using the grepl function to look at only those observations that specifically decrease the target gene's "expression" level. How many observations are there?
2. Similarly, filter the data to identify how many observations there are where the target gene's "expression" level is simply "affected". Create a venn diagram to help visualize any overlap between these two filtered datasets.

:::

# 7.2 Gene Expression Omnibus

This training module was developed by Kyle R. Roell and Julia E. Rager.

All input files (script, data, and figures) can be downloaded from the [UNC-SRP TAME2 GitHub website](https://github.com/UNCSRP/TAME2).


```{r , include=FALSE}
# Set default values for R Markdown "knitting" to HTML, Word, or PDF
knitr::opts_chunk$set(echo = TRUE) #print code chunks
```

## Introduction to Training Module

[GEO](https://www.ncbi.nlm.nih.gov/geo/) is a publicly available database repository of high-throughput gene expression data and hybridization arrays, chips, and microarrays that span genome-wide endpoints of genomics, transcriptomics, and epigenomics. This training module specifically guides trainees through the loading of required packages and data, including the manual upload of GEO data as well as the upload/organization of data leveraging the [GEOquery package](https://www.bioconductor.org/packages/release/bioc/html/GEOquery.html). Data are then further organized and combined with gene annotation information through the merging of platform annotation files. Example visualizations are then produced, including boxplots to evaluate the overall distribution of expression data across samples, as well as heat map visualizations that compare unscaled versus scaled gene expression values. Statistical analyses are then included to identify which genes are significantly altered in expression upon exposure to formaldehyde. Together, this training module serves as a simple example showing methods to access and download GEO data and to perform data organization, analysis, and visualization tasks through applications-based questions.


## Introduction to GEO

The GEO repository is organized and managed by the [The National Center for Biotechnology Information (NCBI)](https://www.ncbi.nlm.nih.gov/), which seeks to advance science and health by providing access to biomedical and genomic information. The three [overall goals](https://www.ncbi.nlm.nih.gov/geo/info/overview.html) of GEO are to: (1) Provide a robust, versatile database in which to efficiently store high-throughput functional genomic data, (2) Offer simple submission procedures and formats that support complete and well-annotated data deposits from the research community, and (3) Provide user-friendly mechanisms that allow users to query, locate, review and download studies and gene expression profiles of interest.

Of high relevance to environmental health, data organized within GEO can be pulled and analyzed to address new environmental health questions, leveraging previously generated data. For example, we have pulled gene expression data from acute myeloid leukemia patients and re-analyzed these data to elucidate new mechanisms of epigenetically-regulated networks involved in cancer, that in turn, may be modified by environmental insults, as previously published in [Rager et al. 2012](https://pubmed.ncbi.nlm.nih.gov/22754483/). We have also pulled and analyzed gene expression data from published studies evaluating toxicity resulting from hexavalent chromium exposure, to further substantiate the role of epigenetic mediators in hexavelent chromium-induced carcinogenesis (see [Rager et al. 2019](https://pubmed.ncbi.nlm.nih.gov/30690063/)). This training exercise leverages an additional dataset that we published and deposited through GEO to evaluate the effects of formaldehyde inhalation exposure, as detailed below.


## Introduction to Example Data

In this training module, data will be pulled from the published GEO dataset recorded through the online series [GSE42394](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE42394). This series represents Affymetrix rat genome-wide microarray data generated from our previous study, aimed at evaluating the transcriptomic effects of formaldehyde across three tissues: the nose, blood, and bone marrow. For the purposes of this training module, we will focus on evaluating gene expression profiles from nasal samples after 7 days of exposure, collected from rats exposed to 2 ppm formaldehyde via inhalation. These findings, in addition to other epigenomic endpoint measures, have been previously published (see [Rager et al. 2014](https://pubmed.ncbi.nlm.nih.gov/24304932/)).


### Training Module's Environmental Health Questions

This training module was specifically developed to answer the following environmental health questions:

(1) What kind of molecular identifiers are commonly used in microarray-based -omics technologies?
(2) How can we convert platform-specific molecular identifiers used in -omics study designs to gene-level information?
(3)	Why do we often scale gene expression signatures prior to heat map visualizations?
(4) What genes are altered in expression by formaldehyde inhalation exposure?
(5) What are the potential biological consequences of these gene-level perturbations?



### Script Preparations

#### Cleaning the global environment
```{r}
rm(list=ls())
```


#### Installing required R packages
If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you
```{r, results=FALSE, message=FALSE}
if (!requireNamespace("tidyverse"))
  install.packages("tidyverse")
if (!requireNamespace("reshape2"))
    install.packages("reshape2")

# GEOquery, this will install BiocManager if you don't have it installed
if (!requireNamespace("BiocManager"))
  install.packages("BiocManager")
BiocManager::install("GEOquery")
```


#### Loading R packages required for this session
```{r, results=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(reshape2)
library(GEOquery)
```
For more information on the **tidyverse package**, see its associated [CRAN webpage](https://cran.r-project.org/web/packages/tidyverse/index.html), primary [webpage](https://www.tidyverse.org/packages/), and peer-reviewed [article released in 2018](https://onlinelibrary.wiley.com/doi/10.1002/sdr.1600).

For more information on the **reshape2 package**, see its associated [CRAN webpage](https://cran.r-project.org/web/packages/reshape2/index.html), [R Documentation](https://www.rdocumentation.org/packages/reshape2/versions/1.4.4), and [helpful website](https://seananderson.ca/2013/10/19/reshape/) providing an introduction to the reshape2 package.

For more information on the **GEOquery package**, see its associated [Bioconductor website](https://www.bioconductor.org/packages/release/bioc/html/GEOquery.html) and [R Documentation file](https://www.rdocumentation.org/packages/GEOquery/versions/2.38.4).



#### Set your working directory
```{r, eval=FALSE, echo=TRUE}
setwd("/filepath to where your input files are")
```

```{r, echo=FALSE}
#setwd("/Users/juliarager/IEHS Dropbox/Julia Rager/Research Projects/1_SRP/4_DMAC/DMAC Training Modules/Training_Modules/3_Chapter 3/3_2_Database_GEO/Clean_Files/")
```

## GEO Data in R

Let's start by loading the GEO dataset needed for this training module. As explained in the introduction, this module walks through two methods of uploading GEO data: manual option vs automatic option using the GEOquery package. These two methods are detailed below.

### 1. Manually Downloading and Uploading GEO Files

In this first method, we will navigate to the dataset within the GEO website, manually download its associated text data file, save it in our working directory, and then upload it into our global environment in R.

For the purposes of this training exercise, we manually downloaded the GEO series matrix file from the GEO series webpage, located at: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE42394. The specific file that was downloaded was noted as "GSE42394_series_matrix.txt", pulled by clicking on the link indicated by the red arrow from the GEO series webpage:

```{r, echo=FALSE, fig.width=4, fig.height=5, fig.align = "center"}
knitr::include_graphics("Module7_2_Input/Module7_2_Image1.png")
```


For simplicity, we also have already pre-filtered this file for the samples we are interested in, focusing on the rat nasal gene expression data after 7 days of exposure to gaseous formaldehyde. This filtered file was saved as "GSE42394_series_matrix_filtered.txt", then renamed "Module7_2_InputData1.txt" for use in this module.


At this point, we can simply read in this pre-filtered text file for the purposes of this training module
```{r}
geodata_manual = read.table(file="Module7_2_Input/Module7_2_InputData1.txt",
                             header=T)
```


Because this is a manual approach, we have to also manually define the treated and untreated samples (based on manually opening the surrounding metadata from the GEO webpage)

Manually defining treated and untreated for these samples of interest:
```{r}
exposed_manual = c("GSM1150940", "GSM1150941", "GSM1150942")
unexposed_manual = c("GSM1150937", "GSM1150938", "GSM1150939")
```



### 2. Loading and Organizing GEO Files through the GEOquery Package
In this second method, we will leverage the GEOquery package, which allows for easier downloading and reading in of data from GEO without having to manually download raw text files, and manually assign sample attributes (e.g., exposed vs unexposed). This package is set-up to automatically merge sample information from GEO metadata files with raw genome-wide datasets.


Let's first use the getGEO function (from the GEOquery package) to load data from our series matrix ("GSE42394_series_matrix.txt", renamed "Module7_2_InputData2.txt" for use in this module). *Note that this line of code may take a couple of minutes to run.*
```{r, message=FALSE}
geo.getGEO.data = getGEO(filename='Module7_2_Input/Module7_2_InputData2.txt')
```



One of the reasons the getGEO package is so helpful is that we can automatically link a dataset with nicely organized sample information using the `pData()` function.
```{r}
sampleInfo = pData(geo.getGEO.data)
```


Let's view this sample information / metadata file, first by viewing what the column headers are.
```{r}
colnames(sampleInfo)
```

Then viewing the first five columns.
```{r}
sampleInfo[1:10,1:5]
```

This shows that each sample is provided with a unique number starting with "GSM", and these are described by information summarized in the "title" column. We can also see that these data were made public on Jan 7, 2014.


Let's view the next five columns.
```{r}
sampleInfo[1:10,6:10]
```

We can see that information is provided here surrounding the type of sample that was analyzed (i.e., RNA), more information on the collected samples within the column `source_name_ch1`, and the organism (rat) is provided in the `organism_ch1` column.


More detailed metadata information is provided throughout this file, as seen when viewing the column headers above.


#### Defining samples

Now, we can use this information to define the samples we want to analyze. Note that this is the same step we did manually above.

In this training exercise, we are focusing on responses in the nose, so we can easily filter for cell type = Nasal epithelial cells (specifically in the `cell type:ch1` variable). We are also focusing on responses collected after 7 days of exposure, which we can filter for using time = 7 day (specifically in the `time:ch1` variable). We will also define exposed and unexposed samples using the variable `treatment:ch1`.

First, let's subset the sampleInfo dataframe to just keep the samples we're interested in
```{r}
# Define a vector variable (here we call it 'keep') that will store rows we want to keep
keep = rownames(sampleInfo[which(sampleInfo$`cell type:ch1`=="Nasal epithelial cells" 
                                  & sampleInfo$`time:ch1`=="7 day"),])

# Then subset the sample info for just those samples we defined in keep variable
sampleInfo = sampleInfo[keep,]
```


Next, we can pull the exposed and unexposed animal IDs. Let's first see how these are labeled within the `treatment:ch1` variable.
```{r}
unique(sampleInfo$`treatment:ch1`)
```


And then search for the rows of data, pulling the sample animal IDs (which are in the variable `geo_accession`).
```{r}
exposedIDs = sampleInfo[which(sampleInfo$`treatment:ch1`=="2 ppm formaldehyde"), 
                          "geo_accession"]
unexposedIDs = sampleInfo[which(sampleInfo$`treatment:ch1`=="unexposed"), 
                            "geo_accession"]
```


The next step is to pull the expression data we want to use in our analyses. The GEOquery function, `exprs()`, allows us to easily pull these data. Here, we can pull the data we're interested in using the `exprs()` function, while defining the data we want to pull based off our previously generated 'keep' vector.
```{r}
# As a reminder, this is what the 'keep' vector includes 
# (i.e., animal IDs that we're interested in)
keep
```

```{r}
# Using the exprs() function
geodata = exprs(geo.getGEO.data[,keep])
```


Let's view the full dataset as is now:
```{r}
head(geodata)
```
This now represents a matrix of data, with animal IDs as column headers and expression levels within the matrix.


#### Simplifying column names
These column names are not the easiest to interpret, so let's rename these columns to indicate which animals were from the exposed vs. unexposed groups.

We need to first convert our expression dataset to a dataframe so we can edit columns names, and continue with downstream data manipulations that require dataframe formats.
```{r}
geodata = data.frame(geodata)
```


Let's remind ourselves what the column names are:
```{r}
colnames(geodata)
```

Which ones of these are exposed vs unexposed animals can be determined by viewing our previously defined vectors.
```{r}
exposedIDs
unexposedIDs
```

With this we can tell that the first three listed IDs are from unexposed animals, and the last three IDs are from exposed animals.

Let's simplify the names of these columns to indicate exposure status and replicate number.
```{r}
colnames(geodata) = c("Control_1", "Control_2", "Control_3", "Exposed_1", 
                       "Exposed_2", "Exposed_3")
```


And we'll now need to re-define our 'exposed' vs 'unexposed' vectors for downstream script.
```{r}
exposedIDs = c("Exposed_1", "Exposed_2", "Exposed_3")
unexposedIDs = c("Control_1", "Control_2", "Control_3")
```



Viewing the data again:
```{r}
head(geodata)
```

These data are now looking easier to interpret/analyze. Still, the row identifiers include 8 digit numbers starting with "107...". We know that this dataset is a gene expression dataset, but these identifiers, in themselves, don't tell us much about what genes these are referring to. These numeric IDs specifically represent microarray probesetIDs, that were produced by the Affymetrix platform used in the original study.

**But how can we tell which genes are represented by these data?!**


#### Adding gene symbol information

Each -omics dataset contained within GEO points to a specific platform that was used to obtain measurements.
In instances where we want more information surrounding the molecular identifiers, we can merge the platform-specific annotation file with the molecular IDs given in the full dataset.

For example, let's pull the platform-specific annotation file for this experiment. Let's revisit the [website](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE42394) that contained the original dataset on GEO. Scroll down to where it lists "Platforms", and there is a hyperlinked platform number "GPL6247" (see arrow below).

```{r, echo=FALSE, fig.width=4, fig.height=5, fig.align = "center"}
knitr::include_graphics("Module7_2_Input/Module7_2_Image2.png")
```


Click on this, and you will be navigated to a different GEO website describing the Affymetrix rat array platform that was used in this analysis. Note that this website also includes information on when this array became available, links to other experiments that have used this platform within GEO, and much more.

Here, we're interested in pulling the corresponding gene symbol information for the probeset IDs. To do so, scroll to the bottom, and click "Annotation SOFT table..." and download the corresponding .gz file within your working directory. Unzip this, and you will find the master annotation file: "GPL6247.annot". 

In this exercise, we've already done these steps and unzipped the file in our working directory. So at this point, we can simply read in this annotation dataset, renamed "Module7_2_InputData2.annot", still using the `GEOquery()` function to help automate.

```{r, warning=FALSE}
geo.annot = GEOquery::getGEO(filename="Module7_2_Input/Module7_2_InputData3.annot")
```

Now we can use the `Table()` function from GEOquery to pull data from the annotation dataset.
```{r}
id.gene.table = GEOquery::Table(geo.annot)[,c("ID", "Gene symbol")]
id.gene.table[1:10,1:2]
```

With these two columns of data, we now have the needed IDs and gene symbols to match with our dataset.

Within the full dataset, we need to add a new column for the probeset ID, taken from the rownames, in preparation for the merging step.
```{r}
geodata$ID = rownames(geodata)
```

We can now merge the gene symbol information by ID with our expression data.
```{r}
geodata_genes = merge(geodata, id.gene.table, by="ID")
head(geodata_genes)
```

Note that many of the probeset IDs do not map to full gene symbols, which is shown here by viewing the top few rows - this is expected in genome-wide analyses based on microarray platforms.

Let's look at the first 25 unique genes in these data:
```{r}
UniqueGenes = unique(geodata_genes$`Gene symbol`)
UniqueGenes[1:25]
```

Again, you can see that the first value listed is blank, representing probesetIDs that do not match to fully annotated gene symbols. Though the rest pertain for gene symbols annotated to the rat genome.

You can also see that some gene symbols have multiple entries, separated by "///"

To simplify identifiers, we can pull just the first gene symbol, and remove the rest by using gsub().
```{r}
geodata_genes$`Gene symbol` = gsub("///.*", "", geodata_genes$`Gene symbol`)
```

Let's alphabetize by main expression dataframe by gene symbol.
```{r}
geodata_genes = geodata_genes[order(geodata_genes$`Gene symbol`),]
```

And then re-view these data:
```{r}
geodata_genes[1:5,]
```

In preparation for the visualization steps below, let's reset the probeset IDs to rownames.
```{r}
rownames(geodata_genes) = geodata_genes$ID

# Can then remove this column within the dataframe
geodata_genes$ID = NULL
```

Finally let's rearrange this dataset to include gene symbols as the first column, right after rownames (probeset IDs).
```{r}
geodata_genes = geodata_genes[,c(ncol(geodata_genes),1:(ncol(geodata_genes)-1))]
geodata_genes[1:5,]
dim(geodata_genes)
```

Note that this dataset includes expression measures across **29,214 probes, representing 14,019 unique genes**.
For simplicity in the final exercises, let's just filter for rows representing mapped genes.

```{r}
geodata_genes = geodata_genes[!(geodata_genes$`Gene symbol` == ""), ]
dim(geodata_genes)
```

Note that this dataset now includes 16,024 rows with mapped gene symbol identifiers.

### Answer to Environmental Health Question 1

:::question
<i>With this, we can now answer **Environmental Health Question 1**:</i>
What kind of molecular identifiers are commonly used in microarray-based -omics technologies?
:::
:::answer
**Answer**:  Platform-specific probeset IDs.
:::


### Answer to Environmental Health Question 2

:::question
<i>We can also answer **Environmental Health Question 2**:</i>
How can we convert platform-specific molecular identifiers used in -omics study designs to gene-level information?
:::
:::answer
**Answer**: We can merge platform-specific IDs with gene-level information using annotation files.
:::
<br>

## Visualizing Data

### Visualizing Gene Expression Data using Boxplots and Heat Maps

To visualize the -omics data, we can generate boxplots, heat maps, any many other types of visualizations. Here, we provide an example to plot a boxplot, which can be used to visualize the variability amongst samples. We also provide an example to plot a heat map, comparing unscaled vs scaled gene expression profiles. These visualizations can be useful to both simply visualize the data as well as identify patterns across samples or genes

#### Boxplot visualizations
For this example, let's simply use R's built in boxplot() function.

We only want to use columns with our expression data (2 to 7), so let's pull those columns when running the boxplot function.
```{r, fig.width=5, fig.height=4, fig.align = "center"}
boxplot(geodata_genes[,2:7])
```

There seem to be a lot of variability within each sample's range of expression levels, with many outliers. This makes sense given that we are analyzing the expression levels across the rat's entire genome, where some genes won't be expressed at all while others will be highly expressed due to biological and/or potential technical variability.
  
To show plots without outliers, we can simply use outline=F.
```{r, fig.width=5, fig.height=4, fig.align = "center"}
boxplot(geodata_genes[,2:7], outline=F)
```
  

#### Heat Map visualizations
Heat maps are also useful when evaluating large datasets.

There are many different packages you can use to generate heat maps. Here, we use the *superheat* package.

It also takes awhile to plot all genes across the genome, so to save time for this training module, let's randomly select 100 rows to plot.

```{r, fig.width=9, fig.height=7, fig.align = "center"}
# To ensure that the same subset of genes are selected each time
set.seed = 101                                     

# Random selection of 100 rows
row.sample = sample(1:nrow(geodata_genes),100) 

# Heat map code
superheat::superheat(geodata_genes[row.sample,2:7], # Only want to plot non-id/gene symbol columns (2 to 7)
                     pretty.order.rows = TRUE,
                     pretty.order.cols = TRUE,
                     col.dendrogram = T,
                     row.dendrogram = T)
```

This produces a heat map with sample IDs along the x-axis and probeset IDs along the y-axis. Here, the values being displayed represent normalized expression values.


One way to improve our ability to distinguish differences between samples is to **scale expression values** across probes. 

**Scaling data**

Z-score is a very common method of scaling that transforms data points to reflect the number of standard deviations they are from the overall mean. Z-score scaling data results in the overall transformation of a dataset to have an overall mean = 0 and standard deviation = 1.

Let's see what happens when we scale this gene expression dataset by z-score across each probe. This can be easily done using the `scale()` function.

This specific `scale()` function works by centering and scaling across columns, but since we want to use it across probesets (organized as rows), we need to first transpose our dataset, then run the scale function.
```{r}
geodata_genes_scaled = scale(t(geodata_genes[,2:7]), center=T, scale=T)
```

Now we can transpose it back to the original format (i.e., before it was transposed).
```{r}
geodata_genes_scaled = t(geodata_genes_scaled)
```


And then view what the normalized and now scaled expression data look like for now a random subset of 100 probesets (representing genes).
```{r, echo=FALSE, fig.width=9, fig.height=7, fig.align = "center"}
superheat::superheat(geodata_genes_scaled[row.sample,],
                     pretty.order.rows = TRUE,
                     pretty.order.cols = TRUE,
                     col.dendrogram = T,
                     row.dendrogram = T)
```

With these data now scaled, we can more easily visualize patterns between samples.


### Answer to Environmental Health Question 3

:::question
*We can also answer **Environmental Health Question 3***:
Why do we often scale gene expression signatures prior to heat map visualizations?
:::
:::answer
**Answer**: To better visualize patterns in expression signatures between samples.
:::

<br>
Now, with these data nicely organized, we can next explore how statistics can help us find which genes show trends in expression associated with formaldehyde exposure.


## Statistical Analyses

### Statistical Analyses to Identify Genes altered by Formaldehyde

A simple way to identify differences between formaldehyde-exposed and unexposed samples is to use a t-test. Because there are so many tests being performed, one for each gene, it is also important to carry out multiple test corrections through  a p-value adjustment method. 

We need to run a t-test for each row of our dataset. This exercise demonstrates two different methods to run a t-test:

+ Method 1: using a 'for loop'
+ Method 2: using the apply function (more computationally efficient)

#### Method 1 (m1): 'For Loop'

Let's first re-save the molecular probe IDs to a column within the dataframe, since we need those values in the loop function.
```{r}
geodata_genes$ID = rownames(geodata_genes)
```


We also need to initially create an empty dataframe to eventually store p-values.
```{r}
pValue_m1 = matrix(0, nrow=nrow(geodata_genes), ncol=3)
colnames(pValue_m1) = c("ID", "pval", "padj")
head(pValue_m1)
```

You can see the empty dataframe that was generated through this code.

Then we can loop through the entire dataset to acquire p-values from t-test statistics, comparing n=3 exposed vs n=3 unexposed samples.
```{r}
for (i in 1:nrow(geodata_genes)) {
  
  #Get the ID
  ID.i = geodata_genes[i, "ID"];
  
  #Run the t-test and get the p-value
  pval.i = t.test(geodata_genes[i,exposedIDs], geodata_genes[i,unexposedIDs])$p.value;
  
  #Store the data in the empty dataframe
  pValue_m1[i,"ID"] = ID.i;
  pValue_m1[i,"pval"] = pval.i
  
}
```

View the results:
```{r}
# Note that we're not pulling the last column (padj) since we haven't calculated these yet
pValue_m1[1:5,1:2] 
```



#### Method 2 (m2): Apply Function
For the second method, we can use the *apply()* function to calculate resulting t-test p-values more efficiently labeled. 

```{r}
pValue_m2 = apply(geodata_genes[,2:7], 1, function(x) t.test(x[unexposedIDs],
                                                              x[exposedIDs])$p.value)
names(pValue_m2) = geodata_genes[,"ID"]
```

We can convert the results into a dataframe to make it similar to m1 matrix we created above.
```{r}
pValue_m2  = data.frame(pValue_m2)

# Now create an ID column
pValue_m2$ID = rownames(pValue_m2)
```

Then we can view at the two datasets to see they result in the same pvalues.
```{r}
head(pValue_m1)
head(pValue_m2)
```
We can see from these results that both methods (m1 and m2) generate the same statistical p-values.

#### Interpreting Results

Let's again merge these data with the gene symbols to tell which genes are significant.

First, let's convert to a dataframe and then merge as before, for one of the above methods as an example (m1).
```{r}
pValue_m1 = data.frame(pValue_m1)
pValue_m1 = merge(pValue_m1, id.gene.table, by="ID")
```

We can also add a multiple test correction by applying a false discovery rate-adjusted p-value; here, using the Benjamini Hochberg (BH) method.
```{r}
# Here fdr is an alias for B-H method
pValue_m1[,"padj"] = p.adjust(pValue_m1[,"pval"], method=c("fdr"))
```

Now, we can sort these statistical results by adjusted p-values.
```{r}
pValue_m1.sorted = pValue_m1[order(pValue_m1[,'padj']),]
head(pValue_m1.sorted)
```

Pulling just the significant genes using an adjusted p-value threshold of 0.05.
```{r}
adj.pval.sig = pValue_m1[which(pValue_m1[,'padj'] < .05),]

# Viewing these genes
adj.pval.sig       
```


### Answer to Environmental Health Question 4

:::question
*With this, we can answer **Environmental Health Question 4***:</i>
What genes are altered in expression by formaldehyde inhalation exposure?
:::
:::answer
**Answer**: Olr633 and Slc7a8.
:::

Finally, let's plot these using a mini heat map.
Note that we can use probesetIDs, then gene symbols, in rownames to have them show in heat map labels.
```{r, echo=FALSE, fig.width=8, fig.height=4, fig.align = "center"}
rownames(geodata_genes) = paste(geodata_genes$ID, ": ",geodata_genes$`Gene symbol`)
superheat::superheat(geodata_genes[which(geodata_genes$ID %in% adj.pval.sig[,"ID"]),2:7])
```

Note that this statistical filter is pretty strict when comparing only n=3 vs n=3 biological replicates. If we loosen the statistical criteria to p-value < 0.05, this is what we can find:
```{r}
pval.sig = pValue_m1[which(pValue_m1[,'pval'] < .05),]
nrow(pval.sig)
```

5327 genes with significantly altered expression!

Note that other filters are commonly applied to further focus these lists (e.g., background and fold change filters) prior to statistical evaluation, which can impact the final results. See [Rager et al. 2013](https://pubmed.ncbi.nlm.nih.gov/24304932/)  for further statistical approaches and visualizations.

<br>

### Answer to Environmental Health Question 5

:::question
*With this, we can answer **Environmental Health Question 5***:
What are the potential biological consequences of these gene-level perturbations?
:::
:::answer
**Answer**: Olr633 stands for 'olfactory receptor 633'. Olr633 is up-regulated in expression, meaning that formaldehyde inhalation exposure has a smell that resulted in 'activated' olfactory receptors in the nose of these exposed rats. Slc7a8 stands for 'solute carrier family 7 member 8'. Slc7a8 is down-regulated in expression, and it plays a role in many biological processes, that when altered, can lead to changes in cellular homeostasis and disease.
:::

<br>

## Concluding Remarks

In conclusion, this training module provides an overview of pulling, organizing, visualizing, and analyzing -omics data from the online repository, Gene Expression Omnibus (GEO). Trainees are guided through the overall organization of an example high dimensional dataset, focusing on transcriptomic responses in the nasal epithelium of rats exposed to formaldehyde. Data are visualized and then analyzed using standard two-group comparisons. Findings are interpreted for biological relevance, yielding insight into the effects resulting from formaldehyde exposure. 

For additional case studies that leverage GEO, see the following publications that also address environmental health questions from our research group:

+ Rager JE, Fry RC. The aryl hydrocarbon receptor pathway: a key component of the microRNA-mediated AML signalisome. Int J Environ Res Public Health. 2012 May;9(5):1939-53. doi: 10.3390/ijerph9051939. Epub 2012 May 18. PMID: 22754483; PMCID: [PMC3386597](https://pubmed.ncbi.nlm.nih.gov/22754483/).

+ Rager JE, Suh M, Chappell GA, Thompson CM, Proctor DM. Review of transcriptomic responses to hexavalent chromium exposure in lung cells supports a role of epigenetic mediators in carcinogenesis. Toxicol Lett. 2019 May 1;305:40-50. PMID: [30690063](https://pubmed.ncbi.nlm.nih.gov/30690063/).




<label class="tykfont">
Test Your Knowledge 
</label>

:::tyk

Using the same dataset that was used in this module, available from the [UNC-SRP TAME2 GitHub website](https://github.com/UNCSRP/TAME2):
1. Load the downloaded GEO dataset into R using the packages and functions mentioned in this tutorial.
2. Filter the data to just those with "cell type" of "Circulating white blood cells".
3. Report the means of the first 5 rows of the gene expression data (10700001, 10700002, 10700003, 10700004, 10700005), across all samples.

:::

# 7.3 CompTox Dashboard Data through APIs


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo = FALSE}
# Redefining the knit_print method to truncate character values to 25 characters
# in each column and to truncate the columns in the print call to prevent 
# wrapping tables with several columns.
#library(ctxR)
knit_print.data.table = function(x, ...) {
  y <- data.table::copy(x)
  y <- y[, lapply(.SD, function(t){
    if (is.character(t)){
      t <- strtrim(t, 25)
    }
    return(t)
  })]
  print(y, trunc.cols = TRUE)
}

registerS3method(
  "knit_print", "data.table", knit_print.data.table,
  envir = asNamespace("knitr")
)
```


```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
if (!library(ctxR, logical.return = TRUE)){
  devtools::load_all('C:/Users/pkruse/ctxR')
}
```

This training module was developed by Paul Kruse and Caroline Ring, with contributions from Julia E. Rager.

All input files (script, data, and figures) can be downloaded from the [UNC-SRP TAME2 GitHub website](https://github.com/UNCSRP/TAME2).

*Disclaimer: The views expressed in this document are those of the authors and do not necessarily reflect the views or policies of the U.S. EPA.*


## Introduction to Training Module

Environmental health research related to chemical exposures often requires accessing and wrangling chemical-specific data. The CompTox Chemicals Dashboard (CCD), developed by the United States Environmental Protection Agency, is a publicly-accessible database that integrates chemical data from multiple domains. Chemical data available on the CCD include physicochemical, environmental fate and transport, exposure, toxicokinetics, functional use, in vivo toxicity, in vitro bioassay, and mass spectra data. The CCD was first described in Williams et al. (2017), and has been continuously expanded since. The CCD is heavily used by researchers who do cheminformatics work of various kinds -- computational toxicology, computational exposure science, analytical chemistry, chemical safety assessment, etc. The CCD is used by cheminformaticians not only at EPA, but across governmental agencies both within the U.S. and worldwide; in private industry; in non-governmental organizations; in academia; and others. It has become an indispensable tool for many researchers.

This training module provides an overview of the physico-chemical, hazard, and bioactivity data available through the CCD; different ways to access these data; and some examples of how these data may be used. We will first introduce the CCD and how to access it. Then we will focus on an automated, programmatic method for retrieving data from the CCD using the *ctxR* R package. Through some basic data visualization and analysis using the R programming language, we will explore some data retrieved from the CCD, and gain insights both in how to wrangle the data and combine different methods of accessing the data to build automated pipelines for use in more complex settings.

Note, as the *ctxR* package accesses data that is periodically updated, some code chunks will produce numbers that may change slightly with data updates. Keep this in mind when running these code chunks in the future.

## Training Module's Environmental Health Questions

This training module was specifically developed to answer the following questions:

1. After automatically pulling the fourth Drinking Water Contaminant Candidate List from the CompTox Chemicals Dashboard, list the properties and property types present in the data. What are the mean values for a specific property when grouped by property type and when ungrouped?

2. The physico-chemical property data are reported with both experimental and predicted values present for many chemicals. Are there differences between the mean predicted and experimental results for a variety of physico-chemical properties?

3. After pulling the genotoxicity data for the different environmental contaminant data sets, list the assays associated with the chemicals in each data set. How many unique assays are there in each data set? What are the different assay categories and how many unique assays for each assay category are there?

4. The genotoxicity data contains information on which assays have been conducted for different chemicals and the results of those assays. How many chemicals in each data set have a ‘positive’, ‘negative’, and ‘equivocal’ value for the assay result?

5. Based on the genotoxicity data reported for the chemical with DTXSID identifier DTXSID0020153, how many assays resulted in a positive/equivocal/negative value? Which of the assays were positive and how many of each were there for the most reported assays?

6. After pulling the hazard data for the different data sets, list the different exposure routes for which there is data. What are the unique risk assessment classes for hazard values for the oral route and for the inhalation exposure route? For each such exposure route, which risk assessment class is most represented by the data sets?

7. There are several types of toxicity values for each exposure route. List the unique toxicity values for the oral and inhalation routes. What are the unique types of toxicity values for the oral route and for the inhalation route? How many of these are common to both the oral and inhalation routes for each data set?

8. When examining different toxicity values, the data may be reported in multiple units. To assess the relative hazard from this data, it is important to take into account the different units and adjust accordingly. List the units reported for the cancer slope factor, reference dose, and reference concentration values associated with the oral and inhalation exposure routes for human hazard. Which chemicals in each data set have the highest cancer slope factor, lowest reference dose, and lowest reference concentration values?

## Script Preparations

### Cleaning the Global Environment

```{r, eval=FALSE}
rm(list=ls())
```


### Installing Required R Packages

```{r, eval=FALSE}
if (!requireNamespace('ctxR'))
  install.packages('ctxR')

if (!requireNamespace('ggplot2'))
  install.packages('ggplot2')
```

### Loading R Packages

```{r}
# Used to interface with CompTox Chemicals Dashboard
library(ctxR)

# Used to visualize data in a variety of plot designs
library(ggplot2)
```


## Introduction to CompTox Chemicals Dashboard

Accessing chemical data and wrangling it is a vital step in many types of workflows related to chemical, biological, and environmental modeling. While there are many resources available from which one can pull data, the [CompTox Chemicals Dashboard](https://comptox.epa.gov/dashboard/) built and maintained by the United States Environmental Protection Agency is particularly well-designed and suitable for these purposes. Originally introduced in [The CompTox Chemistry Dashboard: a community data resource for environmental chemistry](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-017-0247-6), the CCD contains information on over 1.2 million chemicals as of December 2023. 

The CCD includes chemical information from many different domains, including physicochemical, environmental fate and transport, exposure, usage, in vivo toxicity, and in vitro bioassay data (Williams et al., 2017).

The CCD can be searched either one chemical at a time, or using a batch search.

### Searching One Chemical at a Time (Single-substance Search)

In single-substance search, the user types a full or partial chemical identifier (name, CASRN, InChiKey, or DSSTox ID) into a search box on the CCD homepage. Autocomplete provides a list of possible matches; the user selects one by clicking on it, and is then taken to the CCD page for that substance. Here is an example of the CCD page for the chemical Bisphenol A: 

```{r, echo = FALSE, out.width= "90%", fig.align= 'center'}
knitr::include_graphics('Module7_3_Input/Module7_3_Image1.png')
```


The different domains of data available for this chemical are shown by the tabs on the left side of the page: for example, "Physchem Prop." (physico-chemical properties), "Env. Fate/Transport" (environmental fate and transport data), and "Hazard Data" (*in vivo* hazard and toxicity data), among others. 

### Batch Search

In batch search, the user enters a list of search inputs, separated by newlines, into a batch-search box on https://comptox.epa.gov/dashboard/batch-search . The user selects the type(s) of inputs by selecting one or more checkboxes – these may include chemical identifiers, monoisotopic masses, or molecular formulas. Then, the user selects “Display All Chemicals” to display the list of substances matching the batch-search inputs, or “Choose Export Options” to choose options for exporting the batch-search results as a spreadsheet. The exported spreadsheet may include data from most of the domains available on an individual substance’s CCD page.


```{r, echo = FALSE, out.width = "90%", fig.align = 'center'}
knitr::include_graphics('Module7_3_Input/Module7_3_Image2.png')
```

The user can download the selected information in various formats, such as Excel (.xlsx), comma-separated values (.csv), or different types of chemical table files (.e.g, MOL). 

```{r, echo=FALSE, out.width="90%", fig.align='center'}
knitr::include_graphics('Module7_3_Input/Module7_3_Image3.png')
```


The web interface for batch search only allows input of 10,000 identifiers at a time. If a user needs to retrieve information for more than 10,000 chemicals, they will need to separate their identifiers into multiple batches and search each one separately.

### Challenges of Web-based Dashboard Search

Practicing researchers typically end up with a Dashboard workflow that looks something like this:

1.	Start with a dataset that includes your chemical identifiers of interest. These may include chemical names, Chemical Abstract Service Registry Numbers (CASRNs), Distributed Searchable Structure-Toxicity Database (DSSTox) identifiers, or InChIKeys.
2.	Export the chemical identifiers to a spreadsheet. Often, this is done by importing the data into an environment such as R or Python, in order to do some data wrangling (e.g., to select only the unique substance identfiers; to clean up improperly-formatted CASRNs; etc.). Then, the identifiers are saved in a spreadsheet (an Excel, .csv, or .txt file), one chemical identifier per row.
3.	Copy and paste the chemical identifiers from the spreadsheet into the CCD Batch Search box. If there are more than 10,000 total chemical identifiers, divide them into batches of 10,000 or less, and search each batch separately.
4.	Choose your desired export options on the CCD Batch Search page.
5.	Download the exported spreadsheet of CCD data. By default, the downloaded spreadsheet will be given a file name that includes the timestamp of the download.
6.	Repeat steps 3-5 for each batch of 10,000 identifiers produced in step 2. 
7.	Import the downloaded spreadsheet(s) of CCD data into the analysis tool you are using (e.g. R or Python).
8.	Merge the table(s) of downloaded CCD data with your original dataset of interest.
9.	Proceed with research-related data analysis using the chemical data downloaded from the CCD (e.g., statistical modeling, visualization, etc.)

Because each of these workflow steps requires manual interaction with the search and download process, the risk of human error inevitably creeps in. Here are a few real-world possibilities (the authors can neither confirm nor deny that they have personally committed any of these errors):

-	Researchers could copy/paste the wrong identifiers into the CCD batch search, especially if they have more than 10,000 identifiers and have to divide them into batches.
-	Chemical identifiers could be corrupted during the process of exporting to a spreadsheet. For example, if a researcher opens and resaves a CSV file using Microsoft Excel, any information that appears to be in date-like format will be automatically converted to a date (unless the researcher has the most recently-updated version of Excel and has selected the option in Settings that will stop Excel from auto-detecting dates). This behavior has long been identified as a problem in genomics, where gene names can appear date-like to Excel (Abeysooriya et al. 2021). It also affects cheminformatics, where chemical identifiers can appear date-like to Excel. For example, the valid CASRN “1990-07-4” would automatically be converted to “07/04/1990” (if Excel is set to use MM/DD/YYYY date formats). CCD batch search cannot recognize "07/04/1990" as a valid chemical identifier and will be unable to return any chemical data.
-	Researchers could accidentally rename a downloaded CCD data file to overwrite a previous download (for example, when searching multiple batches of identifiers). 
-	Researchers could mistakenly import the wrong CCD download file back into their analysis environment (for example, when searching multiple batches of identifiers). 

Moreover, the manual stages of this kind of workflow are also non-transparent and not easily reproducible. 

## CCTE's CTX Application Programming Interfaces (APIs) for Automated Batch Search of the CCD


Recently, the [Center for Computational Toxicology and Exposure](https://www.epa.gov/aboutepa/about-center-computational-toxicology-and-exposure-ccte) (CCTE) developed a set of Application Programming Interfaces (APIs) that allows programmatic access to the CCD, bypassing the manual steps of the web-based batch search workflow. The Computational Toxicology and Exposure (CTX) APIs effectively automate the process of accessing and downloading data from the web pages that make up the CCD. 


The [CTX APIs](https://www.epa.gov/comptox-tools/computational-toxicology-and-exposure-apis) are publicly available at no cost to the user. However, in order to use the CTX APIs, you must have an API key. The API key uniquely identifies you to the CTX servers and verifies that you have permission to access the database. Getting an API key is free, but requires contacting the API support team at [ccte_api@epa.gov](mailto:ccte_api@epa.gov).



:::txtbx

For more information on the data accessible through the CTX APIs and related tools, please visit the US EPA page on [Computational Toxicology and Exposure Online Resources](https://www.epa.gov/comptox-tools). The CTX APIs are one of many resources developed within this research realm and make available many of the data resources beyond the CCD.

:::




The APIs are organized into four sets of "endpoints" (chemical data domains): `Chemical`, `Hazard`, `Bioactivity`, and `Exposure`. Pictured below is what the `Chemical` section looks like and can be found at [CTX API Chemical Endpoints](https://api-ccte.epa.gov/docs/chemical.html).

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Module7_3_Input/Module7_3_Image4.png')
```

The APIs can be explored through the pictured web interface at https://api-ccte.epa.gov/docs/chemical.html .

### CTX API Authentication

`Authentication` is the first tab on the left. Authentication is required to use the APIs. To authenticate yourself in the API web interface, input your unique API key.

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Module7_3_Input/Module7_3_Image5.png')
```


### CTX API Endpoints


On the left of the API web interface, there are several different tabs, one for each endpoint in the `Chemical` domain. The endpoints are organized by the type of information provided. For instance, the `Chemical Details Resource` endpoint provides basic chemical information; the `Chemical Property Resource` endpoint provides more comprehensive physico-chemical property information; the `Chemical Fate Resource` endpoint provides chemical fate and transport information; and so on. 


### Constructing CTX API Requests

As mentioned above, APIs effectively automate the process of accessing and downloading data from the web pages that make up the CCD. APIs do this by automatically constructing requests using the Hypertext Transfer Protocol (HTTP) that enables communication between clients (e.g. your computer) and servers (e.g. the CCD).

In the CTX API web interface, the colored boxes next to each endpoint indicate the type of the associated HTTP method: either a GET request ("GET", blue) or a a POST request ("POS", green). GET is used to request data from a specific web resource (e.g. a specific URL); POST is used to send data to a server to create or update a web resource. For the CTX APIs, POST requests are used to perform multiple (batch) searches in a single API call; GET requests are used for non-batch searches. You do not need to understand the details of POST and GET requests in order to use the API.

Click on the second item under `Chemical Details Resource`, the tab labeled `Get data by dtxsid`. The following page will appear.

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Module7_3_Input/Module7_3_Image6.png')
```


This page has two subheadings: "Path Parameters" and "Query-String Parameters". "Path Parameters" contains user-specified parameters that are required in order to tell the API what URL (web address) to access. In this case, the required parameter is a string for the DTXSID identifying the chemical to be searched.

"Query-String Parameters" contain user-specific parameters (usually optional) that tell the API what specific type(s) of information to download from the specified URL. In this case, the optional parameter is a `projection` parameter, a string that can take one of five values (`chemicaldetailall`, `chemicaldetailstandard`, `chemicalidentifier`, `chemicalstructure`, `ntatoolkit`). Depending on the value of this string, the API can return different sets of information about the chemical. If the `projection` parameter is left blank, then a default set of chemical information is returned.

The default return format is displayed below and includes a variety of fields with data types represented.

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Module7_3_Input/Module7_3_Image7.png')
```


We show what returned data from searching Bisphenol A looks like using this endpoint with the `chemicaldetailstandard` value for `projection` selected.

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Module7_3_Input/Module7_3_Image8.png')
```


Formatting an http request is not necessarily intuitive nor worth the time for someone not already familiar with the process, so these endpoints may provide a resource that for many would require a significant investment in time and energy to learn how to use. However, there is a solution to this in the form of the R package *ctxR*.

*ctxR* was developed to streamline the process of accessing the information available through the CTX APIs without requiring prior knowledge of how to use APIs. The [*ctxR*](https://CRAN.R-project.org/package=ctxR) package is available in stable form on CRAN and a development version may be found at the [USEPA ctxR GitHub](https://github.com/USEPA/ctxR/) repository. As an example, we demonstrate the ease with which one may retrieve the information given by this endpoint for Bisphenol A using the *ctxR* approach and contrast it with the approach using the CCD website or CTX `Chemical` API Endpoint website.



#### Setting, using, and storing the API key

We store the API key required to access the APIs. To do this for the current session, run the first command. If you want to store your key across multiple sessions, run the second command.

```{r, eval=FALSE}
# This stores the key in the current session
register_ctx_api_key(key = '<YOUR API KEY>')

# This stores the key across multiple sessions and only needs to be run once. 
# If the key changes, rerun this with the new key.
register_ctx_api_key(key = '<YOUR API KEY>', write = TRUE)
```

```{r, echo=FALSE}
# This stores the key in the current session
register_ctx_api_key(key = '706401cd-8bda-469d-9cdb-ac27f489c93a')

# This stores the key across multiple sessions and only needs to be run once. 
# If the key changes, rerun this with the new key.
register_ctx_api_key(key = '706401cd-8bda-469d-9cdb-ac27f489c93a', write = TRUE)
```

To check that your key has successfully been stored for the session, run the following command.

```{r, eval=FALSE}
ctx_key()
```

#### Retrieving chemical details

Now, we demonstrate how to retrieve the information for BPA given by the `Chemical Detail Resource` endpoint under the `chemicaldetailstandard` value for `projection`. Note, this `projection` value is the default value for the function `get_chemical_details()`.

```{r}
BPA_chemical_detail <- get_chemical_details(DTXSID = 'DTXSID7020182')
dim(BPA_chemical_detail)
class(BPA_chemical_detail)
names(BPA_chemical_detail)
```


## Comparing Physico-chemical Properties between Two Important Environmental Contaminant Lists

We study two different data sets contained in the CCD and observe how they relate and how they differ. The two data sets that we will explore are a water contaminant priority list and an air toxics list.  

The fourth Drinking Water Contaminant Candidate List (CCL4) is a set of chemicals that "...are not subject to any proposed or promulgated national primary drinking water regulations, but are known or anticipated to occur in public water systems...." Moreover, this list "...was announced on November 17, 2016. The CCL 4 includes 97 chemicals or chemical groups and 12 microbial contaminants...." The National-Scale Air Toxics Assessments (NATA) is "... EPA's ongoing comprehensive evaluation of air toxics in the United States... a state-of-the-science screening tool for State/Local/Tribal agencies to prioritize pollutants, emission sources and locations of interest for further study in order to gain a better understanding of risks... use general information about sources to develop estimates of risks which are more likely to overestimate impacts than underestimate them...."  

These lists can be found in the CCD at [CCL4](https://comptox.epa.gov/dashboard/chemical-lists/CCL4) with additional information at [CCL4 information](https://www.epa.gov/ccl/contaminant-candidate-list-4-ccl-4-0) and [NATADB](https://comptox.epa.gov/dashboard/chemical-lists/NATADB) with additional information at [NATA information](https://www.epa.gov/national-air-toxics-assessment). The quotes from the previous paragraph were excerpted from list detail descriptions found using the CCD links.


We explore details about these two lists of chemicals before diving into analyzing the data contained in each list.

```{r}
options(width = 100)
ccl4_information <- get_public_chemical_list_by_name('CCL4')
print(ccl4_information, trunc.cols = TRUE)

natadb_information <- get_public_chemical_list_by_name('NATADB')
print(natadb_information, trunc.cols = TRUE)
```

Now we pull the actual chemicals contained in the lists using the APIs.

```{r}
ccl4 <- get_chemicals_in_list('ccl4')
ccl4 <- data.table::as.data.table(ccl4)

natadb <- get_chemicals_in_list('NATADB')
natadb <- data.table::as.data.table(natadb)
```

We examine the dimensions of the data, the column names, and display a single row for illustrative purposes.

```{r}
dim(ccl4)
dim(natadb)

colnames(ccl4)
head(ccl4, 1)
```


### Accessing the Physico-chemical Property Data

Once we have the chemicals in each list, we access their physico-chemical properties. We will use the batch search forms of the function `get_chem_info()`, to which we supply a list of DTXSIDs.

```{r}
ccl4$dtxsid
natadb$dtxsid

ccl4_phys_chem <- get_chem_info_batch(ccl4$dtxsid)
natadb_phys_chem <- get_chem_info_batch(natadb$dtxsid)
```

Observe that this returns a single data.table for each query, and the data.table contains the physico-chemical properties available from the CompTox Chemicals Dashboard for each chemical in the query. Note, a warning message was triggered, `Warning: Setting type to ''!`, which indicates the the parameter `type` was not given a value. A default value is set within the function and more information can be found in the associated documentation. We examine the set of physico-chemical properties for the first chemical in CCL4. 

Before any deeper analysis, let's take a look at the dimensions of the data and the column names.

```{r}
dim(ccl4_phys_chem)
colnames(ccl4_phys_chem)
```
Next, we display the unique values for the columns `propertyID` and `propType`.





```{r}
ccl4_phys_chem[, unique(propertyId)]
ccl4_phys_chem[, unique(propType)]
```

Let's explore this further by examining the mean of the "boiling-point" and "melting-point" data.

```{r}
ccl4_phys_chem[propertyId == 'boiling-point', .(Mean = mean(value))]
ccl4_phys_chem[propertyId == 'boiling-point', .(Mean = mean(value)),
               by = .(propType)]

ccl4_phys_chem[propertyId == 'melting-point', .(Mean = mean(value))]
ccl4_phys_chem[propertyId == 'melting-point', .(Mean = mean(value)),
               by = .(propType)]
```

These results tell us about some of the reported physico-chemical properties of the data sets.

### Answer to Environmental Health Question 1

:::question
*With this, we can answer **Environmental Health Question 1:*** After automatically pulling the fourth Drinking Water Contaminant Candidate List from the CompTox Chemicals Dashboard, list the properties and property types present in the data. What are the mean values for a specific property when grouped by property type and when ungrouped?
:::

:::answer
**Answer:** The mean "boiling-point" is 253.7974 degrees Celsius for CCL4, with mean values of 250.5943 and 255.5472 for experimental and predicted, respectively. The mean "melting-point" is 36.14033 degrees Celsius for CCL4, with mean values of 23.64972 and 51.93584 for experimental and predicted, respectively.
:::

To explore **all** the values of the physico-chemical properties and calculate their means, we can do the following procedure. First we look at all the physico-chemical properties individually, then group them by each property ("boiling-point", "melting-point", etc...), and then additionally group those by property type ("experimental" vs "predicted"). In the grouping, we look at the columns `value`, `unit`, `propertyID` and `propType`. We also demonstrate how take the mean of the values for each grouping. We examine the chemical with `DTXSID` "DTXSID0020153" from CCL4.

```{r}
head(ccl4_phys_chem[dtxsid == 'DTXSID0020153', ])
ccl4_phys_chem[dtxsid == 'DTXSID0020153', .(propType, value, unit),
               by = .(propertyId)]
ccl4_phys_chem[dtxsid == 'DTXSID0020153', .(value, unit), 
               by = .(propertyId, propType)]

ccl4_phys_chem[dtxsid == 'DTXSID0020153', .(Mean_value = sapply(.SD, mean)),
               by = .(propertyId, unit), .SDcols = c("value")]
ccl4_phys_chem[dtxsid == 'DTXSID0020153', .(Mean_value = sapply(.SD, mean)), 
               by = .(propertyId, unit, propType), 
               .SDcols = c("value")][order(propertyId)]
```

### Analyzing and Visualizing Physico-chemical Properties from Two Environmental Contaminant Lists

We consider exploring the differences in mean predicted and experimental values for a variety of physico-chemical properties in an effort to understand better the CCL4 and NATADB lists. In particular, we examine "vapor-pressure", "henrys-law", and "boiling-point" and plot the means by chemical for these using boxplots. We then compare the values by grouping by both data set and `propType` value.



We first examine the vapor pressures for all the chemicals in each list. We then graph these, grouped by `propType` and pooled together in separate plots. For this we will use boxplots.


Group first by DTXSID.

```{r}
ccl4_vapor_all <- ccl4_phys_chem[propertyId %in% 'vapor-pressure', 
                                 .(mean_vapor_pressure = sapply(.SD, mean)), 
                                 .SDcols = c('value'), by = .(dtxsid)]
natadb_vapor_all <- natadb_phys_chem[propertyId %in% 'vapor-pressure', 
                                     .(mean_vapor_pressure = sapply(.SD, mean)),
                                     .SDcols = c('value'), by = .(dtxsid)]
```

Then group by DTXSID and then by property type.

```{r}
ccl4_vapor_grouped <- ccl4_phys_chem[propertyId %in% 'vapor-pressure', 
                                     .(mean_vapor_pressure = sapply(.SD, mean)),
                                     .SDcols = c('value'), 
                                     by = .(dtxsid, propType)]
natadb_vapor_grouped <- natadb_phys_chem[propertyId %in% 'vapor-pressure', 
                                         .(mean_vapor_pressure = 
                                             sapply(.SD, mean)), 
                                         .SDcols = c('value'), 
                                         by = .(dtxsid, propType)]
```

Then examine the summary statistics of the data.

```{r}
summary(ccl4_vapor_all)
summary(ccl4_vapor_grouped)
summary(natadb_vapor_all)
summary(natadb_vapor_grouped)
```

With such a large range of values covering several orders of magnitude, we log transform the data. Since these value are positive, we do not have to worry about illegal transformations.

```{r}
ccl4_vapor_all[, log_transform_mean_vapor_pressure := log(mean_vapor_pressure)]
ccl4_vapor_grouped[, log_transform_mean_vapor_pressure := 
                     log(mean_vapor_pressure)]

natadb_vapor_all[, log_transform_mean_vapor_pressure := 
                   log(mean_vapor_pressure)]
natadb_vapor_grouped[, log_transform_mean_vapor_pressure := 
                       log(mean_vapor_pressure)]
```

Now we plot the log transformed data.

First plot the CCL4 data.
```{r, fig.align='center'}
ggplot(ccl4_vapor_all, aes(log_transform_mean_vapor_pressure)) +
  geom_boxplot() +
  coord_flip()
ggplot(ccl4_vapor_grouped, aes(propType, log_transform_mean_vapor_pressure)) +
  geom_boxplot()
```

Then plot the NATA data.

```{r, fig.align='center'}
ggplot(natadb_vapor_all, aes(log_transform_mean_vapor_pressure)) +
  geom_boxplot() + coord_flip()
ggplot(natadb_vapor_grouped, aes(propType, log_transform_mean_vapor_pressure)) +
  geom_boxplot()
```

Finally, we compare both sets simultaneously. We add in a column to each data.table denoting to which data set the rows correspond and then combine the rows from both data sets together using the function `rbind()`.

```{r}
ccl4_vapor_grouped[, set := 'CCL4']
natadb_vapor_grouped[, set := 'NATADB']

all_vapor_grouped <- rbind(ccl4_vapor_grouped, natadb_vapor_grouped)
```

Now we plot the combined data. First we color the boxplots based on the property type, with mean log transformed vapor pressure plotted for each data set and property type.

```{r, fig.align='center'}
vapor_box <- ggplot(all_vapor_grouped, 
                    aes(set, log_transform_mean_vapor_pressure)) + 
  geom_boxplot(aes(color = propType))
vapor_box
```

Next we color the boxplots based on the data set.

```{r, , fig.align='center'}
vapor <- ggplot(all_vapor_grouped, aes(log_transform_mean_vapor_pressure)) +
  geom_boxplot((aes(color = set))) + 
  coord_flip()
vapor
```

In the plots above, when we graph the data separated both by data set and property type as well as just by data set, we observe the general trend that the NATADB chemicals have a higher mean vapor pressure than the CCL4 chemicals.

We also explore Henry's Law constant and boiling point in a similar fashion.

Group by DTXSID.

```{r}
ccl4_hlc_all <- ccl4_phys_chem[propertyId %in% 'henrys-law', 
                               .(mean_hlc = sapply(.SD, mean)), 
                               .SDcols = c('value'), by = .(dtxsid)]
natadb_hlc_all <- natadb_phys_chem[propertyId %in% 'henrys-law', 
                                   .(mean_hlc = sapply(.SD, mean)), 
                                   .SDcols = c('value'), by = .(dtxsid)]
```

Group by DTXSID and property type.

```{r}
ccl4_hlc_grouped <- ccl4_phys_chem[propertyId %in% 'henrys-law', 
                                   .(mean_hlc = sapply(.SD, mean)), 
                                   .SDcols = c('value'), 
                                   by = .(dtxsid, propType)]
natadb_hlc_grouped <- natadb_phys_chem[propertyId %in% 'henrys-law', 
                                       .(mean_hlc = sapply(.SD, mean)), 
                                       .SDcols = c('value'), 
                                       by = .(dtxsid, propType)]
```

Examine summary statistics.

```{r}
summary(ccl4_hlc_all)
summary(ccl4_hlc_grouped)
summary(natadb_hlc_all)
summary(natadb_hlc_grouped)
```

Again, we log transform the data as it is positive and covers several orders of magnitude.

```{r}
ccl4_hlc_all[, log_transform_mean_hlc := log(mean_hlc)]
ccl4_hlc_grouped[, log_transform_mean_hlc := log(mean_hlc)]

natadb_hlc_all[, log_transform_mean_hlc := log(mean_hlc)]
natadb_hlc_grouped[, log_transform_mean_hlc := log(mean_hlc)]
```

We compare both sets simultaneously. We add in a column to each data.table denoting to which set the rows correspond and then `rbind()` the rows together.


Label and combine data.

```{r}
ccl4_hlc_grouped[, set := 'CCL4']
natadb_hlc_grouped[, set := 'NATADB']

all_hlc_grouped <- rbind(ccl4_hlc_grouped, natadb_hlc_grouped)
```

Plot data.

```{r, , fig.align='center'}
hlc_box <- ggplot(all_hlc_grouped, aes(set, log_transform_mean_hlc)) + 
  geom_boxplot(aes(color = propType))
hlc_box

hlc <- ggplot(all_hlc_grouped, aes(log_transform_mean_hlc)) +
  geom_boxplot(aes(color = set)) +
  coord_flip()
hlc
```

Again, we observe that in both grouping by `propType` and aggregating all results together by data set, that the chemicals in NATADB have a generally higher mean Henry's Law Constant value than those in CCL4.

Finally, we consider boiling point.

Group by DTXSID.

```{r}
ccl4_boiling_all <- ccl4_phys_chem[propertyId %in% 'boiling-point', 
                                   .(mean_boiling_point = sapply(.SD, mean)), 
                                   .SDcols = c('value'), by = .(dtxsid)]
natadb_boiling_all <- natadb_phys_chem[propertyId %in% 'boiling-point', 
                                       .(mean_boiling_point = 
                                           sapply(.SD, mean)), 
                                       .SDcols = c('value'), by = .(dtxsid)]
```

Group by DTXSID and property type.

```{r}
ccl4_boiling_grouped <- ccl4_phys_chem[propertyId %in% 'boiling-point', 
                                       .(mean_boiling_point = 
                                           sapply(.SD, mean)), 
                                       .SDcols = c('value'), 
                                       by = .(dtxsid, propType)]
natadb_boiling_grouped <- natadb_phys_chem[propertyId %in% 'boiling-point', 
                                           .(mean_boiling_point = 
                                               sapply(.SD, mean)), 
                                           .SDcols = c('value'), 
                                           by = .(dtxsid, propType)]
```

Calculate summary statistics.

```{r}
summary(ccl4_boiling_all)
summary(ccl4_boiling_grouped)
summary(natadb_boiling_all)
summary(natadb_boiling_grouped)
```

Since some of the boiling point values have negative values, we cannot log transform these values. If we try, as you will see below, there will be warnings of NaNs produced.

```{r, eval}
ccl4_boiling_all[, log_transform := log(mean_boiling_point)]
ccl4_boiling_grouped[, log_transform := log(mean_boiling_point)]

natadb_boiling_all[, log_transform := log(mean_boiling_point)]
natadb_boiling_grouped[, log_transform := log(mean_boiling_point)]
```

We compare both sets simultaneously. We add in a column to each data.table denoting to which set the rows correspond and then `rbind()` the rows together. We use the values as is rather than transforming them.

Label and combine data.

```{r}
ccl4_boiling_grouped[, set := 'CCL4']
natadb_boiling_grouped[, set := 'NATADB']

all_boiling_grouped <- rbind(ccl4_boiling_grouped, natadb_boiling_grouped)
```

Plot the data.

```{r, , fig.align='center'}
boiling_box <- ggplot(all_boiling_grouped, aes(set, mean_boiling_point)) + 
  geom_boxplot(aes(color = propType))
boiling_box

boiling <- ggplot(all_boiling_grouped, aes(mean_boiling_point)) +
  geom_boxplot(aes(color = set)) + 
  coord_flip()
boiling
```

A visual inspection of this set of graphs is not as clear as in the previous cases. Note that the predicted values for each data set tend to be higher than the experimental. The mean of CCL4, by predicted and experimental appears to be greater than the corresponding means for NATADB, as does the overall mean, but the interquartile ranges of these different groupings yield slightly different results. This gives us a sense that the picture for boiling point is not as clear cut between experimental and predicted for these two data sets as it was in the previous cases of physico-chemical properties we investigated.

### Answer to Environmental Health Question 2

:::question
*Through inspecting the last several plots, we can answer **Environmental Health Question 2:*** The physico-chemical property data are reported with both experimental and predicted values present for many chemicals. Are there differences between the mean predicted and experimental results for a variety of physico-chemical properties? 
:::

:::answer
**Answer**: There are indeed differences between the mean values of various physico-chemical properties when grouped by predicted or experimental. In the case of "vapor-pressure", the means of predicted values tend to be a little lower than experimental, though they are much closer in the case of NATADB than CCL4. The trend of lower predicted means compared to experimental means is more clearly demonstrated for "henrys-law" values in both data sets. In the case of "boiling-point", the predicted values are greater than the experimental values, though this is much more pronounced in CCL4 while the set of means for NATADB are again fairly close. 
:::




## Hazard Data: Genotoxicity

Now, having examined some of the distributions of the physico-chemical properties of the two lists, aggregated between predicted and experimental, we move towards learning more about these chemicals beyond physico-chemical properties. Specifically, we will examine their genotoxicity.

Using the standard CompTox Chemicals Dashboard approach to access genotoxicity, one would again navigate to the individual chemical page 

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Module7_3_Input/Module7_3_Image9.png')
```

Once one navigates to the genotoxicity tab highlighted in the previous page, the following is displayed as seen here:

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Module7_3_Input/Module7_3_Image10.png')
```

This page includes two sets of information, the first of which provides a summary of available genotoxicity data while the second provides the individual reports and samples of such data.

We again use the CTX APIs to streamline the process of retrieving this information in a programmatic fashion. To this end, we will use the genotoxicity endpoints found within the `Hazard` endpoints of the CTX APIs. Pictured below is the particular set of genotoxicity resources available in the `Hazard` endpoints of the CTX APIs.

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Module7_3_Input/Module7_3_Image11.png')
```

There are both summary and detail resources, reflecting the information one can find on the CompTox Chemicals Dashboard Genotoxicity page for a given chemical.

To access the genetox endpoint, we will use the function `get_genetox_summary()`. Since we have a list of chemicals, rather than searching individually for each chemical, we use the batch search version of the function, named `get_genetox_summary_batch()`. We will examine this and then access the details.

Grab the data using the APIs.
```{r}
ccl4_genotox <- get_genetox_summary_batch(DTXSID = ccl4$dtxsid)
natadb_genetox <- get_genetox_summary_batch(DTXSID = natadb$dtxsid)
```

Examine the dimensions.

```{r}
dim(ccl4_genotox)
dim(natadb_genetox)
```

Examine the column names and data from the first six chemicals with genetox data from CCL4.

```{r}
colnames(ccl4_genotox)
head(ccl4_genotox)
```

The information returned is of the first variety highlighted in the image above, that is, the summary data on the available genotoxicity data for each chemical.

Observe that we have information on 71 chemicals from the CCL4 data and 153 from the NATA data. We note the chemicals not included in the results and then dig into the returned results.

```{r}
ccl4[!(dtxsid %in% ccl4_genotox$dtxsid), 
     .(dtxsid, casrn, preferredName, molFormula)]
natadb[!(dtxsid %in% natadb_genetox$dtxsid), 
       .(dtxsid, casrn, preferredName, molFormula)]
```

Now, we access the genotoxicity details of the chemicals in each data set using the function `get_genetox_details_batch()`. We explore the dimensions of the returned queries, the column names, and the first few lines of the data.

Grab the data from the CTX APIs.

```{r}
ccl4_genetox_details <- get_genetox_details_batch(DTXSID = ccl4$dtxsid)
natadb_genetox_details <- get_genetox_details_batch(DTXSID = natadb$dtxsid)
```

Examine the dimensions.

```{r}
dim(ccl4_genetox_details)
dim(natadb_genetox_details)
```

Look at the column names and the first six rows of the data from the CCL4 chemicals.

```{r}
colnames(ccl4_genetox_details)
head(ccl4_genetox_details)
```

We examine the information returned for the first chemical in each set of results, which is DTXSID0020153. Notice that the information is identical in each case as this information is chemical specific and not data set specific.

Look at the dimensions first.

```{r}
dim(ccl4_genetox_details[dtxsid %in% 'DTXSID0020153', ])
dim(natadb_genetox_details[dtxsid %in% 'DTXSID0020153', ])
```

Now examine the first few rows.

```{r}
head(ccl4_genetox_details[dtxsid %in% 'DTXSID0020153', ])
```

Observe that the data is the same for each data set when restricting to the same chemical. This is because the information we are retrieving is specific to the chemical and not dependent on the chemical lists to which the chemical may belong.

```{r}
identical(ccl4_genetox_details[dtxsid %in% 'DTXSID0020153', ], 
          natadb_genetox_details[dtxsid %in% 'DTXSID0020153', ])
```


We now explore the assays present for chemicals in each data set. We first determine the unique values of the `assayCategory` column and then group by these values and determine the number of unique assays for each `assayCategory` value.

Determine the unique assay categories.

```{r}
ccl4_genetox_details[, unique(assayCategory)]
natadb_genetox_details[, unique(assayCategory)]
```
Determine the unique assays for each data set and list them.

```{r}
ccl4_genetox_details[, unique(assayType)]

natadb_genetox_details[, unique(assayType)]
```





Determine the number of assays per unique `assayCategory` value.

```{r}
ccl4_genetox_details[, .(Assays = length(unique(assayType))), 
                     by = .(assayCategory)]

natadb_genetox_details[, .(Assays = length(unique(assayType))),
                       by = .(assayCategory)]
```


We can analyze these results more closely, counting the number of assay results and grouping by `assayCategory`, and `assayType`. We also examine the different numbers of `assayCategory` and `assayTypes` values used.

```{r}
ccl4_genetox_details[, .N, by = .(assayCategory, assayType, assayResult)]
ccl4_genetox_details[, .N, by = .(assayCategory)]
```

We look at the `assayType` values and numbers of each for the three different `assayCategory` values.

```{r}
ccl4_genetox_details[assayCategory == 'in vitro', .N, by = .(assayType)]
ccl4_genetox_details[assayCategory == 'ND', .N, by = .(assayType)]
ccl4_genetox_details[assayCategory == 'in vivo', .N, by = .(assayType)]
```

Now we repeat this for NATADB.

```{r}
natadb_genetox_details[, .N, by = .(assayCategory, assayType, assayResult)]
natadb_genetox_details[, .N, by = .(assayCategory)]
```

Examine the number of rows for each `assayType` value by each `assaycategory` value.

```{r, R.options=list(width=150) }
natadb_genetox_details[assayCategory == 'in vitro', .N, by = .(assayType)]
natadb_genetox_details[assayCategory == 'ND', .N, by = .(assayType)]
natadb_genetox_details[assayCategory == 'in vivo', .N, by = .(assayType)]
```

### Answer to Environmental Health Question 3

:::question
*From these initial explorations of the data, we can answer **Environmental Health Question 3:*** After pulling the genotoxicity data for the different environmental contaminant data sets, list the assays associated with the chemicals in each data set. How many unique assays are there in each data set? What are the different assay categories and how many unique assays for each assay category are there?
:::

:::answer
**Answer**: There are 90 unique assays for CCl4 and 114 unique assays for NATADB. The different assay categories are "in vitro", "ND", and "in vivo", with 65 unique "in vitro" assays for CCl4 and 83 for NATADB, 3 unique "ND" assays for CCL4 and 3 for NATADB, and 22 unique "in vivo" assays for CCL4 and 28 for NATADB.
:::

Next, we dig into the results of the assays. One may be interested in looking at the number of chemicals for which an assay resulted in a positive or negative result for instance. We group by `assayResult` and determine the number of unique `dtxsid` values associated with each `assayResult` value.

```{r}
ccl4_genetox_details[, .(DTXSIDs = length(unique(dtxsid))), by = .(assayResult)]
natadb_genetox_details[, .(DTXSIDs = length(unique(dtxsid))), 
                       by = .(assayResult)]
```

### Answer to Environmental Health Question 4

:::question
*With this data we may now answer **Environmental Health Question 4:*** The genotoxicity data contains information on which assays have been conducted for different chemicals and the results of those assays. How many chemicals in each data set have a ‘positive’, ‘negative’, and ‘equivocal’ value for the assay result?
:::

:::answer
**Answer**: For CCL4, there are 64 unique chemicals that have a negative assay result, 53 that have a positive result, and 15 that have an equivocal result. For NATADB, there are 141 unique chemicals that have a negative assay result, 130 that have a positive result, and 48 that have an equivocal result. Observe that since there are 72 unique `dtxsid` values with assay results in CCL4 and 153 in NATADB, there are several chemicals that have multiple assay results.
:::

We now determine the chemicals from each data set that are known to have genotoxic effects. For this, we look to see which chemicals produce at least one positive response in the `assayResult` column.

```{r}
ccl4_genetox_details[, .(is_positive = any(assayResult == 'positive')), 
                     by = .(dtxsid)][is_positive == TRUE, dtxsid]
natadb_genetox_details[, .(is_positive = any(assayResult == 'positive')),
                       by = .(dtxsid)][is_positive == TRUE, dtxsid]
```

With so much genotoxicity data, let us explore this data for one chemical more deeply to get a sense of the assays and results present for it. We will explore the chemical with DTXSID0020153. We will look at the assays, the number of each type of result, and which correspond to "positive" results. To determine this, we group by `assayResult` and calculate `.N` for each group. We also isolate which were positive and output a data.table with the number of each type.

```{r}
ccl4_genetox_details[dtxsid == 'DTXSID0020153', .(Number = .N), 
                     by = .(assayResult)]
ccl4_genetox_details[dtxsid == 'DTXSID0020153' & assayResult == 'positive', 
                     .(Number_of_assays = .N), by = .(assayType)][order(-Number_of_assays),]
```

### Answer to Environmental Health Question 5

:::question
*With these data.tables, we may answer **Environmental Health Question 5:*** Based on the genotoxicity data reported for the chemical with DTXSID identifier DTXSID0020153, how many assays resulted in a positive/equivocal/negative value? Which of the assays were positive and how many of each were there for the most reported assays?
:::

:::answer
**Answer**: There were five assays that produced a negative result, 22 that produced a positive result, and one that produced an equivocal result. Of the 22 positive assays, "bacterial reverse mutation assay" and "Ames" were the most numerous, with three each.
:::


## Hazard Resource


Finally, we examine the hazard data associated with the chemicals in each data set. For each chemical, there will be potentially hundreds of rows of hazard data, so the returned results will be much larger than in most other API endpoints.

```{r}
ccl4_hazard <- get_hazard_by_dtxsid_batch(DTXSID = ccl4$dtxsid)
natadb_hazard <- get_hazard_by_dtxsid_batch(DTXSID = natadb$dtxsid)
```

We do some preliminary exploration of the data. First we determine the dimensions of the data sets.

```{r}
dim(ccl4_hazard)
dim(natadb_hazard)
```
Next we record the column names and display the first six results in the CCL4 hazard data.

```{r}
colnames(ccl4_hazard)
head(ccl4_hazard)
```

We determine the number of unique values in the `criticalEffect`, `supercategory`, and `toxvalType` columns for each data set.

The number of unique values for `criticalEffect`.

```{r}
length(ccl4_hazard[, unique(criticalEffect)])
length(natadb_hazard[, unique(criticalEffect)])
```
The number of unique values of `supercategory`.

```{r}
length(ccl4_hazard[, unique(supercategory)])
length(natadb_hazard[, unique(supercategory)])
```
The number of unique values for `toxvalType`.

```{r}
length(ccl4_hazard[, unique(toxvalType)])
length(natadb_hazard[, unique(toxvalType)])
```

Now we look at the number of entries per `supercategory`.

```{r}
ccl4_hazard[, .N, by = .(supercategory)]
natadb_hazard[, .N, by = .(supercategory)]

```
With over 22,000 results for the `supercategory` value "Point of Departure" for each data set, we dig into this further.

We determine the number of rows grouped by `toxvalType` that have the "Point of Departure" `supercategory` value, and display this descending.
```{r}
ccl4_hazard[ supercategory %in% 'Point of Departure', .N, 
             by = .(toxvalType)][order(-N),]
natadb_hazard[ supercategory %in% 'Point of Departure', .N, 
               by = .(toxvalType)][order(-N),]
```

We explore "NOEC" and "LOEC" further. Let us look at the the case when `media` value is either "salt water" or "fresh water". For this, we will recover the minimum value of "NOEC" and "LOEC" for each chemical in each data set.

First, we look at soil. We order by `toxvalType` and by the minimum `toxvalNumeric` value in each group, descending.

```{r}
ccl4_hazard[media %in% 'salt water' & toxvalType %in% c('LOEC', 'NOEC'), 
            .(toxvalNumeric = min(toxvalNumeric)), 
            by = .(toxvalType, toxvalUnits, dtxsid)][order(toxvalType,
                                                           -toxvalNumeric)]
natadb_hazard[media %in% 'salt water' & toxvalType %in% c('LOEC', 'NOEC'), 
              .(toxvalNumeric = min(toxvalNumeric)), 
              by = .(toxvalType, toxvalUnits, dtxsid)][order(toxvalType,
                                                             -toxvalNumeric)]
```

Next we look at fresh water, repeating the same grouping and ordering as in the previous case.

```{r}
ccl4_hazard[media %in% 'fresh water' & toxvalType %in% c('LOEC', 'NOEC'), 
            .(toxvalNumeric = min(toxvalNumeric)), 
            by = .(toxvalType, toxvalUnits, dtxsid)][order(toxvalType,
                                                           -toxvalNumeric)]
natadb_hazard[media %in% 'fresh water' & toxvalType %in% c('LOEC', 'NOEC'), 
              .(toxvalNumeric = min(toxvalNumeric)), 
              by = .(toxvalType, toxvalUnits, dtxsid)][order(toxvalType,
                                                             -toxvalNumeric)]
```

Now, let us restrict our attention to human hazard and focus on the exposure routes given by inhalation and oral. 

First, let us determine the exposure routes in general.

```{r}
ccl4_hazard[humanEcoNt %in% 'human health', unique(exposureRoute)]
natadb_hazard[humanEcoNt %in% 'human health', unique(exposureRoute)]
```

Then, let's focus on the inhalation and oral exposure routes for human hazard.



To answer this, filter the data into the corresponding exposure routes, then group by `exposureRoute` and `riskAssessmentClass`, and finally count the number of instances for each grouping. To determine the most represented class, one can order the results descending.

```{r}
ccl4_hazard[humanEcoNt %in% 'human health' & 
              exposureRoute %in% c('inhalation', 'oral'), .(Hits = .N), 
            by = .(exposureRoute, riskAssessmentClass)][order(exposureRoute, 
                                                              -Hits)]
natadb_hazard[humanEcoNt %in% 'human health' & 
                exposureRoute %in% c('inhalation', 'oral'), .(Hits = .N), 
              by = .(exposureRoute, riskAssessmentClass)][order(exposureRoute,
                                                                -Hits)]
```

### Answer to Environmental Health Question 6

:::question
*With these results we may answer **Environmental Health Question 6:*** After pulling the hazard data for the different data sets, list the different exposure routes for which there is data. What are the unique risk assessment classes for hazard values for the oral route and for the inhalation exposure route? For each such exposure route, which risk assessment class is most represented by the data sets?
:::

:::answer
**Answer**: We listed the general exposure routes above for the hazard data associated with the chemicals in each data set. Restricting our attention to human hazard data, the "acute" `riskAssessmentClass` is most represented by the inhalation exposure route and "chronic" for the oral exposure route for both the CCL4 and NATADB data sets. 
:::


We now drill down a little further before moving into a different path for data exploration. We explore the different types of toxicity values present in each data set for the inhalation and oral exposure routes, and then see which of these are common to both exposure routes for each data set.



To answer this, we filter the rows to the "human health" `humanEcoNT` value and "inhalation" or "oral" `exposureRoute` value. Then we return the unique values that `toxvalType` takes.

First we look at CCL4.

```{r}
ccl4_hazard[humanEcoNt %in% 'human health' &
              exposureRoute %in% c('inhalation'), unique(toxvalType)]
ccl4_hazard[humanEcoNt %in% 'human health' &
              exposureRoute %in% c('oral'), unique(toxvalType)]
intersect(ccl4_hazard[humanEcoNt %in% 'human health' & exposureRoute %in% 'inhalation', unique(toxvalType)], ccl4_hazard[humanEcoNt %in% 'human health' & exposureRoute %in% 'oral', unique(toxvalType)])
```

Then we look at NATADB.

```{r}
natadb_hazard[humanEcoNt %in% 'human health' & 
                exposureRoute %in% c('inhalation'), unique(toxvalType)]
natadb_hazard[humanEcoNt %in% 'human health' & 
                exposureRoute %in% c('oral'), unique(toxvalType)]
intersect(natadb_hazard[humanEcoNt %in% 'human health' & exposureRoute %in% 'inhalation', unique(toxvalType)], natadb_hazard[humanEcoNt %in% 'human health' & exposureRoute %in% 'oral', unique(toxvalType)])
```

### Answer to Environmental Health Question 7

:::question
*With the results above, we may answer **Environmental Health Question 7:*** There are several types of toxicity values for each exposure route. List the unique toxicity values for the oral and inhalation routes. What are the unique types of toxicity values for the oral route and for the inhalation route? How many of these are common to both the oral and inhalation routes for each data set?
:::

:::answer
**Answer**: There are 21 toxicity value types shared between the oral and inhalation exposure routes for CCL4 and 36 for NATADB. The lists above indicate the variety of toxicity values present in the hazard data for the two different exposure routes we have considered.
:::


For the next data exploration, We will turn to the `riskAssessmentClass` value of "developmental". We will examine the "NOAEL" and "LOAEL" values for chemicals with oral exposure, human hazard, and a `riskAssessmentClass` value of "developmental". We also examine the units to determine whether any unit conversions are necessary to compare numeric values.

```{r}
ccl4_hazard[humanEcoNt %in% 'human health' & exposureRoute %in% 'oral' &
              riskAssessmentClass %in% 'developmental' & 
              toxvalType %in% c('NOAEL', 'LOAEL'), ]
ccl4_hazard[humanEcoNt %in% 'human health' & exposureRoute %in% 'oral' &
              riskAssessmentClass %in% 'developmental' &
              toxvalType %in% c('NOAEL', 'LOAEL'), unique(toxvalUnits)]
natadb_hazard[humanEcoNt %in% 'human health' & exposureRoute %in% 'oral' &
                riskAssessmentClass %in% 'developmental' &
                toxvalType %in% c('NOAEL', 'LOAEL'), ]
natadb_hazard[humanEcoNt %in% 'human health' & exposureRoute %in% 'oral' &
                riskAssessmentClass %in% 'developmental' & 
                toxvalType %in% c('NOAEL', 'LOAEL'), unique(toxvalUnits)]
```

Observe that for both CCL4 and NATADB, the units are given by "mg/kg-day", "mg/kg" and "-". In this case, we treat "mg/kg-day" and "mg/kg" the same and exclude "-". We group by DTXSID to find the lowest or highest value.

```{r}
ccl4_hazard[humanEcoNt %in% 'human health' & exposureRoute %in% 'oral' &
            riskAssessmentClass %in% 'developmental' & 
            toxvalType %in% c('NOAEL', 'LOAEL') & (toxvalUnits != '-'),
            .(numeric_value = min(toxvalNumeric), 
            units = toxvalUnits[[which.min(toxvalNumeric)]]), 
            by = .(dtxsid, toxvalType)]
natadb_hazard[humanEcoNt %in% 'human health' & exposureRoute %in% 'oral' &
              riskAssessmentClass %in% 'developmental' & 
              toxvalType %in% c('NOAEL', 'LOAEL') & (toxvalUnits != '-'), 
              .(numeric_value = min(toxvalNumeric), 
              units = toxvalUnits[[which.min(toxvalNumeric)]]), 
              by = .(dtxsid, toxvalType)]
```

Now, we also explore the values of "RfD", "RfC", and "cancer slope factor" of the `toxvalType` rows. We first determine the set of units for each, make appropriate conversions if necessary, and then make comparisons.

```{r}
ccl4_hazard[humanEcoNt %in% 'human health' & toxvalType %in% 
            c('cancer slope factor', 'RfD', 'RfC'), .N, 
            by = .(toxvalType, toxvalUnits)][order(toxvalType, -N)]
natadb_hazard[humanEcoNt %in% 'human health' & toxvalType %in%
              c('cancer slope factor', 'RfD', 'RfC'), .N, 
              by = .(toxvalType, toxvalUnits)][order(toxvalType, -N)]
```
For CCL4, there are three inequivalent sets of units that need conversions. We convert to "mg/m3", which means scaling values given in "g/m3" by 1E3 and values given in "ug/m3" by 1E-3. The Rfd units and cancer slope factor units require no conversions although we do remove the cancer slope factor unit of "mg/kg-day".

For NATADB, we need to convert RfC values from ppm to mg/m3, with a conversion factor that relies on the molecular weight of the chemical in question. We will remove these from consideration for now. The units for RfD and cancer slope factor require no conversions although we do remove the cancer slope factor unit of "mg/kg-day".

First, we filter and separate out the relevant data subsets.

```{r}
# Separate out into relevant data subsets
ccl4_csf <- ccl4_hazard[humanEcoNt %in% 'human health' & 
                          toxvalType %in% c('cancer slope factor') & (toxvalUnits != 'mg/kg-day'), ]
ccl4_rfc <- ccl4_hazard[humanEcoNt %in% 'human health' & 
                          toxvalType %in% c('RfC'), ]
ccl4_rfd <- ccl4_hazard[humanEcoNt %in% 'human health' & 
                          toxvalType %in% c('RfD'), ]
```

Then we start to handle the unit conversions.

```{r}
# Set mass by volume units to mg/m3, so scale g/m3 by 1E3 and ug/m3 by 1E-3
ccl4_rfc[toxvalUnits == 'mg/m3', conversion := 1]
ccl4_rfc[toxvalUnits == 'g/m3', conversion := 1E3]
ccl4_rfc[toxvalUnits == 'ug/m3', conversion := 1E-3]
ccl4_rfc[toxvalUnits %in% c('mg/m3', 'g/m3', 'ug/m3'), units := 'mg/m3']
# Set mass by mass units to mg/kg
ccl4_rfd[toxvalUnits %in% c('mg/kg-day', 'mg/kg'), conversion := 1]
ccl4_rfd[toxvalUnits %in% c('mg/kg-day', 'mg/kg'), units := 'mg/kg']
```

Then aggregate the data.

```{r}
# Run data aggregations grouping by dtxsid and taking either the max or the min
# depending on the toxvalType we are considering.
ccl4_csf[,.(numeric_value = max(toxvalNumeric), 
            units = toxvalUnits[which.max(toxvalNumeric)]), 
         by = .(dtxsid)][order(-numeric_value),]
ccl4_rfc[,.(numeric_value = min(toxvalNumeric*conversion), 
            units = units[which.min(toxvalNumeric*conversion)]), 
         by = .(dtxsid)][order(numeric_value),]
ccl4_rfd[,.(numeric_value = min(toxvalNumeric*conversion), 
            units = units[which.min(toxvalNumeric*conversion)]), 
         by = .(dtxsid)][order(numeric_value),]
```

Repeat the process for NATADB, first separating out the relevant subsets of the data.

```{r}
# Separate out into relevant data subsets
natadb_csf <- natadb_hazard[humanEcoNt %in% 'human health' & 
                              toxvalType %in% c('cancer slope factor') & (toxvalUnits != 'mg/kg-day'), ]
natadb_rfc <- natadb_hazard[humanEcoNt %in% 'human health' &
                              toxvalType %in% c('RfC'), ]
natadb_rfd <- natadb_hazard[humanEcoNt %in% 'human health' & 
                              toxvalType %in% c('RfD'), ]
```

Now handle the unit conversions.

```{r}
# Set mass by mass units to mg/kg. Note that ppm is already in mg/kg
natadb_rfc <- natadb_rfc[toxvalUnits != 'ppm',]
natadb_rfd[, units := 'mg/kg-day']
```

Finally, aggregate the data.

```{r}
# Run data aggregations grouping by dtxsid and taking either the max or the min
# depending on the toxvalType we are considering.
natadb_csf[, .(numeric_value = max(toxvalNumeric), 
               units = toxvalUnits[which.max(toxvalNumeric)]), 
           by = .(dtxsid)][order(-numeric_value),]
natadb_rfc[, .(numeric_value = min(toxvalNumeric), 
               units = toxvalUnits[which.min(toxvalNumeric)]), 
           by = .(dtxsid)][order(numeric_value),]
natadb_rfd[, .(numeric_value = min(toxvalNumeric), 
               units = units[which.min(toxvalNumeric)]), 
           by = .(dtxsid)][order(numeric_value),]
```

### Answer to Environmental Health Question 8

:::question
*With these results, we may answer **Environmental Health Question 8:*** When examining different toxicity values, the data may be reported in multiple units. To assess the relative hazard from this data, it is important to take into account the different units and adjust accordingly. List the units reported for the cancer slope factor, reference dose, and reference concentration values associated with the oral and inhalation exposure routes for human hazard. Which chemicals in each data set have the highest cancer slope factor, lowest reference dose, and lowest reference concentration values?
:::

:::answer
**Answer**: The units for these three toxicity value types for CCL4 are given by "mg/m3", "g/m3", "ug/m3" for RfC, "mg/kg-day", "mg/kg" for RfD, and "(mg/kg-day)-1" for Cancer Slope Factor. For NATADB, the units for RfC are given by "mg/m3" and "ppm", for RfD by "mg/kg-day", "mg/kg", and for Cancer Slope Factor by "(mg/kg-day)-1".  For CCL4, the chemical DTXSID2021028 has the highest CsF at 150 (mg/kg-day)-1, the chemical DTXSID1031040 has the lowest RfC value at 6.0e-12 mg/m3, and the chemical DTXSID7021029 has the lowest RfD value at 4e-6 mg/kg. For NATADB, the chemical DTXSID2020137 has the highest CsF at 500 (mg/kg-day)-1, the chemical DTXSID1020516 has the lowest RfC value at 2.0e-6 mg/m3, and the chemical DTXSID7021029 had the lowest RfD at 4e-6 mg/kg-day.
:::

## Concluding Remarks

In conclusion, we explored how one can access publicly available data from the CompTox Chemicals Dashboard programmatically using the CTX APIs via the *ctxR* R package. In the examples above, we investigated different types of data associated with chemicals, visualized and aggregated the data, and employed different data wrangling techniques using data.tables to answer the proposed environmental health questions. With these tools, one can build workflows that take in a list of chemicals and gather and process data associated with those chemicals through the CTX APIs. Consider how you might use this functionality for building models in your own work.

<br>

<label class="tykfont">
Test Your Knowledge 
</label>

:::tyk

Try running the same analysis of physical-chemical properties, genotoxicity data, and hazard data on a different pair of data sets available from the CCD. For instance, try pulling the data set 'BIOSOLIDS2021' and work through the same steps we completed in this module to investigate the chemicals in this list to gain a better understanding of the associated data.
:::



```{r breakdown, echo = FALSE, results = 'hide'}
# This chunk will be hidden in the final product. It serves to undo defining the
# custom print function to prevent unexpected behavior after this module during
# the final knitting process

knit_print.data.table = knitr::normal_print
  
registerS3method(
  "knit_print", "data.table", knit_print.data.table,
  envir = asNamespace("knitr")
)

# A demonstration that this indeed returns the print process back to normal for
# data.table objects
head(ccl4, 1)
```

# 7.4 Database Integration: Air Quality, Mortality, and Environmental Justice Data

The development of this training module was led by Cavin Ward-Caviness with contributions from Alexis Payton.

*Disclaimer: The views expressed in this document are those of the author and do not necessarily reflect the views or policies of the U.S. EPA.*

## Introduction to Training Module

This training module provides an example analysis based on the integration of data across multiple environmental health databases. This module specifically guides trainees through an explanation of how the data were downloaded and organized, and then details the loading of required packages and datasets. Then, code is provided for visualizing county-level air pollution measures, including PM~2.5~, NO~2~, and SO~2~. These measures were obtained through U.S. EPA monitoring stations are visualized here as the yearly average. Air pollution concentrations are then evaluated for potential relationship to the health outcome, mortality. Specifically, age-adjusted mortality rates are organized and statistically related to PM~2.5~ concentrations through linear regression modeling. Crude statistical models are first provided that do not take into account the influence of potential confounders. Then, statistical models are used that adjust for potential confounders, including adult smoking rates, obesity, food environment indicators, physical activity, employment status, rural vs urban living percentages, sex, ethnicity, and race. Results from these models point to the finding that areas with higher percentages of African-Americans may be experiencing higher impacts from PM~2.5~ on mortality. This relationship is of high interest, as it represents a potential environmental justice issue.

## Introduction to Exposure and Health Databases

In this training module, we will use publicly available exposure and health databases to examine associations between air quality and mortality across the entire U.S. Specific databases that we will query include the following:

+ EPA Air Quality data: As an example air pollutant exposure dataset, 2016 annual average data from the EPA Air Quality System database will be analyzed,  using data downloaded and organized from the following website: <https://aqs.epa.gov/aqsweb/airdata/annual_conc_by_monitor_2016.zip>

+ CDC Health Outcome data: As an example health outcome dataset, the 2016 CDC Underlying Cause of Death dataset, from the WONDER (Wide-ranging ONline Data for Epidemiologic Research) website  will be analyzed, using All-Cause Mortality Rates downloaded and organized from the following website: <https://wonder.cdc.gov/ucd-icd10.html>

+ Human covariate data: The potential influence of covariates (e.g., race) and other confounders will be analyzed using data downloaded and organized from the following 2016 county-level resource: <https://www.countyhealthrankings.org/explore-health-rankings/rankings-data-documentation/national-data-documentation-2010-2019>


### Training Module's Environmental Health Questions

This training module was specifically developed to answer the following environmental health questions:

1. What areas of the U.S. are most heavily monitored for air quality?
2. Is there an association between long-term, ambient PM~2.5~ concentrations and mortality at the county level? Stated another way we are asking: Do counties with higher annual average PM~2.5~ concentrations also have higher all-cause mortality rates?
3. What is the difference when running crude statistical models vs. statistical models that adjust for potential confounding, when evaluating the relationship between PM~2.5~ and mortality?
4. Do observed associations differ when comparing between counties with a higher vs. lower percentage of African-Americans which can indicate environmental justice concerns?


### Script Preparations

#### Cleaning the global environment
```{r}
rm(list=ls())
```

#### Installing required R packages
If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you
```{r, results=FALSE, message=FALSE}
if (!requireNamespace("sf"))
  install.packages("sf");
if (!requireNamespace("tidyverse"))
  install.packages("tidyverse");
if (!requireNamespace("ggthemes"))
  install.packages("ggthemes");
```

#### Loading R packages required for this session
```{r, results=FALSE, message=FALSE}
library(sf)
library(tidyverse)
library(ggthemes)
library(DT)
```

#### Set your working directory
```{r, eval=FALSE, echo=TRUE}
setwd("/filepath to where your input files are")
```

Let's start by loading the datasets needed for this training module. As detailed in the introduction, these data were previously downloaded and organized, and specifically made available for this training exercise as a compiled RDataset, containing organized dataframes ready to analyze.

We can now read in these organized data using the `load()` function.
```{r}
load("Module7_4_Input/Module7_4_InputData.RData")
```

First let's take a look at the geographic data, starting with the county-level shapefile (`counties_shapefile`). This dataframe contains the location information for the counties which we will use for visualizations.
```{r}
head(counties_shapefile)
```
These geographic data are represented by the following columns. (Some columns are not described below, since we don't need them for these analyses.): 

+ `STATEFP`: State FIPS code (1 or 2 digits)
+ `COUNTYFP`: County FIPS Code (3 digits)
+ `GEOID`: Geographic identifier that combines the state and county FIPS codes
+ `NAME`: County name
+ `geometry`: Latitude and longitude coordinates

Now let's view the EPA Air Quality Survey (AQS) data (`epa_ap_county`) collected from 2016. This dataframe represents county-level air quality measures, as detailed above. This dataframe is in melted (or long) format, meaning that different air quality measures are organized across rows, with variable measure indicators in the `Parameter.Name`, `Units.of.Measure`, and `County_Avg` columns.
```{r}
head(epa_ap_county)
```
These air quality data are represented by the following columns:

+ `State.Code`: State FIPS code (1 or 2 digits)
+ `State.Name`: State name
+ `County.Code`: County FIPS code (1-3 digits)
+ `County.Name`: County name
+ `State_County_Code`: Combined state and county code (separated by a 0)
+ `Parameter.Name`: Name of the air pollutant
+ `Units.of.Measure`: Units of measurement
+ `County_Avg`: County average

These data can be restructured to view air quality measures as separate variables labeled across columns using:
```{r}
# transform from the "long" to "wide" format for the pollutants
epa_ap_county <- epa_ap_county %>% 
  select(-Units.of.Measure) %>% 
  unique() %>% 
  tidyr::spread(Parameter.Name, County_Avg)

head(epa_ap_county)
```
Note that we can now see the specific pollutant variables `NO2`, `PM25`, and `SO2` on the far right.


### Population-Weighted vs. Unweighted Exposures
Here we pause briefly to speak on population-weighted vs unweighted exposures. The analysis we will be undertaking is known as an "ecological" analysis where we are looking at associations by area, e.g. county. When studying environmental exposures by area a common practice is to try to weight the exposures by the population so that exposures better represent the "burden" faced by the population. Ideally for this you would want a systematic model or assessment of the exposure that corresponded with a fine-scale population estimate so that for each county you could weight exposures within different areas of the county by the population exposed. This sparse monitor data (we will examine the population covered later in the tutorial) is not population weighted, but should you see similar analyses with population weighting of exposures you should simply be aware that this better captures the "burden" of exposure experienced by the population within the area estimated, typically zip code or county.

Now let's view the CDC's mortality dataset collected from 2016 (`cdc_mortality`):
```{r}
head(cdc_mortality)
```
These mortality data are represented by the following columns. We'll just ignore the `Notes` column for our purposes:

 + `County`: County name with the state abbreviation
 + `County.Code`: Combined state and county code (separated by a 0)
 + `Deaths`: Number of deaths in 2016
 + `Population`: County population in 2016
 + `Crude.Rate`: Death rate ($\frac{Number~of~Deaths}{Population}* 100,000$)
 + `Age.Adjusted.Rate`: age-adjusted death rate ($\sum{(Age~Specific~Death~Rate * Standard~Population~Weight)} * 100,000$)
 + `Age.Adjusted.Rate.Standard.Error`: Standard error of the age-adjusted rate

We can create a visualization of the age-adjusted death rate and air pollutants throughout the U.S. to further inform what these data look like:
```{r county plot, fig.align = "center"}
# Can merge them by the FIPS county code which we need to create for the counties_shapefile
counties_shapefile$State_County_Code <- as.character(as.numeric(paste0(counties_shapefile$STATEFP, counties_shapefile$COUNTYFP)))

# Let's merge in the air pollution and mortality data and plot it
counties_shapefile <- merge(counties_shapefile, epa_ap_county, by.x = "State_County_Code", by.y = "State_County_Code", all.x=TRUE)
counties_shapefile <- merge(counties_shapefile, cdc_mortality, by.x = "State_County_Code", by.y = "County.Code")

# Will remove alaska and hawaii just so we can look at the continental USA
county_plot <- subset(counties_shapefile, !STATEFP %in% c("02","15"))

# We can start with a simple plot of age-adjusted mortality rate, PM2.5, NO2, and SO2 levels across the U.S.
plot(county_plot[,c("Age.Adjusted.Rate","PM25","NO2","SO2")])
```

You can see that these result in the generation of four different nationwide plots, showing the distributions of age-adjusted mortality rates, PM~2.5~ concentrations, NO~2~ concentrations, and SO~2~ concentrations, averaged per county.

Let's make a nicer looking plot with `ggplot()`, looking just at PM~2.5~ levels:
```{R fig.align = "center"}
ggplot(data = county_plot) + 
  geom_sf(aes(fill = PM25)) +
  
  # Changing colors
  scale_fill_viridis_c(option ="plasma", name ="PM2.5", 
                       guide = guide_colorbar(
                         direction = "horizontal",
                         barheight = unit(2, units = "mm"),
                         barwidth = unit(50, units = "mm"),
                         draw.ulim = F,
                         title.position = 'top',
                         # some shifting around
                         title.hjust = 0.5,
                         label.hjust = 0.5)) +
  ggtitle(expression(2016~Annual~PM[2.5]~EPA~Monitors)) + 
  theme_map() +
  theme(plot.title = element_text(hjust = 0.5, size = 22)) 

```

### Answer to Environmental Health Question 1
:::question
*With this, we can answer **Environmental Health Question #1***: What areas of the U.S. are most heavily monitored for air quality?
:::

:::answer
**Answer**: We can tell from the PM~2.5~ specific plot that air monitors are densely located in California, and other areas with high populations (including the East Coast), while large sections of central U.S. lack air monitoring data.
:::

<br>

## Analyzing Relationships between PM~2.5~ and Mortality
### Crude Model
Now the primary question is whether counties with higher PM~2.5~ also have higher mortality rates. To answer this question, first we need to perform some data merging in preparation for this analysis.
```{r merging data}
# Merging mortality and air pollution data
model_data <- merge(epa_ap_county, cdc_mortality, by.x = "State_County_Code", by.y = "County.Code")
```

As we saw in the above plot, only a portion of the USA is covered by PM~2.5~ monitors. Let's see what our population coverage is
```{r}
sum(model_data$Population, na.rm = TRUE)
sum(cdc_mortality$Population, na.rm = TRUE)
sum(model_data$Population, na.rm = TRUE)/sum(cdc_mortality$Population, na.rm = TRUE)*100
```

We can do a quick visual inspection of this using a scatter plot which will also let us check for unexpected distributions of the data (always a good idea)
```{r fig.align = "center", warning=FALSE}
ggplot(model_data) + 
  geom_point(aes(x = Age.Adjusted.Rate, y = PM25)) +
  ggtitle(expression(PM[2.5]~by~Mortality~Rate)) + 
  xlab('Age-adjusted Mortality Rate') + ylab(expression(PM[2.5]))  # changing axis labels 
```

The univariate correlation is a simple way of quantifying this potential relationship, though it does not nearly tell the complete story. Just as a starting point, let's run this simple univariate correlation calculation using the `cor()` function.
```{r}
cor(model_data$Age.Adjusted.Rate, model_data$PM25, use = "complete.obs")
```

Now, let's obtain a more complete estimate of the data through regression modeling. As an initial starting point, let's run this model without any confounders (also known as a 'crude' model).

A simple linear regression model in R can be carried out using the `lm()` function. Here, we are evaluating age-adjusted mortality rate (`age.adjusted.rate`) as the dependent variable, and PM~2.5~ as the independent variable. Values used in evaluating this model were weighted to adjust for the fact that some counties have higher precision in their age-adjusted mortality rate (represented by a smaller age-adjusted rate standard error).
```{r}
# running the linear regression model
m <- lm(Age.Adjusted.Rate ~ PM25,
        data = model_data, weights = 1/model_data$Age.Adjusted.Rate.Standard.Error)   
# viewing the model results through the summary function
summary(m)   
```

Shown here are summary level statistics summarizing the results of the linear regression model.

In the model summary we see several features:

+ `Estimate`: the regression coefficient which tells us the relationship between a 1 ug/m3 change (elevation) in PM~2.5~ and the age-adjusted all-cause mortality rate
+ `Std. Error`: the standard error of the estimate
+ `t value`:  represents the T-statistic which is the test statistic for linear regression models and is simply the `Estimate` divided by `Std. Error`. This t value is compared with the Student's T distribution in order to determine the p-value (`Pr(>|t|)`).

The residuals are the difference between the predicted outcome (age-adjusted mortality rate) and known outcome from the data. For linear regression to be valid this should be normally distributed. The residuals from a linear model can be extracted using the `residuals()` function and plotted to see their distribution.

### Answer to Envrionmental Health Question 2
:::question
*With this, we can answer **Environmental Health Question #2***: Is there an association between long-term, ambient PM~2.5~ concentrations and mortality at the county level? 
:::

:::answer
**Answer**: Based on these model results, there may indeed be an association between PM~2.5~ concentrations and mortality (p=0.0019)
:::

<br>

### Adjusting for Covariates

To more thoroughly examine the potential relationship between PM~2.5~ concentrations and mortality it is absolutely essential to adjust for confounders. Let's start by viewing the human covariate data that contains some confounders of interest.
```{r}
datatable(county_health)
```
These covariate data are represented by the following columns:

+ `State.FIPS.Code`: Single digit code assigned to each state
+ `County.FIPS.Code`: FIPS code unique to counties within a particular state (1 digit)
+ `County.5.digit.FIPS.Code`: Unique county FIPS code (5 digits)                    
+ `State.Abbreviation`: State abbreviation
+ `Name`: County Name (or state name that contains the state average)
+ `Release.Year`: Year the data was publicly released 
+ Various confounders indicative of race, ethnicity, socioeconomic status, population density, education, health-related variables in columns 7-36

```{r}
# Merging the covariate data in with the AQS data
model_data <- merge(model_data, county_health, by.x = "State_County_Code", 
                    by.y = "County.5.digit.FIPS.Code", all.x = TRUE)

# Now we add some relevant confounders to the linear regression model
m <- lm(Age.Adjusted.Rate ~ PM25 + Adult.smoking + Adult.obesity + Food.environment.index + 
          Physical.inactivity + High.school.graduation + Some.college + Unemployment + 
          Violent.crime + Percent.Rural + Percent.Females + Percent.Asian + 
          Percent.Non.Hispanic.African.American + Percent.American.Indian.and.Alaskan.Native + 
          Percent.NonHispanic.white, data = model_data, weights = 1/model_data$Age.Adjusted.Rate.Standard.Error)

# And finally we check to see if the statistical association persists
summary(m)
```

### Answer to Envrionmental Health Question 3
:::question
*With this, we can answer **Environmental Health Question #3***: What is the difference when running crude statistical models vs. statistical models that adjust for potential confounding, when evaluating the relationship between PM~2.5~ and mortality?
:::

:::answer
**Answer**: The relationship between PM~2.5~ and mortality remains statistically significant when confounders are considered (p=0.023), though is not as significant as when running the crude model (p=0.0019).
:::

<br>

## Environmental Justice Considerations
**Environmental justice** is the study of how societal inequities manifest in differences in environmental health risks either due to greater exposures or a worse health response to exposures. Racism and racial discrimination are major factors in both how much pollution people are exposed to as well what their responses might be due to other co-existing inequities (e.g. poverty, access to healthcare, food deserts). Race is a commonly used proxy for experiences of racism and racial discrimination.

Here we will consider the race category of `Non-Hispanic African-American` to investigate if pollution levels differ by percent African-Americans in a county and if associations between PM~2.5~ and mortality also differ by this variable, which could indicate environmental justice-relevant issues revealed by this data. We will specifically evaluate data distributions across counties with the highest percentage of African-Americans (top 25%) vs. lowest percentage of African-Americans (bottom 25%).  

First let's visualize the distribution of African-American percentage in these data
```{r fig.align = "center", warning=FALSE, message =FALSE}
ggplot(model_data) + 
  geom_histogram(aes(x = Percent.Non.Hispanic.African.American*100)) + 
  ggtitle("Percent African-American by County") + 
  xlab('Percent')
```

Let's look at a summary of the data
```{r}
summary(model_data$Percent.Non.Hispanic.African.American)
```

We can compute quartiles of the data
```{r}
model_data$AA_quartile <- with(model_data, cut(Percent.Non.Hispanic.African.American, 
                                breaks = quantile(Percent.Non.Hispanic.African.American, probs = seq(0,1, by = 0.25), na.rm = TRUE), 
                                include.lowest = TRUE, ordered_result = TRUE, labels = FALSE))
```

Then we can use these quartiles to see that as the Percent African-American increases so does the PM~2.5~ exposure by county
```{r}
AA_summary <- model_data %>% 
  filter(!is.na(Percent.Non.Hispanic.African.American)) %>% 
  group_by(AA_quartile) %>% 
  summarise(Percent_AA = mean(Percent.Non.Hispanic.African.American, na.rm = TRUE), Mean_PM25 = mean(PM25, na.rm = TRUE))

AA_summary
```

Now that we can see this trend, let's add some statistics.
Let's specifically compare the relationships between PM~2.5~ and mortality within the bottom 25% AA counties (quartile 1); and also the highest 25% AA counties (quartile 4)
```{r}
# first need to subset the data by these quartiles of interest
low_AA <- subset(model_data, AA_quartile == 1)
high_AA <- subset(model_data, AA_quartile == 4)

# then we can run the relevant statistical models
m.low <- lm(Age.Adjusted.Rate ~ PM25 + Adult.smoking + Adult.obesity + Food.environment.index + Physical.inactivity +
          High.school.graduation + Some.college + Unemployment + Violent.crime + Percent.Rural + Percent.Females +
          Percent.Asian + Percent.American.Indian.and.Alaskan.Native + Percent.NonHispanic.white,
        data = low_AA, weights = 1/low_AA$Age.Adjusted.Rate.Standard.Error)

m.high <- lm(Age.Adjusted.Rate ~ PM25 + Adult.smoking + Adult.obesity + Food.environment.index + Physical.inactivity +
          High.school.graduation + Some.college + Unemployment + Violent.crime + Percent.Rural + Percent.Females +
          Percent.Asian + Percent.American.Indian.and.Alaskan.Native + Percent.NonHispanic.white,
        data = high_AA, weights = 1/high_AA$Age.Adjusted.Rate.Standard.Error)


# We see a striking difference in the associations
rbind(c("Bottom 25% AA Counties",round(summary(m.low)$coefficients["PM25",c(1,2,4)],3)),
      c("Top 25% AA Counties",round(summary(m.high)$coefficients["PM25",c(1,2,4)],3)))

```

### Answer to Envrionmental Health Question 4
:::question
*With this, we can answer **Environmental Health Question #4***: Do observed associations differ when comparing between counties with a higher vs. lower percentage of African-Americans which can indicate environmental justice concerns?
:::

:::answer
**Answer**: Yes. Counties with the highest percentage of African-Americans (top 25%) demonstrated a highly significant association between PM~2.5~ and age-adjusted mortality, even when adjusting for confounders (p=0.001), meaning that the association between PM~2.5~ and mortality within these counties may be exacerbated by factors relevant to race. Conversely, counties with the lowest percentages of African-Americans (bottom 25%) did not demonstrate a significant association between PM~2.5~ and age-adjusted mortality, indicating that these counties may have lower environmental health risks due to factors correlated with race.
:::

<br>

## Concluding Remarks
In conclusion, this training module serves as a novel example data integration effort of high relevance to environmental health issues. Databases that were evaluated here span exposure data (i.e., Air Quality System data), health outcome data (i.e., mortality data), and county-level characteristics on healthcare, food environment, and other potentially relevant confounders (i.e., county-level variables that may impact observed relationships), and environmental justice data (e.g., race). Many different visualization and statistical approaches were used, largely based on linear regression modeling and county-level characteristic stratifications. These example statistics clearly support the now-established relationship between PM~2.5~ concentrations in the air and mortality. Importantly, these related methods can be tailored to address new questions to increase our understanding between exposures to chemicals in the environment and adverse health outcomes, as well as the impact of different individual or area characteristics on these relationships - particularly those that might relate to environmental justice concerns.

<br>

<label class="tykfont">
Test Your Knowledge 
</label>

:::tyk
This training module provided some examples looking at PM~2.5~ concentration data. Using the same "Module7_4_InputData.data" file (also saved as "Module7_4_TYKInput.data"), let's ask similar questions but now looking at NO~2~ concentration data.

1. Is there an association between long-term, ambient NO~2~ concentrations and age-adjusted mortality at the county level (i.e., the crude model results)?
2. After adjusting for covariates, is there an association between long-term, ambient NO~2~ concentrations and age-adjusted mortality at the county level (i.e., the adjusted model results)?
:::


