# (PART\*) Chapter 7 Environmental<br> Health Database Mining {-}

# 7.1 Comparative Toxicogenomics Database

This training module was developed by Lauren E. Koval, Kyle R. Roell, and Julia E. Rager. 

All input files (script, data, and figures) can be downloaded from the [UNC-SRP TAME2 GitHub website](https://github.com/UNCSRP/TAME2).

```{r , include=FALSE}
#set default values for R Markdown "knitting" to HTML, Word, or PDF
knitr::opts_chunk$set(echo = TRUE) #print code chunks
```


## Introduction to Training Module

The Comparative Toxicogenomics Database (CTD) is a publicly available, online database that provides manually curated information about chemical-gene/protein interactions, chemical-disease and gene-disease relationships. CTD also recently incorporated curation of exposure data and chemical-phenotype relationships.

CTD is located at: http://ctdbase.org/. Here is a screenshot of the CTD homepage (as of August 5, 2021):
```{r, echo=FALSE, fig.align='center'} 
#knitr::include_graphics("_book/TAME_Toolkit_files/figure-html/Module3_1_CTD_homepage.jpg")
knitr::include_graphics("Chapter_7/Module7_1_Input/Module7_1_Image1.jpg")
```

In this module, we will be using CTD to access and download data to perform data organization and analysis as an applications-based example towards environmental health research. This activity represents a demonstration of basic data manipulation, filtering, and organization steps in R, while highlighting the utility of CTD to identify novel genomic/epigenomic relationships to environmental exposures. Example visualizations are also included in this training module's script, providing visualizations of gene list comparison results.



### Training Module's Environmental Health Questions
This training module was specifically developed to answer the following environmental health questions:

(1) Which genes show altered expression in response to arsenic exposure?
(2) Of the genes showing altered expression, which may be under epigenetic control?



### Script Preparations

#### Cleaning the global environment
```{r}
rm(list=ls())
```


#### Installing required R packages
If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you.
```{r, results=FALSE, message=FALSE}
if (!requireNamespace("tidyverse"))
  install.packages("tidyverse")
if (!requireNamespace("VennDiagram"))
install.packages("VennDiagram")
if (!requireNamespace("grid"))
install.packages("grid")
```


#### Loading R packages required for this session
```{r, results=FALSE, message=FALSE}
library(tidyverse)
library(VennDiagram)
library(grid)
```


#### Set your working directory
```{r, eval=FALSE, echo=TRUE}
setwd("/filepath to where your input files are")
```

 

## CTD Data in R

### Organizing Example Dataset from CTD

CTD requires manual querying of its database, outside of the R scripting environment. Because of this, let's first manually pull the data we need for this example analysis. We can answer both of the example questions by pulling all chemical-gene relationship data for arsenic, which we can do by following the below steps:

Navigate to the main CTD website: http://ctdbase.org/.

Select at the top, 'Search' -> 'Chemical-Gene Interactions'. 
<br>
```{r, echo=FALSE, fig.align='center'} 
knitr::include_graphics("Chapter_7/Module7_1_Input/Module7_1_Image2.jpg")
```

<br>
  
Select to query all chemical-gene interaction data for arsenic.
<br>
```{r, echo=FALSE, fig.align='center'} 
knitr::include_graphics("Chapter_7/Module7_1_Input//Module7_1_Image3.jpg")
```
<br>

  
Note that there are lots of results, represented by many many rows of data! Scroll to the bottom of the webpage and select to download as 'CSV'.
<br>
```{r, echo=FALSE, fig.align='center'} 
knitr::include_graphics("Chapter_7/Module7_1_Input//Module7_1_Image4.jpg")
```
  
<br>
    
This is the file that we can now use to import into the R environment and analyze!
Note that the data pulled here represent data available on August 1, 2021



### Loading the Example CTD Dataset into R



Read in the csv file of the results from CTD query:
```{r, results=FALSE, message=FALSE}
ctd = read_csv("Chapter_7/Module7_1_Input/Module7_1_InputData1.csv")
```



Let's first see how many rows and columns of data this file contains:
```{r}
dim(ctd)
```
This dataset includes 6280 observations (represented by rows) linking arsenic exposure to gene-level alterations
With information spanning across 9 columns



Let's also see what kind of data are organized within the columns:
```{r}
colnames(ctd)
```


```{r}
# Viewing the first five rows of data, across all 9 columns
ctd[1:9,1:5] 
```




#### Filtering data for genes with altered expression



To identify genes with altered expression in association with arsenic, we can leverage the results of our CTD query and filter this dataset to include only the rows that contain the term "expression" in the "Interaction Actions" column.
```{r}
exp_filt = ctd %>% filter(grepl("expression", `Interaction Actions`))
```

We now have 2586 observations, representing instances of arsenic exposure causing a changes in a target gene's expression levels.
```{r}
dim(exp_filt)
```



Let's see how many unique genes this represents:
```{r}
length(unique(exp_filt$`Gene Symbol`))
```
This reflects 1878 unique genes that show altered expression in association with arsenic.



Let's make a separate dataframe that includes only the unique genes, based on the "Gene Symbol" column.
```{r}
exp_genes = exp_filt %>% distinct(`Gene Symbol`, .keep_all=TRUE)

# Removing columns besides gene identifier
exp_genes = exp_genes[,4] 

# Viewing the first 10 genes listed
exp_genes[1:10,] 
```
This now provides us a list of 1878 genes showing altered expression in association with arsenic.


##### Technical notes on running the distinct function within tidyverse:
By default, the distinct function keeps the first instance of a duplicated value. This does have implications if the rest of the values in the rows differ. You will only retain the data associated with the first instance of the duplicated value (which is why we just retained the gene column here). It may be useful to first find the rows with the duplicate value and verify that results are as you would expect before removing observations. For example, in this dataset, expression levels can increase or decrease. If you were looking for just increases in expression, and there were genes that showed increased and decreased expression across different samples, using the distinct function just on "Gene Symbol" would not give you the results you wanted. If the first instance of the gene symbol noted decreased expression, that gene would not be returned in the results even though it might be one you would want. For this example case, we only care about expression change, regardless of direction, so this is not an issue. The distinct function can also take multiple columns to consider jointly as the value to check for duplicates if you are concerned about this.

<br>

### Answer to Environmental Health Question 1

:::question
*With this, we can answer **Environmental Health Question 1***:
Which genes show altered expression in response to arsenic exposure?
:::
:::answer
**Answer**: This list of 1878 genes have published evidence supporting their altered expression levels associated with arsenic exposure.
:::

<br>

## Identifying Genes Under Epigenetic Control


For this dataset, let's focus on gene-level methylation as a marker of epigenetic regulation. Let's return to our main dataframe, representing the results of the CTD query, and filter these results for only the rows that contain the term "methylation" in the "Interaction Actions" column.
```{r}
met_filt = ctd %>% filter(grepl("methylation",`Interaction Actions`))
```

We now have 3211 observations, representing instances of arsenic exposure causing a changes in a target gene's methylation levels.
```{r}
dim(met_filt)
```


Let's see how many unique genes this represents.
```{r}
length(unique(met_filt$`Gene Symbol`))
```
This reflects 3142 unique genes that show altered methylation in association with arsenic



Let's make a separate dataframe that includes only the unique genes, based on the "Gene Symbol" column.
```{r}
met_genes = met_filt %>% distinct(`Gene Symbol`, .keep_all=TRUE)

# Removing columns besides gene identifier
met_genes = met_genes[,4] 
```
This now provides us a list of 3142 genes showing altered methylation in association with arsenic.



With this list of genes with altered methylation, we can now compare it to previous list of genes with altered expression to yeild our final list of genes of interest. To achieve this last step, we present two different methods to carry out list comparisons below.



#### Method 1 for list comparisons: Merging



Merge the expression results with the methylation resuts on the Gene Symbol column found in both datasets.
```{r}
merge_df = merge(exp_genes, met_genes, by = "Gene Symbol")
```
We end up with 315 rows reflecting the 315 genes that show altered expression and altered methylation

Let's view these genes:
```{r}
merge_df[1:315,]
```



### Answer to Environmental Health Question 2

:::question
*With this, we can answer **Environmental Health Question 2***:
Of the genes showing altered expression, which may be under epigenetic control?
:::
:::answer
**Answer**:  We identified 315 genes with altered expression resulting from arsenic exposure, that also demonstrate epigenetic modifications from arsenic. These genes include many high interest molecules involved in regulating cell health, including several cyclin dependent kinases (e.g., CDK2, CDK4, CDK5, CDK6), molecules involved in oxidative stress (e.g., FOSB, NOS2), and cytokines involved in inflammatory response pathways (e.g., IFNG, IL10, IL16, IL1R1, IR1RAP, TGFB1, TGFB3).
:::



#### Method 2 for list comparisons: Intersection
For further training, shown here is another method for pulling this list of interest, through the use of the 'intersection' function.



Obtain a list of the overlapping genes in the overall expression results and the methylation results.
```{r}
inxn = intersect(exp_filt$`Gene Symbol`,met_filt$`Gene Symbol`)
```
Again, we end up with a list of 315 unique genes that show altered expression and altered methylation.



This list can be viewed on its own or converted to a dataframe (df).
```{r}
inxn_df = data.frame(genes=inxn)
```



This list can also be conveniently used to filter the original query results.
```{r}
inxn_df_all_data = ctd %>% filter(`Gene Symbol` %in% inxn)
```



Note that in this last case, the same 315 genes are present, but this time the results contain all records from the original query results, hence the 875 rows (875 records observations reflecting the 315 genes).
```{r}
summary(unique(sort(inxn_df_all_data$`Gene Symbol`))==sort(merge_df$`Gene Symbol`))
dim(inxn_df_all_data)
```


Visually we can represent this as a Venn diagram. Here, we use the ["VennDiagram"](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-35) R package. 

```{r venn, message=F, eval=F, fig.align = "center"}
# Use the data we previously used for intersection in the venn diagram function
venn.plt = venn.diagram(
  x = list(exp_filt$`Gene Symbol`, met_filt$`Gene Symbol`),
  category.names = c("Altered Expression" , "Altered Methylation"),
  filename = NULL,
  
  # Change font size, type, and position
  cat.cex = 1.15,
  cat.fontface = "bold",
  cat.default.pos = "outer",
  cat.pos = c(-27, 27),
    cat.dist = c(0.055, 0.055),
  
  # Change color of ovals
  col=c("#440154ff", '#21908dff'),
  fill = c(alpha("#440154ff",0.3), alpha('#21908dff',0.3)),
)

```

```{r print-venn, fig.width = 7, fig.height = 7, echo=F, message=F, fig.align = "center"}
# Use the data we previously used for intersection in the venn diagram function
venn.plt = venn.diagram(
  x = list(exp_filt$`Gene Symbol`, met_filt$`Gene Symbol`),
  category.names = c("Altered Expression" , "Altered Methylation"),
  filename = NULL,
  output=F,
  
  # Change font size, type, and position
  cat.cex = 1.15,
  cat.fontface = "bold",
  cat.default.pos = "outer",
  cat.pos = c(-27, 27),
  cat.dist = c(0.055, 0.055),
  
  # Change color of ovals
  col=c("#440154ff", '#21908dff'),
  fill = c(alpha("#440154ff",0.3), alpha('#21908dff',0.3)),
)

grid::grid.draw(venn.plt)
```


## Concluding Remarks
In conclusion, we identified 315 genes that show altered expression in response to arsenic exposure that may be under epigenetic control. These genes represent critical mediators of oxidative stress and inflammation, among other important cellular processes. Results yielded an important list of genes representing potential targets for further evaluation, to better understand mechanism of environmental exposure-induced disease. Together, this example highlights the utility of CTD to address environmental health research questions.

For more information, see the recently updated primary CTD publication:  

+ Davis AP, Grondin CJ, Johnson RJ, Sciaky D, Wiegers J, Wiegers TC, Mattingly CJ. Comparative Toxicogenomics Database (CTD): update 2021. Nucleic Acids Res. 2021 Jan 8;49(D1):D1138-D1143. PMID: [33068428](https://pubmed.ncbi.nlm.nih.gov/33068428/).

Additional case studies relevant to environmental health research include the following:

+ An example publication leveraging CTD findings to identify mechanisms of metals-induced birth defects: Ahir BK, Sanders AP, Rager JE, Fry RC. Systems biology and birth defects prevention: blockade of the glucocorticoid receptor prevents arsenic-induced birth defects. Environ Health Perspect. 2013 Mar;121(3):332-8. PMID: [23458687](https://pubmed.ncbi.nlm.nih.gov/23458687/).  

+ An example publication leveraging CTD to help fill data gaps on data poor chemicals, in combination with ToxCast/Tox21 data streams, to elucidate environmental influences on disease pathways: Kosnik MB, Planchart A, Marvel SW, Reif DM, Mattingly CJ. Integration of curated and high-throughput screening data to elucidate environmental influences on disease pathways. Comput Toxicol. 2019 Nov;12:100094. PMID: [31453412](https://pubmed.ncbi.nlm.nih.gov/31453412/).  

+ An example publication leveraging CTD to extract chemical-disease relationships used to derive new chemical risk values, with the goal of prioritizing connections between environmental factors, genetic variants, and human diseases: Kosnik MB, Reif DM. Determination of chemical-disease risk values to prioritize connections between environmental factors, genetic variants, and human diseases. Toxicol Appl Pharmacol. 2019 Sep 15;379:114674. [PMID: 31323264](https://pubmed.ncbi.nlm.nih.gov/31323264/).




<label class="tykfont">
Test Your Knowledge 
</label>

:::tyk

Using the same dataset from this module (available at the GitHub site and as Module7_1_TYKInput.csv): 

1. Filter the data using the grepl function to look at only those observations that specifically decrease the target gene's "expression" level. How many observations are there?
2. Similarly, filter the data to identify how many observations there are where the target gene's "expression" level is simply "affected". Create a venn diagram to help visualize any overlap between these two filtered datasets.

:::

# 7.2 Gene Expression Omnibus

This training module was developed by Kyle R. Roell and Julia E. Rager.

All input files (script, data, and figures) can be downloaded from the [UNC-SRP TAME2 GitHub website](https://github.com/UNCSRP/TAME2).


```{r , include=FALSE}
# Set default values for R Markdown "knitting" to HTML, Word, or PDF
knitr::opts_chunk$set(echo = TRUE) #print code chunks
```

## Introduction to Training Module

[GEO](https://www.ncbi.nlm.nih.gov/geo/) is a publicly available database repository of high-throughput gene expression data and hybridization arrays, chips, and microarrays that span genome-wide endpoints of genomics, transcriptomics, and epigenomics. This training module specifically guides trainees through the loading of required packages and data, including the manual upload of GEO data as well as the upload/organization of data leveraging the [GEOquery package](https://www.bioconductor.org/packages/release/bioc/html/GEOquery.html). Data are then further organized and combined with gene annotation information through the merging of platform annotation files. Example visualizations are then produced, including boxplots to evaluate the overall distribution of expression data across samples, as well as heat map visualizations that compare unscaled versus scaled gene expression values. Statistical analyses are then included to identify which genes are significantly altered in expression upon exposure to formaldehyde. Together, this training module serves as a simple example showing methods to access and download GEO data and to perform data organization, analysis, and visualization tasks through applications-based questions.


## Introduction to GEO

The GEO repository is organized and managed by the [The National Center for Biotechnology Information (NCBI)](https://www.ncbi.nlm.nih.gov/), which seeks to advance science and health by providing access to biomedical and genomic information. The three [overall goals](https://www.ncbi.nlm.nih.gov/geo/info/overview.html) of GEO are to: (1) Provide a robust, versatile database in which to efficiently store high-throughput functional genomic data, (2) Offer simple submission procedures and formats that support complete and well-annotated data deposits from the research community, and (3) Provide user-friendly mechanisms that allow users to query, locate, review and download studies and gene expression profiles of interest.

Of high relevance to environmental health, data organized within GEO can be pulled and analyzed to address new environmental health questions, leveraging previously generated data. For example, we have pulled gene expression data from acute myeloid leukemia patients and re-analyzed these data to elucidate new mechanisms of epigenetically-regulated networks involved in cancer, that in turn, may be modified by environmental insults, as previously published in [Rager et al. 2012](https://pubmed.ncbi.nlm.nih.gov/22754483/). We have also pulled and analyzed gene expression data from published studies evaluating toxicity resulting from hexavalent chromium exposure, to further substantiate the role of epigenetic mediators in hexavelent chromium-induced carcinogenesis (see [Rager et al. 2019](https://pubmed.ncbi.nlm.nih.gov/30690063/)). This training exercise leverages an additional dataset that we published and deposited through GEO to evaluate the effects of formaldehyde inhalation exposure, as detailed below.


## Introduction to Example Data

In this training module, data will be pulled from the published GEO dataset recorded through the online series [GSE42394](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE42394). This series represents Affymetrix rat genome-wide microarray data generated from our previous study, aimed at evaluating the transcriptomic effects of formaldehyde across three tissues: the nose, blood, and bone marrow. For the purposes of this training module, we will focus on evaluating gene expression profiles from nasal samples after 7 days of exposure, collected from rats exposed to 2 ppm formaldehyde via inhalation. These findings, in addition to other epigenomic endpoint measures, have been previously published (see [Rager et al. 2014](https://pubmed.ncbi.nlm.nih.gov/24304932/)).


### Training Module's Environmental Health Questions

This training module was specifically developed to answer the following environmental health questions:

(1) What kind of molecular identifiers are commonly used in microarray-based -omics technologies?
(2) How can we convert platform-specific molecular identifiers used in -omics study designs to gene-level information?
(3)	Why do we often scale gene expression signatures prior to heat map visualizations?
(4) What genes are altered in expression by formaldehyde inhalation exposure?
(5) What are the potential biological consequences of these gene-level perturbations?



### Script Preparations

#### Cleaning the global environment
```{r}
rm(list=ls())
```


#### Installing required R packages
If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you
```{r, results=FALSE, message=FALSE}
if (!requireNamespace("tidyverse"))
  install.packages("tidyverse")
if (!requireNamespace("reshape2"))
    install.packages("reshape2")

# GEOquery, this will install BiocManager if you don't have it installed
if (!requireNamespace("BiocManager"))
  install.packages("BiocManager")
BiocManager::install("GEOquery")
```


#### Loading R packages required for this session
```{r, results=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(reshape2)
library(GEOquery)
```
For more information on the **tidyverse package**, see its associated [CRAN webpage](https://cran.r-project.org/web/packages/tidyverse/index.html), primary [webpage](https://www.tidyverse.org/packages/), and peer-reviewed [article released in 2018](https://onlinelibrary.wiley.com/doi/10.1002/sdr.1600).

For more information on the **reshape2 package**, see its associated [CRAN webpage](https://cran.r-project.org/web/packages/reshape2/index.html), [R Documentation](https://www.rdocumentation.org/packages/reshape2/versions/1.4.4), and [helpful website](https://seananderson.ca/2013/10/19/reshape/) providing an introduction to the reshape2 package.

For more information on the **GEOquery package**, see its associated [Bioconductor website](https://www.bioconductor.org/packages/release/bioc/html/GEOquery.html) and [R Documentation file](https://www.rdocumentation.org/packages/GEOquery/versions/2.38.4).



#### Set your working directory
```{r, eval=FALSE, echo=TRUE}
setwd("/filepath to where your input files are")
```

```{r, echo=FALSE}
#setwd("/Users/juliarager/IEHS Dropbox/Julia Rager/Research Projects/1_SRP/4_DMAC/DMAC Training Modules/Training_Modules/3_Chapter 3/3_2_Database_GEO/Clean_Files/")
```

## GEO Data in R

Let's start by loading the GEO dataset needed for this training module. As explained in the introduction, this module walks through two methods of uploading GEO data: manual option vs automatic option using the GEOquery package. These two methods are detailed below.

### 1. Manually Downloading and Uploading GEO Files

In this first method, we will navigate to the dataset within the GEO website, manually download its associated text data file, save it in our working directory, and then upload it into our global environment in R.

For the purposes of this training exercise, we manually downloaded the GEO series matrix file from the GEO series webpage, located at: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE42394. The specific file that was downloaded was noted as "GSE42394_series_matrix.txt", pulled by clicking on the link indicated by the red arrow from the GEO series webpage:

```{r, echo=FALSE, fig.width=4, fig.height=5, fig.align = "center"}
knitr::include_graphics("Chapter_7/Module7_2_Input/Module7_2_Image1.png")
```


For simplicity, we also have already pre-filtered this file for the samples we are interested in, focusing on the rat nasal gene expression data after 7 days of exposure to gaseous formaldehyde. This filtered file was saved as "GSE42394_series_matrix_filtered.txt", then renamed "Module7_2_InputData1.txt" for use in this module.


At this point, we can simply read in this pre-filtered text file for the purposes of this training module
```{r}
geodata_manual = read.table(file="Chapter_7/Module7_2_Input/Module7_2_InputData1.txt",
                             header=T)
```


Because this is a manual approach, we have to also manually define the treated and untreated samples (based on manually opening the surrounding metadata from the GEO webpage)

Manually defining treated and untreated for these samples of interest:
```{r}
exposed_manual = c("GSM1150940", "GSM1150941", "GSM1150942")
unexposed_manual = c("GSM1150937", "GSM1150938", "GSM1150939")
```



### 2. Loading and Organizing GEO Files through the GEOquery Package
In this second method, we will leverage the GEOquery package, which allows for easier downloading and reading in of data from GEO without having to manually download raw text files, and manually assign sample attributes (e.g., exposed vs unexposed). This package is set-up to automatically merge sample information from GEO metadata files with raw genome-wide datasets.


Let's first use the getGEO function (from the GEOquery package) to load data from our series matrix ("GSE42394_series_matrix.txt", renamed "Module7_2_InputData2.txt" for use in this module). *Note that this line of code may take a couple of minutes to run.*
```{r, message=FALSE}
geo.getGEO.data = getGEO(filename='Chapter_7/Module7_2_Input/Module7_2_InputData2.txt')
```



One of the reasons the getGEO package is so helpful is that we can automatically link a dataset with nicely organized sample information using the `pData()` function.
```{r}
sampleInfo = pData(geo.getGEO.data)
```


Let's view this sample information / metadata file, first by viewing what the column headers are.
```{r}
colnames(sampleInfo)
```

Then viewing the first five columns.
```{r}
sampleInfo[1:10,1:5]
```

This shows that each sample is provided with a unique number starting with "GSM", and these are described by information summarized in the "title" column. We can also see that these data were made public on Jan 7, 2014.


Let's view the next five columns.
```{r}
sampleInfo[1:10,6:10]
```

We can see that information is provided here surrounding the type of sample that was analyzed (i.e., RNA), more information on the collected samples within the column `source_name_ch1`, and the organism (rat) is provided in the `organism_ch1` column.


More detailed metadata information is provided throughout this file, as seen when viewing the column headers above.


#### Defining samples

Now, we can use this information to define the samples we want to analyze. Note that this is the same step we did manually above.

In this training exercise, we are focusing on responses in the nose, so we can easily filter for cell type = Nasal epithelial cells (specifically in the `cell type:ch1` variable). We are also focusing on responses collected after 7 days of exposure, which we can filter for using time = 7 day (specifically in the `time:ch1` variable). We will also define exposed and unexposed samples using the variable `treatment:ch1`.

First, let's subset the sampleInfo dataframe to just keep the samples we're interested in
```{r}
# Define a vector variable (here we call it 'keep') that will store rows we want to keep
keep = rownames(sampleInfo[which(sampleInfo$`cell type:ch1`=="Nasal epithelial cells" 
                                  & sampleInfo$`time:ch1`=="7 day"),])

# Then subset the sample info for just those samples we defined in keep variable
sampleInfo = sampleInfo[keep,]
```


Next, we can pull the exposed and unexposed animal IDs. Let's first see how these are labeled within the `treatment:ch1` variable.
```{r}
unique(sampleInfo$`treatment:ch1`)
```


And then search for the rows of data, pulling the sample animal IDs (which are in the variable `geo_accession`).
```{r}
exposedIDs = sampleInfo[which(sampleInfo$`treatment:ch1`=="2 ppm formaldehyde"), 
                          "geo_accession"]
unexposedIDs = sampleInfo[which(sampleInfo$`treatment:ch1`=="unexposed"), 
                            "geo_accession"]
```


The next step is to pull the expression data we want to use in our analyses. The GEOquery function, `exprs()`, allows us to easily pull these data. Here, we can pull the data we're interested in using the `exprs()` function, while defining the data we want to pull based off our previously generated 'keep' vector.
```{r}
# As a reminder, this is what the 'keep' vector includes 
# (i.e., animal IDs that we're interested in)
keep
```

```{r}
# Using the exprs() function
geodata = exprs(geo.getGEO.data[,keep])
```


Let's view the full dataset as is now:
```{r}
head(geodata)
```
This now represents a matrix of data, with animal IDs as column headers and expression levels within the matrix.


#### Simplifying column names
These column names are not the easiest to interpret, so let's rename these columns to indicate which animals were from the exposed vs. unexposed groups.

We need to first convert our expression dataset to a dataframe so we can edit columns names, and continue with downstream data manipulations that require dataframe formats.
```{r}
geodata = data.frame(geodata)
```


Let's remind ourselves what the column names are:
```{r}
colnames(geodata)
```

Which ones of these are exposed vs unexposed animals can be determined by viewing our previously defined vectors.
```{r}
exposedIDs
unexposedIDs
```

With this we can tell that the first three listed IDs are from unexposed animals, and the last three IDs are from exposed animals.

Let's simplify the names of these columns to indicate exposure status and replicate number.
```{r}
colnames(geodata) = c("Control_1", "Control_2", "Control_3", "Exposed_1", 
                       "Exposed_2", "Exposed_3")
```


And we'll now need to re-define our 'exposed' vs 'unexposed' vectors for downstream script.
```{r}
exposedIDs = c("Exposed_1", "Exposed_2", "Exposed_3")
unexposedIDs = c("Control_1", "Control_2", "Control_3")
```



Viewing the data again:
```{r}
head(geodata)
```

These data are now looking easier to interpret/analyze. Still, the row identifiers include 8 digit numbers starting with "107...". We know that this dataset is a gene expression dataset, but these identifiers, in themselves, don't tell us much about what genes these are referring to. These numeric IDs specifically represent microarray probesetIDs, that were produced by the Affymetrix platform used in the original study.

**But how can we tell which genes are represented by these data?!**


#### Adding gene symbol information

Each -omics dataset contained within GEO points to a specific platform that was used to obtain measurements.
In instances where we want more information surrounding the molecular identifiers, we can merge the platform-specific annotation file with the molecular IDs given in the full dataset.

For example, let's pull the platform-specific annotation file for this experiment. Let's revisit the [website](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE42394) that contained the original dataset on GEO. Scroll down to where it lists "Platforms", and there is a hyperlinked platform number "GPL6247" (see arrow below).

```{r, echo=FALSE, fig.width=4, fig.height=5, fig.align = "center"}
knitr::include_graphics("Chapter_7/Module7_2_Input/Module7_2_Image2.png")
```


Click on this, and you will be navigated to a different GEO website describing the Affymetrix rat array platform that was used in this analysis. Note that this website also includes information on when this array became available, links to other experiments that have used this platform within GEO, and much more.

Here, we're interested in pulling the corresponding gene symbol information for the probeset IDs. To do so, scroll to the bottom, and click "Annotation SOFT table..." and download the corresponding .gz file within your working directory. Unzip this, and you will find the master annotation file: "GPL6247.annot". 

In this exercise, we've already done these steps and unzipped the file in our working directory. So at this point, we can simply read in this annotation dataset, renamed "Module7_2_InputData2.annot", still using the `GEOquery()` function to help automate.

```{r, warning=FALSE}
geo.annot = GEOquery::getGEO(filename="Chapter_7/Module7_2_Input/Module7_2_InputData3.annot")
```

Now we can use the `Table()` function from GEOquery to pull data from the annotation dataset.
```{r}
id.gene.table = GEOquery::Table(geo.annot)[,c("ID", "Gene symbol")]
id.gene.table[1:10,1:2]
```

With these two columns of data, we now have the needed IDs and gene symbols to match with our dataset.

Within the full dataset, we need to add a new column for the probeset ID, taken from the rownames, in preparation for the merging step.
```{r}
geodata$ID = rownames(geodata)
```

We can now merge the gene symbol information by ID with our expression data.
```{r}
geodata_genes = merge(geodata, id.gene.table, by="ID")
head(geodata_genes)
```

Note that many of the probeset IDs do not map to full gene symbols, which is shown here by viewing the top few rows - this is expected in genome-wide analyses based on microarray platforms.

Let's look at the first 25 unique genes in these data:
```{r}
UniqueGenes = unique(geodata_genes$`Gene symbol`)
UniqueGenes[1:25]
```

Again, you can see that the first value listed is blank, representing probesetIDs that do not match to fully annotated gene symbols. Though the rest pertain for gene symbols annotated to the rat genome.

You can also see that some gene symbols have multiple entries, separated by "///"

To simplify identifiers, we can pull just the first gene symbol, and remove the rest by using gsub().
```{r}
geodata_genes$`Gene symbol` = gsub("///.*", "", geodata_genes$`Gene symbol`)
```

Let's alphabetize by main expression dataframe by gene symbol.
```{r}
geodata_genes = geodata_genes[order(geodata_genes$`Gene symbol`),]
```

And then re-view these data:
```{r}
geodata_genes[1:5,]
```

In preparation for the visualization steps below, let's reset the probeset IDs to rownames.
```{r}
rownames(geodata_genes) = geodata_genes$ID

# Can then remove this column within the dataframe
geodata_genes$ID = NULL
```

Finally let's rearrange this dataset to include gene symbols as the first column, right after rownames (probeset IDs).
```{r}
geodata_genes = geodata_genes[,c(ncol(geodata_genes),1:(ncol(geodata_genes)-1))]
geodata_genes[1:5,]
dim(geodata_genes)
```

Note that this dataset includes expression measures across **29,214 probes, representing 14,019 unique genes**.
For simplicity in the final exercises, let's just filter for rows representing mapped genes.

```{r}
geodata_genes = geodata_genes[!(geodata_genes$`Gene symbol` == ""), ]
dim(geodata_genes)
```

Note that this dataset now includes 16,024 rows with mapped gene symbol identifiers.

### Answer to Environmental Health Question 1

:::question
<i>With this, we can now answer **Environmental Health Question 1**:</i>
What kind of molecular identifiers are commonly used in microarray-based -omics technologies?
:::
:::answer
**Answer**:  Platform-specific probeset IDs.
:::


### Answer to Environmental Health Question 2

:::question
<i>We can also answer **Environmental Health Question 2**:</i>
How can we convert platform-specific molecular identifiers used in -omics study designs to gene-level information?
:::
:::answer
**Answer**: We can merge platform-specific IDs with gene-level information using annotation files.
:::
<br>

## Visualizing Data

### Visualizing Gene Expression Data using Boxplots and Heat Maps

To visualize the -omics data, we can generate boxplots, heat maps, any many other types of visualizations. Here, we provide an example to plot a boxplot, which can be used to visualize the variability amongst samples. We also provide an example to plot a heat map, comparing unscaled vs scaled gene expression profiles. These visualizations can be useful to both simply visualize the data as well as identify patterns across samples or genes

#### Boxplot visualizations
For this example, let's simply use R's built in boxplot() function.

We only want to use columns with our expression data (2 to 7), so let's pull those columns when running the boxplot function.
```{r, fig.width=5, fig.height=4, fig.align = "center"}
boxplot(geodata_genes[,2:7])
```

There seem to be a lot of variability within each sample's range of expression levels, with many outliers. This makes sense given that we are analyzing the expression levels across the rat's entire genome, where some genes won't be expressed at all while others will be highly expressed due to biological and/or potential technical variability.
  
To show plots without outliers, we can simply use outline=F.
```{r, fig.width=5, fig.height=4, fig.align = "center"}
boxplot(geodata_genes[,2:7], outline=F)
```
  

#### Heat Map visualizations
Heat maps are also useful when evaluating large datasets.

There are many different packages you can use to generate heat maps. Here, we use the *superheat* package.

It also takes awhile to plot all genes across the genome, so to save time for this training module, let's randomly select 100 rows to plot.

```{r, fig.width=9, fig.height=7, fig.align = "center"}
# To ensure that the same subset of genes are selected each time
set.seed = 101                                     

# Random selection of 100 rows
row.sample = sample(1:nrow(geodata_genes),100) 

# Heat map code
superheat::superheat(geodata_genes[row.sample,2:7], # Only want to plot non-id/gene symbol columns (2 to 7)
                     pretty.order.rows = TRUE,
                     pretty.order.cols = TRUE,
                     col.dendrogram = T,
                     row.dendrogram = T)
```

This produces a heat map with sample IDs along the x-axis and probeset IDs along the y-axis. Here, the values being displayed represent normalized expression values.


One way to improve our ability to distinguish differences between samples is to **scale expression values** across probes. 

**Scaling data**

Z-score is a very common method of scaling that transforms data points to reflect the number of standard deviations they are from the overall mean. Z-score scaling data results in the overall transformation of a dataset to have an overall mean = 0 and standard deviation = 1.

Let's see what happens when we scale this gene expression dataset by z-score across each probe. This can be easily done using the `scale()` function.

This specific `scale()` function works by centering and scaling across columns, but since we want to use it across probesets (organized as rows), we need to first transpose our dataset, then run the scale function.
```{r}
geodata_genes_scaled = scale(t(geodata_genes[,2:7]), center=T, scale=T)
```

Now we can transpose it back to the original format (i.e., before it was transposed).
```{r}
geodata_genes_scaled = t(geodata_genes_scaled)
```


And then view what the normalized and now scaled expression data look like for now a random subset of 100 probesets (representing genes).
```{r, echo=FALSE, fig.width=9, fig.height=7, fig.align = "center"}
superheat::superheat(geodata_genes_scaled[row.sample,],
                     pretty.order.rows = TRUE,
                     pretty.order.cols = TRUE,
                     col.dendrogram = T,
                     row.dendrogram = T)
```

With these data now scaled, we can more easily visualize patterns between samples.


### Answer to Environmental Health Question 3

:::question
*We can also answer **Environmental Health Question 3***:
Why do we often scale gene expression signatures prior to heat map visualizations?
:::
:::answer
**Answer**: To better visualize patterns in expression signatures between samples.
:::

<br>
Now, with these data nicely organized, we can next explore how statistics can help us find which genes show trends in expression associated with formaldehyde exposure.


## Statistical Analyses

### Statistical Analyses to Identify Genes altered by Formaldehyde

A simple way to identify differences between formaldehyde-exposed and unexposed samples is to use a t-test. Because there are so many tests being performed, one for each gene, it is also important to carry out multiple test corrections through  a p-value adjustment method. 

We need to run a t-test for each row of our dataset. This exercise demonstrates two different methods to run a t-test:

+ Method 1: using a 'for loop'
+ Method 2: using the apply function (more computationally efficient)

#### Method 1 (m1): 'For Loop'

Let's first re-save the molecular probe IDs to a column within the dataframe, since we need those values in the loop function.
```{r}
geodata_genes$ID = rownames(geodata_genes)
```


We also need to initially create an empty dataframe to eventually store p-values.
```{r}
pValue_m1 = matrix(0, nrow=nrow(geodata_genes), ncol=3)
colnames(pValue_m1) = c("ID", "pval", "padj")
head(pValue_m1)
```

You can see the empty dataframe that was generated through this code.

Then we can loop through the entire dataset to acquire p-values from t-test statistics, comparing n=3 exposed vs n=3 unexposed samples.
```{r}
for (i in 1:nrow(geodata_genes)) {
  
  #Get the ID
  ID.i = geodata_genes[i, "ID"];
  
  #Run the t-test and get the p-value
  pval.i = t.test(geodata_genes[i,exposedIDs], geodata_genes[i,unexposedIDs])$p.value;
  
  #Store the data in the empty dataframe
  pValue_m1[i,"ID"] = ID.i;
  pValue_m1[i,"pval"] = pval.i
  
}
```

View the results:
```{r}
# Note that we're not pulling the last column (padj) since we haven't calculated these yet
pValue_m1[1:5,1:2] 
```



#### Method 2 (m2): Apply Function
For the second method, we can use the *apply()* function to calculate resulting t-test p-values more efficiently labeled. 

```{r}
pValue_m2 = apply(geodata_genes[,2:7], 1, function(x) t.test(x[unexposedIDs],
                                                              x[exposedIDs])$p.value)
names(pValue_m2) = geodata_genes[,"ID"]
```

We can convert the results into a dataframe to make it similar to m1 matrix we created above.
```{r}
pValue_m2  = data.frame(pValue_m2)

# Now create an ID column
pValue_m2$ID = rownames(pValue_m2)
```

Then we can view at the two datasets to see they result in the same pvalues.
```{r}
head(pValue_m1)
head(pValue_m2)
```
We can see from these results that both methods (m1 and m2) generate the same statistical p-values.

#### Interpreting Results

Let's again merge these data with the gene symbols to tell which genes are significant.

First, let's convert to a dataframe and then merge as before, for one of the above methods as an example (m1).
```{r}
pValue_m1 = data.frame(pValue_m1)
pValue_m1 = merge(pValue_m1, id.gene.table, by="ID")
```

We can also add a multiple test correction by applying a false discovery rate-adjusted p-value; here, using the Benjamini Hochberg (BH) method.
```{r}
# Here fdr is an alias for B-H method
pValue_m1[,"padj"] = p.adjust(pValue_m1[,"pval"], method=c("fdr"))
```

Now, we can sort these statistical results by adjusted p-values.
```{r}
pValue_m1.sorted = pValue_m1[order(pValue_m1[,'padj']),]
head(pValue_m1.sorted)
```

Pulling just the significant genes using an adjusted p-value threshold of 0.05.
```{r}
adj.pval.sig = pValue_m1[which(pValue_m1[,'padj'] < .05),]

# Viewing these genes
adj.pval.sig       
```


### Answer to Environmental Health Question 4

:::question
*With this, we can answer **Environmental Health Question 4***:</i>
What genes are altered in expression by formaldehyde inhalation exposure?
:::
:::answer
**Answer**: Olr633 and Slc7a8.
:::

Finally, let's plot these using a mini heat map.
Note that we can use probesetIDs, then gene symbols, in rownames to have them show in heat map labels.
```{r, echo=FALSE, fig.width=8, fig.height=4, fig.align = "center"}
rownames(geodata_genes) = paste(geodata_genes$ID, ": ",geodata_genes$`Gene symbol`)
superheat::superheat(geodata_genes[which(geodata_genes$ID %in% adj.pval.sig[,"ID"]),2:7])
```

Note that this statistical filter is pretty strict when comparing only n=3 vs n=3 biological replicates. If we loosen the statistical criteria to p-value < 0.05, this is what we can find:
```{r}
pval.sig = pValue_m1[which(pValue_m1[,'pval'] < .05),]
nrow(pval.sig)
```

5327 genes with significantly altered expression!

Note that other filters are commonly applied to further focus these lists (e.g., background and fold change filters) prior to statistical evaluation, which can impact the final results. See [Rager et al. 2013](https://pubmed.ncbi.nlm.nih.gov/24304932/)  for further statistical approaches and visualizations.

<br>

### Answer to Environmental Health Question 5

:::question
*With this, we can answer **Environmental Health Question 5***:
What are the potential biological consequences of these gene-level perturbations?
:::
:::answer
**Answer**: Olr633 stands for 'olfactory receptor 633'. Olr633 is up-regulated in expression, meaning that formaldehyde inhalation exposure has a smell that resulted in 'activated' olfactory receptors in the nose of these exposed rats. Slc7a8 stands for 'solute carrier family 7 member 8'. Slc7a8 is down-regulated in expression, and it plays a role in many biological processes, that when altered, can lead to changes in cellular homeostasis and disease.
:::

<br>

## Concluding Remarks

In conclusion, this training module provides an overview of pulling, organizing, visualizing, and analyzing -omics data from the online repository, Gene Expression Omnibus (GEO). Trainees are guided through the overall organization of an example high dimensional dataset, focusing on transcriptomic responses in the nasal epithelium of rats exposed to formaldehyde. Data are visualized and then analyzed using standard two-group comparisons. Findings are interpreted for biological relevance, yielding insight into the effects resulting from formaldehyde exposure. 

For additional case studies that leverage GEO, see the following publications that also address environmental health questions from our research group:

+ Rager JE, Fry RC. The aryl hydrocarbon receptor pathway: a key component of the microRNA-mediated AML signalisome. Int J Environ Res Public Health. 2012 May;9(5):1939-53. doi: 10.3390/ijerph9051939. Epub 2012 May 18. PMID: 22754483; PMCID: [PMC3386597](https://pubmed.ncbi.nlm.nih.gov/22754483/).

+ Rager JE, Suh M, Chappell GA, Thompson CM, Proctor DM. Review of transcriptomic responses to hexavalent chromium exposure in lung cells supports a role of epigenetic mediators in carcinogenesis. Toxicol Lett. 2019 May 1;305:40-50. PMID: [30690063](https://pubmed.ncbi.nlm.nih.gov/30690063/).




<label class="tykfont">
Test Your Knowledge 
</label>

:::tyk

Using the same dataset that was used in this module, available from the [UNC-SRP TAME2 GitHub website](https://github.com/UNCSRP/TAME2):
1. Load the downloaded GEO dataset into R using the packages and functions mentioned in this tutorial.
2. Filter the data to just those with "cell type" of "Circulating white blood cells".
3. Report the means of the first 5 rows of the gene expression data (10700001, 10700002, 10700003, 10700004, 10700005), across all samples.

:::

# 7.3 CompTox Dashboard Data through APIs


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo = FALSE}
# Redefining the knit_print method to truncate character values to 25 characters
# in each column and to truncate the columns in the print call to prevent 
# wrapping tables with several columns.
#library(ctxR)
knit_print.data.table = function(x, ...) {
  y <- data.table::copy(x)
  y <- y[, lapply(.SD, function(t){
    if (is.character(t)){
      t <- strtrim(t, 25)
    }
    return(t)
  })]
  print(y, trunc.cols = TRUE)
}

registerS3method(
  "knit_print", "data.table", knit_print.data.table,
  envir = asNamespace("knitr")
)
```


```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
if (!library(ctxR, logical.return = TRUE)){
  devtools::load_all('C:/Users/pkruse/ctxR')
}
```

This training module was developed by Paul Kruse and Caroline Ring, with contributions from Julia E. Rager. It was updated and edited November 2025.

All input files (script, data, and figures) can be downloaded from the [UNC-SRP TAME2 GitHub website](https://github.com/UNCSRP/TAME2).

*Disclaimer: The views expressed in this document are those of the authors and do not necessarily reflect the views or policies of the U.S. EPA.*


## Introduction to Training Module

Environmental health research related to chemical exposures often requires accessing and wrangling chemical-specific data. The CompTox Chemicals Dashboard (CCD), developed by the United States Environmental Protection Agency, is a publicly-accessible database that integrates chemical data from multiple domains. Chemical data available on the CCD include physicochemical, environmental fate and transport, exposure, toxicokinetics, functional use, in vivo toxicity, in vitro bioassay, and mass spectra data. The CCD was first described in Williams et al. (2017), and has been continuously expanded since. The CCD is heavily used by researchers who do cheminformatics work of various kinds -- computational toxicology, computational exposure science, analytical chemistry, chemical safety assessment, etc. The CCD is used by cheminformaticians not only at EPA, but across governmental agencies both within the U.S. and worldwide; in private industry; in non-governmental organizations; in academia; and others. It has become an indispensable tool for many researchers.

This training module provides an overview of the physico-chemical, hazard, and bioactivity data available through the CCD; different ways to access these data; and some examples of how these data may be used. We will first introduce the CCD and how to access it. Then we will focus on an automated, programmatic method for retrieving data from the CCD using the *ctxR* R package. Through some basic data visualization and analysis using the R programming language, we will explore some data retrieved from the CCD, and gain insights both in how to wrangle the data and combine different methods of accessing the data to build automated pipelines for use in more complex settings.

Note, as the *ctxR* package accesses data that is periodically updated, some code chunks will produce numbers that may change slightly with data updates. Keep this in mind when running these code chunks in the future.

## Training Module's Environmental Health Questions

This training module was specifically developed to answer the following questions:

1. After automatically pulling the fourth Drinking Water Contaminant Candidate List from the CompTox Chemicals Dashboard, list the properties and property types present in the data. What are the mean values for a specific property when grouped by property type and when ungrouped?

2. The physico-chemical property data are reported with both experimental and predicted values present for many chemicals. Are there differences between the mean predicted and experimental results for a variety of physico-chemical properties?

3. After pulling the genotoxicity data for the different environmental contaminant data sets, list the assays associated with the chemicals in each data set. How many unique assays are there in each data set? What are the different assay categories and how many unique assays for each assay category are there?

4. The genotoxicity data contains information on which assays have been conducted for different chemicals and the results of those assays. How many chemicals in each data set have a positive, negative, and equivocal value for the assay result?

5. Based on the genotoxicity data reported for the chemical with DTXSID identifier DTXSID0020153, how many assays resulted in a positive/equivocal/negative value? Which of the assays were positive and how many of each were there for the most reported assays?

6. After pulling the hazard data for the different data sets, list the different exposure routes for which there is data. What are the unique risk assessment classes for hazard values for the oral route and for the inhalation exposure route? For each such exposure route, which risk assessment class is most represented by the data sets?

7. There are several types of toxicity values for each exposure route. List the unique toxicity values for the oral and inhalation routes. What are the unique types of toxicity values for the oral route and for the inhalation route? How many of these are common to both the oral and inhalation routes for each data set?

8. When examining different toxicity values, the data may be reported in multiple units. To assess the relative hazard from this data, it is important to take into account the different units and adjust accordingly. List the units reported for the cancer slope factor, reference dose, and reference concentration values associated with the oral and inhalation exposure routes for human hazard. Which chemicals in each data set have the highest cancer slope factor, lowest reference dose, and lowest reference concentration values?

## Script Preparations

### Cleaning the Global Environment

```{r, eval=FALSE}
rm(list=ls())
```


### Installing Required R Packages

```{r, eval=FALSE}
if (!requireNamespace('ctxR'))
  install.packages('ctxR')

if (!requireNamespace('ggplot2'))
  install.packages('ggplot2')
```

### Loading R Packages

```{r}
# Used to interface with CompTox Chemicals Dashboard
library(ctxR)

# Used to visualize data in a variety of plot designs
library(ggplot2)
```


## Introduction to CompTox Chemicals Dashboard

Accessing chemical data and wrangling it is a vital step in many types of workflows related to chemical, biological, and environmental modeling. While there are many resources available from which one can pull data, the [CompTox Chemicals Dashboard](https://comptox.epa.gov/dashboard/) built and maintained by the United States Environmental Protection Agency is particularly well-designed and suitable for these purposes. Originally introduced in [The CompTox Chemistry Dashboard: a community data resource for environmental chemistry](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-017-0247-6), the CCD contains information on over 1.2 million chemicals as of December 2023. 

The CCD includes chemical information from many different domains, including physicochemical, environmental fate and transport, exposure, usage, in vivo toxicity, and in vitro bioassay data (Williams et al., 2017).

The CCD can be searched either one chemical at a time, or using a batch search.

### Searching One Chemical at a Time (Single-substance Search)

In single-substance search, the user types a full or partial chemical identifier (name, CASRN, InChiKey, or DSSTox ID) into a search box on the CCD homepage. Autocomplete provides a list of possible matches; the user selects one by clicking on it, and is then taken to the CCD page for that substance. Here is an example of the CCD page for the chemical Bisphenol A: 

```{r, echo = FALSE, out.width= "90%", fig.align= 'center'}
knitr::include_graphics('Chapter_7/Module7_3_Input/Module7_3_Image1.png')
```


The different domains of data available for this chemical are shown by the tabs on the left side of the page: for example, "Physchem Prop." (physico-chemical properties), "Env. Fate/Transport" (environmental fate and transport data), and "Hazard Data" (*in vivo* hazard and toxicity data), among others. 

### Batch Search

In batch search, the user enters a list of search inputs, separated by newlines, into a batch-search box on https://comptox.epa.gov/dashboard/batch-search . The user selects the type(s) of inputs by selecting one or more checkboxes  these may include chemical identifiers, monoisotopic masses, or molecular formulas. Then, the user selects Display All Chemicals to display the list of substances matching the batch-search inputs, or Choose Export Options to choose options for exporting the batch-search results as a spreadsheet. The exported spreadsheet may include data from most of the domains available on an individual substances CCD page.


```{r, echo = FALSE, out.width = "90%", fig.align = 'center'}
knitr::include_graphics('Chapter_7/Module7_3_Input/Module7_3_Image2.png')
```

The user can download the selected information in various formats, such as Excel (.xlsx), comma-separated values (.csv), or different types of chemical table files (.e.g, MOL). 

```{r, echo=FALSE, out.width="90%", fig.align='center'}
knitr::include_graphics('Chapter_7/Module7_3_Input/Module7_3_Image3.png')
```


The web interface for batch search only allows input of 10,000 identifiers at a time. If a user needs to retrieve information for more than 10,000 chemicals, they will need to separate their identifiers into multiple batches and search each one separately.

### Challenges of Web-based Dashboard Search

Practicing researchers typically end up with a Dashboard workflow that looks something like this:

1.	Start with a dataset that includes your chemical identifiers of interest. These may include chemical names, Chemical Abstract Service Registry Numbers (CASRNs), Distributed Searchable Structure-Toxicity Database (DSSTox) identifiers, or InChIKeys.
2.	Export the chemical identifiers to a spreadsheet. Often, this is done by importing the data into an environment such as R or Python, in order to do some data wrangling (e.g., to select only the unique substance identfiers; to clean up improperly-formatted CASRNs; etc.). Then, the identifiers are saved in a spreadsheet (an Excel, .csv, or .txt file), one chemical identifier per row.
3.	Copy and paste the chemical identifiers from the spreadsheet into the CCD Batch Search box. If there are more than 10,000 total chemical identifiers, divide them into batches of 10,000 or less, and search each batch separately.
4.	Choose your desired export options on the CCD Batch Search page.
5.	Download the exported spreadsheet of CCD data. By default, the downloaded spreadsheet will be given a file name that includes the timestamp of the download.
6.	Repeat steps 3-5 for each batch of 10,000 identifiers produced in step 2. 
7.	Import the downloaded spreadsheet(s) of CCD data into the analysis tool you are using (e.g. R or Python).
8.	Merge the table(s) of downloaded CCD data with your original dataset of interest.
9.	Proceed with research-related data analysis using the chemical data downloaded from the CCD (e.g., statistical modeling, visualization, etc.)

Because each of these workflow steps requires manual interaction with the search and download process, the risk of human error inevitably creeps in. Here are a few real-world possibilities (the authors can neither confirm nor deny that they have personally committed any of these errors):

-	Researchers could copy/paste the wrong identifiers into the CCD batch search, especially if they have more than 10,000 identifiers and have to divide them into batches.
-	Chemical identifiers could be corrupted during the process of exporting to a spreadsheet. For example, if a researcher opens and resaves a CSV file using Microsoft Excel, any information that appears to be in date-like format will be automatically converted to a date (unless the researcher has the most recently-updated version of Excel and has selected the option in Settings that will stop Excel from auto-detecting dates). This behavior has long been identified as a problem in genomics, where gene names can appear date-like to Excel (Abeysooriya et al. 2021). It also affects cheminformatics, where chemical identifiers can appear date-like to Excel. For example, the valid CASRN 1990-07-4 would automatically be converted to 07/04/1990 (if Excel is set to use MM/DD/YYYY date formats). CCD batch search cannot recognize "07/04/1990" as a valid chemical identifier and will be unable to return any chemical data.
-	Researchers could accidentally rename a downloaded CCD data file to overwrite a previous download (for example, when searching multiple batches of identifiers). 
-	Researchers could mistakenly import the wrong CCD download file back into their analysis environment (for example, when searching multiple batches of identifiers). 

Moreover, the manual stages of this kind of workflow are also non-transparent and not easily reproducible. 

## CCTE's CTX Application Programming Interfaces (APIs) for Automated Batch Search of the CCD


Recently, the [Center for Computational Toxicology and Exposure](https://www.epa.gov/aboutepa/about-center-computational-toxicology-and-exposure-ccte) (CCTE) developed a set of Application Programming Interfaces (APIs) that allows programmatic access to the CCD, bypassing the manual steps of the web-based batch search workflow. The Computational Toxicology and Exposure (CTX) APIs effectively automate the process of accessing and downloading data from the web pages that make up the CCD. 


The [CTX APIs](https://www.epa.gov/comptox-tools/computational-toxicology-and-exposure-apis) are publicly available at no cost to the user. However, in order to use the CTX APIs, you must have an API key. The API key uniquely identifies you to the CTX servers and verifies that you have permission to access the database. Getting an API key is free, but requires contacting the API support team at [ccte_api@epa.gov](mailto:ccte_api@epa.gov).



:::txtbx

For more information on the data accessible through the CTX APIs and related tools, please visit the US EPA page on [Computational Toxicology and Exposure Online Resources](https://www.epa.gov/comptox-tools). The CTX APIs are one of many resources developed within this research realm and make available many of the data resources beyond the CCD.

:::




The APIs are organized into four sets of "endpoints" (chemical data domains): `Chemical`, `Hazard`, `Bioactivity`, and `Exposure`. Pictured below is what the `Chemical` section looks like and can be found at [CTX API Chemical Endpoints](https://api-ccte.epa.gov/docs/chemical.html).

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Chapter_7/Module7_3_Input/Module7_3_Image4.png')
```

The APIs can be explored through the pictured web interface at https://api-ccte.epa.gov/docs/chemical.html .

### CTX API Authentication

`Authentication` is the first tab on the left. Authentication is required to use the APIs. To authenticate yourself in the API web interface, input your unique API key.

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Chapter_7/Module7_3_Input/Module7_3_Image5.png')
```


### CTX API Endpoints


On the left of the API web interface, there are several different tabs, one for each endpoint in the `Chemical` domain. The endpoints are organized by the type of information provided. For instance, the `Chemical Details Resource` endpoint provides basic chemical information; the `Chemical Property Resource` endpoint provides more comprehensive physico-chemical property information; the `Chemical Fate Resource` endpoint provides chemical fate and transport information; and so on. 


### Constructing CTX API Requests

As mentioned above, APIs effectively automate the process of accessing and downloading data from the web pages that make up the CCD. APIs do this by automatically constructing requests using the Hypertext Transfer Protocol (HTTP) that enables communication between clients (e.g. your computer) and servers (e.g. the CCD).

In the CTX API web interface, the colored boxes next to each endpoint indicate the type of the associated HTTP method: either a GET request ("GET", blue) or a a POST request ("POS", green). GET is used to request data from a specific web resource (e.g. a specific URL); POST is used to send data to a server to create or update a web resource. For the CTX APIs, POST requests are used to perform multiple (batch) searches in a single API call; GET requests are used for non-batch searches. You do not need to understand the details of POST and GET requests in order to use the API.

Click on the second item under `Chemical Details Resource`, the tab labeled `Get data by dtxsid`. The following page will appear.

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Chapter_7/Module7_3_Input/Module7_3_Image6.png')
```


This page has two subheadings: "Path Parameters" and "Query-String Parameters". "Path Parameters" contains user-specified parameters that are required in order to tell the API what URL (web address) to access. In this case, the required parameter is a string for the DTXSID identifying the chemical to be searched.

"Query-String Parameters" contain user-specific parameters (usually optional) that tell the API what specific type(s) of information to download from the specified URL. In this case, the optional parameter is a `projection` parameter, a string that can take one of five values (`chemicaldetailall`, `chemicaldetailstandard`, `chemicalidentifier`, `chemicalstructure`, `ntatoolkit`). Depending on the value of this string, the API can return different sets of information about the chemical. If the `projection` parameter is left blank, then a default set of chemical information is returned.

The default return format is displayed below and includes a variety of fields with data types represented.

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Chapter_7/Module7_3_Input/Module7_3_Image7.png')
```


We show what reRturned data from searching Bisphenol A looks like using this endpoint with the `chemicaldetailstandard` value for `projection` selected.

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Chapter_7/Module7_3_Input/Module7_3_Image8.png')
```


Formatting an http request is not necessarily intuitive nor worth the time for someone not already familiar with the process, so these endpoints may provide a resource that for many would require a significant investment in time and energy to learn how to use. However, there is a solution to this in the form of the R package *ctxR*.

*ctxR* was developed to streamline the process of accessing the information available through the CTX APIs without requiring prior knowledge of how to use APIs. The [*ctxR*](https://CRAN.R-project.org/package=ctxR) package is available in stable form on CRAN and a development version may be found at the [USEPA ctxR GitHub](https://github.com/USEPA/ctxR/) repository. As an example, we demonstrate the ease with which one may retrieve the information given by this endpoint for Bisphenol A using the *ctxR* approach and contrast it with the approach using the CCD website or CTX `Chemical` API Endpoint website.



#### Setting, using, and storing the API key

We store the API key required to access the APIs. To do this for the current session, run the first command. If you want to store your key across multiple sessions, run the second command.

```{r, eval=FALSE}
# This stores the key in the current session
register_ctx_api_key(key = '<YOUR API KEY>')

# This stores the key across multiple sessions and only needs to be run once. 
# If the key changes, rerun this with the new key.
register_ctx_api_key(key = '<YOUR API KEY>', write = TRUE)
```

```{r, echo=FALSE}
# This stores the key in the current session
register_ctx_api_key(key = '706401cd-8bda-469d-9cdb-ac27f489c93a')

# This stores the key across multiple sessions and only needs to be run once. 
# If the key changes, rerun this with the new key.
register_ctx_api_key(key = '706401cd-8bda-469d-9cdb-ac27f489c93a', write = TRUE)
```

To check that your key has successfully been stored for the session, run the following command.

```{r, eval=FALSE}
ctx_key()
```

#### Retrieving chemical details

Now, we demonstrate how to retrieve the information for BPA given by the `Chemical Detail Resource` endpoint under the `chemicaldetailstandard` value for `projection`. Note, this `projection` value is the default value for the function `get_chemical_details()`.

```{r}
BPA_chemical_detail <- get_chemical_details(DTXSID = 'DTXSID7020182')
dim(BPA_chemical_detail)
class(BPA_chemical_detail)
names(BPA_chemical_detail)
```


## Comparing Physico-chemical Properties between Two Important Environmental Contaminant Lists

We study two different data sets contained in the CCD and observe how they relate and how they differ. The two data sets that we will explore are a water contaminant priority list and an air toxics list.  

The fourth Drinking Water Contaminant Candidate List (CCL4) is a set of chemicals that "...are not subject to any proposed or promulgated national primary drinking water regulations, but are known or anticipated to occur in public water systems...." Moreover, this list "...was announced on November 17, 2016. The CCL 4 includes 97 chemicals or chemical groups and 12 microbial contaminants...." The National-Scale Air Toxics Assessments (NATA) is "... EPA's ongoing comprehensive evaluation of air toxics in the United States... a state-of-the-science screening tool for State/Local/Tribal agencies to prioritize pollutants, emission sources and locations of interest for further study in order to gain a better understanding of risks... use general information about sources to develop estimates of risks which are more likely to overestimate impacts than underestimate them...."  

These lists can be found in the CCD at [CCL4](https://comptox.epa.gov/dashboard/chemical-lists/CCL4) with additional information at [CCL4 information](https://www.epa.gov/ccl/contaminant-candidate-list-4-ccl-4-0) and [NATADB](https://comptox.epa.gov/dashboard/chemical-lists/NATADB) with additional information at [NATA information](https://www.epa.gov/national-air-toxics-assessment). The quotes from the previous paragraph were excerpted from list detail descriptions found using the CCD links.


We explore details about these two lists of chemicals before diving into analyzing the data contained in each list.

```{r}
options(width = 100)
ccl4_information <- get_public_chemical_list_by_name('CCL4')
print(ccl4_information, trunc.cols = TRUE)

natadb_information <- get_public_chemical_list_by_name('NATADB')
print(natadb_information, trunc.cols = TRUE)
```

Now we pull the actual chemicals contained in the lists using the APIs.

```{r}
ccl4 <- get_chemicals_in_list('ccl4')
ccl4 <- data.table::as.data.table(ccl4)

natadb <- get_chemicals_in_list('NATADB')
natadb <- data.table::as.data.table(natadb)
```

We examine the dimensions of the data, the column names, and display a single row for illustrative purposes.

```{r}
dim(ccl4)
dim(natadb)

colnames(ccl4)
head(ccl4, 1)
```


### Accessing the Physico-chemical Property Data

Once we have the chemicals in each list, we access their physico-chemical properties. We will use the batch search forms of the function `get_chem_info()`, to which we supply a list of DTXSIDs.

```{r}
ccl4$dtxsid
natadb$dtxsid

ccl4_phys_chem <- get_chem_info_batch(ccl4$dtxsid)
natadb_phys_chem <- get_chem_info_batch(natadb$dtxsid)
```

Observe that this returns a single data.table for each query, and the data.table contains the physico-chemical properties available from the CompTox Chemicals Dashboard for each chemical in the query. Note, a warning message was triggered, `Warning: Setting type to ''!`, which indicates the the parameter `type` was not given a value. A default value is set within the function and more information can be found in the associated documentation. We examine the set of physico-chemical properties for the first chemical in CCL4. 

Before any deeper analysis, let's take a look at the dimensions of the data and the column names.

```{r}
dim(ccl4_phys_chem)
colnames(ccl4_phys_chem)
```
Next, we display the unique values for the columns `propertyID` and `propType`.





```{r}
ccl4_phys_chem[, unique(propName)]
ccl4_phys_chem[, unique(propType)]
```

Let's explore this further by examining the mean of the "boiling-point" and "melting-point" data.

```{r}
ccl4_phys_chem[propName == 'Boiling Point', .(Mean = mean(propValue, na.rm = TRUE))]
ccl4_phys_chem[propName == 'Boiling Point', .(Mean = mean(propValue, na.rm = TRUE)),
               by = .(propType)]

ccl4_phys_chem[propName == 'Melting Point', .(Mean = mean(propValue, na.rm = TRUE))]
ccl4_phys_chem[propName == 'Melting Point', .(Mean = mean(propValue, na.rm = TRUE)),
               by = .(propType)]
```

These results tell us about some of the reported physico-chemical properties of the data sets.

### Answer to Environmental Health Question 1

:::question
*With this, we can answer **Environmental Health Question 1:*** After automatically pulling the fourth Drinking Water Contaminant Candidate List from the CompTox Chemicals Dashboard, list the properties and property types present in the data. What are the mean values for a specific property when grouped by property type and when ungrouped?
:::

:::answer
**Answer:** The mean "Boiling Point" is 237.9223 degrees Celsius for CCL4, with mean values of 249.5600 and 232.7907 for experimental and predicted, respectively. The mean "Melting Point" is 50.29227 degrees Celsius for CCL4, with mean values of 47.95455 and 50.48745 for experimental and predicted, respectively.
:::

To explore **all** the values of the physico-chemical properties and calculate their means, we can do the following procedure. First we look at all the physico-chemical properties individually, then group them by each property ("Boiling Point", "Melting Point", etc...), and then additionally group those by property type ("experimental" vs "predicted"). In the grouping, we look at the columns `propValue`, `unit`, `propName` and `propType`. We also demonstrate how take the mean of the values for each grouping. We examine the chemical with `DTXSID` "DTXSID0020153" from CCL4.

```{r}
head(ccl4_phys_chem[dtxsid == 'DTXSID0020153', ])
ccl4_phys_chem[dtxsid == 'DTXSID0020153', .(propType, propValue, propUnit),
               by = .(propName)]
ccl4_phys_chem[dtxsid == 'DTXSID0020153', .(propValue, propUnit), 
               by = .(propName, propType)]

ccl4_phys_chem[dtxsid == 'DTXSID0020153', .(Mean_value = sapply(.SD, function(t){mean(t, na.rm = TRUE)})),
               by = .(propName, propUnit), .SDcols = c("propValue")]
ccl4_phys_chem[dtxsid == 'DTXSID0020153', .(Mean_value = sapply(.SD, function(t){mean(t, na.rm = TRUE)})), 
               by = .(propName, propUnit, propType), 
               .SDcols = c("propValue")][order(propName)]
```

### Analyzing and Visualizing Physico-chemical Properties from Two Environmental Contaminant Lists

We consider exploring the differences in mean predicted and experimental values for a variety of physico-chemical properties in an effort to understand better the CCL4 and NATADB lists. In particular, we examine "Vapor Pressure", "Henry's Law Constant", and "Boiling Point" and plot the means by chemical for these using boxplots. We then compare the values by grouping by both data set and `propType` value.



We first examine the vapor pressures for all the chemicals in each list. We then graph these, grouped by `propType` and pooled together in separate plots. For this we will use boxplots.


Group first by DTXSID.

```{r}
ccl4_vapor_all <- ccl4_phys_chem[propName %in% 'Vapor Pressure', 
                                 .(mean_vapor_pressure = sapply(.SD, function(t) {mean(t, na.rm = TRUE)})), 
                                 .SDcols = c('propValue'), by = .(dtxsid)]
natadb_vapor_all <- natadb_phys_chem[propName %in% 'Vapor Pressure', 
                                     .(mean_vapor_pressure = sapply(.SD, function(t) {mean(t, na.rm = TRUE)})),
                                     .SDcols = c('propValue'), by = .(dtxsid)]
```

Then group by DTXSID and then by property type.

```{r}
ccl4_vapor_grouped <- ccl4_phys_chem[propName %in% 'Vapor Pressure', 
                                     .(mean_vapor_pressure = sapply(.SD, function(t) {mean(t, na.rm = TRUE)})),
                                     .SDcols = c('propValue'), 
                                     by = .(dtxsid, propType)]
natadb_vapor_grouped <- natadb_phys_chem[propName %in% 'Vapor Pressure', 
                                         .(mean_vapor_pressure = 
                                             sapply(.SD, function(t) {mean(t, na.rm = TRUE)})), 
                                         .SDcols = c('propValue'), 
                                         by = .(dtxsid, propType)]
```

Then examine the summary statistics of the data.

```{r}
summary(ccl4_vapor_all)
summary(ccl4_vapor_grouped)
summary(natadb_vapor_all)
summary(natadb_vapor_grouped)
```

With such a large range of values covering several orders of magnitude, we log transform the data. Since some of these value are non-positive, some transformations may result in non-numeric values. These will be removed when plotting. We expect these values to be positive in general so we go ahead with these transformations.

```{r}
ccl4_vapor_all[, log_transform_mean_vapor_pressure := log(mean_vapor_pressure)]
ccl4_vapor_grouped[, log_transform_mean_vapor_pressure := 
                     log(mean_vapor_pressure)]

natadb_vapor_all[, log_transform_mean_vapor_pressure := 
                   log(mean_vapor_pressure)]
natadb_vapor_grouped[, log_transform_mean_vapor_pressure := 
                       log(mean_vapor_pressure)]
```

Now we plot the log transformed data.

First plot the CCL4 data.
```{r, fig.align='center'}
ggplot(ccl4_vapor_all, aes(log_transform_mean_vapor_pressure)) +
  geom_boxplot() +
  coord_flip()
ggplot(ccl4_vapor_grouped, aes(propType, log_transform_mean_vapor_pressure)) +
  geom_boxplot()
```

Then plot the NATA data.

```{r, fig.align='center'}
ggplot(natadb_vapor_all, aes(log_transform_mean_vapor_pressure)) +
  geom_boxplot() + coord_flip()
ggplot(natadb_vapor_grouped, aes(propType, log_transform_mean_vapor_pressure)) +
  geom_boxplot()
```

Finally, we compare both sets simultaneously. We add in a column to each data.table denoting to which data set the rows correspond and then combine the rows from both data sets together using the function `rbind()`.

```{r}
ccl4_vapor_grouped[, set := 'CCL4']
natadb_vapor_grouped[, set := 'NATADB']

all_vapor_grouped <- rbind(ccl4_vapor_grouped, natadb_vapor_grouped)
```

Now we plot the combined data. First we color the boxplots based on the property type, with mean log transformed vapor pressure plotted for each data set and property type.

```{r, fig.align='center'}
vapor_box <- ggplot(all_vapor_grouped, 
                    aes(set, log_transform_mean_vapor_pressure)) + 
  geom_boxplot(aes(color = propType))
vapor_box
```

Next we color the boxplots based on the data set.

```{r, , fig.align='center'}
vapor <- ggplot(all_vapor_grouped, aes(log_transform_mean_vapor_pressure)) +
  geom_boxplot((aes(color = set))) + 
  coord_flip()
vapor
```

In the plots above, when we graph the data separated both by data set and property type as well as just by data set, we observe the general trend that the NATADB chemicals have a higher mean vapor pressure than the CCL4 chemicals.

We also explore Henry's Law constant and boiling point in a similar fashion.

Group by DTXSID.

```{r}
ccl4_hlc_all <- ccl4_phys_chem[propName %in% "Henry's Law Constant", 
                               .(mean_hlc = sapply(.SD, function(t) {mean(t, na.rm = TRUE)})), 
                               .SDcols = c('propValue'), by = .(dtxsid)]
natadb_hlc_all <- natadb_phys_chem[propName %in% "Henry's Law Constant", 
                                   .(mean_hlc = sapply(.SD, function(t) {mean(t, na.rm = TRUE)})), 
                                   .SDcols = c('propValue'), by = .(dtxsid)]
```

Group by DTXSID and property type.

```{r}
ccl4_hlc_grouped <- ccl4_phys_chem[propName %in% "Henry's Law Constant", 
                                   .(mean_hlc = sapply(.SD, function(t) {mean(t, na.rm = TRUE)})), 
                                   .SDcols = c('propValue'), 
                                   by = .(dtxsid, propType)]
natadb_hlc_grouped <- natadb_phys_chem[propName %in% "Henry's Law Constant", 
                                       .(mean_hlc = sapply(.SD, function(t) {mean(t, na.rm = TRUE)})), 
                                       .SDcols = c('propValue'), 
                                       by = .(dtxsid, propType)]
```

Examine summary statistics.

```{r}
summary(ccl4_hlc_all)
summary(ccl4_hlc_grouped)
summary(natadb_hlc_all)
summary(natadb_hlc_grouped)
```

Again, we log transform the data as it covers several orders of magnitude. We expect these values to be positive in general so we go ahead with these transformations.

```{r}
ccl4_hlc_all[, log_transform_mean_hlc := log(mean_hlc)]
ccl4_hlc_grouped[, log_transform_mean_hlc := log(mean_hlc)]

natadb_hlc_all[, log_transform_mean_hlc := log(mean_hlc)]
natadb_hlc_grouped[, log_transform_mean_hlc := log(mean_hlc)]
```

We compare both sets simultaneously. We add in a column to each data.table denoting to which set the rows correspond and then `rbind()` the rows together.


Label and combine data.

```{r}
ccl4_hlc_grouped[, set := 'CCL4']
natadb_hlc_grouped[, set := 'NATADB']

all_hlc_grouped <- rbind(ccl4_hlc_grouped, natadb_hlc_grouped)
```

Plot data. Some rows are removed due to transformations above that result in non-valid values.

```{r, , fig.align='center'}
hlc_box <- ggplot(all_hlc_grouped, aes(set, log_transform_mean_hlc)) + 
  geom_boxplot(aes(color = propType))
hlc_box

hlc <- ggplot(all_hlc_grouped, aes(log_transform_mean_hlc)) +
  geom_boxplot(aes(color = set)) +
  coord_flip()
hlc
```

Again, we observe that in both grouping by `propType` and aggregating all results together by data set, that the chemicals in NATADB have a generally higher mean Henry's Law Constant value than those in CCL4.

Finally, we consider boiling point.

Group by DTXSID.

```{r}
ccl4_boiling_all <- ccl4_phys_chem[propName %in% 'Boiling Point', 
                                   .(mean_boiling_point = sapply(.SD, function(t) {mean(t, na.rm = TRUE)})), 
                                   .SDcols = c('propValue'), by = .(dtxsid)]
natadb_boiling_all <- natadb_phys_chem[propName %in% 'Boiling Point', 
                                       .(mean_boiling_point = 
                                           sapply(.SD, function(t) {mean(t, na.rm = TRUE)})), 
                                       .SDcols = c('propValue'), by = .(dtxsid)]
```

Group by DTXSID and property type.

```{r}
ccl4_boiling_grouped <- ccl4_phys_chem[propName %in% 'Boiling Point', 
                                       .(mean_boiling_point = 
                                           sapply(.SD, function(t) {mean(t, na.rm = TRUE)})), 
                                       .SDcols = c('propValue'), 
                                       by = .(dtxsid, propType)]
natadb_boiling_grouped <- natadb_phys_chem[propName %in% 'Boiling Point', 
                                           .(mean_boiling_point = 
                                               sapply(.SD, function(t) {mean(t, na.rm = TRUE)})), 
                                           .SDcols = c('propValue'), 
                                           by = .(dtxsid, propType)]
```

Calculate summary statistics.

```{r}
summary(ccl4_boiling_all)
summary(ccl4_boiling_grouped)
summary(natadb_boiling_all)
summary(natadb_boiling_grouped)
```

Since some of the boiling point values have negative values, we cannot log transform these values. If we try, as you will see below, there will be warnings of NaNs produced.

```{r, eval}
ccl4_boiling_all[, log_transform := log(mean_boiling_point)]
ccl4_boiling_grouped[, log_transform := log(mean_boiling_point)]

natadb_boiling_all[, log_transform := log(mean_boiling_point)]
natadb_boiling_grouped[, log_transform := log(mean_boiling_point)]
```

We compare both sets simultaneously. We add in a column to each data.table denoting to which set the rows correspond and then `rbind()` the rows together. We use the values as is rather than transforming them.

Label and combine data.

```{r}
ccl4_boiling_grouped[, set := 'CCL4']
natadb_boiling_grouped[, set := 'NATADB']

all_boiling_grouped <- rbind(ccl4_boiling_grouped, natadb_boiling_grouped)
```

Plot the data.

```{r, , fig.align='center'}
boiling_box <- ggplot(all_boiling_grouped, aes(set, mean_boiling_point)) + 
  geom_boxplot(aes(color = propType))
boiling_box

boiling <- ggplot(all_boiling_grouped, aes(mean_boiling_point)) +
  geom_boxplot(aes(color = set)) + 
  coord_flip()
boiling
```

A visual inspection of this set of graphs is not as clear as in the previous cases. Note that the experimental values for each data set tend to be higher than the predicted. The mean of CCL4, by predicted and experimental appears to be greater than the corresponding means for NATADB, as does the overall mean, but the interquartile ranges of these different groupings yield slightly different results. This gives us a sense that the picture for boiling point is not as clear cut between experimental and predicted for these two data sets as it was in the previous cases of physico-chemical properties we investigated.

### Answer to Environmental Health Question 2

:::question
*Through inspecting the last several plots, we can answer **Environmental Health Question 2:*** The physico-chemical property data are reported with both experimental and predicted values present for many chemicals. Are there differences between the mean predicted and experimental results for a variety of physico-chemical properties? 
:::

:::answer
**Answer**: There are indeed differences between the mean values of various physico-chemical properties when grouped by predicted or experimental. In the case of "Vapor Pressure", the means of experimental values tend to be a little lower than predicted, though they are much closer in the case of NATADB than CCL4. The trend of lower predicted means compared to experimental means is more clearly demonstrated for "Henry's Law Constant" values in both data sets. In the case of "Boiling Point", the experimental values are greater than the predicted values, though this is much more pronounced in CCL4 while the set of means for NATADB are again fairly close. 
:::




## Hazard Data: Genotoxicity

Now, having examined some of the distributions of the physico-chemical properties of the two lists, aggregated between predicted and experimental, we move towards learning more about these chemicals beyond physico-chemical properties. Specifically, we will examine their genotoxicity.

Using the standard CompTox Chemicals Dashboard approach to access genotoxicity, one would again navigate to the individual chemical page 

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Chapter_7/Module7_3_Input/Module7_3_Image9.png')
```

Once one navigates to the genotoxicity tab highlighted in the previous page, the following is displayed as seen here:

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Chapter_7/Module7_3_Input/Module7_3_Image10.png')
```

This page includes two sets of information, the first of which provides a summary of available genotoxicity data while the second provides the individual reports and samples of such data.

We again use the CTX APIs to streamline the process of retrieving this information in a programmatic fashion. To this end, we will use the genotoxicity endpoints found within the `Hazard` endpoints of the CTX APIs. Pictured below is the particular set of genotoxicity resources available in the `Hazard` endpoints of the CTX APIs.

```{r, echo = FALSE, out.width = "90%", fig.align='center'}
knitr::include_graphics('Chapter_7/Module7_3_Input/Module7_3_Image11.png')
```

There are both summary and detail resources, reflecting the information one can find on the CompTox Chemicals Dashboard Genotoxicity page for a given chemical.

To access the genetox endpoint, we will use the function `get_genetox_summary()`. Since we have a list of chemicals, rather than searching individually for each chemical, we use the batch search version of the function, named `get_genetox_summary_batch()`. We will examine this and then access the details.

Grab the data using the APIs.
```{r}
ccl4_genotox <- get_genetox_summary_batch(DTXSID = ccl4$dtxsid)
natadb_genetox <- get_genetox_summary_batch(DTXSID = natadb$dtxsid)
```

Examine the dimensions.

```{r}
dim(ccl4_genotox)
dim(natadb_genetox)
```

Examine the column names and data from the first six chemicals with genetox data from CCL4.

```{r}
colnames(ccl4_genotox)
head(ccl4_genotox)
```

The information returned is of the first variety highlighted in the image above, that is, the summary data on the available genotoxicity data for each chemical.

Observe that we have information on 71 chemicals from the CCL4 data and 153 from the NATA data. We note the chemicals not included in the results and then dig into the returned results.

```{r}
ccl4[!(dtxsid %in% ccl4_genotox$dtxsid), 
     .(dtxsid, casrn, preferredName, molFormula)]
natadb[!(dtxsid %in% natadb_genetox$dtxsid), 
       .(dtxsid, casrn, preferredName, molFormula)]
```

Now, we access the genotoxicity details of the chemicals in each data set using the function `get_genetox_details_batch()`. We explore the dimensions of the returned queries, the column names, and the first few lines of the data.

Grab the data from the CTX APIs.

```{r}
ccl4_genetox_details <- get_genetox_details_batch(DTXSID = ccl4$dtxsid)
natadb_genetox_details <- get_genetox_details_batch(DTXSID = natadb$dtxsid)
```

Examine the dimensions.

```{r}
dim(ccl4_genetox_details)
dim(natadb_genetox_details)
```

Look at the column names and the first six rows of the data from the CCL4 chemicals.

```{r}
colnames(ccl4_genetox_details)
head(ccl4_genetox_details)
```

We examine the information returned for the first chemical in each set of results, which is DTXSID0020153. Notice that the information is identical in each case as this information is chemical specific and not data set specific.

Look at the dimensions first.

```{r}
dim(ccl4_genetox_details[dtxsid %in% 'DTXSID0020153', ])
dim(natadb_genetox_details[dtxsid %in% 'DTXSID0020153', ])
```

Now examine the first few rows.

```{r}
head(ccl4_genetox_details[dtxsid %in% 'DTXSID0020153', ])
```

Observe that the data is the same for each data set when restricting to the same chemical. This is because the information we are retrieving is specific to the chemical and not dependent on the chemical lists to which the chemical may belong.


We now explore the assays present for chemicals in each data set. We first determine the unique values of the `assayCategory` column and then group by these values and determine the number of unique assays for each `assayCategory` value.

Determine the unique assay categories.

```{r}
ccl4_genetox_details[, unique(assayCategory)]
natadb_genetox_details[, unique(assayCategory)]
```
Determine the unique assays for each data set and list them.

```{r}
ccl4_genetox_details[, unique(assayType)]

natadb_genetox_details[, unique(assayType)]
```





Determine the number of assays per unique `assayCategory` value.

```{r}
ccl4_genetox_details[, .(Assays = length(unique(assayType))), 
                     by = .(assayCategory)]

natadb_genetox_details[, .(Assays = length(unique(assayType))),
                       by = .(assayCategory)]
```


We can analyze these results more closely, counting the number of assay results and grouping by `assayCategory`, and `assayType`. We also examine the different numbers of `assayCategory` and `assayTypes` values used.

```{r}
ccl4_genetox_details[, .N, by = .(assayCategory, assayType, assayResult)]
ccl4_genetox_details[, .N, by = .(assayCategory)]
```

We look at the `assayType` values and numbers of each for the three different `assayCategory` values.

```{r}
ccl4_genetox_details[assayCategory == 'in vitro', .N, by = .(assayType)]
ccl4_genetox_details[assayCategory == 'ND', .N, by = .(assayType)]
ccl4_genetox_details[assayCategory == 'in vivo', .N, by = .(assayType)]
```

Now we repeat this for NATADB.

```{r}
natadb_genetox_details[, .N, by = .(assayCategory, assayType, assayResult)]
natadb_genetox_details[, .N, by = .(assayCategory)]
```

Examine the number of rows for each `assayType` value by each `assaycategory` value.

```{r, R.options=list(width=150) }
natadb_genetox_details[assayCategory == 'in vitro', .N, by = .(assayType)]
natadb_genetox_details[assayCategory == 'ND', .N, by = .(assayType)]
natadb_genetox_details[assayCategory == 'in vivo', .N, by = .(assayType)]
```

### Answer to Environmental Health Question 3

:::question
*From these initial explorations of the data, we can answer **Environmental Health Question 3:*** After pulling the genotoxicity data for the different environmental contaminant data sets, list the assays associated with the chemicals in each data set. How many unique assays are there in each data set? What are the different assay categories and how many unique assays for each assay category are there?
:::

:::answer
**Answer**: There are 87 unique assays for CCl4 and 113 unique assays for NATADB. The different assay categories are "in vitro", "ND", and "in vivo", with 62 unique "in vitro" assays for CCl4 and 82 for NATADB, 2 unique "ND" assays for CCL4 and 2 for NATADB, and 23 unique "in vivo" assays for CCL4 and 29 for NATADB.
:::

Next, we dig into the results of the assays. One may be interested in looking at the number of chemicals for which an assay resulted in a positive or negative result for instance. We group by `assayResult` and determine the number of unique `dtxsid` values associated with each `assayResult` value.

```{r}
ccl4_genetox_details[, .(DTXSIDs = length(unique(dtxsid))), by = .(assayResult)]
natadb_genetox_details[, .(DTXSIDs = length(unique(dtxsid))), 
                       by = .(assayResult)]
```

### Answer to Environmental Health Question 4

:::question
*With this data we may now answer **Environmental Health Question 4:*** The genotoxicity data contains information on which assays have been conducted for different chemicals and the results of those assays. How many chemicals in each data set have a positive, negative, and equivocal value for the assay result?
:::

:::answer
**Answer**: For CCL4, there are 63 unique chemicals that have a negative assay result, 53 that have a positive result, and 14 that have an equivocal result. For NATADB, there are 139 unique chemicals that have a negative assay result, 129 that have a positive result, and 47 that have an equivocal result. Observe that since there are 71 unique `dtxsid` values with assay results in CCL4 and 153 in NATADB, there are several chemicals that have multiple assay results.
:::

We now determine the chemicals from each data set that are known to have genotoxic effects. For this, we look to see which chemicals produce at least one positive response in the `assayResult` column.

```{r}
ccl4_genetox_details[, .(is_positive = any(assayResult == 'positive')), 
                     by = .(dtxsid)][is_positive == TRUE, dtxsid]
natadb_genetox_details[, .(is_positive = any(assayResult == 'positive')),
                       by = .(dtxsid)][is_positive == TRUE, dtxsid]
```

With so much genotoxicity data, let us explore this data for one chemical more deeply to get a sense of the assays and results present for it. We will explore the chemical with DTXSID0020153. We will look at the assays, the number of each type of result, and which correspond to "positive" results. To determine this, we group by `assayResult` and calculate `.N` for each group. We also isolate which were positive and output a data.table with the number of each type.

```{r}
ccl4_genetox_details[dtxsid == 'DTXSID0020153', .(Number = .N), 
                     by = .(assayResult)]
ccl4_genetox_details[dtxsid == 'DTXSID0020153' & assayResult == 'positive', 
                     .(Number_of_assays = .N), by = .(assayType)][order(-Number_of_assays),]
```

### Answer to Environmental Health Question 5

:::question
*With these data.tables, we may answer **Environmental Health Question 5:*** Based on the genotoxicity data reported for the chemical with DTXSID identifier DTXSID0020153, how many assays resulted in a positive/equivocal/negative value? Which of the assays were positive and how many of each were there for the most reported assays?
:::

:::answer
**Answer**: There were five assays that produced a negative result, 20 that produced a positive result, and one that produced an equivocal result. Of the 20 positive assays, "InVitroCA", "InVitroMLA", "Ames", "Sister-chromatid exchange (SCE) in vitro", bacterial reverse mutation assay" and "Rec-assay, DNA effects (bacterial DNA repair)" were the most numerous, with two each.
:::


## Hazard Resource


Finally, we examine the hazard data associated with the chemicals in each data set. For each chemical, there will be potentially hundreds of rows of hazard data, so the returned results will be much larger than in most other API endpoints.

```{r}
ccl4_hazard <- get_hazard_by_dtxsid_batch(DTXSID = ccl4$dtxsid)
natadb_hazard <- get_hazard_by_dtxsid_batch(DTXSID = natadb$dtxsid)
```

We do some preliminary exploration of the data. First we determine the dimensions of the data sets.

```{r}
dim(ccl4_hazard)
dim(natadb_hazard)
```
Next we record the column names and display the first six results in the CCL4 hazard data.

```{r}
colnames(ccl4_hazard)
head(ccl4_hazard)
```

We determine the number of unique values in the `criticalEffect`, `toxvalTypeSuperCategory`, and `toxvalType` columns for each data set.

The number of unique values for `criticalEffect`.

```{r}
length(ccl4_hazard[, unique(criticalEffect)])
length(natadb_hazard[, unique(criticalEffect)])
```
The number of unique values of `toxvalTypeSuperCategory`.

```{r}
length(ccl4_hazard[, unique(toxvalTypeSuperCategory)])
length(natadb_hazard[, unique(toxvalTypeSuperCategory)])
```
The number of unique values for `toxvalType`.

```{r}
length(ccl4_hazard[, unique(toxvalType)])
length(natadb_hazard[, unique(toxvalType)])
```

Now we look at the number of entries per `toxvalTypeSuperCategory`.

```{r}
ccl4_hazard[, .N, by = .(toxvalTypeSuperCategory)]
natadb_hazard[, .N, by = .(toxvalTypeSuperCategory)]

```
With over 7,000 results for the `toxvalTypeSuperCategory` value "Dose Response Summary Value" for each data set, we dig into this further.

We determine the number of rows grouped by `toxvalType` that have the "Dose Response Summary Value" `toxvalTypeSuperCategory` value, and display this descending.
```{r}
ccl4_hazard[toxvalTypeSuperCategory %in% 'Dose Response Summary Value', .N, 
             by = .(toxvalType)][order(-N),]
natadb_hazard[toxvalTypeSuperCategory %in% 'Dose Response Summary Value', .N, 
               by = .(toxvalType)][order(-N),]
```

We explore "NOAEL", "LOAEL", and "NOEL" further. Let us look at the the case when `media` value is either "food" or "culture". For this, we will recover the minimum value of "NOAEL", "LOAEL", and "NOEL" for each chemical in each data set.

First, we look at "food". We order by `toxvalType` and by the minimum `toxvalNumeric` value in each group, descending.

```{r}
ccl4_hazard[media %in% 'food' & toxvalType %in% c('LOAEL', 'NOAEL', 'NOEL'), 
            .(toxvalNumeric = min(toxvalNumeric)), 
            by = .(toxvalType, toxvalUnits, dtxsid)][order(toxvalType,
                                                           -toxvalNumeric)]
natadb_hazard[media %in% 'food' & toxvalType %in% c('LOAEL', 'NOAEL', 'NOEL'), 
              .(toxvalNumeric = min(toxvalNumeric)), 
              by = .(toxvalType, toxvalUnits, dtxsid)][order(toxvalType,
                                                             -toxvalNumeric)]
```

Next we look at "culture", repeating the same grouping and ordering as in the previous case.

```{r}
ccl4_hazard[media %in% 'culture' & toxvalType %in% c('LOAEL', 'NOAEL', 'NOEL'), 
            .(toxvalNumeric = min(toxvalNumeric)), 
            by = .(toxvalType, toxvalUnits, dtxsid)][order(toxvalType,
                                                           -toxvalNumeric)]
natadb_hazard[media %in% 'culture' & toxvalType %in% c('LOAEL', 'NOAEL', 'NOEL'), 
              .(toxvalNumeric = min(toxvalNumeric)), 
              by = .(toxvalType, toxvalUnits, dtxsid)][order(toxvalType,
                                                             -toxvalNumeric)]
```

Now, let us restrict our attention to human hazard and focus on the exposure routes given by inhalation and oral. 

First, let us determine the exposure routes in general.

```{r}
ccl4_hazard[humanEco %in% 'human health', unique(exposureRoute)]
natadb_hazard[humanEco %in% 'human health', unique(exposureRoute)]
```

Then, let's focus on the inhalation and oral exposure routes for human hazard.



To answer this, filter the data into the corresponding exposure routes, then group by `exposureRoute` and `riskAssessmentClass`, and finally count the number of instances for each grouping. To determine the most represented class, one can order the results descending.

```{r}
ccl4_hazard[humanEco %in% 'human health' & 
              exposureRoute %in% c('inhalation', 'oral'), .(Hits = .N), 
            by = .(exposureRoute, riskAssessmentClass)][order(exposureRoute, 
                                                              -Hits)]
natadb_hazard[humanEco %in% 'human health' & 
                exposureRoute %in% c('inhalation', 'oral'), .(Hits = .N), 
              by = .(exposureRoute, riskAssessmentClass)][order(exposureRoute,
                                                                -Hits)]
```

### Answer to Environmental Health Question 6

:::question
*With these results we may answer **Environmental Health Question 6:*** After pulling the hazard data for the different data sets, list the different exposure routes for which there is data. What are the unique risk assessment classes for hazard values for the oral route and for the inhalation exposure route? For each such exposure route, which risk assessment class is most represented by the data sets?
:::

:::answer
**Answer**: We listed the general exposure routes above for the hazard data associated with the chemicals in each data set. Restricting our attention to human hazard data, the "air" `riskAssessmentClass` is most represented by the inhalation exposure route and "water" for the oral exposure route for both the CCL4 and NATADB data sets. 
:::


We now drill down a little further before moving into a different path for data exploration. We explore the different types of toxicity values present in each data set for the inhalation and oral exposure routes, and then see which of these are common to both exposure routes for each data set.



To answer this, we filter the rows to the "human health" `humanEco` value and "inhalation" or "oral" `exposureRoute` value. Then we return the unique values that `toxvalType` takes.

First we look at CCL4.

```{r}
ccl4_hazard[humanEco %in% 'human health' &
              exposureRoute %in% c('inhalation'), unique(toxvalType)]
ccl4_hazard[humanEco %in% 'human health' &
              exposureRoute %in% c('oral'), unique(toxvalType)]
intersect(ccl4_hazard[humanEco %in% 'human health' & exposureRoute %in% 'inhalation', unique(toxvalType)], ccl4_hazard[humanEco %in% 'human health' & exposureRoute %in% 'oral', unique(toxvalType)])
```

Then we look at NATADB.

```{r}
natadb_hazard[humanEco %in% 'human health' & 
                exposureRoute %in% c('inhalation'), unique(toxvalType)]
natadb_hazard[humanEco %in% 'human health' & 
                exposureRoute %in% c('oral'), unique(toxvalType)]
intersect(natadb_hazard[humanEco %in% 'human health' & exposureRoute %in% 'inhalation', unique(toxvalType)], natadb_hazard[humanEco %in% 'human health' & exposureRoute %in% 'oral', unique(toxvalType)])
```

### Answer to Environmental Health Question 7

:::question
*With the results above, we may answer **Environmental Health Question 7:*** There are several types of toxicity values for each exposure route. List the unique toxicity values for the oral and inhalation routes. What are the unique types of toxicity values for the oral route and for the inhalation route? How many of these are common to both the oral and inhalation routes for each data set?
:::

:::answer
**Answer**: There are 18 toxicity value types shared between the oral and inhalation exposure routes for CCL4 and 29 for NATADB. The lists above indicate the variety of toxicity values present in the hazard data for the two different exposure routes we have considered.
:::


For the next data exploration, we will examine the "NOAEL" and "LOAEL" values for chemicals with oral exposure  and human hazard. We also examine the units to determine whether any unit conversions are necessary to compare numeric values.

```{r}
ccl4_hazard[humanEco %in% 'human health' & exposureRoute %in% 'oral' & 
              toxvalType %in% c('NOAEL', 'LOAEL'), ]
ccl4_hazard[humanEco %in% 'human health' & exposureRoute %in% 'oral' &
              toxvalType %in% c('NOAEL', 'LOAEL'), unique(toxvalUnits)]
natadb_hazard[humanEco %in% 'human health' & exposureRoute %in% 'oral' &
                toxvalType %in% c('NOAEL', 'LOAEL'), ]
natadb_hazard[humanEco %in% 'human health' & exposureRoute %in% 'oral' & 
                toxvalType %in% c('NOAEL', 'LOAEL'), unique(toxvalUnits)]
```

Observe that for both CCL4 and NATADB, the units are given by "mg/kg-day", "ppm", "mg/L" and additionally "-" for NATADB. In this case, we treat "mg/kg-day" and "ppm" the same and exclude "-" and "mg/L". We group by DTXSID to find the lowest or highest value.

```{r}
ccl4_hazard[humanEco %in% 'human health' & exposureRoute %in% 'oral' & 
            toxvalType %in% c('NOAEL', 'LOAEL') & !(toxvalUnits %in% c('-', 'mg/L')),
            .(numeric_value = min(toxvalNumeric), 
            units = toxvalUnits[[which.min(toxvalNumeric)]]), 
            by = .(dtxsid, toxvalType)]
natadb_hazard[humanEco %in% 'human health' & exposureRoute %in% 'oral' & 
              toxvalType %in% c('NOAEL', 'LOAEL') & !(toxvalUnits %in% c('-', 'mg/L')), 
              .(numeric_value = min(toxvalNumeric), 
              units = toxvalUnits[[which.min(toxvalNumeric)]]), 
              by = .(dtxsid, toxvalType)]
```

Now, we also explore the values of "RfD", "RfC", and "cancer slope factor" of the `toxvalType` rows. We first determine the set of units for each, make appropriate conversions if necessary, and then make comparisons.

```{r}
ccl4_hazard[humanEco %in% 'human health' & toxvalType %in% 
            c('cancer slope factor', 'RfD', 'RfC'), .N, 
            by = .(toxvalType, toxvalUnits)][order(toxvalType, -N)]
natadb_hazard[humanEco %in% 'human health' & toxvalType %in%
              c('cancer slope factor', 'RfD', 'RfC'), .N, 
              by = .(toxvalType, toxvalUnits)][order(toxvalType, -N)]
```
For CCL4 and NATADB, there is a single unit type for each `toxvalType` value, so no unit conversions are necessary. 

First, we filter and separate out the relevant data subsets.

```{r}
# Separate out into relevant data subsets
ccl4_csf <- ccl4_hazard[humanEco %in% 'human health' & 
                          toxvalType %in% c('cancer slope factor') & (toxvalUnits != 'mg/kg-day'), ]
ccl4_rfc <- ccl4_hazard[humanEco %in% 'human health' & 
                          toxvalType %in% c('RfC'), ]
ccl4_rfd <- ccl4_hazard[humanEco %in% 'human health' & 
                          toxvalType %in% c('RfD'), ]
```

While there are no unit conversions needed, we demonstrate how we would convert units if they were required. 

```{r}
# Set mass by volume units to mg/m3, so scale g/m3 by 1E3 and ug/m3 by 1E-3
ccl4_rfc[toxvalUnits == 'mg/m3', conversion := 1]
ccl4_rfc[toxvalUnits == 'g/m3', conversion := 1E3]
ccl4_rfc[toxvalUnits == 'ug/m3', conversion := 1E-3]
ccl4_rfc[toxvalUnits %in% c('mg/m3', 'g/m3', 'ug/m3'), units := 'mg/m3']
# Set mass by mass units to mg/kg
ccl4_rfd[toxvalUnits %in% c('mg/kg-day', 'mg/kg'), conversion := 1]
ccl4_rfd[toxvalUnits %in% c('mg/kg-day', 'mg/kg'), units := 'mg/kg']
```

Then aggregate the data.

```{r}
# Run data aggregations grouping by dtxsid and taking either the max or the min
# depending on the toxvalType we are considering.
ccl4_csf[,.(numeric_value = max(toxvalNumeric), 
            units = toxvalUnits[which.max(toxvalNumeric)]), 
         by = .(dtxsid)][order(-numeric_value),]
ccl4_rfc[,.(numeric_value = min(toxvalNumeric*conversion), 
            units = units[which.min(toxvalNumeric*conversion)]), 
         by = .(dtxsid)][order(numeric_value),]
ccl4_rfd[,.(numeric_value = min(toxvalNumeric*conversion), 
            units = units[which.min(toxvalNumeric*conversion)]), 
         by = .(dtxsid)][order(numeric_value),]
```

Repeat the process for NATADB, first separating out the relevant subsets of the data.

```{r}
# Separate out into relevant data subsets
natadb_csf <- natadb_hazard[humanEco %in% 'human health' & 
                              toxvalType %in% c('cancer slope factor') & (toxvalUnits != 'mg/kg-day'), ]
natadb_rfc <- natadb_hazard[humanEco %in% 'human health' &
                              toxvalType %in% c('RfC'), ]
natadb_rfd <- natadb_hazard[humanEco %in% 'human health' & 
                              toxvalType %in% c('RfD'), ]
```

Now handle the unit conversions.

```{r}
# Set mass by mass units to mg/kg. Note that ppm is already in mg/kg
natadb_rfc <- natadb_rfc[toxvalUnits != 'ppm',]
natadb_rfd[, units := 'mg/kg-day']
```

Finally, aggregate the data.

```{r}
# Run data aggregations grouping by dtxsid and taking either the max or the min
# depending on the toxvalType we are considering.
natadb_csf[, .(numeric_value = max(toxvalNumeric), 
               units = toxvalUnits[which.max(toxvalNumeric)]), 
           by = .(dtxsid)][order(-numeric_value),]
natadb_rfc[, .(numeric_value = min(toxvalNumeric), 
               units = toxvalUnits[which.min(toxvalNumeric)]), 
           by = .(dtxsid)][order(numeric_value),]
natadb_rfd[, .(numeric_value = min(toxvalNumeric), 
               units = units[which.min(toxvalNumeric)]), 
           by = .(dtxsid)][order(numeric_value),]
```

### Answer to Environmental Health Question 8

:::question
*With these results, we may answer **Environmental Health Question 8:*** When examining different toxicity values, the data may be reported in multiple units. To assess the relative hazard from this data, it is important to take into account the different units and adjust accordingly. List the units reported for the cancer slope factor, reference dose, and reference concentration values associated with the oral and inhalation exposure routes for human hazard. Which chemicals in each data set have the highest cancer slope factor, lowest reference dose, and lowest reference concentration values?
:::

:::answer
**Answer**: The units for these three toxicity value types for CCL4 are given by "mg/m3" for RfC, "mg/kg-day"for RfD, and "(mg/kg-day)-1" for Cancer Slope Factor. For NATADB, the units for RfC are given by "mg/m3", for RfD by "mg/kg-day", and for Cancer Slope Factor by "(mg/kg-day)-1".  For CCL4, the chemical DTXSID2021028 has the highest CsF at 150 (mg/kg-day)-1, the chemical DTXSID5020023 has the lowest RfC value at 2.0e-5 mg/m3, and the chemical DTXSID8031865 has the lowest RfD value at 3e-8 mg/kg. For NATADB, the chemical DTXSID2020137 has the highest CsF at 500 (mg/kg-day)-1, the chemical DTXSID1020516 has the lowest RfC value at 2.0e-6 mg/m3, and the chemical DTXSID7021029 had the lowest RfD at 4e-6 mg/kg-day.
:::

## Concluding Remarks

In conclusion, we explored how one can access publicly available data from the CompTox Chemicals Dashboard programmatically using the CTX APIs via the *ctxR* R package. In the examples above, we investigated different types of data associated with chemicals, visualized and aggregated the data, and employed different data wrangling techniques using data.tables to answer the proposed environmental health questions. With these tools, one can build workflows that take in a list of chemicals and gather and process data associated with those chemicals through the CTX APIs. Consider how you might use this functionality for building models in your own work.

<br>

<label class="tykfont">
Test Your Knowledge 
</label>

:::tyk

Try running the same analysis of physical-chemical properties, genotoxicity data, and hazard data on a different pair of data sets available from the CCD. For instance, try pulling the data set 'BIOSOLIDS2021' and work through the same steps we completed in this module to investigate the chemicals in this list to gain a better understanding of the associated data.
:::



```{r breakdown, echo = FALSE, results = 'hide'}
# This chunk will be hidden in the final product. It serves to undo defining the
# custom print function to prevent unexpected behavior after this module during
# the final knitting process

knit_print.data.table = knitr::normal_print
  
registerS3method(
  "knit_print", "data.table", knit_print.data.table,
  envir = asNamespace("knitr")
)

# A demonstration that this indeed returns the print process back to normal for
# data.table objects
head(ccl4, 1)
```
