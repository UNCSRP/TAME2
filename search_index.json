[["index.html", "TAME 2.0: An Update to the TAME Toolkit for Introductory Data Science, Chemical-Biological Analyses, Machine Learning and Predictive Modeling, and Database Mining for Environmental Health Research Preface", " TAME 2.0: An Update to the TAME Toolkit for Introductory Data Science, Chemical-Biological Analyses, Machine Learning and Predictive Modeling, and Database Mining for Environmental Health Research Rager Lab 2024-10-17 Preface Background Research in exposure science, toxicology, and environmental health is becoming increasingly reliant upon data science and computational methods that can more efficiently extract information from complex datasets. These methods can be used to better identify relationships between exposures to chemicals in the environment and human disease outcomes. Still, there remains a critical gap surrounding the training of researchers on these in silico methods. Objectives We aimed to address this critical gap by developing the inTelligence And Machine lEarning (TAME) Toolkit, promoting trainee-driven data generation, management, and analysis methods to “TAME” data in environmental health studies. This toolkit encompasses training modules, organized as chapters within this Github Bookdown site. TAME site users are welcome to participate in training modules by viewing them online and/or downloading all underlying script, data, and figure input files through the UNC-SRP TAME2 GitHub website. Contributors to TAME 2.0 Julia E. Rager, PhD, MSEE (UNC-Chapel Hill) directed the developed of TAME 2.0. All training resources were developed by combining modules drafted by multiple authors and experts in the field of environmental health and data science. Authors are specifically recognized at the beginning of each module section. Contributors specifically include the following, listed in alphabetical order: Oyemwenosa N. Avenbuan, University of North Carolina at Chapel Hill (UNC-Chapel Hill) Rebecca Boyles, MSPH, Research Triangle Institute (RTI) International Jessie Chappel, PhD, University of North Carolina at Chapel Hill (UNC-Chapel Hill) Chloe K. Chou, PhD Candidate, University of North Carolina at Chapel Hill (UNC-Chapel Hill) Lauren Eaves, PhD, University of North Carolina at Chapel Hill (UNC-Chapel Hill) Rebecca C. Fry, PhD, University of North Carolina at Chapel Hill (UNC-Chapel Hill) Elise Hickman, PhD, University of North Carolina at Chapel Hill (UNC-Chapel Hill) Ilona Jaspers, PhD, University of North Carolina at Chapel Hill (UNC-Chapel Hill) Lauren E. Koval, PhD, University of North Carolina at Chapel Hill (UNC-Chapel Hill) Paul Kruse, PhD, U.S. Environmental Protection Agency (U.S. EPA) Sarah Miller, PhD Candidate, University of North Carolina at Chapel Hill (UNC-Chapel Hill) David M. Reif, PhD, Division of Translational Toxicology (DTT), National Institutes of Environmental Health Science (NIEHS) Cynthia Rider, PhD, Division of Translational Toxicology (DTT), National Institutes of Environmental Health Science (NIEHS) Kyle Roell, PhD, University of North Carolina at Chapel Hill (UNC-Chapel Hill) Alexis Payton, MS, University of North Carolina at Chapel Hill (UNC-Chapel Hill) Grace Patlewicz, PhD, U.S. Environmental Protection Agency (U.S. EPA) Caroline Ring, PhD, U.S. Environmental Protection Agency (U.S. EPA) Cavin Ward-Caviness, PhD, U.S. Environmental Protection Agency (U.S. EPA) Module Development Overview Training modules were developed to provide applications-driven examples of data organization and analysis methods that can be used to address environmental health questions. Target audiences for these modules include students and professionals in academia, government, and industry that are interested in expanding their skillset. Modules were developed by study coauthors using annotated script formatted for R/RStudio coding language and interface and are now organized into seven chapters. The first group of modules focuses on introductory data science, which included the following topics: data management and sharing practices, file management, and basic data wrangling. The second chapter of modules provides an overview of scripting, using R as the example language. Topics span downloading and programming in R, coding best practices, data manipulation and reshaping, and coding efficiencies. The third chapter demonstrates methods for basic data analysis and visualizations, spanning an introduction to data visualizations, methods to improve data visualizations, normality tests and data transformations, and introduction to statistical tests. The fourth chapter focuses on the translation of wet lab data into dry lab analyses. Topics in this chapter include an introduction to key experimental features and terms; data importing, cleaning and summary statistics from wet lab sources; two-group comparisons and visualizations; multigroup comparisons and visualizations; advanced multigroup comparisons; and advanced data export and sharing. The fifth chapter provides an introduction to machine learning and artifical intelligence (ML/AI). Topics in this chapter include a general introduction of ML/AI and then provides examples of unsupervised machine learning methods, supervised machine learning methods, and interpretation of developed ML models. The sixth chapter provides applications in toxicology and exposure science, spanning descriptive cohort analyses; -omics analyses; mixtures methodologies; toxicokinetic modeling; and chemical read-across.The seventh chapter delves into examples of environmental health database mining, and highlights the helpful databases of Comparative Toxicogenomics Database, Gene Expression Omnibus, Computational Toxicology Dashboard access through APIs, and database integration using geospatial methods. The overall organization of this TAME toolkit is summarized below. Modules are organized into seven chapters that are listed on the left side of this website. Ways to use TAME 2.0 and Underlying Site License Site viewers are encouraged to follow along each module, either starting at the very beginning and reading through the material like a classical book or clicking on specific modules of interest and learning topics of greatest interest to their data science development. Each module is designed to stand-alone. Site license This site is covered by the BY-NC-ND Creative Commons license. This license allows reusers to copy and distribute the material in any medium or format in unadapted form only, for noncommercial purposes only, and only so long as attribution is given to the creator. Figure credit Figures included in TAME 2.0 were either generated in R, created within Biorender.com, and/or pulled from online publications with site licensing and author attribution throughout. Concluding Remarks Together, this valuable resource provides unique opportunities to obtain introductory-level training on current data analysis methods applicable to 21st century exposure science, toxicology, and environmental health. These modules serve as applications-based examples on how to “TAME” data within the environmental health research field, expanding the toolbox for career development and cross-training of scientists in multiple specialties, as well as supporting the next generation of data scientists. Funding This study was supported by the National Institutes of Health (NIH) from the National Institute of Environmental Health Sciences, including the following grant funds and associated programs: P42ES031007: The University of North Carolina (UNC)-Superfund Research Program (SRP) seeks to develop new solutions for reducing exposure to inorganic arsenic and prevent arsenic-induced diabetes through mechanistic and translational research. The UNC-SRP Data Analysis and Management Core (UNC-SRP-DMAC) provides the UNC-SRP with critical expertise in bioinformatics, statistics, data management, and data integration. T32ES007126: The UNC Curriculum in Toxicology and Environmental Medicine (CiTEM) seeks to provide a cutting edge research and mentoring environment to train students and postdoctoral fellows in environmental health and toxicology. Towards this goal, the CiTEM has a T32 Training Program for Pre- and Postdoctoral Training in Toxicology to support the development of future investigators in environmental health and toxicology. Support was additionally provided through the Institute for Environmental Health Solutions (IEHS) at the University of North Carolina (UNC) Gillings School of Global Public Health. The IEHS is aimed at protecting those who are particularly vulnerable to diseases caused by environmental factors, putting solutions directly into the hands of individuals and communities of North Carolina and beyond. "],["fair-data-management-practices.html", "1.1 FAIR Data Management Practices Introduction to Training Module Introduction to FAIR Breaking Down FAIR, Letter-by-Letter What Does This Mean for You? Data Repositories for Sharing of Data Recent Shifts in Regulatory Policies for Data Sharing Additional Training Resources on FAIR", " 1.1 FAIR Data Management Practices This training module was developed by Rebecca Boyles, with contributions from Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module This training module provides a description of FAIR data management practices, and points participants to important resources to help ensure generated data meet current FAIR guidelines. This training module is descriptive content-based (as opposed to coding-based), in order to present information clearly and serve as an important resource alongside the other scripted training activities. Training Module’s Environmental Heatlh Questions This training module was specifically developed to answer the following questions: What is FAIR? When was FAIR first developed? When making data ‘Findable’, who and what should be able to find your data? When saving/formatting your data, which of the following formats is preferred to meet FAIR principles: .pdf, .csv, or a proprietary output file from your lab instrument? How can I find a suitable data repository for my data? Introduction to FAIR Proper data management is of utmost importance while leading data analyses within the field of environmental health science. A method to ensure proper data management is the implementation of Findability, Accessibility, Interoperability, and Reusability (FAIR) practices. A landmark paper that describes FAIR practices in environmental health research is the following: Wilkinson MD, Dumontier M, Aalbersberg IJ, et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data. 2016 Mar 15. PMID: 26978244. The FAIR principles describe a framework for data management and stewardship aimed at increasing the value of data by enabling sharing and reuse. These principles were originally developed from discussions during the Jointly Designing a Data FAIRport meeting at the Lorentz Center in Leiden, The Netherlands in 2014, which brought together stakeholders to discuss the creation of an environment for virtual computational science. The resulting principles are technology agnostic, discipline independent, community driven, and internationally adopted. Below is a schematic providing an overview of this guiding principle: Answer to Environmental Health Question 1 &amp; 2 With this background, we can answer Environmental Health Question #1 and #2: What is FAIR and when was it first developed? Answer: FAIR is guiding framework that was recently established to promote best data management practices, to ensure that data are Findable, Accessibility, Interoperable, and Reusable. It was first developed in 2014- which means that these principles are very new and continuing to evolve! Breaking Down FAIR, Letter-by-Letter The aspects of the FAIR principles apply to data and metadata with the aim of making the information available to people and computers as described in the seminal paper by Wilkinson et al., 2016. F (Findable) in FAIR The F in FAIR identifies components of the principles needed to make the meta(data) findable through the application of unique persistent identifiers, thoroughly described, reference the unique identifiers, and that the descriptive information (i.e., metadata) could be searched by both humans and computer systems. F1. (Meta)data are assigned a globally unique and persistent identifier Each dataset is assigned a globally unique and persistent identifier (PID), for example a DOI. These identifiers allow to find, cite and track (meta)data. A DOI looks like: https://doi.org/10.1109/5.771073 Action: Ensure that each dataset is assigned a globally unique and persistent identifier. Certain repositories automatically assign identifiers to datasets as a service. If not, obtain a PID via a PID registration service. F2. Data are described with rich metadata Each dataset is thoroughly (see R1) described: these metadata document how the data was generated, under what term (license) and how it can be (re)used and provide the necessary context for proper interpretation. This information needs to be machine-readable. Action: Fully document each dataset in the metadata, which may include descriptive information about the context, quality and condition, or characteristics of the data. Another researcher in any field, or their computer, should be able to properly understand the nature of your dataset. Be as generous as possible with your metadata (see R1). F3. Metadata clearly and explicitly include the identifier of the data it describes Explanation: The metadata and the dataset they describe are separate files. The association between a metadata file and the dataset is obvious thanks to the mention of the dataset’s PID in the metadata. Action: Make sure that the metadata contains the dataset’s PID. F4. (Meta)data are registered or indexed in a searchable resource Explanation: Metadata are used to build easily searchable indexes of datasets. These resources will allow to search for existing datasets similarly to searching for a book in a library. Action: Provide detailed and complete metadata for each dataset (see F2). Answer to Environmental Health Question 3 With this, we can answer Environmental Health Question #3: When making data ‘Findable’, who and what should be able to find your data? Answer: Both humans and computer systems should be able to find your data. A (Accessible) in FAIR The A components are designed to enable meta(data) be available long-term, accessed by humans and machines using standard communication protocols with clearly described limitations on reuse. A1. (Meta)data are retrievable by their identifier using a standardized communications protocol Explanation: If one knows a dataset’s identifier and the location where it is archived, one can access at least the metadata. Furthermore, the user knows how to proceed to get access to the data. Action: Clearly define who can access the actual data and specify how. It is possible that data will not be downloaded, but rather reused in situ. If so, the metadata must specify the conditions under which this is allowed (sometimes versus the conditions needed to fulfill for external usage/“download”). A1.1 The protocol is open, free, and universally implementable Explanation: Anyone with a computer and an internet connection can access at least the metadata. A1.2 The protocol allows for an authentication and authorization procedure, where necessary Explanation: It often makes sense to request users to create a user account on a repository. This allows to authenticate the owner (or contributor) of each dataset, and to potentially set user specific rights. A2. Metadata are accessible, even when the data are no longer available Explanation: Maintaining all datasets in a readily usable state eternally would require an enormous amount of curation work (adapting to new standards for formats, converting to different format if specifically needed software is discontinued, etc). Keeping the metadata describing each dataset accessible, however, can be done with fewer resources. This allows to build comprehensive data indexes including all current, past, and potentially arising datasets. Action: Provide detailed and complete metadata for each dataset (see R1). I (Interoperable) in FAIR The I components of the principles address needs for data exchange and interpretation by humans and machines which includes the use of controlled vocabularies or ontologies to describe meta(data) and to describe provenance relationships through appropriate data citation. I1. (Meta)data use a formal, accessible, shared, and broadly applicable language Explanation: Interoperability typically means that each computer system has at least knowledge of the other system’s formats in which data is exchanged. If (meta)data are to be searchable and if compatible data sources should be combinable in a (semi)automatic way, computer systems need to be able to decide if the content of datasets are comparable. Action: Provide machine readable data and metadata in an accessible language, using a well-established formalism. Data and metadata are annotated with resolvable vocabularies/ontologies/thesauri that are commonly used in the field (see I2). I2. (Meta)data use vocabularies that follow FAIR principles Explanation: The controlled vocabulary (e.g., MESH) used to describe datasets needs to be documented. This documentation needs to be easily findable and accessible by anyone who uses the dataset. Action: The vocabularies/ontologies/thesauri are themselves findable, accessible, interoperable and thoroughly documented, hence FAIR. Lists of these standards can be found at: NCBO BioPortal, FAIRSharing, OBO Foundry. I3. (Meta)data include qualified references to other (meta)data Explanation: If the dataset builds on another dataset, if additional datasets are needed to complete the data, or if complementary information is stored in a different dataset, this needs to be specified. In particular, the scientific link between the datasets needs to be described. Furthermore, all datasets need to be properly cited (i.e. including their persistent identifiers). Action: Properly cite relevant/associated datasets, by providing their persistent identifiers, in the metadata, and describe the scientific link/relation to your dataset. R (Reusable) in FAIR The R components highlight needs for the meta(data) to be reused and support integration such as sufficient description of the data and data use limitations. R1. Meta(data) are richly described with a plurality of accurate and relevant attributes Explanation: Description of a dataset is required at two different levels: Metadata describing the dataset: what does the dataset contain, how was the data generated, how has it been processed, how can it be reused. Metadata describing the data: any needed information to properly use the data, such as definitions of the variable names Action: Provide complete metadata for each data file. Scope of your data: for what purpose was it generated/collected? Particularities or limitations about the data that other users should be aware of. Date of the dataset generation, lab conditions, who prepared the data, parameter settings, name and version of the software used. Variable names are explained or self-explanatory. Version of the archived and/or reused data is clearly specified and documented. What Does This Mean for You? We advise the following as ‘starting-points’ for participants to start meeting FAIR guidances: Learn how to create a Data Management Plan Keep good documentation (project &amp; data-level) while working Do not use proprietary file formats (.csv is a great go-to formats for your data!) When able, use a domain appropriate metadata standard or ontology Ruthlessly document any steps in a project Most of FAIR can be handled by selecting a good data or software repository Don’t forget to include a license! Answer to Environmental Health Question 4 With these, we can answer Environmental Health Question #4: When saving/formatting your data, which of the following formats is preferred to meet FAIR principles: .pdf, .csv, or a proprietary output file from your lab instrument? Answer: A .csv file is preferred to enhance data sharing. Data Repositories for Sharing of Data When you are organizing your data to deposit online, it is important to identify an appropriate repository to publish your dataset it. A good starting place is a repository registry such as FAIRsharing.org or re3data.org. Journals can also provide helpful resources and starting repository lists, such as Nature and PLOS, which both have published a list of recommended repositories. Funding agencies, including the NIH, can also inform specific repositories. Below are some examples of two main categories of data repositories: 1. Domain Agnostic Data Repositories Domain agnostic repositories allow the deposition of any data type. Some examples include the following: Data in Brief Articles (e.g., Elsevier’s Data in Brief Journal) Dryad Figshare The Dataverse Project Zenodo 2. Domain Specific Data Repositories Domain specific repositories allow the deposition of specific types of data, produced from specific types of technologies or within specific domains. Some examples include the following: Database of Genotypes and Phenotypes Gene Expression Omnibus The Immunology Database and Analysis Portal Metabolomics Workbench (National Metabolomics Data Repository) Microphysiology Systems Database Mouse Genome Informatics Mouse Phenome Database OpenNeuro Protein Data Bank ProteomeXchange Rat Genome Database The Database of Genotypes and Phenotypes Zebrafish Model Organism Database and many, many, many others… Answer to Environmental Health Question 5 With these, we can answer Environmental Health Question #5: How can I find a suitable data repository for my data? Answer: I can search through a data repository registry service or look for recommendations from NIH or other funding agencies. Recent Shifts in Regulatory Policies for Data Sharing The NIH Data Management and Sharing Policy NIH’s data management and sharing (DMS) policy became effective January 2023. This policy specifically lists the expectations that investigators must comply with in order to promote the sharing of scientific data. Information about this recent policy can be found through updated NIH websites. Information about writing an official Data Management and Sharing (DMS) plan for your research can be found through NIH’s Guidance on Writing a Data Management &amp; Sharing Plan. The 2018 Evidence Act The Evidence Act, or Foundations for Evidence-Based Policymaking Act of 2018, was signed into U.S. law on January 14, 2019. The Act requires federal agencies to build the capacity to use evidence and data in their decision-making and policymaking. It also requires agencies to: Develop an evidence-building plan as part of their quadrennial strategic plan &amp; Develop an evaluation plan concurrent with their annual performance plan. The Evidence Act also: Mandates that data be “open by default” Specifies that a comprehensive data inventory should be created for each agency’s open data assets How Does the NIH Data Management and Sharing Policy Intersect with the 2018 Evidence Act? Making your data FAIR, by definition, makes it more shareable and reusable. Many of the requirements in the NIH DMS and the Evidence Act policy overlap with the FAIR principles. The CARE Principles for Indigenous Data Governance While we are experiencing increased requirements for the open sharing of data, it is important to recognize that there are circumstances and populations that should, at the same time, be carefully protected. Examples include human clinical or epidemiological data that may become identifiable upon the sharing of sensitive data. Another example includes the consideration of Indigenous populations. A recent article by Carroll et al. 2021 describes in their abstract: As big data, open data, and open science advance to increase access to complex and large datasets for innovation, discovery, and decision-making, Indigenous Peoples’ rights to control and access their data within these data environments remain limited. Operationalizing the FAIR Principles for scientific data with the CARE Principles for Indigenous Data Governance enhances machine actionability and brings people and purpose to the fore to resolve Indigenous Peoples’ rights to and interests in their data across the data lifecycle. Additional Training Resources on FAIR Many organizations, from specific programs to broad organizations, provide training and resources for scientists in FAIR principles. Some of the notable global organizations organizing and providing training that offer opportunities for community involvement are: Committee on Data for Science and Technology (CODATA) Global Alliance for Genomics &amp; Health GoFAIR Force11 Research Data Alliance Example Workshops discussing FAIR: NAS Implementing FAIR Data for People and Machines: Impacts and Implications (2019). Available at: https://www.nationalacademies.org/our-work/implementing-fair-data-for-people-and-machines-impacts-and-implications NIH Catalyzing Knowledge-driven Discovery in Environmental Health Sciences Through a Harmonized Language, Virtual Workshop (2021). Available at: https://www.niehs.nih.gov/news/events/pastmtg/2021/ehslanguage/index.cfm NIH Trustworthy Data Repositories Workshop (2019). Available at: https://datascience.nih.gov/data-ecosystem/trustworthy-data-repositories-workshop NIH Virtual Workshop on Data Metrics (2020). Available at: https://datascience.nih.gov/data-ecosystem/nih-virtual-workshop-on-data-metrics NIH Workshop on the Role of Generalist Repositories to Enhance Data Discoverability and Reuse: Workshop Summary (2020). Available at: https://datascience.nih.gov/data-ecosystem/nih-data-repository-workshop-summary Example Government Report Documents on FAIR: Collins S, Genova F, Harrower N, Hodson S, Jones S, Laaksonen L, Mietchen D, Petrauskaite R, Wittenburg P. Turning FAIR into reality: Final report and action plan from the European Commission expert group on FAIR data: European Union; 2018. Available at: https://www.vdu.lt/cris/handle/20.500.12259/103794. EU. FAIR Data Advanced Use Cases: From Principles to Practice in the Netherlands. 2018. European Union. Available at: doi:10.5281/zenodo.1250535. NIH. Final NIH Policy for Data Management and Sharing and Supplemental Information. National Institutes of Health. Federal Register, vol. 85, 2020-23674, 30 Oct. 2020, pp. 68890–900. Available at: https://www.federalregister.gov/d/2020-23674. NIH. NIH Strategic Plan for Data Science 2018. National Institutes of Health. Available at: https://datascience.nih.gov/strategicplan. NLM. NLM Strategic Plan 2017 to 2027. U.S. National Library of Medicine, Feb. 2018. Available at: https://www.nlm.nih.gov/about/strategic-plan.html. Example Related Publications on FAIR: Comess S, Akbay A, Vasiliou M, Hines RN, Joppa L, Vasiliou V, Kleinstreuer N. Bringing Big Data to Bear in Environmental Public Health: Challenges and Recommendations. Front Artif Intell. 2020 May;3:31. doi: 10.3389/frai.2020.00031. Epub 2020 May 15. PMID: 33184612; PMCID: PMC7654840. Koers H, Bangert D, Hermans E, van Horik R, de Jong M, Mokrane M. Recommendations for Services in a FAIR Data Ecosystem. Patterns (N Y). 2020 Jul 7;1(5):100058. doi: 10.1016/j.patter.2020.100058. Erratum in: Patterns (N Y). 2020 Sep 11;1(6):100104. PMID: 33205119. Kush RD, Warzel D, Kush MA, Sherman A, Navarro EA, Fitzmartin R, Pétavy F, Galvez J, Becnel LB, Zhou FL, Harmon N, Jauregui B, Jackson T, Hudson L. FAIR data sharing: The roles of common data elements and harmonization. J Biomed Inform. 2020 Jul;107:103421. doi: 10.1016/j.jbi.2020.103421. Epub 2020 May 12. PMID: 32407878. Lin D, Crabtree J, Dillo I, Downs RR, Edmunds R, Giaretta D, De Giusti M, L’Hours H, Hugo W, Jenkyns R, Khodiyar V, Martone ME, Mokrane M, Navale V, Petters J, Sierman B, Sokolova DV, Stockhause M, Westbrook J. The TRUST Principles for digital repositories. Sci Data. 2020 May 14;7(1):144. PMID: 32409645. Thessen AE, Grondin CJ, Kulkarni RD, Brander S, Truong L, Vasilevsky NA, Callahan TJ, Chan LE, Westra B, Willis M, Rothenberg SE, Jarabek AM, Burgoon L, Korrick SA, Haendel MA. Community Approaches for Integrating Environmental Exposures into Human Models of Disease. Environ Health Perspect. 2020 Dec;128(12):125002. PMID: 33369481. Roundtable on Environmental Health Sciences, Research, and Medicine; Board on Population Health and Public Health Practice; Health and Medicine Division; National Academies of Sciences, Engineering, and Medicine. Principles and Obstacles for Sharing Data from Environmental Health Research: Workshop Summary. Washington (DC): National Academies Press (US); 2016 Apr 29. PMID: 27227195. Test Your Knowledge Let’s imagine that you’re a researcher who is planning on gathering a lot of data using the zebrafish model. In order to adequately prepare your studies and steps to ensure data are deposited into proper repositories, you have the idea to check repository information obtained in FAIRsharing.org. What are some example repositories and relevant ontology resources that you could use to organize, deposit, and share your zebrafish data (hint: use the search tool)? "],["data-sharing-through-online-repositories.html", "1.2 Data Sharing through Online Repositories An Overview and Example with the Dataverse Repository Introduction to Training Module Data Repositories The Dataverse Project What is a Dataverse? Metadata Creating a Dataverse Creating a Dataset Concluding Remarks", " 1.2 Data Sharing through Online Repositories An Overview and Example with the Dataverse Repository This training module was developed by Kyle R. Roell, Alexis Payton, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Submitting data to publicly available repositories is an essential part of ensuring data meet FAIR guidelines, as discussed in detail in the previous training module. There are many benefits to sharing and submitting your researching, such as: Making more use out of data that are generated in your lab More easily sharing and integrating across datasets Ensuring reproducibility in analysis findings and conclusions Improving the tracking and archiving of data sources, and data updates Increasing the awareness and attention surrounding your research as others locate your data through additional online queries Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: How should I structure my data for upload into online repositories? What does the term ‘metadata’ mean and what does it look like? This module will introduce some of the repositories that are commonly used to deposit data, how to set up metadata files, and how to organize example data in preparation for sharing. We will also provide information surrounding best practices for data organization and sharing through these repositories. Additional resources are also provided throughout, as there are many ways to organize, share, and deposit data depending on your data types and structures and overall research goals. Data Repositories There are many publicly available repositories that we should consider when depositing data. Some general repository registries that are helpful to search through include FAIRsharing.org or re3data.org. Journals can also provide helpful resources and starting repository lists, such as Nature and PLOS, which both have published a list of recommended repositories. As detailed in the FAIR training module, there are two main categories of data repositories: 1. Domain Agnostic Data Repositories Domain agnostic repositories allow the deposition of any data type. Some examples include: Data in Brief Articles (e.g., Elsevier’s Data in Brief Journal) Dryad Figshare The Dataverse Project Zenodo 2. Domain Specific Data Repositories Domain specific repositories allow the deposition of specific types of data, produced from specific types of technologies or within specific domains. Some examples include: Database of Genotypes and Phenotypes Gene Expression Omnibus The Immunology Database and Analysis Portal Metabolomics Workbench (National Metabolomics Data Repository) Microphysiology Systems Database Mouse Genome Informatics Mouse Phenome Database OpenNeuro Protein Data Bank ProteomeXchange Rat Genome Database The Database of Genotypes and Phenotypes Zebrafish Model Organism Database and many, many, many others… This training module focuses on providing an example of how to organize and upload data into the Dataverse; though many of the methods described below pertain to other data repositories as well, and also incorporate general data organization and sharing best practices. The Dataverse Project Dataverse, organized through The Dataverse Project, is a popular repository option that allows for upload of most types of material, without any stringent requirements. The Dataverse organization also provides ample resources on how to organize, upload, and share data through Dataverse. These resources include very thorough, readable, and user guides and best practices. Screenshot of the main page of The Dataverse Project An easier way to think about Dataverse is to interpret it similar to a folder system on your computer. A Dataverse is just an online folder that contains files, data, or datasets that are all related to some topic, project, etc. Although Dataverse was started at Harvard and the base Dataverse lives there, there are many versions of Dataverse that are specific to and supported by various institutions. For example, these training modules are being developed primarily by faculty, staff, and students at the University of North Carolina at Chapel Hill. As such, the examples contained in this module will specifically connect with the UNC Dataverse; though many of the methods outlined here are applicable to other Dataverses and additional online repositories, in general. What is a Dataverse? Remember how we pointed out that a Dataverse is similar to a folder system on a computer? Well, here we are going to show you what that actually looks like. But first, something that can be confusing when starting to work with Dataverse is the fact that the term Dataverse is used for both the overarching repository as well as individual subsections (or folders) in which data are stored. For example, the UNC Dataverse is called a Dataverse, but to upload data, you need to upload it to a specific sub-Dataverse. So, what is the difference between the high level UNC Dataverse and smaller, sub-dataverses? Well, nothing, really. The UNC Dataverse is similar to a large folder that says, these are all the projects and research related to or contained within UNC. From there, we want to be more specific about where we store our research, so we are creating more sub-Dataverses (folders) within that higher, overarching UNC Dataverse. As an example, using the UNC Dataverse, here we can see various sub-Dataverses that have been created as repositories for specific projects or types of data. As another example looking within a specific Dataverse, here we can see the Dataverse that hosts datasets and publications for Dr. Julia Rager’s lab, the Ragerlab-Dataverse. Within this Datavere, we can see various datasets produced by her lab. It is worth noting that the datasets may not necessarily be directly related to each other in terms of exact topic, for example, the Ragerlab-Dataverse hosts data pertaining to wildfire smoke exposure as well as chemical exposures and breast cancer. But they are all pertaining to experiments and analyses run within her specific lab. Let’s now start talking more specifically about how to organize data and format files for Dataverse, create your own “Dataverse”, upload datasets, and what this all means! Dataset Structure Before uploading your data to any data repository, it is important to structure your data efficiently and effectively, making it easy for others to navigate, understand, and utilize. While we will cover this in various sections throughout these training modules, here are some basic tips for data structure and organization. Keep all data for one participant or subject within one column (or row) of your dataset Genomic data and other analytical assays tend to have subjects on columns and genes, expression, etc. as the rows Descriptive and demographic data often tend to have subjects or participants as the rows and each descriptor variable (including demographics and any other subject variables) as columns Create succinct, descriptive variable names For example, do not use something like “This Variable Contains Information Regarding Smoking Status”, and instead just using something like, “Smoking_Status” Be aware of using spacing, special characters, and capitalization within variable names Think about transforming data from wide to long format depending on your specific dataset and general conventions Be sure to follow specific guidelines of repository when appropriate TAME 2.0 Module 1.1 FAIR Data Management Practices and TAME 2.0 Module 1.4 Data Wrangling in Excel are also helpful resources to reference when thinking about organizing your data. A general example of an organized, long format dataset in Excel in provided below: Only .csv or .txt files can be uploaded to dataverse; therefore, the metadata and data tabs in an excel file will need to saved and uploaded as two separate .csv or .txt files. Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question 1: How should I structure my data for upload into online repositories? Answer: It is ideal to have data clearly organized and filled, with succinct and descriptive variable names clearly labeled and values filled in. Most commonly, datasets should be saved as separate .csv or .txt files for upload into data repositories. Metadata There are many different definitions of what a metadata file is. Helpful explanations, for example, are provided by the UNC University Libraries: There are many definitions of metadata, but one of the simplest is data about data. More specifically… Metadata (in terms of data management) describe a dataset: how they were collected; when they were collected; what assumptions were made in their methodology; their geographic scope; if there are multiple files, how they relate to one another; the definitions of individual variables and, if applicable, what possible answers were (i.e., to survey questions); the calibration of any equipment used in data collection; the version of software used for analysis; etc. Very often, a dataset that has no metadata is incomprehensible. Metadata ARE data. They are pieces of information that have some meaning in relation to another piece of information. They can be created, managed, stored, and preserved like any other data. Metadata can be applied to anything. A computer file can be described in the same way that a book or piece of art can be described. For example, both can have a title, an author, and a year created. Metadata should be documented for research outputs of any kind. Metadata generally has little value on their own. Metadata adds value to other information, but are usually not valuable in themselves. There are exceptions to this rule, such as text transcription of an audio file. There are three kinds of metadata: Descriptive metadata consist of information about the content and context of your data. Examples: title, creator, subject keywords, and description (abstract) Structural metadata describe the physical structure of compound data. Examples: camera used, aperture, exposure, file format, and relation to other data or files Administrative metadata are information used to manage your data. Examples: when and how they were created, who can access them, software required to use them, and copyright permissions Therefore, after having organized your primary dataset for submission into online repositories, it is equally important to have a metadata file for easy comprehension and utilization of your data for future researchers or anyone downloading your data. While most repositories capture some metadata on the dataset page (e.g., descripton of data, upload date, contact information), there is generally little information about the specific data values and variables. In this section, we review some general guidelines and tips to better annotate your data. First, keep in mind, depending on the specific repository you are using, you may have to follow their metadata standards. But, if uploading to more generalist repository, this may be up to you to define. Generally, a metadata file consists of a set of descriptors for each variable in the data. If you are uploading data that contains many covariates or descriptive variables, it is essential that you provide a metadata file that describes these covariates. Both a description of the variable as well as any specific levels of any categorical or factor type variables. From the dataset presented previously, here we present an example of an associated metadata file: Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question 2: What does the term ‘metadata’ mean and what does it look like? Answer: Metadata refers to the information that describes and explains data. It looks like an additional dataset that provides context with details such as the source, type, owner, and relationships to other datasets. This file can help users understand the relevance of a specific dataset and provide guidance on how to use it. Creating a Dataverse Now, let’s review how to actually create a Dataverse. First, navigate to the parent Dataverse that you would like to use as your primary host website. For example, our group uses the UNC Dataverse. If you do not already have one, create a username and login. Then, from the home Dataverse page, click “Add Data” and select “New Dataverse”. And fill in the information necessary. And that is it. After creating your Dataverse site, you will need to publish it; however, before it is accessible to the public, note that you can actually create a Dataverse within another Dataverse (similar to a folder within a folder on your computer). This makes sense even when you are creating a new Dataverse at the home, UNC Dataverse level, you are still technically creating a new Dataverse within an existing one (the large UNC Dataverse). Here are some tips as you create your Dataverse: Do not recreate a Dataverse that already exists Choose a name that is specific, but general enough that it doesn’t only pertain to one specific dataset You can add more than one contact email, if necessary Creating a Dataset Creating a dataset creates a page for your data containing information about that data, a citation for the data (something valuable and somewhat unique to Dataverse), as well the place from where you data can be directly accessed or downloaded. First, decide the specific Dataverse your data will live and navigate to that specific Dataverse site. Then carry out the following steps to create a dataset: Navigate to the Dataverse page under which your dataset will live Click “Add Data” and then select “New Dataset” Fill in the necessary information Upload your data and metadata file(s) structured as detailed above Now, you have a dataset within your Dataverse. Again, you will have to publish the dataset for someone to have access to it. The easy part of using a more generalist repository like Dataverse, is that you do not have to have a strict data structure adherence. However, this means it is up to you to make sure your data is readable and useable. Concluding Remarks In this training module, we set out to express the importance of uploading data to online repositories, demonstrate what the upload process may look like using a generalist repository (Dataverse), and give some examples and tips on structuring data for upload and creating metadata files. It is important to choose the appropriate repository for your data based on your field of study and specifications of your work. Test Your Knowledge Try creating your own Dataverse repository, format your files to be uploaded to Dataverse, and upload those files to your new repository! "],["file-management-using-github.html", "1.3 File Management using Github Introduction to Training Module Creating an Account Creating a Repository Uploading Code Adding Subfolders in a Repository Updating Code Updating Repository Titles and Structure to Support a Manuscript Tracking Code Changes using Github Branches Concluding Remarks", " 1.3 File Management using Github This training module was developed by Alexis Payton, Lauren E. Koval, Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Good data practices like file management and code tracking are imperative for data analysis initiatives, especially when working in research teams and/or shared project folders. Often times analyses and manuscripts are edited many times prior to being submitted for a grant or publication. Analysis methods are also shared between members of a research team and to external communities, as further detailed in TAME 2.0 Module 1.1 FAIR Data Management Practices. Therefore, Github has emerged as an effective way to manage, share, and track how code changes over time. Github is an open source or publicly accessible platform designed to facilitate version control and issue tracking of code. It is used by us and many of our colleagues to not only document versions of script written for data analysis and visualization, but to also make our code publicly available for open communication and dissemination of results. This training module serves a launch pad for getting acclimated with Github and includes… Creating an account Uploading code Creating a repository and making it legible for manuscript submission Creating an Account First, users must create their own accounts within github to start uploading/sharing code. To do this, navigate to github.com, click “Sign Up”, and follow the on screen instructions. Creating a Repository A repository, also known as a “repo”, is similar to a project folder that will contain all code pertaining to a specific project (which can be used for specific research programs, grants, or manuscripts, as examples). A repository can be set to public or private. If a repo is initially set to private to keep findings confidential prior to publication, it can always be updated to public once findings are ready for public dissemination. Multiple people can be allowed to work on a project together within a single repository. To access the repositories that are currently available to you through your user account, click the circle in top right-hand corner and click “Your repositories”. To create a new repository, click on the green button that says “New”. Then give your repository a descriptive name. We often edit the repo titles to match the title of specific manuscripts, though specific titling formats are up to the users/team’s preference. For more information, visit Github’s Create a repo documentation. Then click “Add a README file” to initiate the README file, which is important to continually edit to provide analysis-specific background information, and any additional information that would be helpful during and after code is drafted to better facilitate tracking information and project details. We provide further details surrounding specific information that can be included within the README file below. Uploading Code The simplest way to upload code is to first navigate to the repository that you would like to upload your code/associated files to. Note that this could represent a repo that you created or that someone granted you access to. Click “Add file” then click “Upload files”. Drag and drop your file containing your script into github and click “Commit changes”. A more advanced way to upload code is by using the command line, which allows a user to directly interact with the computer or software application. Further documentation can be found here. Adding Subfolders in a Repository To keep the repository organized, it might be necessary to create a new folder (like the folder labeled “1.1. Summary Statistics” in the above screenshot). Files can be grouped into these folders based on the type of analysis. To do so, click on the new file and then click on the pencil icon next to the “Blame” button. Click on the box that contains the title of the file. Write the title of your new folder and then end with a forward slash (/). In the screenshot below, we’re creating a new folder entitled “New Folder”. Click “Commit changes” and your file should now be in a new folder. Updating Code Saving iterations of code can save valuable time later as analyses are constantly being updated and edited. If your code undergoes substantial changes, (e.g., adding/ removing steps or if there’s code that is likely to be beneficial later on, but is no longer relevant to the current analysis), it is helpful to save that version in Github for future reference. To do so, create a subfolder named “Archive” and move the old file into it. If you have multiple versions of a file with the same name, add the current date to prevent the file from being overwritten later on as seen in the screenshot below. Once the old file version has been archived, now upload the most recent version of your code to the main folder. Based on the screenshot above, that would be under “3. ML Visualizations”. Note: If a file is uploaded with the same name it will be overwritten, which can’t be undone! Therefore, put the older file into the archive folder if you’d like it to be saved PRIOR to uploading the new version. Updating Repository Titles and Structure to Support a Manuscript If the code is for a manuscript, it’s helpful to include the table or figure name it pertains to in the manuscript in parentheses. For example, “Baseline Clusters (Figure 3)”. This allows viewers to find find the code for each table or figure faster. Using a README.md file A README.md file is used to describe the overall aims and purpose of the analyses in the repository or a folder within a repository. It is often the first file that someone will look at in a repo/folder, so it is important to include information that would be valuable to an outsider trying to make use of the work. To add a README.md file, click “Add file” and then “Create new file”. Name your file “README.md”. A README.md file uses R markdown syntax. This type of syntax is very helpful as you continue to develop R coding skills, as it provides a mechanism through which your code’s output can be visualized and saved as a rendered file version. There are many helpful resources for R markdown, including some that we find helpful: R Markdown Cheatsheet R Markdown Syntax Overview The final README.md file for the OVERALL repository for manuscript submission should look something like the screenshot below. Always include… The main goal of the project The final manuscript name, year it was published, Pub Med ID (if applicable) Graphical abstract (if needed for publication) Names and brief descriptions of each file Include both the goal of the analysis and the methodology used (ie. Using chi square tests to determine if there are statistically significant differences across demographic groups) If the code was written in the software Jupyter (ie. has the extension .ipynb not .R or .Rmd), NBViewer is a website that can render jupyter notebooks (files). This is helpful, because sometimes the files take too long to render, so link the repository from the NB viewer website. Go to nbviewer.org –&gt; type in the name of the repository –&gt; copy the url and add it to the README.md file The final README.md file for the a subfolder within a repository should look something like the screenshot below. Always include… The name of each file Brief description of each file Include both the goal of the analysis and the methodology used Table or Figure name in the corresponding manuscript (if applicable) Note: That the organization structure for the README.md files are simply recommendations and should be changed based on needs of the project. However, it is important to include information and organize the repository in a way that helps other readers and colleagues navigate it who aren’t familiar with the project. Example Repositories Below are links to repositories that contain code for analyses used in published manuscripts. These are examples of well organized Github repositories. Wildfires and Environmental Justice: Future Wildfire Events Predicted to Disproportionally Impact Socioeconomically Vulnerable Communities in North Carolina Plasma sterols and vitamin D are correlates and predictors of ozone-induced inflammation in the lung: A pilot study Cytokine signature clusters as a tool to compare changes associated with tobacco product use in upper and lower airway samples Tracking Code Changes using Github Branches Github is a useful platform for managing and facilitating code tracking performed by different collaborators through branches. When creating a repository on Github, it automatically creates a default branch entitled “main”. It’s possible to create a new branch which allows a programmer to make changes to files in a repository in isolation from the main branch. This is beneficial, because the same file can be compared across branches, potentially created by different scientists, and merged together to reflect those changes. Note: In order for this to work the file in main branch has to have the same name and the file in the newly created branch. Let’s start by creating a new branch. First, navigate to a repository, select “main” and then “View all branches”. Click “New branch”, give your branch a title, and click “Create new branch”. In the screenshot, you’ll see the new branch entitled “jr-changes”. As a new collaborator interested in comparing and merging code changes to a file, click on the new branch that was just created. Based on the screenshot, that means click “jr-changes”. After uploading the file(s) to this branch, you’ll see a notification that this branch is now a certain number of commits ahead of the main branch. A commit records the number of changes to files in a branch. Based on the screenshot, “jr-changes” is now 2 commits ahead of “main”. Click on “2 commits ahead” and scroll down to compare versions between the “main” and “jr-changes” branches. A pull request will need to be created. A pull request allows other collaborators to see changes made to a file within a branch. These proposed changes can be discussed and amended before merging them into the main branch. For more information, visit Github’s branches, pull requests and comparing branches in pull requests documentation. Go ahead and click on “Create pull request”. Click on “Create pull request” again on the next screen. Select “Merge pull request” and then “Confirm merge”. Concluding Remarks In summary, this training module serves as a basic tutorial for sharing code on Github in a way that is beneficial for scientific research. Concepts discussed include uploading and updating code, making a repository easily readable for manuscript submissions, and tracking code changes across collaborators. We encourage trainees and data scientists to implement code tracking and sharing through Github and to also keep up with current trends in data analysis documentation that continue to evolve over time. Test Your Knowledge Try creating your own Github profile, set up a practice repo with subfolders, and a detailed READ.md file paralleling the suggested formatting and content detailed above for your own data analyses! "],["data-wrangling-in-excel.html", "1.4 Data Wrangling in Excel Introduction to Training Module Save a Copy of the Soon-To-Be Organized and Cleaned Dataset as a New File Remove Extraneous White Space Replace Missing Data with “NA” Create a Metadata Tab Abbreviate and Capitalize Categorical Data Alphabetize (Sort) the Data by the Categorical Variable of Interest Create a New Subject Number Column Remove Special Symbols and Dashes Bold all Column Names and Center all Data Create a Subject Identifier Column Separate Subject Demographic Data from Experimental Measurements Convert Data from Wide to Long Format Pivoting Data from a Wide to Long Format Generating Summary-Level Statistics with Pivot Tables Excel vs. R: Which Should You Use? Concluding Remarks", " 1.4 Data Wrangling in Excel This training module was developed by Alexis Payton, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module This module is intended to be a starting guide to cleaning and organizing an example toxicology dataset in Excel. Data wrangling involves cleaning, removing of erroneous data, and restructuring necessary for to preparing wet lab generated data for downstream analyses. These steps will ensure that: Data are amenable to downstream analyses in R, or your preferred programming language Data are clear and easily interpretable by collaborators, reviewers, and readers Click here for more information on data wrangling. In this training tutorial, we’ll make use of an example dataset that needs to be wrangled. The dataset contains concentration values for molecules that were measured using protein-based ELISA technologies. These molecules specifically span 17 sterols and cytokines, selected based upon their important roles in mediating biological responses. These measures were derived from human serum samples. Demographic information also exists for each subject. The following steps detailed in this training module are by no means exhaustive! Further resources are provided at the end. This module provides example steps that are helpful when wrangling your data in Excel. Datasets often come in many different formats from our wet bench colleagues, therefore some steps will likely need to be added, removed, or amended depending on your specific data. Save a Copy of the Soon-To-Be Organized and Cleaned Dataset as a New File Open Microsoft Excel and prior to ANY edits, click “File” –&gt; “Save As” to save a new version of the file that can serve as the cleaned version of the data. This is very important for file tracking purposes, and can help in the instance that the original version needs to be referred back to (e.g., if data are accidentally deleted or modified during downstream steps). The file needs to be named something indicative of the data it contains followed by the current date (e.g., “Allostatic Mediator Data_061622”). The title should be succinct and descriptive. It is okay to use dashes or underscores in the name of the title. Do not include special characters, such as $, #, @, !, %, &amp;, *, (, ), and +. Special characters tend to generate errors on local hard drives when syncing to cloud-based servers, and they are difficult to upload into programming software. Let’s first view what the dataset currently looks like: Helpful Excel Keyboard Shortcuts The following keyboard shortcuts can help you work more efficiently in Excel: Move to the last cell in use on the sheet Control + Fn + Right arrow key (Mac users) Control + End (PC users) Move to the beginning of the sheet Control + Fn + Left arrow key, then same Control + Fn + Up arrow key (Mac users) Control + Home (PC users) Highlight and grab all data Click on the first cell in the upper left hand corner then click and hold Shift + Command + Down arrow key + Right arrow key (Mac users) Shift + Command + Down arrow key + Right arrow key (PC users) Note: This only works if there are no cells with missing information or gaps in the columns/rows used to define the peripheral area. For more available shortcuts on various operating systems click here. Remove Extraneous White Space Before we can begin organizing the data, we need to remove the entirely blank rows of cells. This reduces the file size and allows for the use of the filter function in Excel, as well as other organizing functions, which will be used in the next few steps. This step also makes the data look more tidy and amenable to import for coding purposes. Excel Trick #1: Select all lines that need to be removed and press Control + minus key for Mac and PC users. (Note that there are other ways to do this for larger datasets, but this works fine for this small example.) Excel Trick #2: An easier way to remove blank rows and cells for larger datasets, includes clicking “Find &amp; Select”–&gt; “Special” –&gt; “Blanks” –&gt; click “OK” to select all blank rows and cells. Click “Delete” within the home tab –&gt; “Delete sheet rows”. After removing the blank rows, the file should look like the screenshot below. Replace Missing Data with “NA” There are many ways missing data can be encoded in datasets. This includes values like “blank”, “N/A”, “NA”, or leaving a cell blank. Replacing all missing values with “NA” values is done for 2 reasons: To confirm that the data is indeed missing R reads in “NA” values as missing values To check for missing values, the filter function can be used on each column and only select cells with missing values. You may need to scroll to the bottom of the filter pop up window for numerical data. Enter “NA” into the cell of the filtered column. Double click the bottom right corner of the cell to copy the “NA” down the rest of the column. There was no missing data in this dataset, so this step can be skipped. Create a Metadata Tab Metadata explains what each column represents in the dataset. Metadata is now a required component of data sharing, so it is best to initiate this process prior to data analysis. Ideally, this information is filled in by the scientist(s) who generated the data. Create a new tab (preferably as the first tab) and label it “XXXXX_METADATA” (ie., “Allostatic_METADATA”) Then relabel the original data tab as “XXXX_DATA” (ie., “Allostatic_DATA). Within the metadata tab, create three columns: the first, “Column Identifier”, contains each of the column names found in the data tab; the second, “Code”, contains the individual variable/ abbreviation for each column identifier; the third, “Description” contains additional information and definitions for abbreviations. Abbreviate and Capitalize Categorical Data Categorical data are easier to handle in programming languages when they are capitalized and abbreviated. It also helps reduce typos and potential typing mistakes within your script. For this dataset, the following variables were edited: Group “control” became “NS” for non-smoker “smoker” became “CS” for cigarette smoker Sex “f” became “F” for female “m” became “M” for male Race “AA” became “B” for Black “White” became “W” for White Excel Trick: To change cells that contain the same data simultaneously, navigate to “Edit”, click “Find”, and then “Replace”. Once the categorical data have been abbreviated, add those abbreviations to the metadata and describe what they symbolize. Alphabetize (Sort) the Data by the Categorical Variable of Interest For this dataset, we will sort by the column “Group”. This organizes the data and sets it up for the next step. Highlight all the column headers. Click on the “Sort &amp; Filter” button and click “Filter”. Click on the arrow on cell that contains the column name “Group” and click “Ascending”. Create a New Subject Number Column Analysis-specific subjects are created to give an ordinal subject number to each subject, which allows the scientist to easily identify the number of subjects. In addition, these new ordinal subject numbers will be used to create a subject identifier that combines both a subject’s group and subject number that is helpful for downstream visualization analyses. Relabel the subject number/identifier column as “Original_Subject_Number” and create an ordinal subject number column labeled “Subject_Number”. R reads in spaces between words as periods, therefore it’s common practice to replace spaces with underscores when doing data analysis in R. Avoid using dashes in column names or anywhere else in the dataset. Remove Special Symbols and Dashes Programming languages, in general, do not operate well with special symbols and dashes, particularly when included in column identifiers. For this reason, it is best to remove these while cleaning up your data, prior to importing it into R or your preferred programming software. In this case, this dataset contains dashes and Greek letters within some of the column header identifiers. Here, it is beneficial to remove these dashes (e.g., change IL-10 to IL10) and replace the Greek letters with first letter of the word in English (e.g., change TNF-\\(\\alpha\\) to TNFa). Bold all Column Names and Center all Data These data will likely be shared with collaborators, uploaded onto data deposition websites, and used as supporting information in published manuscripts. For these purposes, it is nice to format data in Excel such that it is visually appealing and easy to digest. For example, here, it is nice to bold column identifiers and center the data, as shown below: Create a Subject Identifier Column The subject identifier column labeled, “Group_Subject_No”, combines the subject number with the variable of interest (ie. Group for this dataset). This is useful for analyses to identify outliers by the subject number and the group. Insert 2 additional columns where the current “Sex” column is. To combine values from two different columns, type “=CONCAT(D1,” _ “,C1)” in the first cell in the first column inserted. Double click the right corner of the cell for the formula to be copied to last row in the dataset. Copy the entire column and paste only the values in the second column by navigating to the drop down arrow next to “Paste” and click “Paste Values”. Label the second column “Group_Subject_No” and delete the first column. Separate Subject Demographic Data from Experimental Measurements This example dataset is very small, so the demographic data (e.g., sex, race, age) was kept within the same file as the experimentally measured molecules. Though in larger datasets (e.g., genome-wide data, exposomic data, etc), it is often beneficial to separate the demographic data into one file that can be labeled according to the following format: “XXX_Subject_Info_061622” (ie. “Allostatic_Subject_Info_061622”). This step was not completed for this current data, since it had a smaller size and the downstream analyses were simple. Convert Data from Wide to Long Format A wide format contains values that DO NOT repeat the subject identifier column. For this dataset, each subject has one row containing all of its data, therefore the subject identifier occurs once in the dataset. Wide Format A long format contains values that DO repeat the subject identifier column. For this dataset, that means a new column was created entitled “Variable” containing all the mediator names and a column entitled “Value” containing all their corresponding values. In the screenshot, an additional column, “Category”, was added to help with the categorization of mediators in R analyses. Long Format The reason a long format is preferred is because it makes visualizations and statistical analyses more efficient in R. In the long format, we were able to add a column entitled “Category” to categorize the mediators into “AL Biomarker” or “Cytokine” allowing us to more easily subset the mediators in R. Read more about wide and long formats here. To convert the data from a wide to long format, follow the steps below: Pivoting Data from a Wide to Long Format To do this, a power query in Excel will be used. Note: If you are working on a Mac, you will need to have at least Excel 2016 installed to follow this tutorial, as Power Query is not avaialble for earlier versions. Add-ins are available for Windows users. See this link for more details. Start by copying all of the data, including the column titles. (Hint: Try using the keyboard shortcut mentioned above.) Click the tab at the top that says “Data”. Then click “Get Data (Power Query)” at the far left. It will ask you to choose a data source. Click “Blank table” in the bottom row. Paste the data into the table. (Hint: Use the shortcut Ctrl + “v”). At this point, your screen should look like the screenshot below. Click “Use first row as headers” and then click “Next” in the bottom right hand corner. Select all the columns with biomarker names. That should be the column “Cortisol” through the end. Click the “Transform” button in the upper left hand corner. Then click “Unpivot columns” in the middle of the pane. The final result should look like the sceenshot below with all the biomarkers now in one column entitled “Attribute” and their corresponding values in another column entitled “Value”. To save this, go back to the “Home” tab and click “Close &amp; load”. You should see something similar to the screenshot below. In the upper right with all the shaded tables (within the “Table” tab), click the arrow to the left of the green table until you see one with no shading. Then click the table with no colors. Click “Convert to Range” within the “Table” tab. This removes the power query capabilities, so that the data is a regular excel sheet. Now the “Category” column can be created to identify the types of biomarkers in the dataset. The allostatic load (AL) biomarkers denoted in the “Category” column include the variables Cortisol, CRP, Fibrinogen, Hba1c, HDL, and Noradrenaline. The rest of the variables were labeled as cytokines. Additionally, we can make this data more closely resemble the final long format screenshot by bolding the headers, centering all the data, etc. We have successfully wrangled our data and the final dataset now looks like this: Generating Summary-Level Statistics with Pivot Tables A PivotTable is a tool in Excel used to summarize numerical data. It’s called a pivot table, because it pivots or changes how the data is displayed to make statistical inferences. This can be useful for generating initial summary-level statistics to guage the distribution of data. To create a PivotTable, start by selecting all of the data. (Hint: Try using the keyboard shortcut mentioned above.) Click “Insert” tab on the upper left-hand side, click “PivotTable”, and click “OK”. The new PivotTable should be available in a new sheet as seen in the screenshot below. A PivotTable will be constructed based on the column headers that can be dragged into the PivotTable fields located on the right-hand side. For example, what if we were interested in determining if there were differences in average expression between non-smokers and cigarette smokers in each category of biomarkers? As seen below, drag the “Group” variable under the “Rows” field and drag the “Value” variable under the “Values” field. Notice that it automatically calculates the sum of the expression values for each group. To change the function to average, click the “i” icon and select “Average”. The output should mirror what’s below with non-smokers having an average expression that’s more than double that of cigarette smokers. Excel vs. R: Which Should You Use? For the most part, it’s better to perform final analyses in R (or another programming language) rather than Excel for the following reasons… R clearly shows the code (instructions), which makes editing, interpretability, and sharing easier. This makes analyses more reproducible and can save time. R has packages that makes more complex analyses possible (i.e., machine learning and heatmaps) that aren’t available in Excel. R can handle larger data sets. R can compute and process data faster. However, Excel is still a software that has many benefits for running analyses including… Excel is user-friendly and most people have experience in navigating the software at a basic level. Excel can be faster for rudimentary statistical analyses and visualizations. Depending on each scientist’s skill-level and the complexity of the analysis, Excel or R could be beneficial. Concluding Remarks In summary, this training module highlights the importance of data wrangling and how to do so in Microsoft Excel for downstream analyses. Concepts discussed include helpful Excel features like power queries and pivot tables and when to use Microsoft Excel vs. R. Additional Resources Data wrangling in Excel can be expedited with knowledge of useful features and functions to format data. Check out the resources below for additional information on Excel tricks. Data Analysis in Excel Excel Spreesheet Hacks Excel for Beginners Test Your Knowledge Try wrangling the “Module1_4_TYKInput.xlsx” to mimic the cleaned versions of the data found in “Module1_4_TYKSolution.xlsx”. This dataset includes sterol and cytokine concentration levels extracted from induced sputum samples collected after ozone exposure. After wrangling, you should end up with a sheet for subject information and a sheet for experimental data. Using the a PivotTable on the cleaned dataset, find the standard deviation of each cytokine variable stratified by the disease status. "],["downloading-and-programming-in-r.html", "2.1 Downloading and Programming in R Introduction to Training Module General Introduction and Installation of R and RStudio Introduction to R Packages Scripting Basics Code Troubleshooting Concluding Remarks", " 2.1 Downloading and Programming in R This training module was developed by Kyle Roell, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module In this training module, we will provide a brief introduction of: R R Studio Packages in R Scripting basics Code troubleshooting General Introduction and Installation of R and RStudio What is R? R is a programming language. Computer script (lines of code) can be used to increase data analysis reproducibility, transparency, and methods sharing, and is becoming increasingly incorporated into exposure science, toxicology, and environmental health research. One of the most commonly used coding languages in the field of environmental health science is the R language. Some advantages of using R include the following: Free, open-source programming language that is licensed under the Free Software Foundation’s GNU General Public License Can be run across all major platforms and operating systems, including Unix, Windows, and MacOS Publicly available packages help you carry out analyses efficiently (without you having to code for everything yourself) Large, diverse collection of packages Comprehensive documentation When code is efficiently tracked during development/execution, it promotes reproducible analyses Because of these advantages, R has emerged as an avenue for world-wide collaboration in data science. Other commonly implemented scripting languages in the field of environmental health research include Python and SAS, among others; and these training tutorials focus on R as an important introductory-level example that also houses many relevant packages and example datasets as further described throughout TAME. Downloading and Installing R To download R, first navigate to https://cran.rstudio.com/ and download the .pkg file for your operating system. Install this file according to your computer’s typical program installation steps. What is RStudio? RStudio is an Integrated Development Environment (IDE) for R, which makes it more ‘user friendly’ when developing and using R script. It is a desktop application that can be downloaded for free, online. Downloading and Installing RStudio To download RStudio: Navigate to: https://posit.co/download/rstudio-desktop/ Scroll down and select “Download RStudio” Install according to your computer’s typical program installation steps RStudio Orientation Here is a screenshot demonstrating what the RStudio desktop app looks like: The default RStudio layout has four main panes (numbered above in the blue boxes): Source Editor: allows you to open and edit script files and view data. Console: where you can type code that will execute immediately when you press enter/return. This is also where code from script files will appear when you run the code. Environment: shows you the objects in your environment. Viewer: has a number of useful tabs, including: Files: a file manager that allows you to navigate similar to Finder or File Explorer Plots: where plots you generate by executing code will appear Packages: shows you packages that are loaded (checked) and those that can be loaded (unchecked) Help: where help pages will appear for packages and functions (see below for further instructions on the help option) Under “Tools” → “Global Options,” RStudio panes can be customized to appear in different configurations or with different color themes. A number of other options can also be changed. For example, you can choose to have colors highlighted the color they appear or rainbow colored parentheses that can help you visualize nested code. Introduction to R Packages One of the major benefits to coding in the R language is access to the continually expanding resource of thousands of user-developed packages. Packages represent compilations of code and functions fitted for a specialized focus or purpose. These are often written by R users and submitted to the CRAN, or another host such as BioConductor or Github. Packages aid in improved data analyses and methods sharing. Packages have varying utilities, spanning basic organization and manipulation of data, visualizing data, and more advanced approaches to parse and analyze data, with examples included in all of the proceeding training modules. Examples of some common packages that we’ll be using throughout these training modules include the following: tidyverse: A collection of open source R packages that share an underlying design philosophy, grammar, and data structures of tidy data. For more information on the tidyverse package, see its associated CRAN webpage, primary webpage, and peer-reviewed article released in 2018. ggplot2: A system for creating graphics. Users provide the data and tell R what type of graph to use, how to map variables to aesthetics (elements of the graph), and additional stylistic elements to include in the graph. For more information on the ggplot2 package, see its associated CRAN webpage and R Documentation. More information on these packages, as well as many others, is included throughout TAME training modules. Downloading/Installing R Packages R packages often do not need to be downloaded from a website. Instead, you can install packages and load them through running script in R. Note that you only need to install packages one time, but packages must be loaded each time you start a new R session. # Install the package install.packages(“tidyverse”) # Load the package for use library(tidyverse) Many packages also exist as part of the baseline configuration of an R working environment, and do not require manual loading each time you launch R. These include the following packages: datasets graphics methods stats utils You can learn more about a function by typing one question mark before the name of the function, which will bring up documentation in the Help tab of the Viewer window. Importantly, this documentation includes a description of the different arguments that can be passed to the function and examples for how to use the function. ?install.packages ## starting httpd help server ... done You can learn more about a package by typing two question marks before the name of the package. This will bring up vingettes and help pages associated with that package. ??tidyverse Scripting Basics Data Types Before writing any script, let’s first review different data types in R. Data types are what they imply – the type of data you are handling. It is important to understand data types because functions often require a specific data type as input. R has 5 basic data types: Logical (e.g., TRUE or FALSE) Integer (e.g., 1, 2, 3) Numeric (real or decimal) Character (e.g., ”apple”) Complex (e.g., 1 + 0i) Numeric variables are often stored as “double” values (sometimes shown as &lt; dbl &gt;), or a decimal type with at least two decimal places. Character variables can also be stored as factors, which are data structures that are implemented to store categorical data in a specific order (also known as levels). Data are stored in data structures. There are many different data structures in R. Some packages even implement unique data structures. The most common data structures are: Vectors: also known as an atomic vector, can contain characters, logical values, integers, or numeric values (but all elements must be the same data type). Matrices: a vector with multiple dimensions. Elements must still be all the same data type. Data frames: similar to a matrix but can contain different data types and additional attributes such as row names (and is one of the most common data structures in environmental health research). Tibbles are a stricter type of data frame implemented in the tidyverse package. Lists: a special type of vector that acts as a container – other data structures can be stored within the list, and lists can contain other lists. Lists can contain elements that are different data structures. Figure 1: Created with BioRender.com Writing Script R code is written line by line. It may take just one line or many lines of code for one step to be executed, depending on the number of arguments to the function you are using. R code is executed (run) by selecting the line(s) of code to run and pressing return/enter (or a keyboard shortcut), or by clicking “Run” in the upper right corner of the script. A very simple example of running code is as follows: 3 + 4 ## [1] 7 We can see that when we ran our code, the answer was returned. But what if we want to store that answer? We can assign that number to a variable named x using the assignment operator &lt;-: x &lt;- 3 + 4 Then, if we run a line of code with our variable, we will get that value: x ## [1] 7 The assignment operator can also be used to assign values to any of the data structures discussed above, such as vectors and data frames, as shown here: # Creating a vector of values called my_values my_values &lt;- c(7, 3, 8, 9) # Viewing the vector my_values ## [1] 7 3 8 9 # Creating a data frame of values corresponding to colors my_df &lt;- data.frame(values = my_values, color = c(&quot;Blue&quot;, &quot;Red&quot;, &quot;Yellow&quot;, &quot;Purple&quot;)) # Viewing the data frame my_df ## values color ## 1 7 Blue ## 2 3 Red ## 3 8 Yellow ## 4 9 Purple Comments You may have noticed in the code chunks above that there were # followed by phrases describing the code. R allows for scripts to contain non-code elements, called comments, that will not be run or interpreted. Comments are useful to help make code more interpretable for others or to add reminders of what and why parts of code may have been written. To make a comment, simply use a # followed by the comment. A # only comments out a single line of code. In other words, only that line will be commented and therefore not be run, but lines directly above/below it will still be run: # This is an R comment! For more on comments, see TAME 2.0 Module 2.2 Coding Best Practices. Autofilling RStudio will autofill function names and object names as you type, which can save a lot of time. When you are typing a variable or function name, you can press tab while typing. RStudio will look for variables or functions that match the first few letters you’ve typed. If multiple matches are found, RStudio will provide you with a drop down list to select from, which may be useful when searching through newly installed packages or trying to quickly type variable names in an R script. For example, let’s say we instead named our example data frame something much longer, and we had two data frames with similar names. If we start typing in my_ and pause our typing, all of the objects that start with that name will appear as options in a list. To select which one to autofill, navigate down the list and click return/enter. my_df_with_really_long_name &lt;- data.frame(values = my_values, color = c(&quot;Blue&quot;, &quot;Red&quot;, &quot;Yellow&quot;, &quot;Purple&quot;)) my_df_with_really_long_name_2 &lt;- data.frame(values = my_values, color = c(&quot;Green&quot;, &quot;Teal&quot;, &quot;Magenta&quot;, &quot;Orange&quot;)) Finding and Setting Your Working Directory Another step that is commonly done at the very beginning of your code is setting your working direction. This tells your computer where to look for files that you want to import and where to deposit output files produced during your scripted activities. To view your current working directory, run the following: getwd() To set or change the location of your working directory, run the following: setwd(&quot;/file path to where your input files are&quot;) Note that macOS file paths use / to separate folders, whereas PC file paths use \\. You can easily find the file path to your desired working directory by navigating to “Session”, then “Set Working Directory”, and “Choose Directory”: In the popup box, navigate to the folder you want to set as your working directory and click “Open.” Look in the R console, which will now contain a line of code with setwd() containing your file path. You can copy this line of code to the top of your script for future use. Alternatively, you can navigate to the folder you want in Finder or File Explorer and right click to see the file path. Within your working directory, you can make sub-folders to keep your analyses organized. Here is an example folder hierarchy: How you set up your folder hierarchy is highly dependent on your specific analysis and coding style. However, we recommend that you: Name your script something concise, but descriptive (no acronyms) Consider using dates when appropriate Separate your analysis into logical sections so that script doesn’t get too long or hard to follow Revisit and adapt your organization as the project evolves! Archive old code so you can revisit it A Quick Note About Projects Creating projects allows you to store your progress (open script, global environment) for one project in an R Project File. This facilitates quick transitions between multiple projects. Find detailed information about how to set up projects here. Importing Files After setting the working directory, you can import and export files using various functions based on the type of file being imported or exported. Often, it is easiest to import data into R that are in a comma separated value / comma delimited file (.csv) or tab / text delimited file (.txt). Other datatypes such as SAS data files or large .csv files may require different functions to be more efficiently read in, and some of these file formats will be discussed in future modules. Files can also be imported and exported from Excel using the openxlsx package. Below, we will demonstrate how to read in .csv and .txt files: # Read in the .csv data that&#39;s located in our working directory csv.dataset &lt;- read.csv(&quot;Module2_1_Input/Module2_1_InputData1.csv&quot;) # Read in the .txt data txt.dataset &lt;- read.table(&quot;Module2_1_Input/Module2_1_InputData1.txt&quot;) These datasets now appear as saved dataframes (“csv.dataset” and “txt.dataset”) in our working environment. Viewing Data After data have been loaded into R, or created within R, you will likely want to view what these datasets look like. Datasets can be viewed in their entirety, or datasets can be subsetted to quickly look at part of the data. Here’s some example script to view just the beginnings of a dataframe using the head() function: head(csv.dataset) ## Sample Var1 Var2 Var3 ## 1 sample1 1 2 1 ## 2 sample2 2 4 4 ## 3 sample3 3 6 9 ## 4 sample4 4 8 16 ## 5 sample5 5 10 25 Here, you can see that this automatically brings up a view of the first five rows of the dataframe. Another way to view the first five rows of a dataframe is to run the following: csv.dataset[1:5,] ## Sample Var1 Var2 Var3 ## 1 sample1 1 2 1 ## 2 sample2 2 4 4 ## 3 sample3 3 6 9 ## 4 sample4 4 8 16 ## 5 sample5 5 10 25 This brings us to an important concept - indexing! Brackets are used in R to index. Within the bracket, the first argument represents the row numbers, and the second argument represents the column numbers. A colon between two numbers means to select all of the columns in between the left and right numbers. The above line of code told R to select rows 1 to 5, and, by leaving the column argument blank, all of the columns. Expanding on this, to view the first 5 rows and 2 columns, we can run the following: csv.dataset[1:5, 1:2] ## Sample Var1 ## 1 sample1 1 ## 2 sample2 2 ## 3 sample3 3 ## 4 sample4 4 ## 5 sample5 5 For another example: What if we want to only view the first and third row, and first and fourth column? We can use a vector within the index to do this: csv.dataset[c(1, 3), c(1, 4)] ## Sample Var3 ## 1 sample1 1 ## 3 sample3 9 To view the entire dataset, use the View() function: View(csv.dataset) Another way to view a dataset is to just click on the name of the data in the environment pane. The view window will pop up in the same way that it did with the View() function. Determining Data Structures and Data Types As discussed above, there are a number of different data structures and types that can be used in R. Here, we will demonstrate functions that can be used to identify data structures and types within R objects. The glimpse() function, which is part of the tidyverse package, is helpful because it allows us to see an overview of our column names and the types of data contained within those columns. # Load tidyverse package library(tidyverse) glimpse(csv.dataset) ## Rows: 5 ## Columns: 4 ## $ Sample &lt;chr&gt; &quot;sample1&quot;, &quot;sample2&quot;, &quot;sample3&quot;, &quot;sample4&quot;, &quot;sample5&quot; ## $ Var1 &lt;int&gt; 1, 2, 3, 4, 5 ## $ Var2 &lt;int&gt; 2, 4, 6, 8, 10 ## $ Var3 &lt;int&gt; 1, 4, 9, 16, 25 Here, we see that our Sample column is a character column, while the rest are integers. The class() function is also helpful for understanding objects in our global environment: # What class (data structure) is our object? class(csv.dataset) ## [1] &quot;data.frame&quot; # What class (data type) is a specific column in our data? class(csv.dataset$Sample) ## [1] &quot;character&quot; These functions are particularly helpful when introducing new functions or troubleshooting code because functions often require input data to be a specific structure or data type. Exporting Data Now that we have these datasets saved as dataframes, we can use these as examples to export data files from the R environment back into our local directory. There are many ways to export data in R. Data can be written out into a .csv file, tab delimited .txt file, or RData file, for example. There are also many functions within packages that write out specific datasets generated by that package. To write out to a .csv file: write.csv(csv.dataset, &quot;Module2_1_SameCSVFileNowOut.csv&quot;) To write out a .txt tab delimited file: write.table(txt.dataset, &quot;Module2_1_SameTXTFileNowOut.txt&quot;) R also allows objects to be saved in RData files. These files can be read into R, as well, and will load the object into the current workspace. Entire workspaces are also able to be saved in RData files, such that when you open an RData file, your script and Global Environment will be just as you saved them. Below includes example code to carry out these tasks, and though these files are not provided, they are just example code for future reference. # Read in saved single R data object r.obj = readRDS(&quot;data.rds&quot;) # Write single R object to file saveRDS(object, &quot;single_object.rds&quot;) # Read in multiple saved R objects load(&quot;multiple_data.RData&quot;) # Save multiple R objects save(object1, object2, &quot;multiple_objects.RData&quot;) # Save entire workspace save.image(&quot;entire_workspace.RData&quot;) # Load entire workspace load(&quot;entire_workspace.RData&quot;) Code Troubleshooting Learning how to code is an iterative, exploratory process. The secret to coding is to… Make sure to include “R” and the package and/or function name in your search. Don’t be afraid to try out different solutions until you find one that works for you, but also know when it is time to ask for help. For example, when you have tried solutions available on forums, but they aren’t working for you, or you know a colleague has already spent a significant amount of time developing code for this specific task. Note that when reading question/answer forums, make sure to look at how recent a post is, as packages are updated frequently, and old answers may or may not work. Some common reasons that code doesn’t work and potential solutions to these problems include: Two packages are loaded that have functions with the same name, and the default function is not the one you are intending to run. Solutions: specify the package that you want the function to be called from each time you use it (e.g., dplyr::select()) or re-assign that function at the beginning of your script (e.g., select &lt;- dplyr::select) Your data object is the wrong input type (is a data frame and needs to be a matrix, is character but needs to be numeric) Solution: double check the documentation (?functionname) for the input/variable type needed You accidentally wrote over your data frame or variable with another section of code Solution: re-run your code from the beginning, checking that your input is in the correct format There is a bug in the function/package you are trying to use (this is most common after packages are updated or after you update your version of R) Solution: post an issue on GitHub for that package (or StackOverflow if there is not a GitHub) using a reproducible example There are a number of forums that can be extremely helpful when troubleshooting your code, such as: Stack Overflow: one of the most common forums to post questions related to coding and will often be the first few links in a Google search about any code troubleshooting. It is free to make an account, which allows you to post and answer questions. Cross Validated: a forum focused on statistics, including machine learning, data analysis, data mining, and data visualization, and is best for conceptual questions related to how statistical tests are carried out, when to use specific tests, and how to interpret tests (rather than code execution questions, which are more appropriate to post on Stack Overflow). BioConductor Forum: provides a platform for specific coding and conceptual questions about BioConductor packages. GitHub: can also be used to create posts about specific issues/bugs for functions within that package. Before you post a question, make sure you have thoroughly explored answers to existing similar questions and are able to explain in your question why those haven’t worked for you. You will also need to provide a reproducible example of your error or question, meaning that you provide all information (input data, packages, code) needed such that others can reproduce your exact issues. While demonstrating a reproducible example is beyond the scope of this module, see the below links and packages for help getting started: Detailed step-by-step guides for how to make reproducible examples: How to Reprex by Ariel Muldoon What’s a reproducible example (reprex) and how do I create one? Helpful packages: reprex: part of tidyverse, useful for preparing reproducible code for posting to forums. datapasta: useful for creating code you can copy and paste that creates a new data frame as a subset of your original data. Concluding Remarks Together, this training module provides introductory level information on installing and loading packages in R, scripting basics, importing and exporting data, and code troubleshooting. Additional Resources Coursera Stack Overflow How to Learn R R for Data Science Test Your Knowledge Install R and RStudio on your computer. Launch RStudio and explore installing packages (e.g., tidyverse) and understanding data types using the built-in datasets in R. Make a vector of the letters A-E. Make a data frame of the letters A-E in one column and their corresponding number in the alphabet order in the second column (e.g., A corresponds with 1). "],["coding-best-practices.html", "2.2 Coding “Best” Practices Introduction to Training Module Scripting File Types Script Headers and Annotation Coding Style Script Organization Concluding Remarks", " 2.2 Coding “Best” Practices This training module was developed by Kyle Roell, Alexis Payton, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module In this training module, we will be going over coding “best” practices. The reason we put “best” in quotes is because these practices are what we currently consider best or better, though everyone has different coding styles, annotation styles, etc that also change over time. Here, we hope to give you a sense of what we do when coding, why we do it, and why we think it is important. We will also be pointing out other guides to style, annotations, and best practices that we suggest implementing into your own coding. Some of the questions we hope to answer in this module are: What type of scripting file should I use? What should I name my script? What should I put at the top of every script and why is it important? How should I annotate my code? Why are annotations important? How do I implement these coding practices into my own code? Where can I find other resources to help with coding best practices? In the following sections, we will be addressing these questions. Keep in mind that the advice and suggestions in this section are just that: advice and suggestions. So please take them into consideration and integrate them into your own coding style as appropriate. Scripting File Types Two of the most common scripting file types applicable to the R language are .R (normal R files) and .Rmd (R Markdown). Normal R files appear as plain text and can be used for running any normal R code. R Markdown files are used for more intensive documentation of code and allow for a combination of code, non-code text explaining the code, and viewing of code output, tables, and figures that are rendered together into an output file (typically .html, although other formats such as .pdf are also offered). For example, TAME is coded using R Markdown, which allows us to include blocks of non-code text, hyperlinks, annotated code, schematics, and output figures all in one place. We highly encourage the use of R Markdown as the default scripting file type for R-based projects because it produces a polished final document that is easy for others to follow, whereas .R files are more appropriate for short, one-off analyses and writing in-depth functions and packages. However, code executed in normal .R files and R Markdown will produce the same results, and ultimately, which file type to use is personal preference. See below for screenshots that demonstrate some of the stylistic differences between .R, .Rmd, and .Rmd knitted to HTML format: If you are interested in learning more about the basic features of R Markdown and how to use them, see the following resources: RStudio introduction to R Markdown R Markdown Cheat Sheet Bookdown R Markdown guide Including external images in R Markdown with knitr Interactive plots with plotly Interactive data tables with DT Naming the Script File The first thing we need to talk about, which is sometimes overlooked in the discussion of coding practices, is script file naming conventions and high level descriptive headers within a script. It is important to remember to name your code something concise, but descriptive. You want to be able to easily recognize what the script is for and does without a cumbersome, lengthy title. Some tips for naming conventions: Be concise, but descriptive Use dates when appropriate Avoid special characters Use full words if possible, avoiding non-standard acronyms Keep in mind that each script should have a clear purpose within a given project. And, it is sometimes necessary, and often common, to have multiple scripts within one project that all pertain to different parts of the analysis. For example, it may be appropriate to have one script for data cleaning and pre-processing and another script for analyzing data. When scripting an analysis with multiple sub-analyses, some prefer to keep code for each sub-analysis separate (e.g., one file for an ANOVA and one file for a k-means analysis on the same data input), while others prefer to have longer code files with more subsections. Whichever method you choose, we recommend maintaining clear documentation that indicates locations for input and output files for each sub-analysis (e.g., whether global environment objects or output files from a previous script are needed to run the current script). Script Headers and Annotation Script Header Once your script is created and named, it is generally recommended to include a header at the top of the script. The script header can be used for describing: Title of Script - This can be a longer or more readable name than script file name. Author(s) - Who wrote the script? Date - When was the script developed? Description - Provides a more detailed description of the purpose of the script and any notes or special considerations for this particular script. In R, it is common to include multiple #, the comment operator, or a # followed by another special character, to start and end a block of coding annotation or the script header. An example of this in an .R file is shown below: ######################################################################## ######################################################################## ### Script Longer Title ### ### Description of what this script does! ### Also can include special notes or anything else here. ### ### Created by: Kyle Roell and Julia Rager ### Last updated: 01 May 2023 ######################################################################## ######################################################################## This block of comment operators is common in .R but not .Rmd files because .Rmd files have their own specific type of header, known as the YAML, which contains the title, author, date, and formatting outputs for the .Rmd file: We will now review how annotations within the script itself can make a huge difference in understanding the code within. Annotations Before we review coding style considerations, it is important to address code annotating. So, what are annotations and why are they important? Annotations are notes embedded within your code as comments that will not be run. The beauty of annotating your code is that not only others, but future you, will be able to read through and better understand what a particular piece of code does. We suggest annotating your code while you write it and incorporate a lot of description. While not every single line needs an annotation, or a very detailed one, it is helpful to provide comments and annotation as much as you can while maintaining feasibility. General annotation style In general, annotations will be short sentences that describe what your code does or why you are executing that specific code. This can be helpful when you are defining a covariate a specific way, performing a specific analytical technique, or just generally explaining why you are doing what you’re doing. # Performing logistic regression to assess association between xyz and abc # Regression confounders: V1, V2, V3 ... xyz.regression.output = glm(xyz ~ abc + V1 + V2 + V3, family=binomial(), data=example.data) Mid-script headings Another common approach to annotations is to use mid-script type headings to separate out the script into various sections. For example, you might want to create distinct sections for “Loading Packages, Data, and Setup”, “Covariate Definition”, “Correlation Analysis”, “Regression Analysis”, etc. This can help you, and others, reading your script, to navigate the script more easily. It also can be more visually pleasing to see the script split up into multiple sections as opposed to one giant chunk of code interspersed with comments. Similar to above, the following example is specific to .R files. For .Rmd files, sub headers can be created by increasing the number of # before the header. ########################################################################### ########################################################################### ### ### Regression Analyses ### ### You can even add some descriptions or notes here about this section! ### ########################################################################### # Performing logistic regression to assess association between xyz and abc # Regression confounders: V1, V2, V3 ... xyz.regression.output = glm(xyz ~ abc + V1 + V2 + V3, family=binomial(), data=example.data) General tips for annotations: Make comments that are useful and meaningful You don’t need to comment every single line In general, you probably won’t over-comment your script, so more is generally better That being said, don’t write super long paragraphs every few lines Split up your script into various sections using mid-script headings when appropriate Quick, short comments and annotations While it is important to provide descriptive annotations, not every one needs to be a sentence or longer. As stated previously, it is not necessary to comment every single line. Here is an example of very brief commenting: # Loading necessary packages library(ggplot2) # Plotting package In the example above, we can see that these short comments clearly convey what the script does – load the necessary package and indicate what the package is needed for. Short, one line annotations can also be placed after lines to clarify that specific line or within the larger mid-script headings to split up these larger sections of code. Coding Style Coding style is often a contentious topic! There are MANY styles of coding, and no two coders have the same exact style, even if they are following the same reference. Here, we will provide some guides to coding style and go over some of the basic, general tips for making your code readable and efficient. Here is an example showing how you can use spacing to align variable assignment: # Example of using spacing for alignment of variable assignment Longer_variable_name_x = 1 Short_name_y = 2 Note that guides will suggest you use &lt;- as the assignment operator. However, for most situations, &lt;- and = will do the same thing. For spacing around certain symbols and operators: Include a space after if, before parenthesis Include a space on either side of symbols such as &lt; The first (opening) curly brace should not be on its own line, but the second (closing) should # Example of poor style if(Longer_variable_name_x &lt;Short_name_y) {Short_name_y = 0} # Example of better style if (Longer_variable_name_x &lt; Short_name_y) { Short_name_y = 0 } Summary of general tips for coding style: Variable names Make them intuitive, short, but descriptive Use the same convention throughout (ex: separating words with . or _) Data names Use _datatype (e.g., cytokines_df for a dataframe data type) following a short, concise name for your data so that readers know what type of input is being used AND/OR Clearly define important data frames or other input in your code comments (e.g., “cytokines” refers to a data frame containing all of the cytokine data from this experiment) Separate long lines onto two or more lines (typically for loops or functions) Use &lt;- for assignment operator Using spacing appropriately for readability Alignment of lines After certain keywords when appropriate Be consistent throughout Example: if you use Tidyverse conventions, continue to use it throughout your script Try to make your code as readable as possible Common style guides: Google’s Style Guide Tidyverse Style Guide R Blogger’s Best Coding Practices RMarkdown for Scientists Script Organization Lastly, it is important to note that organizing your script efficiently can help with readability as well. In general, as stated before, the beginning of your script should contain some sort of header lines to describe the script. The basic ordering we suggest for most scripts is: Header section Loading libraries and data Function definitions (if any user defined functions exist) Data and variables manipulation Analyses While following this exact organization isn’t absolute, using this structure or something similar can greatly improve the clarity of your analyses and make them easier for others to follow. Concluding Remarks In this module, we demonstrate basic coding style and best practices. Please reference additional style guides (above) and create your own style from best practices that work for you. Aim to make your code understandable and readable, and consider what a future reader, including yourself, will need to understand why you wrote your code the way you did and how to apply your code to new analyses. Test Your Knowledge Using the input file provided (“Module2_2_TYKInput.R”): Convert the script and annotations into R Markdown format. Improve the organization, comments, and scripting to follow the coding best practices described in this module. List the changes you made at the bottom of the new R Markdown file. Notes on the starting code: This starting code uses dummy data to demonstrate how to make a graph in R that includes bars representing the mean, with standard deviation error bars overlaid. You don’t need to understand every step in the code to be able to improve the existing coding style! You can run each step of the code if needed to understand better what it does. "],["data-manipulation-and-reshaping.html", "2.3 Data Manipulation and Reshaping Introduction to Training Module Data Manipulation Using Base R Introduction to Tidyverse Concluding Remarks", " 2.3 Data Manipulation and Reshaping This training module was developed by Kyle Roell, Alexis Payton, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Data within the fields of exposure science, toxicology, and public health are very rarely prepared and ready for downstream statistical analyses and visualization code. The beginning of almost any scripted analysis includes important formatting steps that make the data easier to read and work with. This can be done in several ways, including: Base R operations and functions, or A collection of packages (and philosophy) known as The Tidyverse. In this training tutorial we will review some of the most common ways you can organize and manipulate data, including: Merging data Filtering and subsetting data Pivoting data wider and longer (also known as casting and melting) These approaches will first be demonstrated using the functions available in base R. Then, the exact same approaches will be demonstrated using the functions and syntax that are part of the Tidyverse package. We will demonstrate these data manipulation and organization methods using an environmentally relevant example data set from a human cohort. This dataset was generated by creating data distributions randomly pulled from our previously published cohorts, resulting in a unique data set for these training purposes. The dataset contains environmental exposure metrics from metal levels obtained using sources of drinking water and human urine samples and associated demographic data. Training Module’s Environmental Health Question This training module was specifically developed to answer the following environmental health question using data manipulation and reshaping approaches: What is the average urinary chromium concentration across different maternal education levels? We’ll use base R and Tidydverse to answer this question, but let’s start with Base R. Workspace Preparation and Data Import Set your working directory In preparation, first let’s set our working directory to the folder path that contains our input files: setwd(&quot;/file path to where your input files are&quot;) Note that macOS file paths use / as folder separators, and PC file paths use \\. Importing example datasets Next, let’s read in our example data sets: demographic_data &lt;- read.csv(&quot;Module2_3_Input/Module2_3_InputData1.csv&quot;) chemical_data &lt;- read.csv(&quot;Module2_3_Input/Module2_3_InputData2.csv&quot;) Viewing example datasets Let’s see what these datasets look like: dim(demographic_data) ## [1] 200 6 dim(chemical_data) ## [1] 200 7 The demographic data set includes 200 rows x 7 columns, while the chemical measurement data set includes 200 rows x 7 columns. We can preview the demographic data frame by using the head() function, which displays all the columns and the first 6 rows of a data frame: head(demographic_data) ## ID BMI MAge MEdu BW GA ## 1 1 27.7 22.99928 3 3180.058 34 ## 2 2 26.8 30.05142 3 3210.823 43 ## 3 3 33.2 28.04660 3 3311.551 40 ## 4 4 30.1 34.81796 3 3266.844 32 ## 5 5 37.4 42.68440 3 3664.088 35 ## 6 6 33.3 24.94960 3 3328.988 40 These demographic data are organized according to subject ID (first column) followed by the following subject information: ID: subject number BMI: body mass index MAge: maternal age in years MEdu: maternal education level; 1 = “less than high school”, 2 = “high school or some college”, 3 = “college or greater” BW: body weight in grams GA: gestational age in weeks We can also preview the chemical dataframe: head(chemical_data) ## ID DWAs DWCd DWCr UAs UCd UCr ## 1 1 6.426464 1.292941 51.67987 10.192695 0.7537104 42.60187 ## 2 2 7.832384 1.798535 50.10409 11.815088 0.9789506 41.30757 ## 3 3 7.516569 1.288461 48.74001 10.079057 0.1903262 36.47716 ## 4 4 5.906656 2.075259 50.92745 8.719123 0.9364825 42.47987 ## 5 5 7.181873 2.762643 55.16882 9.436559 1.4977829 47.78528 ## 6 6 9.723429 3.054057 51.14812 11.589403 1.6645837 38.26386 These chemical data are organized according to subject ID (first column), followed by measures of: DWAs: drinking water arsenic levels in µg/L DWCd: drinking water cadmium levels in µg/L DWCr: drinking water chromium levels in µg/L UAs: urinary arsenic levels in µg/L UCd: urinary cadmium levels in µg/L UCr: urinary chromium levels in µg/L Data Manipulation Using Base R Merging Data Using Base R Syntax Merging datasets represents the joining together of two or more datasets, using a common identifier (generally some sort of ID) to connect the rows. This is useful if you have multiple datasets describing different aspects of the study, different variables, or different measures across the same samples. Samples could correspond to the same study participants, animals, cell culture samples, environmental media samples, etc, depending on the study design. In the current example, we will be joining human demographic data and environmental metals exposure data collected from drinking water and human urine samples. Let’s start by merging the example demographic data with the chemical measurement data using the base R function merge(). To learn more about this function, you can type ?merge, which brings up helpful information in the R console. To merge these datasets with the merge function, use the following code. The by = argument specifies the column used to match the rows of data. full.data &lt;- merge(demographic_data, chemical_data, by = &quot;ID&quot;) dim(full.data) ## [1] 200 12 This merged dataframe contains 200 rows x 12 columns. Viewing this merged dataframe, we can see that the merge() function retained the first column in each original dataframe (ID), though did not replicate it since it was used as the identifier for merging. All other columns include their original data, just merged together by the IDs in the first column. head(full.data) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 These datasets were actually quite easy to merge, since they had the same exact column identifier and number of rows. You can edit your script to include more specifics in instances when these may differ across datasets that you would like to merge. This option allows you to edit the name of the column that is used in each dataframe. Here, these are still the same “ID”, but you can see that adding the by.x and by.y arguments allows you to specify instances when different column names are used in the two datasets. full.data &lt;- merge(demographic_data, chemical_data, by.x = &quot;ID&quot;, by.y = &quot;ID&quot;) # Viewing data head(full.data) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 Note that after merging datasets, it is always helpful to check that the merging was done properly before proceeding with your data analysis. Helpful checks could include viewing the merged dataset, checking the numbers of rows and columns to make sure chunks of data are not missing, and searching for values (or strings) that exist in one dataset but not the other, among other mechanisms of QA/QC. Filtering and Subsetting Data Using Base R Syntax Filtering and subsetting data are useful tools when you need to focus on specific parts of your dataset for downstream analyses. These could represent, for example, specific samples or participants that meet certain criteria that you are interested in evaluating. It is also useful for removing unneeded variables or samples from dataframes as you are working through your script. Note that in the examples that follow, we will create new dataframes that are distinguished from our original dataframe by adding sequential numbers to the end of the dataframe name (e.g., subset.data1, subset.data2, subset.data3). This style of dataframe naming is useful for the simple examples we are demonstrating, but in a full scripted analysis, we encourage the use of more descriptive dataframe names. For example, if you are subsetting your data to include only the first 100 rows, you could name that dataframe “data.first100.” For this example, let’s first define a vector of columns that we want to keep in our analysis, then subset the data by keeping only the columns specified in our vector: # Defining a vector of columns to keep in the analysis subset.columns &lt;- c(&quot;BMI&quot;, &quot;MAge&quot;, &quot;MEdu&quot;) # Subsetting the data by selecting the columns represented in the defined &#39;subset.columns&#39; vector subset.data1 &lt;- full.data[,subset.columns] # Viewing the top of this subsetted dataframe head(subset.data1) ## BMI MAge MEdu ## 1 27.7 22.99928 3 ## 2 26.8 30.05142 3 ## 3 33.2 28.04660 3 ## 4 30.1 34.81796 3 ## 5 37.4 42.68440 3 ## 6 33.3 24.94960 3 We can also easily subset data based on row numbers. For example, to keep only the first 100 rows: subset.data2 &lt;- full.data[1:100,] # Viewing the dimensions of this new dataframe dim(subset.data2) ## [1] 100 12 To remove the first 100 rows, we use the same code as above, but include a - sign before our vector to indicate that these rows should be removed: subset.data3 &lt;- full.data[-c(1:100),] # Viewing the dimensions of this new dataframe dim(subset.data3) ## [1] 100 12 Conditional statements are also written to filter and subset data. A conditional statement is written to execute one block of code if the statement is true and a different block of code if the statement is false. A conditional statement requires a Boolean or true/false statement that will be either TRUE or FALSE. A couple of the more commonly used functions used to create conditional statements include… if(){} or an if statement means “execute R code when the condition is met”. if(){} else{} or an if/else statement means “execute R code when condition 1 is met, if not execute R code for condition 2”. ifelse() is a function that executes the same logic as an if/else statement. The first argument specifies a condition to be met. If that condition is met, R code in the second argument is executed, and if that condition is not met, R code in the third argument is executed. There are six comparison operators that are used to created these Boolean values: == means “equals”. != means “not equal”. &lt; means “less than”. &gt; means “greater than”. &lt;= means “less than or equal to”. &gt;= mean “greater than or equal to”. There are also three logical operators that are used to create these Boolean values: &amp; means “and”. | means “or”. ! means “not”. We can filter data based on conditions using the subset() function. For example, the following code filters for subjects whose BMI is greater than 25 and who have a college education: subset.data4 &lt;- subset(full.data, BMI &gt; 25 &amp; MEdu == 3) Additionally, we can subset and select specific columns we would like to keep, using the select argument within the subset() function: # Filtering for subjects whose BMI is less than 22 or greater than 27 # Also selecting the BMI, maternal age, and maternal education columns subset.data5 &lt;- subset(full.data, BMI &lt; 22 | BMI &gt; 27, select = subset.columns) For more information on the subset() function, see its associated documentation. Melting and Casting Data using Base R Syntax Melting and casting refers to the conversion of data to “long” or “wide” form as discussed previously in TAME 2.0 Module 1.4 Data Wrangling in Excel. You will often see data within the environmental health field in wide format, though long format is necessary for some procedures, such as plotting with ggplot2 and performing certain analyses. Here, we’ll illustrate some example script to melt and cast data using the reshape2 package. Let’s first install and load the reshape2 package: if (!requireNamespace(&quot;reshape2&quot;)) install.packages(&quot;reshape2&quot;); library(reshape2) ## ## Attaching package: &#39;reshape2&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## smiths Using the fully merged dataframe, let’s remind ourselves what these data look like in the current dataframe format: head(full.data) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 These data are represented by single subject identifiers listed as unique IDs per row, with associated environmental measures and demographic data organized across the columns. Thus, this dataframe is currently in wide (also known as casted) format. Let’s convert this dataframe to long (also known as melted) format. Here, will will specify that we want a row for each unique sample ID + variable measure pair by using id = \"ID\": full.melted &lt;- melt(full.data, id = &quot;ID&quot;) # Viewing this new dataframe head(full.melted) ## ID variable value ## 1 1 BMI 27.7 ## 2 2 BMI 26.8 ## 3 3 BMI 33.2 ## 4 4 BMI 30.1 ## 5 5 BMI 37.4 ## 6 6 BMI 33.3 You can see here that each measure that was originally contained as a unique column has been reoriented, such that the original column header is now listed throughout the second column labeled variable. Then, the third column contains the value of this variable. Let’s see an example view of the middle of this new dataframe: full.melted[1100:1110,1:3] ## ID variable value ## 1100 100 DWAs 7.928885 ## 1101 101 DWAs 8.677403 ## 1102 102 DWAs 8.115183 ## 1103 103 DWAs 7.134189 ## 1104 104 DWAs 8.816142 ## 1105 105 DWAs 7.487227 ## 1106 106 DWAs 7.541973 ## 1107 107 DWAs 6.313516 ## 1108 108 DWAs 6.654474 ## 1109 109 DWAs 7.564429 ## 1110 110 DWAs 7.357122 Here, we can see a different variable (DWAs) now being listed. This continues throughout the entire dataframe, which has the following dimensions: dim(full.melted) ## [1] 2200 3 Let’s now re-cast this dataframe back into wide format using the dcast() function. Here, we are telling the dcast() function to give us a sample (ID) for every variable in the column labeled variable. The column names from the variable column and corresponding values from the value column are then used to fill in the dataset: full.cast &lt;- dcast(full.melted, ID ~ variable) head(full.cast) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 Here, we can see that this dataframe is back in its original casted (or wide) format. Now that we’re familiar with some base R functions to reshape our data, let’s answer our original question: What is the average urinary chromium concentration for each maternal education level? Although it is not necessary to calculate the average, we could first subset our data frame to only include the two columns we are interested in (MEdu and UCr): subset.data6 &lt;- full.data[,c(&quot;MEdu&quot;, &quot;UCr&quot;)] head(subset.data6) ## MEdu UCr ## 1 3 42.60187 ## 2 3 41.30757 ## 3 3 36.47716 ## 4 3 42.47987 ## 5 3 47.78528 ## 6 3 38.26386 Next, we will make a new data frame for each maternal education level: # Creating new data frames based on maternal education category data.matedu.1 &lt;- subset(subset.data6, MEdu == 1) data.matedu.2 &lt;- subset(subset.data6, MEdu == 2) data.matedu.3 &lt;- subset(subset.data6, MEdu == 3) # Previewing the first data frame to make sure our function is working as specified head(data.matedu.1) ## MEdu UCr ## 14 1 38.59349 ## 18 1 47.77878 ## 37 1 35.33980 ## 63 1 34.72255 ## 66 1 34.13982 ## 76 1 31.38145 Last, we can calculate the average urinary chromium concentration using each of our data frames: mean(data.matedu.1$UCr) ## [1] 39.88055 mean(data.matedu.2$UCr) ## [1] 40.61807 mean(data.matedu.3$UCr) ## [1] 40.41556 With this, we can answer our Environmental Health Question: What is the average urinary chromium concentration across different maternal education levels? Answer: The average urinary Chromium concentrations are 39.9 µg/L for participants with less than high school education, 40.6 µg/L for participants with high school or some college education, and 40.4 µg/L for participants with college education or greater. Introduction to Tidyverse Tidyverse is a collection of packages that are commonly used to more efficiently organize and manipulate datasets in R. This collection of packages has its own specific type of syntax and formatting that differ slightly from base R functions. There are eight core tidyverse packages: For data visualization and exploration: ggplot2 For data wrangling and transformation: dplyr tidyr stringr forcats For data import and management: tibble readr For functional programming: purr Here, we will carry out all the of the same data organization exercises demonstrated above using packages that are part of The Tidyverse, specifically using functions that are part of the dplyr and tidyr packages. Downloading and Loading the Tidyverse Package If you don’t have tidyverse already installed, you will need to install it using: if(!require(tidyverse)) install.packages(&quot;tidyverse&quot;) And then load the tidyverse package using: library(tidyverse) Note that by loading the tidyverse package, you are also loading all of the packages included within The Tidyverse and do not need to separately load these packages. Merging Data Using Tidyverse Syntax To merge the same example dataframes using tidyverse, you can run the following script: full.data.tidy &lt;- inner_join(demographic_data, chemical_data, by = &quot;ID&quot;) head(full.data.tidy) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 Note that you can still merge dataframes that have different ID column names with the argument by = c(\"ID.x\", \"ID.y\"). tidyverse also has other join, functions, shown in the graphic below (source): inner_join keeps only rows that have matching ID variables in both datasets full_join keeps the rows in both datasets left_join matches rows based on the ID variables in the first dataset (and omits any rows from the second dataset that do not have matching ID variables in the first dataset) right_join matches rows based on ID variables in the second dataset (and omits any rows from the first dataset that do not have matching ID variables in the second dataset) anti_join(x,y) keeps the rows that are unique to the first dataset anti_join(y,x) keeps the rows that are unique to the second dataset The Pipe Operator One of the most important elements of Tidyverse syntax is use of the pipe operator (%&gt;%). The pipe operator can be used to chain multiple functions together. It takes the object (typically a dataframe) to the left of the pipe operator and passes it to the function to the right of the pipe operator. Multiple pipes can be used in chain to execute multiple data cleaning steps without the need for intermediate dataframes. The pipe operator can be used to pass data to functions within all of the Tidyverse universe packages, not just the functions demonstrated here. Below, we can see the same code executed above, but this time with the pipe operator. The demographic_data dataframe is passed to inner_join() as the first argument to that function, with the following arguments remaining the same. full.data.tidy2 &lt;- demographic_data %&gt;% inner_join(chemical_data, by = &quot;ID&quot;) head(full.data.tidy2) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 Because the pipe operator is often used in a chain, it is best practice is to start a new line after each pipe operator, with the new lines of code indented. This makes code with multiple piped steps easier to follow. However, if just one function is being executed, the pipe operator can be used on the same line as the input and function or omitted altogether (as shown in the previous two code chunks). Here is an example of placing the function to the right of the pipe operator on a new line, with placeholder functions shown as additional steps: full.data.tidy3 &lt;- demographic_data %&gt;% inner_join(chemical_data, by = &quot;ID&quot;) %&gt;% additional_function_1() %&gt;% additional_function_2() Filtering and Subsetting Data Using Tidyverse Syntax Column-wise functions The select() function is used to subset columns in Tidyverse. Here, we can use our previously defined vector subset.columns in the select() function to keep only the columns in our subset.columns vector. The all_of() function tells the select() to keep all of the columns that match elements of the subset.columns vector. subset.tidy1 &lt;- full.data.tidy %&gt;% select(all_of(subset.columns)) head(subset.tidy1) ## BMI MAge MEdu ## 1 27.7 22.99928 3 ## 2 26.8 30.05142 3 ## 3 33.2 28.04660 3 ## 4 30.1 34.81796 3 ## 5 37.4 42.68440 3 ## 6 33.3 24.94960 3 There are many different ways that select() can be used. See below for some examples using dummy variable names: # Select specific ranges in the dataframe data &lt;- data %&gt;% select(start_column_1:end_column_1) data &lt;- data %&gt;% select(c(start_column_1:end_column_1, start_column_2:end_column_2)) # Select columns that match the elements in a character vector an an additional range of columns data &lt;- data %&gt;% select(c(all_of(character_vector), start_column_1:end_column_1)) To select columns that have names that contain specific strings, you can use functions such as starts_with(), ends_with(), and contains(). These functions allow you to ignore the case of the strings with ignore.case = TRUE. These arguments can be combined with specific column names and other selection ranges. data &lt;- data %&gt;% select(starts_with(&quot;starting_string&quot;)) data &lt;- data %&gt;% select(other_column_to_keep, starts_with(&quot;starting_string&quot;)) To remove columns using tidyverse, you can use similar code, but include a - sign before the argument defining the columns. # Removing columns subset.tidy2 &lt;- full.data.tidy %&gt;% select(-all_of(subset.columns)) # Viewing this new dataframe head(subset.tidy2) ## ID BW GA DWAs DWCd DWCr UAs UCd UCr ## 1 1 3180.058 34 6.426464 1.292941 51.67987 10.192695 0.7537104 42.60187 ## 2 2 3210.823 43 7.832384 1.798535 50.10409 11.815088 0.9789506 41.30757 ## 3 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 0.1903262 36.47716 ## 4 4 3266.844 32 5.906656 2.075259 50.92745 8.719123 0.9364825 42.47987 ## 5 5 3664.088 35 7.181873 2.762643 55.16882 9.436559 1.4977829 47.78528 ## 6 6 3328.988 40 9.723429 3.054057 51.14812 11.589403 1.6645837 38.26386 Row-wise functions The slice() function can be used to keep or remove a certain number of rows based on their position within the dataframe. For example, we can retain only the first 100 rows using the following code: subset.tidy3 &lt;- full.data.tidy %&gt;% slice(1:100) dim(subset.tidy3) ## [1] 100 12 Or, we can remove the first 100 rows: subset.tidy4 &lt;- full.data.tidy %&gt;% slice(-c(1:100)) dim(subset.tidy4) ## [1] 100 12 The related functions slice_min() and slice_max() can be used to select rows with the smallest or largest values of a variable. The filter() function can be used to keep or remove specific rows based on conditional statements. For example, we can keep only rows where BMI is greater than 25 and age is greater than 31: subset.tidy5 &lt;- full.data.tidy %&gt;% filter(BMI &gt; 25 &amp; MAge &gt; 31) dim(subset.tidy5) ## [1] 49 12 Combining column and row-wise functions Now, we can see how Tidyverse makes it easy to chain together multiple data manipulation steps. Here, we first filter rows based on values for BMI and age, then we select our columns of interest: subset.tidy6 &lt;- full.data.tidy %&gt;% filter(BMI &gt; 25 &amp; MAge &gt; 31) %&gt;% select(BMI, MAge, MEdu) head(subset.tidy6) ## BMI MAge MEdu ## 1 30.1 34.81796 3 ## 2 37.4 42.68440 3 ## 3 36.9 33.58589 3 ## 4 33.7 33.82961 3 ## 5 25.7 37.08028 3 ## 6 28.4 47.85761 3 Melting and Casting Data Using Tidyverse Syntax To melt and cast data in Tidyverse, you can use the pivot functions (i.e., pivot_longer() or pivot_wider()). The first argument in the pivot_longer() function specifies which columns should be pivoted. This can be specified with either positive or negative selection - i.e., naming columns to pivot with a vector or range or naming columns not to pivot with a - sign. Here, we are telling the function to pivot all of the columns except the ID column, which we need to keep to be able to trace back which values came from which subject. The names_to = argument allows you to set what you want to name the column that stores the variable names (the column names in wide format). The values_to = argument allows you to set what you want to name the column that stores the values. We almost always call these columns “var” and “value”, respectively, but you can name them anything that makes sense for your dataset. full.pivotlong &lt;- full.data.tidy %&gt;% pivot_longer(-ID, names_to = &quot;var&quot;, values_to = &quot;value&quot;) head(full.pivotlong, 15) ## # A tibble: 15 × 3 ## ID var value ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 BMI 27.7 ## 2 1 MAge 23.0 ## 3 1 MEdu 3 ## 4 1 BW 3180. ## 5 1 GA 34 ## 6 1 DWAs 6.43 ## 7 1 DWCd 1.29 ## 8 1 DWCr 51.7 ## 9 1 UAs 10.2 ## 10 1 UCd 0.754 ## 11 1 UCr 42.6 ## 12 2 BMI 26.8 ## 13 2 MAge 30.1 ## 14 2 MEdu 3 ## 15 2 BW 3211. To pivot our data back to wide format, we can use pivot_wider(), which will pull the column names from the column specified in the names_from = argument and the corresponding values from the column specified in the values_from = argument. full.pivotwide &lt;- full.pivotlong %&gt;% pivot_wider(names_from = &quot;var&quot;, values_from = &quot;value&quot;) head(full.pivotwide) ## # A tibble: 6 × 12 ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs UCd UCr ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 27.7 23.0 3 3180. 34 6.43 1.29 51.7 10.2 0.754 42.6 ## 2 2 26.8 30.1 3 3211. 43 7.83 1.80 50.1 11.8 0.979 41.3 ## 3 3 33.2 28.0 3 3312. 40 7.52 1.29 48.7 10.1 0.190 36.5 ## 4 4 30.1 34.8 3 3267. 32 5.91 2.08 50.9 8.72 0.936 42.5 ## 5 5 37.4 42.7 3 3664. 35 7.18 2.76 55.2 9.44 1.50 47.8 ## 6 6 33.3 24.9 3 3329. 40 9.72 3.05 51.1 11.6 1.66 38.3 Now that we’re familiar with some tidyverse functions to reshape our data, let’s answer our original question: What is the average urinary Chromium concentration for each maternal education level? We can use the group_by() function to group our dataset by education class, then the summarize function to calculate the mean of our variable of interest within each class. Note how much shorter and more efficient this code is than the code we used to calculate the same values using base R! full.data %&gt;% group_by(MEdu) %&gt;% summarize(Avg_UCr = mean(UCr)) ## # A tibble: 3 × 2 ## MEdu Avg_UCr ## &lt;int&gt; &lt;dbl&gt; ## 1 1 39.9 ## 2 2 40.6 ## 3 3 40.4 For more detailed and advanced examples of pivoting in Tidyverse, see the Tidyverse Pivoting Vignette. Concluding Remarks This training module provides an introductory level overview of data organization and manipulation basics in base R and Tidyverse, including merging, filtering, subsetting, melting, and casting, and demonstrates these methods with an environmentally relevant dataset. These methods are used regularly in scripted analyses and are important preparation steps for almost all downstream analyses and visualizations. Test Your Knowledge What subjects, arranged from highest to lowest drinking water cadmium levels, had babies at at least 35 weeks and had urinary cadmium levels of at least 1.5 µg/L? Hint: Try using the arrange() function from the tidyverse package. "],["improving-coding-efficiencies.html", "2.4 Improving Coding Efficiencies Introduction to Training Module Loops Functions List operations Concluding Remarks Additional Resources", " 2.4 Improving Coding Efficiencies This training module was developed by Elise Hickman, Alexis Payton, Kyle Roell, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module In this module, we’ll explore how to improve coding efficiency. Coding efficiency involves performing a task in as few lines as possible and can… Shorten code by eliminating redundancies Reduce the number of typos Help other coders understand script better Specific approaches that we will discuss in this module include loops, functions, and list operations, which can all be used to make code more succinct. A loop is employed when we want to perform a repetitive task, while a function contains a block of code organized together to perform one specific task. List operations, in which the same function is applied to a list of dataframes, can also be used to code more efficiently. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Are there statistically significant differences in drinking water arsenic, cadmium, and chromium between normal weight (BMI &lt; 25) and overweight (BMI \\(\\geq\\) 25) subjects? Are there statistically significant differences in drinking water arsenic, cadmium, and chromium between underweight (BMI &lt; 18.5) and non-underweight (BMI \\(\\geq\\) 18.5) subjects? Are there statistically significant difference in drinking water arsenic, cadmium, and chromium between non-obese (BMI &lt; 29.9) and obese (BMI \\(\\geq\\) 29.9) subjects? We will demonstrate how this analysis can be approached using for loops, functions, or list operations. We will introduce the syntax and structure of each approach first, followed by application of the approach to our data. First, let’s prepare the workspace and familiarize ourselves with the dataset we are going to use. Data Import and Workspace Preparation Installing required packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you. We will be using the tidyverse package for data manipulation steps and the rstatix package for statistical tests, as it provides pipe friendly adaptations of the base R statistical tests and returns results in a dataframe rather than a list format, making results easier to access. This brings up an important aspect of coding efficiency - sometimes, there is already a package that has been designed with functions to help you execute your desired analysis in an efficient way, so you don’t need to write custom functions yourself! So, don’t forget to explore packages relevant to your analysis before spending a lot of time developing custom solutions (although, sometimes this is necessary). if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;) if (!requireNamespace(&quot;rstatix&quot;)) install.packages(&quot;rstatix&quot;) Loading required packages library(tidyverse) library(rstatix) Setting your working directory setwd(&quot;/file path to where your input files are&quot;) Importing example dataset The first example dataset contains subject demographic data, and the second dataset contains corresponding chemical data. Familiarize yourself with these data used previously in TAME 2.0 Module 2.3 Data Manipulation and Reshaping. # Load the demographic data demographic_data &lt;- read.csv(&quot;Module2_4_Input/Module2_4_InputData1.csv&quot;) # View the top of the demographic dataset head(demographic_data) ## ID BMI MAge MEdu BW GA ## 1 1 27.7 22.99928 3 3180.058 34 ## 2 2 26.8 30.05142 3 3210.823 43 ## 3 3 33.2 28.04660 3 3311.551 40 ## 4 4 30.1 34.81796 3 3266.844 32 ## 5 5 37.4 42.68440 3 3664.088 35 ## 6 6 33.3 24.94960 3 3328.988 40 # Load the chemical data chemical_data &lt;- read.csv(&quot;Module2_4_Input/Module2_4_InputData2.csv&quot;) # View the top of the chemical dataset head(chemical_data) ## ID DWAs DWCd DWCr UAs UCd UCr ## 1 1 6.426464 1.292941 51.67987 10.192695 0.7537104 42.60187 ## 2 2 7.832384 1.798535 50.10409 11.815088 0.9789506 41.30757 ## 3 3 7.516569 1.288461 48.74001 10.079057 0.1903262 36.47716 ## 4 4 5.906656 2.075259 50.92745 8.719123 0.9364825 42.47987 ## 5 5 7.181873 2.762643 55.16882 9.436559 1.4977829 47.78528 ## 6 6 9.723429 3.054057 51.14812 11.589403 1.6645837 38.26386 Preparing the example dataset For ease of analysis, we will merge these two datasets before proceeding. # Merging data full_data &lt;- inner_join(demographic_data, chemical_data, by = &quot;ID&quot;) # Previewing new data head(full_data) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 Continuous demographic variables, like BMI, are often dichotomized (or converted to a categorical variable with two categories representing higher vs. lower values) to increase statistical power in analyses. This is particularly important for clinical data that tend to have smaller sample sizes. In our initial dataframe, BMI is a continuous or numeric variable; however, our questions require us to dichotomize BMI. We can use the following code, which relies on if/else logic (see TAME 2.0 Module 2.3 Data Manipulation and Reshaping for more information) to generate a new column representing our dichotomized BMI variable for our first environmental health question. # Adding dichotomized BMI column full_data &lt;- full_data %&gt;% mutate(Dichotomized_BMI = ifelse(BMI &lt; 25, &quot;Normal&quot;, &quot;Overweight&quot;)) # Previewing new data head(full_data) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr Dichotomized_BMI ## 1 0.7537104 42.60187 Overweight ## 2 0.9789506 41.30757 Overweight ## 3 0.1903262 36.47716 Overweight ## 4 0.9364825 42.47987 Overweight ## 5 1.4977829 47.78528 Overweight ## 6 1.6645837 38.26386 Overweight We can see that we now have created a new column entitled Dichotomized_BMI that we can use to perform a statistical test to assess if there are differences between drinking water metals between normal and overweight subjects. Loops We will start with loops. There are three main types of loops in R: for, while, and repeat. We will focus on for loops in this module, but for more in-depth information on loops, including the additional types of loops, see here. Before applying loops to our data, let’s discuss how for loops work. The basic structure of a for loop is shown here: # Basic structure of a for loop for (i in 1:4){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 for loops always start with for followed by a statement in parentheses. The argument in the parentheses tells R how to iterate (or repeat) through the code in the curly brackets. Here, we are telling R to iterate through the code in curly brackets 4 times. Each time we told R to print the value of our iterator, or i, which has a value of 1, 2, 3, and then 4. Loops can also iterate through columns in a dataset. For example, we can use a for loop to print the ages of each subject: # Creating a smaller dataframe for our loop example full_data_subset &lt;- full_data[1:6, ] # Finding the total number of rows or subjects in the dataset number_of_rows &lt;- length(full_data_subset$MAge) # Creating a for loop to iterate from 1 to the last row for (i in 1:number_of_rows){ # Printing each subject age # Need to put `[i]` to index the correct value corresponding to the row we are evaluating print(full_data_subset$MAge[i]) } ## [1] 22.99928 ## [1] 30.05142 ## [1] 28.0466 ## [1] 34.81796 ## [1] 42.6844 ## [1] 24.9496 Now that we know how a for loop works, how can we apply this approach to determine whether there are statistically significant differences in drinking water arsenic, cadmium, and chromium between normal weight (BMI &lt; 25) and overweight (BMI \\(\\geq\\) 25) subjects. Because our data are normally distributed and there are two groups that we are comparing, we will use a t-test applied to each metal measured in drinking water. Testing for assumptions is outside the scope of this module, but see TAME 2.0 Module 3.3 Normality Tests and Data Transformation for more information on this topic. Running a t-test in R is very simple, which we can demonstrate by running a t-test on the drinking water arsenic data: # Running t-test and storing results in t_test_res t_test_res &lt;- full_data %&gt;% t_test(DWAs ~ Dichotomized_BMI) # Viewing results t_test_res ## # A tibble: 1 × 8 ## .y. group1 group2 n1 n2 statistic df p ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 DWAs Normal Overweight 96 104 -0.728 192. 0.468 We can see that our p-value is 0.468. Because this is greater than 0.05, we cannot reject the null hypothesis that normal weight and overweight subjects are exposed to the same drinking water arsenic concentrations. Although this was a very simple line of code to run, what if we have many columns we want to run the same t-test on? We can use a for loop to iterate through these columns. Let’s break down the steps of our for loop before executing the code. First, we will define the variables (columns) we want to run our t-test on. This is different from our approach above, because in those code chunks, we were using numbers to indicate the number of iterations through the loop. Here, we are naming the specific variables instead, and R will iterate though each of these variables. Note that we could omit this step and instead use the numeric column index of our variables of interest [7:9]. However, naming the specific columns makes this approach more robust because if additional data are added to or removed from our dataframe, the numeric column index of our variables could change. Which approach you choose really depends on the purpose of your loop! Second, we will create an empty dataframe where we will store the results generated by our for loop. Third, we will actually run our for loop. This will tell R: for each variable in our vars_of_interest vector, run a t-test with that variable (and store the results in a temporary dataframe called “res”), then add those results to our final results dataframe. A row will be added to the results dataframe each time R iterates through a new variable, resulting in a dataframe that stores the results of all of our t-tests. # Defining variables (columns) we want to run a t-test on vars_of_interest &lt;- c(&quot;DWAs&quot;, &quot;DWCd&quot;, &quot;DWCr&quot;) # Creating an empty dataframe to store results t_test_res_DW &lt;- data.frame() # Running for loop for (i in vars_of_interest) { # Storing the results of each iteration of the loop in a temporary results dataframe res &lt;- full_data %&gt;% # Writing the formula needed for each iteration of the loop t_test(as.formula(paste(i, &quot;~ Dichotomized_BMI&quot;, sep = &quot;&quot;))) # Adding a row to the results dataframe each time the loop is iterated t_test_res_DW &lt;- bind_rows(t_test_res_DW, res) } # Viewing our results t_test_res_DW ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Normal Overweight 96 104 -0.7279621 192.3363 0.468 ## 2 DWCd Normal Overweight 96 104 -0.5894360 196.1147 0.556 ## 3 DWCr Normal Overweight 96 104 0.1102933 197.9870 0.912 With this, we can answer Environmental Health Question #1: Are there statistically significant differences in drinking water arsenic, cadmium, and chromium between normal weight (BMI &lt; 25) and overweight (BMI \\(\\geq\\) 25) subjects? Answer: No, there are not any statistically significant differences in drinking water metals between normal weight and overweight subjects. Formulas and Pasting Note the use of the code as.formula(paste0(i, \"~ Dichotomized_BMI\")). Let’s take a quick detour to discuss the use of the as.formula() and paste() functions, as these are important functions often used in loops and user-defined functions. Many statistical test functions and regression functions require one argument to be a formula, which is typically formatted as y ~ x, where y is the dependent variable of interest and x is an independent variable. For some functions, additional variables can be included on the right side of the formula to represent covariates (additional variables of interest). The function as.formula() returns the argument in parentheses in formula format so that it can be correctly passed to other functions. We can demonstrate that here by assigning a dummy variable j the character string var1: # Assigning variable j &lt;- &quot;var1&quot; # Demonstrating output of as.formula() as.formula(paste(j, &quot; ~ Dichotomized_BMI&quot;, sep = &quot;&quot;)) ## var1 ~ Dichotomized_BMI We can use the paste() function to combine strings of characters. The paste function takes each argument (as many arguments as is needed) and pastes them together into one character string, with the separator between arguments set by the sep = argument. When our y variable is changing with each iteration of our for loop, we can use the paste() function to write our formula correctly by telling the function to paste the variable i, followed by the rest of our formula, which stays the same for each iteration of the loop. Let’s examine the output of just the paste() part of our code: paste(j, &quot; ~ Dichotomized_BMI&quot;, sep = &quot;&quot;) ## [1] &quot;var1 ~ Dichotomized_BMI&quot; The paste() function is very flexible and can be useful in many other settings when you need to create one character string from arguments from different sources! Notice that the output looks different from the output of as.formula(). There is a returned index ([1]), and there are quotes around the character string. The last function we will highlight here is the noquote() function, which can be helpful if you’d like a string without quotes: noquote(paste(j, &quot; ~ Dichotomized_BMI&quot;, sep = &quot;&quot;)) ## [1] var1 ~ Dichotomized_BMI However, this still returns an indexed number, so there are times when it will not allow code to execute properly (for example, when we need a formula format). Next, we will learn about functions and apply them to our dataset to answer our additional environmental health questions. Functions Functions are useful when you want to execute a block of code organized together to perform one specific task, and you want to be able to change parameters for that task easily rather than having to copy and paste code over and over that largely stays the same but might have small modifications in certain arguments. The basic structure of a function is as follows: function_name &lt;- function(parameter_1, parameter_2...){ # Function body (where the code goes) insert_code_here # What the function returns return() } A function requires you to name it as we did with function_name. In parentheses, the function requires you to specify the arguments or parameters. Parameters (i.e., parameter_1) act as placeholders in the body of the function. This allows us to change the values of the parameters each time a function is called, while the majority of the code remains the same. Lastly, we have a return() statement, which specifies what object (i.e., vector, dataframe, etc.) we want to retrieve from a function. Although a function can display the last expression from the function body in the absence of a return() statement, it’s a good habit to include it as the last expression. It is important to note that, although functions can take many input parameters and execute large code chunks, they can only return one item, whether that is a value, vector, dataframe, plot, code output, or list. When writing your own functions, it is important to describe the purpose of the function, its input, its parameters, and its output so that others can understand what your functions does and how to use it. This can be defined either in text above a code chunk if you are using R Markdown or as comments within the code itself. We’ll start with a simple function. Let’s say we want to convert temperatures from Fahrenheit to Celsius. We can write a function that takes the temperature in Fahrenheit and converts it to Celsius. Note that we have given our parameters descriptive names (fahrenheit_temperature, celsius_temperature), which makes our code more readable than if we assigned them dummy names such as x and y. # Function to convert temperatures in Fahrenheit to Celsius ## Parameters: temperature in Fahrenheit (input) ## Output: temperature in Celsius fahrenheit_to_celsius &lt;- function(fahrenheit_temperature){ celsius_temperature &lt;- (fahrenheit_temperature - 32) * (5/9) return(celsius_temperature) } Notice that the above code block was run, but there isn’t an output. Rather, running the code assigns the function code to that function. When you run code defining a function, that function will appear in your Global Environment under the “Functions” section. We can see the output of the function by providing an input value. Let’s start by converting 41 degrees Fahrenheit to Celsius: # Calling the function # Here, 41 is the `fahrenheit_temperature` in the function fahrenheit_to_celsius(41) ## [1] 5 41 degrees Fahrenheit is equivalent to 5 degrees Celsius. We can also have the function convert a vector of values. # Defining vector of temperatures vector_of_temperatures &lt;- c(81,74,23,65) # Calling the function fahrenheit_to_celsius(vector_of_temperatures) ## [1] 27.22222 23.33333 -5.00000 18.33333 Before getting back to answer our environmental health related questions, let’s look at one more example of a function. This time we’ll create a function that can calculate the circumference of a circle based on its radius in inches. Here you can also see a different style of commenting to describe the function’s purpose, inputs, and outputs. circle_circumference &lt;- function(radius){ # Calculating a circle&#39;s circumference based on the radius inches # :parameters: radius # :output: circumference and radius # Calculating diameter first diameter &lt;- 2 * radius # Calculating circumference circumference &lt;- pi * diameter return(circumference) } # Calling function circle_circumference(3) ## [1] 18.84956 So, if a circle had a radius of 3 inches, its circumference would be ~19 inches. What if we were interested in seeing the diameter to double check our code? diameter ## Error: object &#39;diameter&#39; not found R throws an error, because the variable diameter was created inside the function and the function only returned the circumference variable. This is actually one of the ways that functions can improve coding efficiency - by not needing to store intermediate variables that aren’t of interest to the main goal of the code or analysis. However, there are two ways we can still see the diameter variable: Put print statements in the body of the function (print(diameter)). Have the function return a different variable or list of variables (c(circumference, diameter)). See the below section on List Operation for more on this topic. We can now move on to using a more complicated function to answer all three of our environmental health questions without repeating our earlier code three times. The main difference between each of our first three environmental health questions is the BMI cutoff used to dichotomize the BMI variable, so we can use that as one of the parameters for our function. We can also use arguments in our function to name our groups. We can adapt our previous for loop code into a function that will take different BMI cutoffs and return statistical results by including parameters to define the parts of the analysis that will change with each unique question. For example: Changing the BMI cutoff from a number (in our previous code) to our parameter name that specifies the cutoff Changing the group names for assigning category (in our previous code) to our parameter names # Function to dichotomize BMI into different categories and return results of t-test on drinking water metals between dichotomized groups ## Parameters: ### input_data: dataframe containing BMI and drinking water metals levels ### bmi_cutoff: numeric value specifying the cut point for dichotomizing BMI ### lower_group_name: name for the group of subjects with BMIs lower than the cutoff ### upper_group_name: name for the group of subjects with BMIs higher than the cutoff ### variables: vector of variable names that statistical test should be run on ## Output: dataframe with statistical results for each variable in the variables vector bmi_DW_ttest &lt;- function(input_data, bmi_cutoff, lower_group_name, upper_group_name, variables){ # Creating dichotomized variable dichotomized_data &lt;- input_data %&gt;% mutate(Dichotomized_BMI = ifelse(BMI &lt; bmi_cutoff, lower_group_name, upper_group_name)) # Creating an empty dataframe to store results t_test_res_DW &lt;- data.frame() # Running for loop for (i in variables) { # Storing the results of each iteration of the loop in a temporary results dataframe res &lt;- dichotomized_data %&gt;% # Writing the formula needed for each iteration of the loop t_test(as.formula(paste(i, &quot;~ Dichotomized_BMI&quot;, sep = &quot;&quot;))) # Adding a row to the results dataframe each time the loop is iterated t_test_res_DW &lt;- bind_rows(t_test_res_DW, res) } # Return results return(t_test_res_DW) } For the first example of using the function, we have included the name of each argument for clarity, but this isn’t necessary if you pass in the arguments in the order they were defined when writing the function. # Defining variables (columns) we want to run a t-test on vars_of_interest &lt;- c(&quot;DWAs&quot;, &quot;DWCd&quot;, &quot;DWCr&quot;) # Apply function for normal vs. overweight (bmi_cutoff = 25) bmi_DW_ttest(input_data = full_data, bmi_cutoff = 25, lower_group_name = &quot;Normal&quot;, upper_group_name = &quot;Overweight&quot;, variables = vars_of_interest) ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Normal Overweight 96 104 -0.7279621 192.3363 0.468 ## 2 DWCd Normal Overweight 96 104 -0.5894360 196.1147 0.556 ## 3 DWCr Normal Overweight 96 104 0.1102933 197.9870 0.912 Here, we can see the same results as above in the Loops section. We can next apply the function to answer our additional environmental health questions: # Apply function for underweight vs. non-underweight (bmi_cutoff = 18.5) bmi_DW_ttest(full_data, 18.5, &quot;Underweight&quot;, &quot;Non-Underweight&quot;, vars_of_interest) ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Non-Underweight Underweight 166 34 -0.86947835 53.57143 0.388 ## 2 DWCd Non-Underweight Underweight 166 34 -0.97359810 55.45450 0.334 ## 3 DWCr Non-Underweight Underweight 166 34 0.04305105 56.08814 0.966 # Apply function for non-obese vs. obese (bmi_cutoff = 29.9) bmi_DW_ttest(full_data, 29.9, &quot;Non-Obese&quot;, &quot;Obese&quot;, vars_of_interest) ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Non-Obese Obese 144 56 -1.9312097 86.80253 0.0567 ## 2 DWCd Non-Obese Obese 144 56 0.3431076 94.52209 0.7320 ## 3 DWCr Non-Obese Obese 144 56 -0.6878311 89.61818 0.4930 With this, we can answer Environmental Health Questions #2 &amp; #3: Are there statistically significant differences in drinking water arsenic, cadmium, and chromium between underweight (BMI &lt; 18.5) and non-underweight (BMI \\(\\geq\\) 18.5) subjects or between non-obese (BMI &lt; 29.9) and obese (BMI \\(\\geq\\) 29.9) subjects? Answer: No, there are not any statistically significant differences in drinking water metals between underweight and non-underweight subjects or between non-obese and obese subjects. Here, we were able to answer all three of our environmental health questions within relatively few lines of code by using a function to efficiently assess different variations on our analysis. In the last section of this module, we will demonstrate how to use list operations to improve coding efficiency. List operations Lists are a data type in R that can store other data types (including lists, to make nested lists). This allows you to store multiple dataframes in one object and apply the same functions to each dataframe in the list. Lists can also be helpful for storing the results of a function if you would like to be able to access multiple outputs. For example, if we return to our example of a function that calculates the circumference of a circle, we can store both the diameter and circumference as list objects. The function will then return a list containing both of these values when called. # Adding list element to our function circle_circumference_4 &lt;- function(radius){ # Calculating a circle&#39;s circumference and diameter based on the radius in inches # :parameters: radius # :output: list that contains diameter [1] and circumference [2] # Calculating diameter first diameter &lt;- 2 * radius # Calculating circumference circumference &lt;- pi * diameter # Storing results in a named list results &lt;- list(&quot;diameter&quot; = diameter, &quot;circumference&quot; = circumference) # Return results results } # Calling function circle_circumference_4(10) ## $diameter ## [1] 20 ## ## $circumference ## [1] 62.83185 We can also call the results individually using the following code: # Storing results of function circle_10 &lt;- circle_circumference_4(10) # Viewing only diameter ## Method 1 circle_10$diameter ## [1] 20 ## Method 2 circle_10[1] ## $diameter ## [1] 20 # Viewing only circumference ## Method 1 circle_10$circumference ## [1] 62.83185 ## Method 2 circle_10[2] ## $circumference ## [1] 62.83185 In the context of our dataset, we can use list operations to clean up and combine our results from all three BMI stratification approaches. This is often necessary to prepare data to share with collaborators or for supplementary tables in a manuscript. Let’s revisit our code for producing our statistical results, this time assigning our results to a dataframe rather than viewing them. # Defining variables (columns) we want to run a t-test on vars_of_interest &lt;- c(&quot;DWAs&quot;, &quot;DWCd&quot;, &quot;DWCr&quot;) # Normal vs. overweight (bmi_cutoff = 25) norm_vs_overweight &lt;- bmi_DW_ttest(input_data = full_data, bmi_cutoff = 25, lower_group_name = &quot;Normal&quot;, upper_group_name = &quot;Overweight&quot;, variables = vars_of_interest) # Underweight vs. non-underweight (bmi_cutoff = 18.5) under_vs_nonunderweight &lt;- bmi_DW_ttest(full_data, 18.5, &quot;Underweight&quot;, &quot;Non-Underweight&quot;, vars_of_interest) # Non-obese vs. obese (bmi_cutoff = 29.9) nonobese_vs_obese &lt;- bmi_DW_ttest(full_data, 29.9, &quot;Non-Obese&quot;, &quot;Obese&quot;, vars_of_interest) # Viewing one results dataframe as an example norm_vs_overweight ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Normal Overweight 96 104 -0.7279621 192.3363 0.468 ## 2 DWCd Normal Overweight 96 104 -0.5894360 196.1147 0.556 ## 3 DWCr Normal Overweight 96 104 0.1102933 197.9870 0.912 For publication purposes, let’s say we want to make the following formatting changes: Keep only the comparison of interest (for example Normal vs. Overweight) and the associated p-value, removing columns that are not as useful for interpreting or sharing the results Rename the .y. column so that its contents are clearer Collapse all of our data into one final dataframe We can first write a function to execute these cleaning steps: # Function to clean results dataframes ## Parameters: ### input_data: dataframe containing results of t-test ## Output: cleaned dataframe data_cleaning &lt;- function(input_data) { data &lt;- input_data %&gt;% # Rename .y. column rename(&quot;Variable&quot; = &quot;.y.&quot;) %&gt;% # Merge group1 and group2 unite(Comparison, group1, group2, sep = &quot; vs. &quot;) %&gt;% # Keep only columns of interest select(c(Variable, Comparison, p)) return(data) } Then, we can make a list of our dataframes to clean and apply: # Making list of dataframes t_test_res_list &lt;- list(norm_vs_overweight, under_vs_nonunderweight, nonobese_vs_obese) # Viewing list of dataframes head(t_test_res_list) ## [[1]] ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Normal Overweight 96 104 -0.7279621 192.3363 0.468 ## 2 DWCd Normal Overweight 96 104 -0.5894360 196.1147 0.556 ## 3 DWCr Normal Overweight 96 104 0.1102933 197.9870 0.912 ## ## [[2]] ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Non-Underweight Underweight 166 34 -0.86947835 53.57143 0.388 ## 2 DWCd Non-Underweight Underweight 166 34 -0.97359810 55.45450 0.334 ## 3 DWCr Non-Underweight Underweight 166 34 0.04305105 56.08814 0.966 ## ## [[3]] ## .y. group1 group2 n1 n2 statistic df p ## 1 DWAs Non-Obese Obese 144 56 -1.9312097 86.80253 0.0567 ## 2 DWCd Non-Obese Obese 144 56 0.3431076 94.52209 0.7320 ## 3 DWCr Non-Obese Obese 144 56 -0.6878311 89.61818 0.4930 And we can apply the cleaning function to each of the dataframes using the lapply() function, which takes a list as the first argument and the function to apply to each list element as the second argument: # Applying cleaning function t_test_res_list_cleaned &lt;- lapply(t_test_res_list, data_cleaning) # Vieweing cleaned dataframes head(t_test_res_list_cleaned) ## [[1]] ## Variable Comparison p ## 1 DWAs Normal vs. Overweight 0.468 ## 2 DWCd Normal vs. Overweight 0.556 ## 3 DWCr Normal vs. Overweight 0.912 ## ## [[2]] ## Variable Comparison p ## 1 DWAs Non-Underweight vs. Underweight 0.388 ## 2 DWCd Non-Underweight vs. Underweight 0.334 ## 3 DWCr Non-Underweight vs. Underweight 0.966 ## ## [[3]] ## Variable Comparison p ## 1 DWAs Non-Obese vs. Obese 0.0567 ## 2 DWCd Non-Obese vs. Obese 0.7320 ## 3 DWCr Non-Obese vs. Obese 0.4930 Last, we can collapse our list down into one dataframe using the do.call() and rbind.data.frame() functions, which together, take the elements of the list and collapse them into a dataframe by binding the rows together: t_test_res_cleaned &lt;- do.call(rbind.data.frame, t_test_res_list_cleaned) # Viewing final dataframe t_test_res_cleaned ## Variable Comparison p ## 1 DWAs Normal vs. Overweight 0.4680 ## 2 DWCd Normal vs. Overweight 0.5560 ## 3 DWCr Normal vs. Overweight 0.9120 ## 4 DWAs Non-Underweight vs. Underweight 0.3880 ## 5 DWCd Non-Underweight vs. Underweight 0.3340 ## 6 DWCr Non-Underweight vs. Underweight 0.9660 ## 7 DWAs Non-Obese vs. Obese 0.0567 ## 8 DWCd Non-Obese vs. Obese 0.7320 ## 9 DWCr Non-Obese vs. Obese 0.4930 The above example is just that - an example to demonstrate the mechanics of using list operations. However, there are actually a couple of even more efficient ways to execute the above cleaning steps: Build cleaning steps into the analysis function if you know you will not need to access the raw results dataframe. Bind all three dataframes together, then execute the cleaning steps. We will demonstrate #2 below: # Start by binding the rows of each of the results dataframes t_test_res_cleaned_2 &lt;- bind_rows(norm_vs_overweight, under_vs_nonunderweight, nonobese_vs_obese) %&gt;% # Rename .y. column rename(&quot;Variable&quot; = &quot;.y.&quot;) %&gt;% # Merge group1 and group2 unite(Comparison, group1, group2, sep = &quot; vs. &quot;) %&gt;% # Keep only columns of interest select(c(Variable, Comparison, p)) # Viewing results t_test_res_cleaned_2 ## Variable Comparison p ## 1 DWAs Normal vs. Overweight 0.4680 ## 2 DWCd Normal vs. Overweight 0.5560 ## 3 DWCr Normal vs. Overweight 0.9120 ## 4 DWAs Non-Underweight vs. Underweight 0.3880 ## 5 DWCd Non-Underweight vs. Underweight 0.3340 ## 6 DWCr Non-Underweight vs. Underweight 0.9660 ## 7 DWAs Non-Obese vs. Obese 0.0567 ## 8 DWCd Non-Obese vs. Obese 0.7320 ## 9 DWCr Non-Obese vs. Obese 0.4930 As you can see, this dataframe is the same as the one we produced using list operations. It was produced using fewer lines of code and without the need for a user-defined function! For our purposes, this was a more efficient approach. However, we felt it was important to demonstrate the mechanics of list operations because there may be times where you do need to keep dataframes separate during specific analyses. Concluding Remarks This module provided an introduction to loops, functions, and list operations and demonstrated how to use them to efficiently analyze an environmentally relevant dataset. When and how you implement these approaches depends on your coding style and the goals of your analysis. Although here we were focused on statistical tests and data cleaning, these flexible approaches can be used in a variety of data analysis steps. We encourage you to implement loops, functions, and list operations in your analyses when you find the need to iterate through statistical tests, visualizations, data cleaning, or other common workflow elements! Additional Resources Intro2r Loops Intro2r Functions in R Hadley Wickham Advanced R - Functionals Test Your Knowledge Use the same input data we used in this module to answer the following questions and produce a cleaned, publication-ready data table of results. Note that these data are normally distributed, so you can use a t-test. Are there statistically significant differences in urine metal concentrations (ie. arsenic levels, cadmium levels, etc.) between younger (MAge &lt; 40) and older (MAge \\(\\geq\\) 40) mothers? Are there statistically significant differences in urine metal concentrations (ie. arsenic levels, cadmium levels, etc.) between between normal weight (BMI &lt; 25) and overweight (BMI \\(\\geq\\) 25) subjects? "],["data-visualizations.html", "3.1 Data Visualizations Introduction to Data Visualizations Introduction to Training Module Density Plot Visualization Boxplot Visualization Correlation Visualizations Heatmap Visualization Concluding Remarks", " 3.1 Data Visualizations This training module was developed by Alexis Payton, Kyle Roell, Lauren E. Koval, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Data Visualizations Selecting an approach to visualize data is an important consideration when presenting scientific research, given that figures have the capability to summarize large amounts of data efficiently and effectively. (At least that’s the goal!) This module will focus on basic data visualizations that we view to be most commonly used, both in and outside of the field of environmental health research, many of which you have likely seen before. This module is not meant to be an exhaustive representation of all figure types, rather it serves as an introduction to some types of figures and how to approach choosing the one that most optimally displays your data and primary findings. When selecting a data visualization approach, here are some helpful questions you should first ask yourself: What message am I trying to convey with this figure? How does this figure highlight major findings from the paper? Who is the audience? What type of data am I working with? A Guide To Getting Data Visualization Right is a great resource for determining which figure is best suited for various types of data. More complex methodology-specific charts are presented in succeeding TAME modules. These include visualizations for: Two Group Comparisons (e.g.,boxplots and logistic regression) in Module 3.4 Introduction to Statistical Tests and Module 4.4 Two Group Comparisons and Visualizations Multi-Group Comparisons (e.g.,boxplots) in Module 3.4 Introduction to Statistical Tests and Module 4.5 Mult-Group Comparisons and Visualizations Supervised Machine Learning (e.g.,decision boundary plots, variable importance plots) in Module 5.3 Supervised ML Model Interpretation Unsupervised Machine Learning Principal Component Analysis (PCA) plots and heatmaps in Module 5.4 Unsupervised Machine Learning I: K-Means Clustering &amp; PCA Dendrograms, clustering visualizations, heatmaps, and variable contribution plots in Module 5.5 Unsupervised Machine Learning II: Additional Clustering Applications -Omics Expression (e.g.,MA plots and volcano plots) in Module 6.2 -Omics and Systems Biology: Transcriptomic Applications Mixtures Methods Forest Plots in Module 6.3 Mixtures I: Overview and Quantile G-Computation Application Trace Plots in Module 6.4 Mixtures II: BKMR Application Sufficient Similarity (e.g.,heatmaps, clustering) in Module 6.5 Mixtures III: Sufficient Similarity Toxicokinetic Modeling (e.g.,line graph, dose response) in Module 6.6 Toxicokinetic Modeling Introduction to Training Module Visualizing data is an important step in any data analysis, including those carried out in environmental health research. Often, visualizations allow scientists to better understand trends and patterns within a particular dataset under evaluation. Even after statistical analysis of a dataset, it is important to then communicate these findings to a wide variety of target audiences. Visualizations are a vital part of communicating complex data and results to target audiences. In this module, we highlight some figures that can be used to visualize larger, more high-dimensional datasets using figures that are more simple (but still relevant!) than methods presented later on in TAME. This training module specifically reviews the formatting of data in preparation of generating visualizations, scaling datasets, and then guides users through the generation of the following example data visualizations: Density plots Boxplots Correlation plots Heatmaps These visualization approaches are demonstrated using a large environmental chemistry dataset. This example dataset was generated through chemical speciation analysis of smoke samples collected during lab-based simulations of wildfire events. Specifically, different biomass materials (eucalyptus, peat, pine, pine needles, and red oak) were burned under two combustion conditions of flaming and smoldering, resulting in the generation of 12 different smoke samples. These data have been previously published in the following environmental health research studies, with data made publicly available: Rager JE, Clark J, Eaves LA, Avula V, Niehoff NM, Kim YH, Jaspers I, Gilmour MI. Mixtures modeling identifies chemical inducers versus repressors of toxicity associated with wildfire smoke. Sci Total Environ. 2021 Jun 25;775:145759. doi: 10.1016/j.scitotenv.2021.145759. Epub 2021 Feb 10. PMID: 33611182. Kim YH, Warren SH, Krantz QT, King C, Jaskot R, Preston WT, George BJ, Hays MD, Landis MS, Higuchi M, DeMarini DM, Gilmour MI. Mutagenicity and Lung Toxicity of Smoldering vs. Flaming Emissions from Various Biomass Fuels: Implications for Health Effects from Wildland Fires. Environ Health Perspect. 2018 Jan 24;126(1):017011. doi: 10.1289/EHP2200. PMID: 29373863. GGplot2 ggplot2 is a powerful package used to create graphics in R. It was designed based on the philosophy that every figure can be built using a dataset, a coordinate system, and a geom that specifies the type of plot. As a result, it is fairly straightforward to create highly customizable figures and is typically preferred over using base R to generate graphics. We’ll generate all of the figures in this module using ggplot2. For additional resources on ggplot2 see ggplot2 Posit Documentation and Data Visualization with ggplot2. Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;GGally&quot;)) install.packages(&quot;GGally&quot;); if (!requireNamespace(&quot;corrplot&quot;)) install.packages(&quot;corrplot&quot;); if (!requireNamespace(&quot;pheatmap&quot;)) install.packages(&quot;pheatmap&quot;); Loading R packages required for this session library(tidyverse) library(GGally) library(corrplot) library(reshape2) library(pheatmap) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example dataset Then let’s read in our example dataset. As mentioned in the introduction, this example dataset represents chemical measurements across 12 different biomass burn scenarios representing potential wildfire events. Let’s upload and view these data: # Load the data smoke_data &lt;- read.csv(&quot;Module3_1_Input/Module3_1_InputData.csv&quot;) # View the top of the dataset head(smoke_data) ## Chemical.Category Chemical CASRN Eucalyptus_Smoldering ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 0.06 ## 2 n-Alkanes 3-Methylnonadecane 6418-45-7 0.04 ## 3 n-Alkanes Docosane 629-97-0 0.21 ## 4 n-Alkanes Dodecylcyclohexane 1795-17-1 0.04 ## 5 n-Alkanes Eicosane 112-95-8 0.11 ## 6 n-Alkanes Heneicosane 629-94-7 0.13 ## Eucalyptus_Flaming Peat_Smoldering Peat_Flaming Pine_Smoldering Pine_Flaming ## 1 0.06 1.36 0.06 0.06 0.06 ## 2 0.04 1.13 0.90 0.47 0.04 ## 3 0.25 9.46 0.57 0.16 0.48 ## 4 0.04 0.25 0.04 0.04 0.04 ## 5 0.25 7.55 0.54 0.17 0.29 ## 6 0.28 6.77 0.34 0.13 0.42 ## Pine_Needles_Smoldering Pine_Needles_Flaming Red_Oak_Smoldering ## 1 0.06 0.06 0.06 ## 2 0.04 0.72 0.04 ## 3 0.32 0.18 0.16 ## 4 0.12 0.04 0.04 ## 5 0.28 0.16 0.15 ## 6 0.30 0.13 0.13 ## Red_Oak_Flaming Units ## 1 0.13 ng_per_uL ## 2 0.77 ng_per_uL ## 3 0.36 ng_per_uL ## 4 0.04 ng_per_uL ## 5 0.38 ng_per_uL ## 6 0.69 ng_per_uL Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: How do the distributions of the chemical concentration data differ based on each biomass burn scenario? Are there correlations between biomass burn conditions based on the chemical concentration data? Under which biomass burn conditions are concentrations of certain chemical categories the highest? We can create a density plot to answer the first question. Similar to a histogram, density plots are an effective way to show overall distributions of data and can be useful to compare across various test conditions or other stratifications of the data. In this example of a density plot, we’ll visualize the distributions of chemical concentration data on the x axis. A density plot automatically displays where values are concentrated on the y axis. Additionally, we’ll want to have multiple density plots within the same figure for each biomass burn condition. Before the data can be visualized, it needs to be converted from a wide to long format. This is because we need to have variable or column names entitled Chemical_Concentration and Biomass_Burn_Condition that can be placed into ggplot(). For review on converting between long and wide formats and using other tidyverse tools, see TAME 2.0 Module 2.3 Data Manipulation &amp; Reshaping. longer_smoke_data = pivot_longer(smoke_data, cols = 4:13, names_to = &quot;Biomass_Burn_Condition&quot;, values_to = &quot;Chemical_Concentration&quot;) head(longer_smoke_data) ## # A tibble: 6 × 6 ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Flaming ## # ℹ 1 more variable: Chemical_Concentration &lt;dbl&gt; Scaling dataframes for downstream data visualizations A data preparation method that is commonly used to convert values into those that can be used to better illustrate overall data trends is data scaling. Scaling can be achieved through data transformations or normalization procedures, depending on the specific dataset and goal of analysis/visualization. Scaling is often carried out using data vectors or columns of a dataframe. For this example, we will normalize the chemical concentration dataset using a basic scaling and centering procedure using the base R function, scale(). This algorithm results in the normalization of a dataset using the mean value and standard deviation. This scaling step will convert chemical concentration values in our dataset into normalized values across samples, such that each chemical’s concentration distributions are more easily comparable between the different biomass burn conditions. For more information on the scale() function, see its associated RDocumentation and helpful tutorial on Implementing the scale() function in R. scaled_longer_smoke_data = longer_smoke_data %&gt;% # scaling within each chemical group_by(Chemical) %&gt;% mutate(Scaled_Chemical_Concentration = scale(Chemical_Concentration)) %&gt;% ungroup() head(scaled_longer_smoke_data) # see the new scaled values now in the last column (column 7) ## # A tibble: 6 × 7 ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Flaming ## # ℹ 2 more variables: Chemical_Concentration &lt;dbl&gt;, ## # Scaled_Chemical_Concentration &lt;dbl[,1]&gt; We can see that in the Scaled_Chemical_Concentration column, each chemical is scaled based on a normal distribution centered around 0, with values now less than or greater than zero. Now that we have our dataset formatted, let’s plot it. Density Plot Visualization The following code can be used to generate a density plot: ggplot(scaled_longer_smoke_data, aes(x = Scaled_Chemical_Concentration, color = Biomass_Burn_Condition)) + geom_density() Answer to Environmental Health Question 1, Method I With this method, we can answer Environmental Health Question #1: How do the distributions of the chemical concentration data differ based on each biomass burn scenario? Answer: In general, there are a high number of chemicals that were measured at relatively lower abundances across all smoke samples (hence, the peak in occurrence density occurring towards the left, before 0). The three conditions of smoldering peat, flaming peat, and flaming pine contained the most chemicals at the highest relative concentrations (hence, these lines are the top three lines towards the right). Boxplot Visualization A boxplot can also be used to answer our first environmental health question: How do the distributions of the chemical concentration data differ based on each biomass burn scenario?. A boxplot also displays a data’s distribution, but it incorporates a visualization of a five number summary (i.e., minimum, first quartile, median, third quartile, and maximum). Any outliers are displayed as dots. For this example, let’s have Scaled_Chemical_Concentration on the x axis and Biomass_Burn_Condition on the y axis. The scaled_longer_smoke_data dataframe is the format we need, so we’ll use that for plotting. ggplot(scaled_longer_smoke_data, aes(x = Scaled_Chemical_Concentration, y = Biomass_Burn_Condition, color = Biomass_Burn_Condition)) + geom_boxplot() Answer to Environmental Health Question 1, Method II With this alternative method, we can answer, in a different way, Environmental Health Question #1: How do the distributions of the chemical concentration data differ based on each biomass burn scenario? Answer, Method II: The median chemical concentration is fairly low (less than 0) for all biomass burn conditions. Overall, there isn’t much variation in chemical concentrations with the exception of smoldering peat, flaming peat, and flaming eucalyptus. Correlation Visualizations Let’s turn our attention to the second environmental health question: Are there correlations between biomass burn conditions based on the chemical concentration data? We’ll use two different correlation visualizations to answer this question using the GGally package. GGally is a package that serves as an extension of ggplot2, the baseline R plotting system based on the grammar of graphics. GGally is very useful for creating plots that compare groups or features within a dataset, among many other utilities. Here we will demonstrate the ggpairs() function within GGally using the scaled chemistry dataset. This function will produce an image that shows correlation values between biomass burn sample pairs and also illustrates the overall distributions of values in the samples. For more information on GGally, see its associated RDocumentation and example helpful tutorial. GGally requires a wide dataframe with ids (i.e.,Chemical) as the rows and the variables that will be compared to each other (i.e.,Biomass_Burn_Condition) as the columns. Let’s create that dataframe. # first selecting the chemical, biomass burn condition, and # the scaled chemical concentration columns wide_scaled_data = scaled_longer_smoke_data %&gt;% pivot_wider(id_cols = Chemical, names_from = &quot;Biomass_Burn_Condition&quot;, values_from = &quot;Scaled_Chemical_Concentration&quot;) %&gt;% # converting the chemical names to row names column_to_rownames(var = &quot;Chemical&quot;) head(wide_scaled_data) ## Eucalyptus_Smoldering Eucalyptus_Flaming Peat_Smoldering ## 2-Methylnonadecane -0.3347765 -0.3347765 2.841935 ## 3-Methylnonadecane -0.8794448 -0.8794448 1.649829 ## Docosane -0.3465132 -0.3327216 2.842787 ## Dodecylcyclohexane -0.4240624 -0.4240624 2.646734 ## Eicosane -0.3802202 -0.3195928 2.841691 ## Heneicosane -0.3895328 -0.3166775 2.835527 ## Peat_Flaming Pine_Smoldering Pine_Flaming ## 2-Methylnonadecane -0.3347765 -0.3347765 -0.3347765 ## 3-Methylnonadecane 1.1161291 0.1183422 -0.8794448 ## Docosane -0.2223890 -0.3637526 -0.2534201 ## Dodecylcyclohexane -0.4240624 -0.4240624 -0.4240624 ## Eicosane -0.1940076 -0.3542370 -0.3022707 ## Heneicosane -0.2875354 -0.3895328 -0.2486793 ## Pine_Needles_Smoldering Pine_Needles_Flaming ## 2-Methylnonadecane -0.3347765 -0.3347765 ## 3-Methylnonadecane -0.8794448 0.6984509 ## Docosane -0.3085863 -0.3568568 ## Dodecylcyclohexane 0.7457649 -0.4240624 ## Eicosane -0.3066012 -0.3585675 ## Heneicosane -0.3069635 -0.3895328 ## Red_Oak_Smoldering Red_Oak_Flaming ## 2-Methylnonadecane -0.3347765 -0.1637228 ## 3-Methylnonadecane -0.8794448 0.8144726 ## Docosane -0.3637526 -0.2947948 ## Dodecylcyclohexane -0.4240624 -0.4240624 ## Eicosane -0.3628981 -0.2632960 ## Heneicosane -0.3895328 -0.1175398 By default, ggpairs() displays Pearson’s correlations. To show Spearman’s correlations takes more nuance, but can be done using the code that has been commented out below. # ggpairs with Pearson&#39;s correlations wide_scaled_data = data.frame(as.matrix(wide_scaled_data)) ggpairs(wide_scaled_data) # ggpairs with Spearman&#39;s correlations # pearson_correlations = cor(wide_scaled_data, method = &quot;spearman&quot;) # ggpairs(wide_scaled_data, upper = list(continuous = wrap(ggally_cor, method = &quot;spearman&quot;))) Many of these biomass burn conditions have significant correlations denoted by the asterisks. ’*’: p value &lt; 0.1 ’**’: p value &lt; 0.05 ’***’: p value &lt; 0.01 The upper right portion displays the correlation values, where a value less than 0 indicates negative correlation and a value greater than 0 signifies positive correlation. The diagonal shows the density plots for each variable. The lower left portion visualizes the values of the two variables compared using a scatterplot. Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: Are there correlations between biomass burn conditions based on the chemical concentration data? Answer: There is low correlation between many of the variables (-0.5 &lt; correlation value &lt; 0.5). Eucalyptus flaming and pine flaming are significantly positively correlated along with peat flaming and pine needles flaming (correlation value ~0.7 and p value &lt; 0.001). We can visualize correlations another way using the other function from GGally, ggcorr(), which visualizes each correlation as a square. Note that this function calculates Pearson’s correlations by default. However, this can be changed using the method parameter shown in the code commented out below. # Pearson&#39;s correlations ggcorr(wide_scaled_data) # Spearman&#39;s correlations # ggcorr(wide_scaled_data, method = &quot;spearman&quot;) We’ll visualize correlations between each of the groups using one more figure using the corrplot() function from the corrplot package. # Need to supply corrplot with a correlation matrix, here, using the &#39;cor&#39; function corrplot(cor(wide_scaled_data)) Each of these correlation figures displays the same information, but the one you choose to use is a matter of personal preference. Click on the following resources for additional information on ggpairs() and corrplot(). Heatmap Visualization Last, we’ll turn our attention to answering the final environmental health question: Under which biomass burn conditions are concentrations of certain chemical categories the highest? This can be addressed with the help of a heatmap. Heatmaps are a highly effective method of viewing an entire dataset at once. Heatmaps can appear similar to correlation plots, but typically illustrate other values (e.g., concentrations, expression levels, presence/absence, etc) besides correlation values. They are used to draw patterns between two variables of highest interest (that comprise the x and y axis, though additional bars can be added to display other layers of information). In this instance, we’ll use a heatmap to determine whether there are patterns apparent between chemical categories and biomass burn condition on chemical concentrations. For this example, we can plot Biomass_Burn_Condition and Chemical.Category on the axes and fill in the values with Scaled_Chemical_Concentration. When generating heatmaps, scaled values are often used to better distinguish patterns between groups/samples. In this example, we also plan to display the median scaled concentration value within the heatmap as an additional layer of helpful information to aid in interpretation. To do so, we’ll need to take the median chemical concentration for each biomass burn condition within each chemical category. However, since we want ggplot() to visualize the median scaled values with the color of the tiles this step was already necessary. # We&#39;ll find the median value and add that data to the dataframe as an additional column heatmap_df = scaled_longer_smoke_data %&gt;% group_by(Biomass_Burn_Condition, Chemical.Category) %&gt;% mutate(Median_Scaled_Concentration = median(Scaled_Chemical_Concentration)) head(heatmap_df) ## # A tibble: 6 × 8 ## # Groups: Biomass_Burn_Condition, Chemical.Category [6] ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Flaming ## # ℹ 3 more variables: Chemical_Concentration &lt;dbl&gt;, ## # Scaled_Chemical_Concentration &lt;dbl[,1]&gt;, Median_Scaled_Concentration &lt;dbl&gt; Now we can plot the data and add the Median_Scaled_Concentration to the figure using geom_text(). Note that specifying the original Scaled_Chemical_Concentration in the fill parameter will NOT give you the same heatmap as specifying the median values in ggplot(). ggplot(data = heatmap_df, aes(x = Chemical.Category, y = Biomass_Burn_Condition, fill = Median_Scaled_Concentration)) + geom_tile() + # function used to specify a heatmap for ggplot geom_text(aes(label = round(Median_Scaled_Concentration, 2))) # adding concentration values as text, rounding to two values after the decimal Answer to Environmental Health Question 3 With this, we can answer Environmental Health Question #3: Under which biomass burn conditions are concentrations of certain chemical categories the highest? Answer: Peat flaming has the highest concentrations of inorganics and ions. Eucalyptus smoldering has the highest concentrations of levoglucosans. Pine smoldering has the highest concentrations of methoxyphenols. Peat smoldering has the highest concentrations of n-alkanes. Pine needles smoldering has highest concentrations of PAHs. This same heatmap can be achieved another way using the pheatmap() function from the pheatmap package. Using this function requires us to use a wide dataset, which we need to create. It will contain Chemical.Category, Biomass_Burn_Condition and Scaled_Chemical_Concentration. heatmap_df2 = scaled_longer_smoke_data %&gt;% group_by(Biomass_Burn_Condition, Chemical.Category) %&gt;% # using the summarize function instead of mutate function as was done previously since we only need the median values now summarize(Median_Scaled_Concentration = median(Scaled_Chemical_Concentration)) %&gt;% # transforming the data to a wide format pivot_wider(id_cols = Biomass_Burn_Condition, names_from = &quot;Chemical.Category&quot;, values_from = &quot;Median_Scaled_Concentration&quot;) %&gt;% # converting the chemical names to row names column_to_rownames(var = &quot;Biomass_Burn_Condition&quot;) head(heatmap_df2) ## Inorganics Ions Levoglucosan Methoxyphenols ## Eucalyptus_Flaming 0.05405359 0.05273246 0.4208870 -0.44781893 ## Eucalyptus_Smoldering -0.68595076 -0.80160192 1.7772753 -0.06449444 ## Peat_Flaming 2.24332901 1.77515899 -0.9383328 -0.51488738 ## Peat_Smoldering -0.51860591 -0.36146158 -0.8041211 0.05720971 ## Pine_Flaming -0.02063532 -0.05999543 -0.1992054 -0.50269422 ## Pine_Needles_Flaming 0.36405527 0.82229035 -0.8570130 -0.46331332 ## PAHs n-Alkanes ## Eucalyptus_Flaming 1.2885776 -0.3790357 ## Eucalyptus_Smoldering -0.4724635 -0.3465132 ## Peat_Flaming -0.5369746 -0.3093608 ## Peat_Smoldering -0.3162278 2.8238921 ## Pine_Flaming 1.7825403 -0.3347765 ## Pine_Needles_Flaming -0.4179505 -0.3850613 Now let’s generate the same heatmap this time using the pheatmap() function: pheatmap(heatmap_df2, # removing the clustering option from both rows and columns cluster_rows = FALSE, cluster_cols = FALSE, # adding the values for each cell, making those values black, and changing the font size display_numbers = TRUE, number_color = &quot;black&quot;, fontsize = 12) Notice that the pheatmap() function does not include axes or legend titles as with ggplot(), however those can be added to the figure after exporting from R in MS Powerpoint or Adobe. Additional parameters, including cluster_rows, for the pheatmap() function are discussed further in TAME 2.0 Module 5.4 Unsupervised Machine Learning. For basic heatmaps like the ones shown here, ggplot() or pheatmap() can both be used however, both have their pros and cons. For example, ggplot() figures tend to be more customizable and easily combined with other figures, while pheatmap() has additional parameters built into the function that can make plotting certain features advantageous like clustering. Concluding Remarks In conclusion, this training module provided example code to create highly customizable data visualizations using ggplot2 pertinent to environmental health research. Test Your Knowledge Replicate the figure below! The heatmap still visualizes the median chemical concentrations, but this time we’re separating the burn conditions, allowing us to determine if the concentrations of chemicals released are contingent upon the burn condition. For additional figures available and to view aspects of figures that can be changed in GGplot2, check out this GGPlot2 Cheat Sheet. You might need it to make this figure! Hint 1: Use the separate() function from tidyverse to split Biomass_Burn_Condition into Biomass and Burn_Condition. Hint 2: Use the function facet_wrap() within ggplot() to separate the heatmaps by Burn_Condition. "],["improving-data-visualizations.html", "3.2 Improving Data Visualizations Introduction to Data Visulization Conventions Introduction to Training Module Creating an Improved Boxplot Visualization Creating an Improved Heatmap Visualization Creating Multi-Plot Figures Concluding Remarks", " 3.2 Improving Data Visualizations This training module was developed by Alexis Payton, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Data Visulization Conventions Data visualizations are used to convey key takeaways from a research study’s findings in a clear, succinct manner to highlight data trends, patterns, and/or relationships. In environmental health research, this is of particular importance for high-dimensional datasets that can typically be parsed using multiple methods, potentially resulting in many different approaches to visualize data. As a consequence, researchers are often faced with an overwhelming amount of options when deciding which visualization scheme(s) most optimally translate their results for effective dissemination. Effective data visualization approaches are vital to a researcher’s success for many reasons. For instance, manuscript readers or peer reviewers often scroll through a study’s text and focus on the quality and novelty of study figures before deciding whether to read/review the paper. Therefore, the importance of data visualizations cannot be understated in any research field. As a high-level introduction, it is important that we first communicate some traits that we think are imperative towards ensuring a successful data visualization approach as described in more detail below. Keys to successful data visualizations: Consider your audience, data type, and research question prior to selecting a figure to visualize your data For example, if more computationally complex methods are used in a manuscript that is intended for a journal with an audience that doesn’t have that same level of expertise, consider spending time focusing on how those results are presented in an approachable way for that audience. For a review of how to choose a rudimentary chart based on the data type, check out How to Choose the Right Data Visualization. Some of these basic charts will be presented in this module, while more complex analysis-specific visualizations, especially ones developed for high-dimensional data will be presented in later modules. Take the legibility of the figure into account This includes avoiding abbreviations when possible. (If they can’t be avoided explain them in the caption.) All titles should be capitalized, including titles for the legend(s) and axes. Underscores and periods between words should be replaced with spaces. Consider the legibility of the figure if printed in black and white. (However, that’s not as important these days.) Lastly, feel free to describe your plot in further detail in the caption to aid the reader in understanding the results presented. Minimize text Main titles aren’t necessary for single paneled figures (like the examples below), because in a publication the title of the figure is right underneath each figure. It’s good practice to remove this kind of extraneous text, which can make the figure seem more cluttered. Titles can be helpful in multi-panel figures, especially if there are multiple panels with the same figure type that present slightly different results. For example, in the Test Your Knowledge section, you’ll need to create two heatmaps, but one displays data under smoldering conditions and the other displays data under flaming conditions. In general, try to reduce the amount of extraneous text in a plot to keep a reader focused on the most important elements and takeaways in the plot. Use the minimal number of figures you need to support your narrative It is important to include an optimal number of figures within manuscripts and scientific reports. Too many figures might overwhelm the overall narrative, while too few might not provide enough substance to support your main findings. It can be helpful to also consider placing some figures in supplemental material to aid in the overall flow of your scientific writing. Select an appropriate color palette Packages have been developed to offer color palettes including MetBrewer and RColorBrewer. In addition, ggsci is a package that offers a collection of color palettes used in various scientific journals. For more information, check out MetBrewer, see its associated RDocumentation and example tutorial. For more information on RColorBrewer, see its associated RDocumentation and example tutorial. For more information on ggsci, see its associated RDocumentation. In general, it’s better to avoid bright and flashy colors that can be difficult to read. It’s advisable to use colors for manuscript figures that are color-blind friendly. Check out these Stack overflow answers about color blind-safe color palettes and packages. Popular packages for generating colorblind-friendly palettes include viridis and rcartocolor. Use color strategically Color can be used to visualize a variable. There are three ways to categorize color schemes - sequential, diverging, and qualitative. Below, definitions are provided for each along with example figures that we’ve previously published that illustrate each color scheme. In addition, figure titles and captions are also provided for context. Note that some of these figures have been simplified from what was originally published to show more streamlined examples for TAME. Sequential: intended for ordered categorical data (i.e., disease severity, likert scale, quintiles). The choropleth map below is from Winker, Payton et. al. Figure 1. Geospatial distribution of the risk of future wildfire events across North Carolina. Census tracts in North Carolina were binned into quintiles based on Wildfire Hazard Potential (WHP) with 1 (pale orange) having the lowest risk and 5 (dark red) having the highest risk. Figure regenerated here in alignment with its published Creative Commons Attribution 4.0 International License] Diverging: intended to emphasize continuous data at extremes of the data range (typically using darker colors) and mid-range values (typically using lighter colors). This color scheme is ideal for charts like heatmaps. The heatmap below is from Payton, Perryman et. al. Figure 2. Individual cytokine expression levels across all subjects. Cytokine concentrations were derived from nasal lavage fluid samples. On the x axis, subjects were ordered first according to tobacco use status, starting with non-smokers then cigarette smokers and e-cigarette users. Within tobacco use groups, subjects are ordered from lowest to highest average cytokine concentration from left to right. Within each cluster shown on the y axis, cytokines are ordered from lowest to highest average cytokine concentration from bottom to top. Figure regenerated here in alignment with its published Creative Commons Attribution 4.0 International License Qualitative: intended for nominal categorical data to visualize clear differences between groups (i.e., soil types and exposure groups). The dendrogram below is from Koval et. al. Figure 3. Translating chemical use inventory data to inform human exposure patterning. Groups A-I illustrate the identified clusters of exposure source categories. Figure regenerated here in alignment with its published Creative Commons Attribution 4.0 International License Consider ordering axes to reveal patterns relevant to the research questions Ordering the axes can reveal potential patterns that may not be clear in the visualization otherwise. In the cytokine expression heatmap above, there are not clear differences in cytokine expression across the tobacco use groups. However, e-cigarette users seem to have slightly more muted responses compared to non-smokers and cigarette smokers in clusters B and C, which was corroborated in subsequent statistical analyses. It is also evident that Cluster A had the lowest cytokine concentrations, followed by Cluster B, and then Cluster C with the greatest concentrations. What makes these figures so compelling is how the aspects introduced above were thoughtfully incorporated. In the next section, we’ll put those principles into practice using data that were described and referenced previously in TAME 2.0 Module 3.1 Data Visualizations. Introduction to Training Module In this module, ggplot2, R’s data visualization package will be used to walk through ways to improve data visualizations. We’ll recreate two figures (i.e., the boxplot and heatmap) constructed previously in TAME 2.0 Module 3.1 Data Visualizations and improve them so they are publication-ready. Additionally, we’ll write figure titles and captions to contextualize the results presented for each visualization. When writing figure titles and captions, it is helpful to address the research question or overall concept that the figure seeks to capture rather than getting into the weeds of specific methods the plot is based on. This is especially important when visualizing more complex methods that your audience might not have as much knowledge on. Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;MetBrewer&quot;)) install.packages(&quot;MetBrewer&quot;); if (!requireNamespace(&quot;RColorBrewer&quot;)) install.packages(&quot;RColorBrewer&quot;); if (!requireNamespace(&quot;pheatmap&quot;)) install.packages(&quot;pheatmap&quot;); if (!requireNamespace(&quot;cowplot&quot;)) install.packages(&quot;cowplot&quot;); Loading required R packages library(tidyverse) library(MetBrewer) library(RColorBrewer) library(pheatmap) library(cowplot) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example dataset Let’s now read in our example dataset. As mentioned in the introduction, this example dataset represents chemical measurements across 12 different biomass burn scenarios, representing chemicals emitted during potential wildfire events. Let’s upload and view these data: # Load the data smoke_data &lt;- read.csv(&quot;Module3_2_Input/Module3_2_InputData.csv&quot;) # View the top of the dataset head(smoke_data) ## Chemical.Category Chemical CASRN Eucalyptus_Smoldering ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 0.06 ## 2 n-Alkanes 3-Methylnonadecane 6418-45-7 0.04 ## 3 n-Alkanes Docosane 629-97-0 0.21 ## 4 n-Alkanes Dodecylcyclohexane 1795-17-1 0.04 ## 5 n-Alkanes Eicosane 112-95-8 0.11 ## 6 n-Alkanes Heneicosane 629-94-7 0.13 ## Eucalyptus_Flaming Peat_Smoldering Peat_Flaming Pine_Smoldering Pine_Flaming ## 1 0.06 1.36 0.06 0.06 0.06 ## 2 0.04 1.13 0.90 0.47 0.04 ## 3 0.25 9.46 0.57 0.16 0.48 ## 4 0.04 0.25 0.04 0.04 0.04 ## 5 0.25 7.55 0.54 0.17 0.29 ## 6 0.28 6.77 0.34 0.13 0.42 ## Pine_Needles_Smoldering Pine_Needles_Flaming Red_Oak_Smoldering ## 1 0.06 0.06 0.06 ## 2 0.04 0.72 0.04 ## 3 0.32 0.18 0.16 ## 4 0.12 0.04 0.04 ## 5 0.28 0.16 0.15 ## 6 0.30 0.13 0.13 ## Red_Oak_Flaming Units ## 1 0.13 ng_per_uL ## 2 0.77 ng_per_uL ## 3 0.36 ng_per_uL ## 4 0.04 ng_per_uL ## 5 0.38 ng_per_uL ## 6 0.69 ng_per_uL Now that we’ve been able to view the dataset, let’s come up with questions that can be answered with our boxplot and heatmap figure. This will inform how we format the dataframe for visualization. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Boxplot: How do the distributions of the chemical concentration data differ based on each biomass burn scenario? Heatmap: Which classes of chemicals show the highest concentrations across the evaluated biomass burn conditions? How can these figures be combined into a single plot that can be then be exported from R? Formatting dataframes for downstream visualization code First, format the dataframe by changing it from a wide to long format and normalizing the chemical concentration data. For more details on this data reshaping visit TAME 2.0 Module 2.3 Data Manipulation &amp; Reshaping. scaled_longer_smoke_data = pivot_longer(smoke_data, cols = 4:13, names_to = &quot;Biomass_Burn_Condition&quot;, values_to = &quot;Chemical_Concentration&quot;) %&gt;% # scaling within each chemical group_by(Chemical) %&gt;% mutate(Scaled_Chemical_Concentration = scale(Chemical_Concentration)) %&gt;% ungroup() head(scaled_longer_smoke_data) ## # A tibble: 6 × 7 ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus_Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat_Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine_Flaming ## # ℹ 2 more variables: Chemical_Concentration &lt;dbl&gt;, ## # Scaled_Chemical_Concentration &lt;dbl[,1]&gt; Creating an Improved Boxplot Visualization As we did in the previous module, a boxplot will be constructed to answer the first environmental heath question: How do the distributions of the chemical concentration data differ based on each biomass burn scenario?. Let’s remind ourselves of the original figure from the previous module. Based on the figure above, peat smoldering has the highest median scaled chemical concentration. However, this was difficult to determine given that the burn conditions aren’t labeled on the x axis and a sequential color palette was used, making it difficult to identify the correct boxplot with its burn condition in the legend. If you look closely, the colors in the legend are in a reverse order of the colors assigned to the boxplots. Let’s identify some elements of this graph that can be modified to make it easier to answer our research question. There are four main aspects we can adjust on this figure: 1. The legibility of the text in the legend and axes. Creating spaces between the text or exchanging the underscores for spaces improves the legibility of the figure. 2. The order of the boxplots. Ordering the biomass burn conditions from highest to lowest based on their median scaled chemical concentration allows the reader to easily determine the biomass burn condition that had the greatest or least chemical concentrations relative to each other. In R, this can be done by putting the Biomass_Burn_Condition variable into a factor. 3. Use of color. Variables can be visualized using color, text, size, etc. In this figure, it is redundant to have the biomass burn condition encoded in the legend and the color. Instead this variable can be put on the y axis and the legend will be removed to be more concise. The shades of the colors will also be changed, but to keep each burn condition distinct from each other, colors will be chosen that are distinct from one another. Therefore, we will choose a qualitative color scheme. 4. Show all data points when possible. Many journals now require that authors report every single value when making data visualizations, particularly for small n studies using bar graphs and boxplots to show results. Instead of just displaying the mean/median and surrounding data range, it is advised to show how every replicate landed in the study range when possible. Note that this requirement is not feasible for studies with larger sample sizes though should be considered for smaller in vitro and animal model studies. Let’s start with addressing #1: Legibility of Axis Text. The legend title and axis titles can easily be changed with ggplot(), so that will be done later. To remove the underscore from the Biomass_Burn_Condition column, we can use the function gsub(), which will replace all of the underscores with spaces, resulting in a cleaner-looking graph. # First adding spaces between the biomass burn conditions scaled_longer_smoke_data = scaled_longer_smoke_data %&gt;% mutate(Biomass_Burn_Condition = gsub(&quot;_&quot;, &quot; &quot;, Biomass_Burn_Condition)) # Viewing dataframe head(scaled_longer_smoke_data) ## # A tibble: 6 × 7 ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine Flaming ## # ℹ 2 more variables: Chemical_Concentration &lt;dbl&gt;, ## # Scaled_Chemical_Concentration &lt;dbl[,1]&gt; #2. Reordering the boxplots based on the median scaled chemical concentration. After calculating the median scaled chemical concentration for each biomass burn condition, the new dataframe will be arranged from lowest to highest median scaled concentration from the top of the dataframe to the bottom. This order will be saved in a vector, median_biomass_order. Although the biomass burn conditions are saved from lowest to highest concentration, ggplot() will plot them in reverse order with the highest concentration at the top and the lowest at the bottom of the y axis. Axis reordering can also be accomplished using reorder within the ggplot() function as described here and here. median_biomass = scaled_longer_smoke_data %&gt;% group_by(Biomass_Burn_Condition) %&gt;% summarize(Median_Concentration = median(Scaled_Chemical_Concentration)) %&gt;% # arranges dataframe from lowest to highest from top to bottom arrange(Median_Concentration) head(median_biomass) ## # A tibble: 6 × 2 ## Biomass_Burn_Condition Median_Concentration ## &lt;chr&gt; &lt;dbl&gt; ## 1 Red Oak Smoldering -0.459 ## 2 Eucalyptus Smoldering -0.451 ## 3 Pine Smoldering -0.424 ## 4 Pine Needles Smoldering -0.370 ## 5 Pine Needles Flaming -0.350 ## 6 Red Oak Flaming -0.337 # Saving that order median_biomass_order = median_biomass$Biomass_Burn_Condition # Putting into factor to organize the burn conditions scaled_longer_smoke_data$Biomass_Burn_Condition = factor(scaled_longer_smoke_data$Biomass_Burn_Condition, levels = median_biomass_order) # Final dataframe to be used for plotting head(scaled_longer_smoke_data) ## # A tibble: 6 × 7 ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine Flaming ## # ℹ 2 more variables: Chemical_Concentration &lt;dbl&gt;, ## # Scaled_Chemical_Concentration &lt;dbl[,1]&gt; Now that the dataframe has been finalized, we can plot the new boxplot. The final revision, #3: Making Use of Color, will be addressed with ggplot(). However, a palette can be chosen from the MetBrewer package. # Choosing the &quot;Jurarez&quot; palette from the `MetBrewer` package # `n = 12`, since there are 12 biomass burn conditions juarez_colors = met.brewer(name = &quot;Juarez&quot;, n = 12)[1:12] #4. Show all data points when possible will also be addressed with ggplot() by simply using geom_point(). FigureX1 = ggplot(scaled_longer_smoke_data, aes(x = Scaled_Chemical_Concentration, y = Biomass_Burn_Condition, color = Biomass_Burn_Condition)) + geom_boxplot() + # jittering the points, so they&#39;re not all on top of each other and adding transparency geom_point(position = position_jitter(h = 0.1), alpha = 0.7) + theme_light() + # changing the theme theme(axis.text = element_text(size = 9), # changing size of axis labels axis.title = element_text(face = &quot;bold&quot;, size = rel(1.5)), # changes axis titles legend.position = &quot;none&quot;) + # removes legend xlab(&#39;Scaled Chemical Concentration (pg/uL)&#39;) + ylab(&#39;Biomass Burn Condition&#39;) + # changing axis labels scale_color_manual(values = c(juarez_colors)) # changing the colors FigureX1 An appropriate title for this figure could be: “Figure X. Chemical concentration distributions of biomass burn conditions. The boxplots are based on the scaled chemical concentration values, which used the raw chemical concentrations values scaled within each chemical. The individual dots represent the concentrations of each chemical. The biomass burn conditions on the y axis are ordered from greatest (top) to least (bottom) based on median scaled chemical concentration.” Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question #1: Which biomass burn condition has the highest total chemical concentration? Answer: Smoldering peat has the highest median chemical concentration, however the median concentrations are comparable across all biomass burn conditions. All the flaming conditions have the highest median chemical concentrations and more overall variation than their respective smoldering conditions with the exception of smoldering peat. You may notice that the scaled chemical concentration was put on the x axis and burn condition was put on the y axis and not vice versa. When names are longer in length, they are more legible if placed on the y axis. Other aspects of the figure were changed in the latest version, but those are minor compared to changing the order of the boxplots, revamping the text, and changing the usage of color. For example, the background was changed from gray to white. Figure backgrounds are generally white, because the figure is easier to read if the paper is printed in black and white. A plot’s background can easily be changed to white in R using theme_light(), theme_minimal(), or theme_bw(). Posit provides a very helpful GGplot2 cheat sheet for changing a figure’s parameters. Creating an Improved Heatmap Visualization We’ll use a heatmap to answer the second environmental health question: Which classes of chemicals show the highest concentrations across the evaluated biomass burn conditions? Let’s view the original heatmap from the previous module and find aspects of it that can be improved. # Changing the biomass condition variable back to a character from a factor scaled_longer_smoke_data$Biomass_Burn_Condition = as.character(scaled_longer_smoke_data$Biomass_Burn_Condition) # Calculating the median value within each biomass burn condition and category scaled_longer_smoke_data = scaled_longer_smoke_data %&gt;% group_by(Biomass_Burn_Condition, Chemical.Category) %&gt;% mutate(Median_Scaled_Concentration = median(Scaled_Chemical_Concentration)) # Plotting ggplot(data = scaled_longer_smoke_data, aes(x = Chemical.Category, y = Biomass_Burn_Condition, fill = Median_Scaled_Concentration)) + geom_tile() + geom_text(aes(label = round(Median_Scaled_Concentration, 2))) # adding concentration values as text, rounding to two values after the decimal From the figure above, it’s clear that certain biomass burn conditions are associated with higher chemical concentrations for some of the chemical categories. For example, peat flaming exposure was associated with higher levels of inorganics and ions, while pine smoldering exposure was associated with higher levels of methoxyphenols. Although these are important findings, it is still difficult to determine if there are greater similarities in chemical profiles based on the biomass or the incineration temperature. Therefore, let’s identify some elements of this chart that can be modified to make it easier to answer our research question. There are three main aspects we can adjust on this figure: 1. The legibility of the text in the legend and axes. Similar to what we did previously, we’ll replace underscores and periods with spaces in the axis labels and titles. 2. The order of the axis labels. Ordering the biomass burn condition and chemical category from highest to lowest based on their median scaled chemical concentration allows the reader to easily determine the biomass burn condition that had the greatest or least total chemical concentrations relative to each other. From the previous boxplot figure, biomass burn condition is already in this order, however we need to order the chemical category by putting the variable into a factor. 3. Use of color. Notice that in the boxplot we used a qualitative palette, which is best for creating visual differences between different classes or groups. In this heatmap, we’ll choose a diverging color palette that uses two or more contrasting colors. A diverging color palette is able to highlight mid range with a lighter color and values at either extreme with a darker color or vice versa. #1: Legibility of Text can be addressed in ggplot() and so can #2: Reordering the heatmap. Biomass_Burn_Condition has already been reordered and put into a factor, but we need to do the same with Chemical.Category. Similar to before, median scaled chemical concentration for each chemical category will be calculated. However, this time the new dataframe will be arranged from highest to lowest median scaled concentration from the top of the dataframe to the bottom. ggplot() will plot them in the SAME order with the highest concentration on the left side and the lowest on the right side of the figure. # Order the chemical category by the median scaled chemical concentration median_chemical = scaled_longer_smoke_data %&gt;% group_by(Chemical.Category) %&gt;% summarize(Median_Concentration = median(Scaled_Chemical_Concentration)) %&gt;% arrange(-Median_Concentration) head(median_chemical) ## # A tibble: 6 × 2 ## Chemical.Category Median_Concentration ## &lt;chr&gt; &lt;dbl&gt; ## 1 Inorganics -0.265 ## 2 n-Alkanes -0.335 ## 3 Ions -0.359 ## 4 Levoglucosan -0.417 ## 5 Methoxyphenols -0.434 ## 6 PAHs -0.459 # Saving that order median_chemical_order = median_chemical$Chemical.Category # Putting into factor to organize the chemical categories scaled_longer_smoke_data$Chemical.Category = factor(scaled_longer_smoke_data$Chemical.Category, levels = median_chemical_order) # Putting burn conditons back into a factor to organize them scaled_longer_smoke_data$Biomass_Burn_Condition = factor(scaled_longer_smoke_data$Biomass_Burn_Condition, levels = median_biomass_order) # Viewing the dataframe to be plotted head(scaled_longer_smoke_data) ## # A tibble: 6 × 8 ## # Groups: Biomass_Burn_Condition, Chemical.Category [6] ## Chemical.Category Chemical CASRN Units Biomass_Burn_Condition ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus Smoldering ## 2 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Eucalyptus Flaming ## 3 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat Smoldering ## 4 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Peat Flaming ## 5 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine Smoldering ## 6 n-Alkanes 2-Methylnonadecane 1560-86-7 ng_per_… Pine Flaming ## # ℹ 3 more variables: Chemical_Concentration &lt;dbl&gt;, ## # Scaled_Chemical_Concentration &lt;dbl[,1]&gt;, Median_Scaled_Concentration &lt;dbl&gt; Now that the dataframe has been finalized, we can plot the new boxplot. The final revision, #3: Making Use of Color, will be addressed with ggplot(). Here a palette is chosen from the RColorBrewer package. # Only needed to choose 2 colors for &#39;low&#39; and &#39;high&#39; in the heatmap # `n = 8` in the code to generate more colors that can be chosen from rcolorbrewer_colors = brewer.pal(n = 8, name = &#39;Accent&#39;) FigureX2 = ggplot(data = scaled_longer_smoke_data, aes(x = Chemical.Category, y = Biomass_Burn_Condition, fill = Median_Scaled_Concentration)) + geom_tile(color = &#39;white&#39;) + # adds white space between the tiles geom_text(aes(label = round(Median_Scaled_Concentration, 2))) + # adding concentration values as text theme_minimal() + # changing the theme theme(axis.text = element_text(size = 9), # changing size of axis labels axis.title = element_text(face = &quot;bold&quot;, size = rel(1.5)), # changes axis titles legend.title = element_text(face = &#39;bold&#39;, size = 10), # changes legend title legend.text = element_text(size = 9)) + # changes legend text labs(x = &#39;Chemical Category&#39;, y = &#39;Biomass Burn Condition&#39;, fill = &quot;Scaled Chemical\\nConcentration (pg/mL)&quot;) + # changing axis labels scale_fill_gradient(low = rcolorbrewer_colors[5], high = rcolorbrewer_colors[6]) # changing the colors FigureX2 An appropriate title for this figure could be: “Figure X. Chemical category concentrations across biomass burn conditions. Scaled chemical concentration values are based on the raw chemical concentration values scaled within each chemical. Chemical category on the x axis is ordered from highest to lowest median concentration from left to right. Biomass burn condition on the y axis is ordered from the highest to lowest median concentration from top to bottom. The values in each tile represent the median scaled chemical concentration.” Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: Which classes of chemicals show the highest concentrations across the evaluated biomass burn conditions? Answer: Ordering the axes from highest to lowest concentration didn’t help organize the data as much as we would’ve liked given some of the variance of chemical concentrations across the chemical categories. Nevertheless, it’s still clear that peat flaming produces the highest concentration of inorganics and ions, peat smoldering with n-Alkanes, eucalyptus smoldering with Levoglucosan, pine smoldering with methoxyphenols, and pine flaming with PAHs. In addition, flaming conditions seem to have higher levels of inorganics and ions while smoldering conditions seem to have higher levels of levoglucosan and PAHs. It would be helpful if there was a way to group these chemical profiles based on similarity and that’s where the pheatmap() function can be helpful when it can be difficult to spot those patterns using visual inspection alone. Just for fun, let’s briefly visualize a hierarchical clustering heatmap, which will be used to group both the biomass burn conditions and chemical categories based on their chemical concentrations. In this module, we’ll focus only on the pheatmap() visualization, but more information on hierarchical clustering can be found in Module 5.5 Unsupervised Machine Learning II: Additional Clustering Applications. As we showed in the previous module, this function requires a wide dataframe which we’ll need to create. It will contain Chemical.Category, Biomass_Burn_Condition and Scaled_Chemical_Concentration. heatmap_df2 = scaled_longer_smoke_data %&gt;% group_by(Biomass_Burn_Condition, Chemical.Category) %&gt;% # using the summarize function instead of mutate function as was done previously since we only need the median values now summarize(Median_Scaled_Concentration = median(Scaled_Chemical_Concentration)) %&gt;% # transforming the data to a wide format pivot_wider(id_cols = Biomass_Burn_Condition, names_from = &quot;Chemical.Category&quot;, values_from = &quot;Median_Scaled_Concentration&quot;) %&gt;% # converting the chemical names to row names column_to_rownames(var = &quot;Biomass_Burn_Condition&quot;) head(heatmap_df2) ## Inorganics n-Alkanes Ions Levoglucosan ## Red Oak Smoldering -0.6236516 -0.3895328 -0.8011155 1.1042911 ## Eucalyptus Smoldering -0.6859508 -0.3465132 -0.8016019 1.7772753 ## Pine Smoldering -0.8012749 -0.3637526 -0.8016019 0.9961855 ## Pine Needles Smoldering -0.8436790 -0.3076779 -0.4611222 -0.6357051 ## Pine Needles Flaming 0.3640553 -0.3850613 0.8222903 -0.8570130 ## Red Oak Flaming 0.4858796 -0.3016948 0.6793662 -0.8642615 ## Methoxyphenols PAHs ## Red Oak Smoldering 0.04674277 -0.5179641 ## Eucalyptus Smoldering -0.06449444 -0.4724635 ## Pine Smoldering 2.74452541 -0.4580922 ## Pine Needles Smoldering -0.22835290 -0.3162278 ## Pine Needles Flaming -0.46331332 -0.4179505 ## Red Oak Flaming -0.51488738 -0.5318127 Now let’s generate the same heatmap this time using the pheatmap() function: # creating a color palette blue_pink_palette = colorRampPalette(c(rcolorbrewer_colors[5], rcolorbrewer_colors[6])) pheatmap(heatmap_df2, # changing the color scheme color = blue_pink_palette(40), # hierarchical clustering of the biomass burn conditions cluster_rows = TRUE, # creating white space between the two largest clusters cutree_row = 2, # adding the values for each cell and making those values black display_numbers = TRUE, number_color = &quot;black&quot;, # changing the font size and the angle of the column names fontsize = 12, angle_col = 45) By using incorporating the dendrogram into the visualization, it’s easier to see that the chemical profiles have greater similarities within incineration temperatures rather than biomasses (with the exception of pine needles smoldering). Creating Multi-Plot Figures We can combine figures using the plot_grid() function from the cowplot package. For additional information on the plot_grid() function and parameters that can be changed see Arranging Plots in a Grid. Other packages that have figure combining capabilities include the patchwork package and the grid_arrange() function from the gridExtra package. Figures can also be combined after they’re exported from R using other applications like MS powerpoint and Adobe pdf. FigureX = plot_grid(FigureX1, FigureX2, # Adding labels, changing size their size and position labels = &quot;AUTO&quot;, label_size = 15, label_x = 0.04, rel_widths = c(1, 1.5)) FigureX An appropriate title for this figure could be: “Figure X. Chemical concentration distributions across biomass burn conditions. (A) The boxplots are based on the scaled chemical concentration values, which used the raw chemical concentrations values scaled within each chemical. The individual dots represent the concentrations of each chemical. The biomass burn conditions on the y axis are ordered from greatest (top) to least (bottom) based on median scaled chemical concentration. (B) The heatmap visualizes concentrations across chemical categories. Chemical category on the x axis is ordered from highest to lowest median concentration from left to right. Biomass burn condition on the y axis is ordered from the highest to lowest median concentration from top to bottom. The values in each tile represent the median scaled chemical concentration. By putting these two figures side by side, it’s now easier to compare the distributions of each biomass burn condition in figure A alongside the median chemical category concentrations in figure B that are responsible for the variation seen on the left. Concluding Remarks In conclusion, this training module provided information and example code for improving, streamlining, and making ggplot2 figures publication ready. Keep in mind that concepts and ideas presented in this module can be subjective and might need to be amended given the situation, dataset, and visualization. Additional Resources Beginner’s Guide to Data Visualizations and Improving Data Visualizations in R Generating Colors for Visualizations Additional Hands on Training Brewer, Cynthia A. 1994. Color use guidelines for mapping and visualization. Chapter 7 (pp. 123-147) in Visualization in Modern Cartography Hattab, G., Rhyne, T.-M., &amp; Heider, D. (2020). Ten simple rules to colorize biological data visualization. PLOS Computational Biology, 16(10), e1008259. PMID: 33057327 Lastly, for researchers who are newer to R programming, ggpubr is a package specifically designed to create publication-ready graphs similar to ggplot2 with more concise syntax. This package is particularly useful for statistically relevant visualizations, which are further explored in later modules including, TAME 2.0 Module 3.4 Introduction to Statistical Tests, TAME 2.0 Module 4.4 Two Group Comparisons and Visualizations, and TAME 2.0 Module 4.5 Multigroup Comparisons and Visualizations. Test Your Knowledge Replicate the figure below! The heatmap is the same as the “Test Your Knowledge” figure from TAME 2.0 Module 3.1 Data Visualizations. This time we’ll focus on making the figure look more publication ready by cleaning up the titles, cleaning up the labels, and changing the colors. The heatmap still visualizes the median chemical concentrations, but this time we’re separating the burn conditions, allowing us to determine if the concentrations of chemicals released are contingent upon the burn condition. Hint: To view additional aspects of figures that can be changed in ggplot2 check out this GGPlot2 Cheat Sheet. It might come in handy! "],["normality-tests-and-data-transformations.html", "3.3 Normality Tests and Data Transformations Introduction to Training Module What is a Normal Distribution? Qualitative Assessment of Normality Quantitative Normality Assessment Data Transformation Additional Considerations Regarding Normality Concluding Remarks", " 3.3 Normality Tests and Data Transformations This training module was developed by Elise Hickman, Alexis Payton, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module When selecting the appropriate statistical tests to evaluate potential trends in your data, selection often relies upon whether or not underlying data are normally distributed. Many statistical tests and methods that are commonly implemented in exposure science, toxicology, and environmental health research rely on assumptions of normality. Applying a statistical test intended for data with a specific distribution when your data do not fit within that distribution can generate unreliable results, with the potential for false positive and false negative findings. Thus, one of the most common statistical tests to perform at the beginning of an analysis is a test for normality. In this training module, we will: Review the normal distribution and why it is important Demonstrate how to test whether your variable distributions are normal… Qualitatively, with histograms and Q-Q plots Quantitatively, with the Shapiro-Wilk test Discuss data transformation approaches Demonstrate log2 data transformation for non-normal data Discuss additional considerations related to normality We will demonstrate normality assessment using example data derived from a study in which chemical exposure profiles were collected across study participants through silicone wristbands. This exposure monitoring technique has been described through previous publications, including the following examples: O’Connell SG, Kincl LD, Anderson KA. Silicone wristbands as personal passive samplers. Environ Sci Technol. 2014 Mar 18;48(6):3327-35. doi: 10.1021/es405022f. Epub 2014 Feb 26. PMID: 24548134; PMCID: PMC3962070. Kile ML, Scott RP, O’Connell SG, Lipscomb S, MacDonald M, McClelland M, Anderson KA. Using silicone wristbands to evaluate preschool children’s exposure to flame retardants. Environ Res. 2016 May;147:365-72. doi: 10.1016/j.envres.2016.02.034. Epub 2016 Mar 3. PMID: 26945619; PMCID: PMC4821754. Hammel SC, Hoffman K, Phillips AL, Levasseur JL, Lorenzo AM, Webster TF, Stapleton HM. Comparing the Use of Silicone Wristbands, Hand Wipes, And Dust to Evaluate Children’s Exposure to Flame Retardants and Plasticizers. Environ Sci Technol. 2020 Apr 7;54(7):4484-4494. doi: 10.1021/acs.est.9b07909. Epub 2020 Mar 11. PMID: 32122123; PMCID: PMC7430043. Levasseur JL, Hammel SC, Hoffman K, Phillips AL, Zhang S, Ye X, Calafat AM, Webster TF, Stapleton HM. Young children’s exposure to phenols in the home: Associations between house dust, hand wipes, silicone wristbands, and urinary biomarkers. Environ Int. 2021 Feb;147:106317. doi: 10.1016/j.envint.2020.106317. Epub 2020 Dec 17. PMID: 33341585; PMCID: PMC7856225. In the current example dataset, chemical exposure profiles were obtained from the analysis of silicone wristbands worn by 97 participants for one week. Chemical concentrations on the wristbands were measured with gas chromatography mass spectrometry. The subset of chemical data used in this training module are all phthalates, a group of chemicals used primarily in plastic products to increase flexibility and durability. Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;openxlsx&quot;)) install.packages(&quot;openxlsx&quot;); if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); if (!requireNamespace(&quot;ggpubr&quot;)) install.packages(&quot;ggpubr&quot;); Loading R packages required for this session library(openxlsx) # for importing data library(tidyverse) # for manipulating and plotting data library(ggpubr) # for making Q-Q plots with ggplot Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example dataset # Import data wrist_data &lt;- read.xlsx(&quot;Module3_3_Input/Module3_3_InputData.xlsx&quot;) # Viewing the data head(wrist_data) ## S_ID Age DEP DBP BBP DEHA DEHP DEHT ## 1 1 24.76986 335.58857 574.5443 40.67286 755.8157 10621.7029 30420.68 ## 2 2 25.39452 56.38286 1075.7114 243.48857 2716.7314 3036.5757 23991.82 ## 3 3 34.55068 515.65429 121.1657 205.86857 3286.5886 3056.2743 46188.06 ## 4 4 23.83562 1009.00714 373.4957 66.97571 3966.5371 729.7971 17900.74 ## 5 5 39.29315 33.74143 104.0629 77.17286 1654.3317 2599.7129 13597.44 ## 6 6 36.15616 168.79714 503.8300 61.98429 398.6314 1492.6143 29875.76 ## DINP TOTM ## 1 26534.290 1447.86000 ## 2 10073.704 39.46143 ## 3 1842.949 112.67714 ## 4 78779.567 92.31000 ## 5 3682.956 161.84571 ## 6 23845.493 182.56429 Our example dataset contains subject IDs (S_ID), subject ages, and measurements of 8 different phthalates from silicone wristbands: DEP: Diethyl phthalate DBP : Dibutyl phthalate BBP : Butyl benzyl phthalate DEHA : Di(2-ethylhexyl) adipate DEHP : Di(2-ethylhexyl) phthalate DEHT: Di(2-ethylhexyl) terephthalate DINP : Diisononyl phthalate TOTM : Trioctyltrimellitate The units for the chemical data are nanogram of chemical per gram of silicone wristband (ng/g) per day the participant wore the wristband. One of the primary questions in this study was whether there were significant differences in chemical exposure between subjects with different levels of social stress or between subjects with differing demographic characteristics. However, before we can analyze the data for significant differences between groups, we first need to assess whether our numeric variables are normally distributed. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Are these data normally distributed? How does the distribution of data influence the statistical tests performed on the data? Before answering these questions, let’s define normality and how to test for it in R. What is a Normal Distribution? A normal distribution is a distribution of data in which values are distributed roughly symmetrically out from the mean such that 68.3% of values fall within one standard deviation of the mean, 95.4% of values fall within 2 standard deviations of the mean, and 99.7% of values fall within three standard deviations of the mean. Figure Credit: D Wells, CC BY-SA 4.0 https://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons Common parametric statistical tests, such as t-tests, one-way ANOVAs, and Pearson correlations, rely on the assumption that data fall within the normal distribution for calculation of z-scores and p-values. Non-parametric tests, such as the Wilcoxon Rank Sum test, Kruskal-Wallis test, and Spearman Rank correlation, do not rely on assumptions about data distribution. Some of the aforementioned between-group comparisons were introduced in TAME 2.0 Module 3.4 Introduction to Statistical Tests. They, along with non-parametric tests, are explored further in later modules including TAME 2.0 Module 4.4 Two-Group Comparisons &amp; Visualizations and TAME 2.0 Module 4.5 Multi-group Comparisons &amp; Visualizations. Qualitative Assessment of Normality We can begin by assessing the normality of our data through plots. For example, plotting data using histograms, densities, or Q-Q plots can graphically help inform if a variable’s values appear to be normally distributed or not. We will start with visualizing our data distributions with histograms. Histograms Let’s start with visualizing the distribution of the participant’s ages using the hist() function that is part of base R. hist(wrist_data$Age) We can edit some of the parameters to improve this basic histogram visualization. For example, we can decrease the size of each bin using the breaks parameter: hist(wrist_data$Age, breaks = 10) The hist() function is useful for plotting single distributions, but what if we have many variables that need normality assessment? We can leverage ggplot2’s powerful and flexible graphics functions such as geom_histogram() and facet_wrap() to inspect histograms of all of our variables in one figure panel. For more information on data manipulation in general, see TAME 2.0 Module 2.3 Data Manipulation &amp; Reshaping and for more on ggplot2 including the use of facet_wrap(), see TAME 2.0 Module 3.2 Improving Data Visualizations. First, we’ll pivot our data to longer to prepare for plotting. Then, we’ll make our plot. We can use the theme_set() function to set a default graphing theme for the rest of the script. A graphing theme represents a set of default formatting parameters (mostly colors) that ggplot will use to make your graphs. theme_bw() is a basic theme that includes a white background for the plot and dark grey axis text and minor axis lines. The theme that you use is a matter of personal preference. For more on the different themes available through ggplot2, see here. # Pivot data longer to prepare for plotting wrist_data_long &lt;- wrist_data %&gt;% pivot_longer(!S_ID, names_to = &quot;variable&quot;, values_to = &quot;value&quot;) # Set theme for graphing theme_set(theme_bw()) # Make figure panel of histograms ggplot(wrist_data_long, aes(value)) + geom_histogram(fill = &quot;gray40&quot;, color = &quot;black&quot;, binwidth = function(x) {(max(x) - min(x))/25}) + facet_wrap(~ variable, scales = &quot;free&quot;) + labs(y = &quot;# of Observations&quot;, x = &quot;Value&quot;) From these histograms, we can see that our chemical variables do not appear to be normally distributed. Q-Q Plots Q-Q (quantile-quantile) plots are another way to visually assess normality. Similar to the histogram above, we can create a single Q-Q plot for the age variable using base R functions. Normal Q-Q plots (Q-Q plots where the theoretical quantiles are based on a normal distribution) have theoretical quantiles on the x-axis and sample quantiles, representing the distribution of the variable of interest from the dataset, on the y-axis. If the variable of interest is normally distributed, the points on the graph will fall along the reference line. # Plot points qqnorm(wrist_data$Age) # Add a reference line for theoretically normally distributed data qqline(wrist_data$Age) Small variations from the reference line, as seen above, are to be expected for the most extreme values. Overall, we can see that the age data are relatively normally distributed, as the points fall along the reference line. To make a figure panel with Q-Q plots for all of our variables of interest, we can use the ggqqplot() function within the ggpubr package. This function generates Q-Q plots and has arguments that are similar to ggplot2. ggqqplot(wrist_data_long, x = &quot;value&quot;, facet.by = &quot;variable&quot;, ggtheme = theme_bw(), scales = &quot;free&quot;) With this figure panel, we can see that the chemical data have very noticeable deviations from the reference, suggesting non-normal distributions. To answer our first environmental health question, age is the only variable that appears to be normally distributed in our dataset. This is based on our histograms and Q-Q plots with data centered in the middle and spreading with a distribution on both the lower and upper sides that follow typical normal data distributions. However, chemical concentrations appear to be non-normally distributed. Next, we will implement a quantitative approach to assessing normality, based on a statistical test for normality. Quantitative Normality Assessment Single Variable Normality Assessment We will use the Shapiro-Wilk test to quantitatively assess whether our data distribution is normal, again looking at the age data. This test can be carried out simply using the shapiro.test() function from the base R stats package. When using this test and interpreting its results, it is important to remember that the null hypothesis is that the sample distribution is normal, and a significant p-value means the distribution is non-normal. shapiro.test(wrist_data$Age) ## ## Shapiro-Wilk normality test ## ## data: wrist_data$Age ## W = 0.9917, p-value = 0.8143 This test resulted in a p-value of 0.8143, so we cannot reject the null hypothesis (that data are normally distributed). This means that we can assume that age is normally distributed, which is consistent with our visualizations above. Multiple Variable Normality Assessment With a large dataset containing many variables of interest (e.g., our example data with multiple chemicals), it is more efficient to test each column for normality and then store those results in a dataframe. We can use the base R function apply() to apply the Shapiro Wilk test over all of the numeric columns of our dataframe. This function generates a list of results, with a list element for each variable tested. There are also other ways that you could iterate through each of your columns, such as a for loop or a function as discussed in TAME 2.0 Module 2.4 Improving Coding Efficiencies. # Apply Shapiro Wilk test shapiro_res &lt;- apply(wrist_data %&gt;% select(-S_ID), 2, shapiro.test) # View first three list elements glimpse(shapiro_res[1:3]) ## List of 3 ## $ Age:List of 4 ## ..$ statistic: Named num 0.992 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;W&quot; ## ..$ p.value : num 0.814 ## ..$ method : chr &quot;Shapiro-Wilk normality test&quot; ## ..$ data.name: chr &quot;newX[, i]&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;htest&quot; ## $ DEP:List of 4 ## ..$ statistic: Named num 0.225 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;W&quot; ## ..$ p.value : num 2.74e-20 ## ..$ method : chr &quot;Shapiro-Wilk normality test&quot; ## ..$ data.name: chr &quot;newX[, i]&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;htest&quot; ## $ DBP:List of 4 ## ..$ statistic: Named num 0.658 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;W&quot; ## ..$ p.value : num 1.08e-13 ## ..$ method : chr &quot;Shapiro-Wilk normality test&quot; ## ..$ data.name: chr &quot;newX[, i]&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;htest&quot; We can then convert those list results into a dataframe. Each variable is now in a row, with columns describing outputs of the statistical test. # Create results dataframe shapiro_res &lt;- do.call(rbind.data.frame, shapiro_res) # View results dataframe shapiro_res ## statistic p.value method data.name ## Age 0.9917029 8.143367e-01 Shapiro-Wilk normality test newX[, i] ## DEP 0.2248611 2.736536e-20 Shapiro-Wilk normality test newX[, i] ## DBP 0.6584967 1.076529e-13 Shapiro-Wilk normality test newX[, i] ## BBP 0.2367689 3.757059e-20 Shapiro-Wilk normality test newX[, i] ## DEHA 0.6646692 1.454576e-13 Shapiro-Wilk normality test newX[, i] ## DEHP 0.6163531 1.519572e-14 Shapiro-Wilk normality test newX[, i] ## DEHT 0.8072684 6.315917e-10 Shapiro-Wilk normality test newX[, i] ## DINP 0.5741864 2.486638e-15 Shapiro-Wilk normality test newX[, i] ## TOTM 0.3397424 6.901903e-19 Shapiro-Wilk normality test newX[, i] Finally, we can clean up our results dataframe and add a column that will quickly tell us whether our variables are normally or non-normally distributed based on the Shapiro-Wilk normality test results. # Clean dataframe shapiro_res &lt;- shapiro_res %&gt;% # Add normality conclusion mutate(normal = ifelse(p.value &lt; 0.05, F, T)) %&gt;% # Remove columns that do not contain informative data select(c(p.value, normal)) # View cleaned up dataframe shapiro_res ## p.value normal ## Age 8.143367e-01 TRUE ## DEP 2.736536e-20 FALSE ## DBP 1.076529e-13 FALSE ## BBP 3.757059e-20 FALSE ## DEHA 1.454576e-13 FALSE ## DEHP 1.519572e-14 FALSE ## DEHT 6.315917e-10 FALSE ## DINP 2.486638e-15 FALSE ## TOTM 6.901903e-19 FALSE The results from the Shapiro-Wilk test demonstrate that age data are normally distributed, while the chemical concentration data are non-normally distributed. These results support the conclusions we made based on our qualitative assessment above with histograms and Q-Q plots. Answer to Environmental Health Question 1 With this, we can now answer Environmental Health Question #1: Are these data normally distributed? Answer: Age is normally distributed, while chemical concentrates are non-normally distributed. Answer to Environmental Health Question 2 We can also answer Environmental Health Question #2: How does the distribution of data influence the statistical tests performed on the data? Answer: Parametric statistical tests should be used when analyzing the age data, and non-parametric tests should be used when analyzing the chemical concentration data Data Transformation There are a number of approaches that can be used to change the range and/or distribution of values within each variable. Typically, the purpose for applying these changes is to reduce bias in a dataset, remove known sources of variation, or prepare data for specific downstream analyses. The following are general definitions for common terms used when discussing these changes: Transformation refers to any process used to change data into other, related values. Normalization and standardization are types of data transformation. Transformation can also refer to performing the same mathematical operation on every value in your dataframe. For example, taking the log2 or log10 of every value is referred to as log transformation. Normalization is the process of transforming variables so that they are on a similar scale and therefore are comparable. This can be important when variables in a dataset contain a mixture of data types that are represented by vastly different numeric magnitudes or when there are known sources of variability across samples. Normalization methods are highly dependent on the type of input data. One example of normalization is min-max scaling, which results in a range for each variable of 0 to 1. Although normalization in computational methodologies typically refers to min-max scaling or other similar methods where the variable’s range is bounded by specific values, wet-bench approaches also employ normalization - for example, using a reference gene for RT-qPCR assays or dividing a total protein amount for each sample by the volume of each sample to obtain a concentration. Standardization, also known as Z-score normalization, is a specific type of normalization that involves subtracting each value from the mean of that variable and dividing by that variable’s standard deviation. The standardized values for each variable will have a mean of 0 and a standard deviation of 1. The scale() function in R performs standardization by default when the data are centered (argument center = TRUE is included within the scale function). Transformation of example data When data are non-normally distributed, such as with the chemical concentrations in our example dataset, it may be desirable to transform the data so that the distribution becomes closer to a normal distribution, particularly if there are only parametric tests available to test your hypothesis. A common transformation used in environmental health research is log2 transformation, in which data are transformed by taking the log2 of each value in the dataframe. Let’s log2 transform our chemical data and examine the resulting histograms and Q-Q plots to qualitatively assess whether data appear more normal following transformation. We will apply a pseudo-log2 transformation, where we will add 1 to each value before log2 transforming so that all resulting values are positive and any zeroes in the dataframe do not return -Inf. # Apply psuedo log2 (pslog2) transformation to chemical data wrist_data_pslog2 &lt;- wrist_data %&gt;% mutate(across(DEP:TOTM, ~ log2(.x + 1))) # Pivot data longer wrist_data_pslog2_long &lt;- wrist_data_pslog2 %&gt;% pivot_longer(!S_ID, names_to = &quot;variable&quot;, values_to = &quot;value&quot;) # Make figure panel of histograms ggplot(wrist_data_pslog2_long, aes(value)) + geom_histogram(fill = &quot;gray40&quot;, color = &quot;black&quot;, binwidth = function(x) {(max(x) - min(x))/25}) + facet_wrap(~ variable, scales = &quot;free&quot;) + labs(y = &quot;# of Observations&quot;, x = &quot;Value&quot;) # Make a figure panel of Q-Q plots ggqqplot(wrist_data_pslog2_long, x = &quot;value&quot;, facet.by = &quot;variable&quot;, ggtheme = theme_bw(), scales = &quot;free&quot;) Both the histograms and the Q-Q plots demonstrate that our log2 transformed data are more normally distributed than the raw data graphed above. Let’s apply the Shapiro-Wilk test to our log2 transformed data to determine if the chemical distributions are normally distributed. # Apply Shapiro Wilk test shapiro_res_pslog2 &lt;- apply(wrist_data_pslog2 %&gt;% select(-S_ID), 2, shapiro.test) # Create results dataframe shapiro_res_pslog2 &lt;- do.call(rbind.data.frame, shapiro_res_pslog2) # Clean dataframe shapiro_res_pslog2 &lt;- shapiro_res_pslog2 %&gt;% ## Add normality conclusion mutate(normal = ifelse(p.value &lt; 0.05, F, T)) %&gt;% ## Remove columns that do not contain informative data select(c(p.value, normal)) # View cleaned up dataframe shapiro_res_pslog2 ## p.value normal ## Age 0.814336705 TRUE ## DEP 0.001335217 FALSE ## DBP 0.368954224 TRUE ## BBP 0.052805523 TRUE ## DEHA 0.979072298 TRUE ## DEHP 0.304963678 TRUE ## DEHT 0.770066136 TRUE ## DINP 0.883662530 TRUE ## TOTM 0.004399442 FALSE The results from the Shapiro-Wilk test demonstrate that the the log2 chemical concentration data are more normally distributed than the raw data. Overall, the p-values, even for the chemicals that are still non-normally distributed, are much higher, and only 2 out of the 8 chemicals are non-normally distributed by the Shapiro-Wilk test. We can also calculate average p-values across all variables for our raw and log2 transformed data to further demonstrate this point. # Calculate the mean Shapiro-Wilk p-value for the raw chemical data mean(shapiro_res$p.value) ## [1] 0.09048186 # Calculate the mean Shapiro-Wilk p-value for the pslog2 transformed chemical data mean(shapiro_res_pslog2$p.value) ## [1] 0.4643995 Therefore, the log2 chemical data would be most appropriate to use if researchers are wanting to perform parametric statistical testing (and particularly if there is not a non-parametric statistical test for a given experimental design). It is important to note that if you proceed to statistical testing using log2 or other transformed data, graphs you make of significant results should use the transformed values on the y-axis, and findings should be interpreted in the context of the transformed values. Additional Considerations Regarding Normality The following sections detail additional considerations regarding normality. Similar to other advice in TAME, appropriate methods for handling normality assessment and normal versus non-normal data can be dependent on your field, lab, endpoints of interest, and downstream analyses. We encourage you to take those elements of your study into account, alongside the guidance provided here, when assessing normality. Regardless of the specific steps you take, be sure to report normality assessment steps and the data transformation or statistical test decisions you make based on them in your final report or manuscript. Determining which data should go through normality testing: Values for all samples (rows) that will be going into statistical testing should be tested for normality. If you are only going to be statistically testing a subset of your data, perform the normality test on that subset. Another way to think of this is that data points that are on the same graph together and/or that have been used as input for a statistical test should be tested for normality together. Analyzing datasets with a mixture of normally and non-normally distributed variables: There are a couple of different routes you can pursue if you have a mixture of normally and non-normally distributed variables in your dataframe: Perform parametric statistical tests on the normally distributed variables and non-parametric tests on the non-normally distributed variable. Perform the statistical test across all variables that fits with the majority of the variable distributions in your dataset. Our preference is to perform one test across all variables of the same data type/endpoint (e.g., all chemical concentrations, all cytokine concentrations). Aim to choose an approach that fits best rather than perfectly. Improving efficiency for normality assessment: If you find yourself frequently performing the same normality assessment workflow, consider writing a function that will execute each normality testing step (making a histogram, making a Q-Q plot, determining Shapiro-Wilk normality variable by variable, and determining the average Shapiro-Wilk p-value across all variables) and store the results in a list for easy inspection. Concluding Remarks In conclusion, this training module serves as an introduction to and step by step tutorial for normality assessment and data transformations. Approaches described in this training module include visualizations to qualitatively assess normality, statistical tests to quantitatively assess normality, data transformation, and other distribution considerations relating to normality. These methods are an important step in data characterization and exploration prior to downstream analyses and statistical testing, and they can be applied to nearly all studies carried out in environmental health research. Additional Resources Descriptive Statistics and Normality Tests for Statistical Data STHDA Normality Test in R Normalization vs. Standardization Test Your Knowledge Use the input file provided (“Module3_3_TYKInput.xlsx”), which represents a similar dataset to the one used in the module, to answer the following questions: Are any variables normally distributed in the raw data? Does psuedo log2 transforming the values make the distributions overall more or less normally distributed? What are the average Shapiro-Wilk p-values for the raw and psuedo log2 transformed data? "],["intoduction-to-statistical-tests.html", "3.4 Intoduction to Statistical Tests Introduction to Training Module Assessing Normality &amp; Homogeneity of Variance Two-Group Visualizations and Statistical Comparisons using the T-Test Three-Group Visualizations and Statistical Comparisons using an ANOVA Regression Modeling and Visualization: Linear and Logistic Regressions Statistical Evaluations of Categorical Data using the Chi-Squared Test and Fisher’s Exact Test Concluding Remarks", " 3.4 Intoduction to Statistical Tests This training module was developed by Alexis Payton, Kyle Roell, Elise Hickman, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module This training module provides a brief introduction to some of the most commonly implemented statistics and associated visualizations used in exposure science, toxicology, and environmental health studies. This module first uploads an example dataset that is similar to the data used in TAME 2.0 Module 2.3 Data Manipulation &amp; Reshaping], though it includes some expanded subject information data to allow for more example statistical tests. Then, methods to evaluate data normality are presented, including visualization-based and statistical-based approaches. Basic statistical tests discussed in this module include: T test Analysis of Variance (ANOVA) with a Tukey’s Post-Hoc test Regression Modeling (Linear and Logistic) Chi-squared test Fisher’s exact test These statistical tests are very simple, with more extensive examples and associated descriptions of statistical models in the proceeding applications-based training modules in: TAME 2.0 Module 4.4 Two-Group Comparisons &amp; Visualizations TAME 2.0 Module 4.5 Multi-Group Comparisons &amp; Visualizations TAME 2.0 Module 4.6 Advanced Multi-Group Comparisons &amp; Visualizations Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); if (!requireNamespace(&quot;car&quot;)) install.packages(&quot;car&quot;); if (!requireNamespace(&quot;ggpubr&quot;)) install.packages(&quot;ggpubr&quot;); if(!requireNamespace(&quot;effects&quot;)) install.packages(&quot;effects&quot;); Loading R packages required for this session library(tidyverse) # all tidyverse packages, including dplyr and ggplot2 library(car) # package for statistical tests library(ggpubr) # ggplot2 based plots library(effects) # for linear modeling Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example datasets Let’s read in our example dataset. Note that these data are similar to those used previously, except that demographic and chemical measurement data were previously merged, and a few additional columns of subject information/demographics were added to serve as more thorough examples of data for use in this training module. # Loading data full.data &lt;- read.csv(&quot;Module3_4_Input/Module3_4_InputData.csv&quot;) Let’s view the top of the first 9 columns of data in this dataframe: full.data[1:10,1:9] ## ID BMI BMIcat MAge MEdu BW GA Smoker Smoker3 ## 1 1 27.7 Overweight 22.99928 College_Degree 3180.058 34 NS Never ## 2 2 26.8 Overweight 30.05142 College_Degree 3210.823 43 S Never ## 3 3 33.2 Overweight 28.04660 College_Degree 3311.551 40 NS Never ## 4 4 30.1 Overweight 34.81796 College_Degree 3266.844 32 S Never ## 5 5 37.4 Overweight 42.68440 College_Degree 3664.088 35 NS Never ## 6 6 33.3 Overweight 24.94960 College_Degree 3328.988 40 NS Never ## 7 7 24.8 Overweight 29.54798 College_Degree 3061.949 30 NS Never ## 8 8 16.9 Underweight 24.94954 College_Degree 3332.539 38 NS Current ## 9 9 36.9 Overweight 33.58589 College_Degree 3260.482 39 NS Never ## 10 10 21.7 Normal 39.29018 College_Degree 3141.723 35 NS Current These represent the subject information/demographic data, which include the following columns: ID: subject number BMI: body mass index BMIcat: BMI &lt;= 18.5 binned as “Underweight”, 18.5 &lt; BMI &lt;= 24.5 binned as “Normal”, BMI &gt; 24.5 binned as “Overweight” MAge: maternal age in years MEdu: maternal education level; “No_HS_Degree” = “less than high school”, “No_College_Degree” = “high school or some college”, “College_Degree” = “college or greater” BW: body weight in grams GA: gestational age in weeks Smoker: “NS” = non-smoker, “S” = smoker Smoker3: “Never”, “Former”, or “Current” smoking status Let’s now view the remaining columns (columns 10-15) in this dataframe: full.data[1:10,10:15] ## DWAs DWCd DWCr UAs UCd UCr ## 1 6.426464 1.292941 51.67987 10.192695 0.7537104 42.60187 ## 2 7.832384 1.798535 50.10409 11.815088 0.9789506 41.30757 ## 3 7.516569 1.288461 48.74001 10.079057 0.1903262 36.47716 ## 4 5.906656 2.075259 50.92745 8.719123 0.9364825 42.47987 ## 5 7.181873 2.762643 55.16882 9.436559 1.4977829 47.78528 ## 6 9.723429 3.054057 51.14812 11.589403 1.6645837 38.26386 ## 7 6.268547 1.218410 52.08578 8.887948 0.6347667 39.45535 ## 8 6.718448 1.414975 54.96740 9.304968 0.6658849 45.09987 ## 9 9.074928 2.727755 55.72826 10.818153 1.6585757 42.58577 ## 10 5.771691 2.410993 47.06552 8.747217 1.7354305 34.80661 These columns represent the environmental exposure measures, including: DWAs: drinking water arsenic levels in µg/L DWCd: drinking water cadmium levels in µg/L DWCr: drinking water chromium levels in µg/L UAs: urinary arsenic levels in µg/L UCd: urinary cadmium levels in µg/L UCr: urinary chromium levels in µg/L Now that the script is prepared and the data are uploaded, we can start by asking some initial questions about the data that can be answered by running some basic statistical tests and visualizations. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Are there statistically significant differences in BMI between non-smokers and smokers? Are there statistically significant differences in BMI between current, former, and never smokers? Is there a relationship between maternal BMI and birth weight? Are maternal age and gestational age considered potential covariates in the relationship between maternal BMI and birth weight? Are there statistically significant differences in gestational age based on whether a subject is a non-smoker or a smoker? Is there a relationship between smoking status and BMI? Assessing Normality &amp; Homogeneity of Variance Statistical test selection often relies upon whether or not the underlying data are normally distributed and that variance across the groups is the same (homogeneity of variances). Many statistical tests and methods that are commonly implemented in exposure science, toxicology, and environmental health research rely on assumptions of normality. Thus, one of the most common statistical tests to perform at the beginning of an analysis is a test for normality. As discussed in the previous module, there are a few ways to evaluate the normality of a dataset: First, you can visually gauge whether a dataset appears to be normally distributed through plots. For example, plotting data using histograms, densities, or Q-Q plots can graphically help inform if a variable’s values appear to be normally distributed or not. Second, you can evaluate normality using statistical tests, such as the Kolmogorov-Smirnov (K-S) test and Shapiro-Wilk test. When using these tests and interpreting their results, it is important to remember that the null hypothesis is that the sample distribution is normal, and a significant p-value means the distribution is non-normal. Let’s start with the first approach based on data visualizations. In this module, we’ll primarily be generating figures using the ggubr package which is specifically designed to generate ggplot2-based figures using more streamlined coding syntax. In addition, this package has statistical parameters for plotting that are useful for basic statistical analysis, especially for people with introductory experience to plotting in R. For further documentation on ggubr, click here. Let’s begin with a histogram to view the distribution of BMI data using the gghistogram() function from the ggubr package: gghistogram(data = full.data, x = &quot;BMI&quot;, bins = 20) Let’s also view the Q–Q (quantile-quantile) plot using the ggqqplot() function also from the ggubr package: ggqqplot(full.data$BMI, ylab = &quot;BMI&quot;) From these visualizations, the BMI variable appears to be normally distributed, with data centered in the middle and spreading with a distribution on both the lower and upper sides that follow typical normal data distributions. Let’s now implement the second approach based on statistical tests for normality. Here, let’s use the Shapiro-Wilk test as an example, again looking at the BMI data. shapiro.test(full.data$BMI) ## ## Shapiro-Wilk normality test ## ## data: full.data$BMI ## W = 0.99232, p-value = 0.3773 This test resulted in a p-value of 0.3773, so we cannot reject the null hypothesis (that the BMI data are normally distributed). These findings support the assumption that these data are normally distributed. Next, we’ll assess homogeneity of variance using the Levene’s test. This will be done using the leveneTest()function from the car package: # First convert the smoker variable to a factor full.data$Smoker = factor(full.data$Smoker, levels = c(&quot;NS&quot;, &quot;S&quot;)) leveneTest(BMI ~ Smoker, data = full.data) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 0.2631 0.6086 ## 198 The p value, (Pr&gt;F), is 0.6086 indicating that variance in BMI across the smoking groups is the same. Therefore, the assumptions of a t-test, including normality and homogeneity of variance, have been met. Two-Group Visualizations and Statistical Comparisons using the T-Test T-tests are commonly used to test for a significant difference between the means of two groups in normally distributed data. In this example, we will be answering Environmental Health Question 1: Are there statistically significant differences in BMI between non-smokers and smokers? We will specifically implement a two sample t-test (or independent samples t-test). Let’s first visualize the BMI data across these two groups using boxplots: ggboxplot(data = full.data, x = &quot;Smoker&quot;, y = &quot;BMI&quot;) From this plot, it looks like non-smokers (labeled “NS”) may have significantly higher BMI than smokers (labeled “S”), though we need statistical evaluation of these data to more thoroughly evaluate this potential data trend. It is easy to perform a t-test on these data using the t.test() function from the base R stats package: t.test(data = full.data, BMI ~ Smoker) ## ## Welch Two Sample t-test ## ## data: BMI by Smoker ## t = 2.5372, df = 80.362, p-value = 0.01311 ## alternative hypothesis: true difference in means between group NS and group S is not equal to 0 ## 95 percent confidence interval: ## 0.583061 4.823447 ## sample estimates: ## mean in group NS mean in group S ## 26.11176 23.40851 Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question #1: Are there statistically significant differences in BMI between non-smokers and smokers? Answer: From this statistical output, we can see that the overall mean BMI in non-smokers (group “NS”) is 26.1, and the overall mean BMI in smokers (group “S”) is 23.4. We can also see that the resulting p-value comparison between the means of these two groups is, indeed, significant (p-value = 0.013), meaning that the means between these groups are significantly different (i.e., are not equal). It’s also helpful to save these results into a variable within the R global environment, which then allows us to access specific output values and extract them more easily for our records. For example, we can run the following to specifically extract the resulting p-value from this test: ttest.res &lt;- t.test(data = full.data, BMI ~ Smoker) # making a list in the R global environment with the statistical results signif(ttest.res$p.value, 2) # pulling the p-value and using the `signif` function to round to 2 significant figures ## [1] 0.013 Three-Group Visualizations and Statistical Comparisons using an ANOVA Analysis of Variance (ANOVA) is a statistical method that can be used to compare means across three or more groups in normally distributed data. To demonstrate an ANOVA test on this dataset, let’s answer Environmental Health Question 2: Are there statistically significant differences in BMI between current, former, and never smokers? To do this we’ll use the Smoker3 variable from our dataset. Let’s again start by viewing these data distributions using a boxplot: ggboxplot(data = full.data, x = &quot;Smoker3&quot;, y = &quot;BMI&quot;) From this cursory review of the data, it looks like the current smokers likely demonstrate significantly different BMI measures than the former and never smokers, though we need statistical tests to verify this potential trend. We also require statistical tests to evaluate potential differences (or lack of differences) between former and never smokers. Let’s now run the ANOVA to compare BMI between smoking groups, using the aov() function to fit an ANOVA model: smoker_anova = aov(data = full.data, BMI ~ Smoker3) smoker_anova ## Call: ## aov(formula = BMI ~ Smoker3, data = full.data) ## ## Terms: ## Smoker3 Residuals ## Sum of Squares 2046.713 6817.786 ## Deg. of Freedom 2 197 ## ## Residual standard error: 5.882861 ## Estimated effects may be unbalanced We need to extract the typical ANOVA results table using either the summary() or anova() function on the resulting fitted object: anova(smoker_anova) ## Analysis of Variance Table ## ## Response: BMI ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Smoker3 2 2046.7 1023.36 29.57 5.888e-12 *** ## Residuals 197 6817.8 34.61 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This table outputs a lot of information, including the F value referring to the resulting F-statistic, Pr(&gt;F) referring to the p-value of the F-statistic, and other values that are described in detail through other available resources including this helpful video through PennState’s statistics online resources. Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: Are there statistically significant differences in BMI between current, former, never smokers? Answer: From this ANOVA output table, we can conclude that the group means across all three groups are not equal given that the p value, written as Pr(&gt;F) is significant (p value = 5.88 x 10-12). However, it doesn’t tell us which groups differ from each other and that’s where post hoc tests like Tukey’s are useful. Let’s run a Tukey’s post hoc test using the TukeyHSD() function in base R to determine which of the current, former, and never smokers have significant differences in BMI: smoker_tukey = TukeyHSD(smoker_anova) smoker_tukey ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = BMI ~ Smoker3, data = full.data) ## ## $Smoker3 ## diff lwr upr p adj ## Former-Current 7.4436765 4.203006 10.684347 0.0000005 ## Never-Current 8.1167857 5.595470 10.638102 0.0000000 ## Never-Former 0.6731092 -2.011764 3.357982 0.8245448 Although the above Tukey object contains a column p adj, those are the raw unadjusted p values. It is common practice to adjust p values from multiple comparisons to prevent the reporting of false positives or reporting of a significant difference that doesn’t actually exist (Feise, 2002). There are a couple of different methods that are used to adjust p values including the Bonferroni and the Benjamini &amp; Hochberg approaches. For this example, we’ll use the p.adjust() function to obtain the Benjamini &amp; Hochberg adjusted p values. Check out the associated RDocumentation to discover other methods that can be used to adjust p values using the p.adjust() function: # First converting the Tukey object into a dataframe smoker_tukey_df = data.frame(smoker_tukey$Smoker3) %&gt;% # renaming the `p adj` to `P Value` for clarity rename(`P Value` = p.adj) # Adding a column with the adjusted p values smoker_tukey_df$`P Adj` = p.adjust(smoker_tukey_df$`P Value`, method = &quot;fdr&quot;) smoker_tukey_df ## diff lwr upr P Value P Adj ## Former-Current 7.4436765 4.203006 10.684347 5.064863e-07 7.597295e-07 ## Never-Current 8.1167857 5.595470 10.638102 3.432921e-12 1.029876e-11 ## Never-Former 0.6731092 -2.011764 3.357982 8.245448e-01 8.245448e-01 Answer to Environmental Health Question 2 We can use this additional information to further answer Environmental Health Question #2: Are there statistically significant differences in BMI between current, former, and never smokers? Answer: Current smokers have significantly lower BMIs than people who have never smoked and people who have formerly smoked. This is made evident by the 95% confidence intervals (lwr and upr) that don’t cross 0 and the p values that are less than 0.05 even after adjusting. Regression Modeling and Visualization: Linear and Logistic Regressions Regression modeling aims to find a relationship between a dependent variable (or outcome, response, y) and an independent variable (or predictor, explanatory variable, x). There are many forms of regression analysis, but here we will focus on two: linear regression and logistic regression. In brief, linear regression is generally used when you have a continuous dependent variable and there is assumed to be some sort of linear relationship between the dependent and independent variables. Conversely, logistic regression is often used when the dependent variable is dichotomous. Let’s first run through an example linear regression model to answer Environmental Health Question 3: Is there a relationship between maternal BMI and birth weight? Linear Regression We will first visualize the data and a run simple correlation analysis to evaluate whether these data are generally correlated. Then, we will run a linear regression to evaluate the relationship between these variables in more detail. Plotting the variables against one another and adding a linear regression line using the function ggscatter() from the ggubr package: ggscatter(full.data, x = &quot;BMI&quot;, y = &quot;BW&quot;, # Adding a linear line with 95% condfidence intervals as the shaded region add = &quot;reg.line&quot;, conf.int = TRUE, # Customize reg. line add.params = list(color = &quot;blue&quot;, fill = &quot;lightgray&quot;), # Adding Pearson&#39;s correlation coefficient cor.coef = TRUE, cor.method = &quot;pearson&quot;, cor.coeff.args = list(label.sep = &quot;\\n&quot;)) We can also run a basic correlation analysis between these two variables using the cor.test() function. This function uses the Pearson’s correlation test as default, which we can implement here due to the previously discussed assumption of normality for this dataset. Note that other tests are needed in instances when data are not normally distributed (e.g., Spearman Rank). This function is used here to extract the Pearson’s correlation coefficient and p-value (which also appear above in the upper left corner of the graph): cor.res &lt;- cor.test(full.data$BW, full.data$BMI) signif(cor.res$estimate, 2) ## cor ## 0.25 signif(cor.res$p.value, 2) ## [1] 0.00039 Together, it looks like there may be an association between BW and BMI, based on these correlation results, demonstrating a significant p-value of 0.0004. To test this further, let’s run a linear regression analysis using the lm() function, using BMI (X) as the independent variable and BW as the dependent variable (Y): crude_lm &lt;- lm(data = full.data, BW ~ BMI) summary(crude_lm) # viewing the results summary ## ## Call: ## lm(formula = BW ~ BMI, data = full.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -598.39 -116.72 8.11 136.54 490.11 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3069.201 52.576 58.38 &lt; 2e-16 *** ## BMI 7.208 1.997 3.61 0.000388 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 188 on 198 degrees of freedom ## Multiple R-squared: 0.06176, Adjusted R-squared: 0.05702 ## F-statistic: 13.03 on 1 and 198 DF, p-value: 0.0003876 Answer to Environmental Health Question 3 With this, we can answer Environmental Health Question #3: Is there a relationship between maternal BMI and birth weight? Answer: Not only is there a slight positive correlation between maternal BMI and BW as indicated by ~0.25 correlation coefficient, this linear relationship is significant due to the p-value being ~0.0004. Additionally, we can derive confidence intervals for the BMI estimate using: confint(crude_lm)[&quot;BMI&quot;,] ## 2.5 % 97.5 % ## 3.270873 11.145740 Notice that the r-squared (R2) value in regression output is the squared value of the previously calculated correlation coefficient (R). signif(sqrt(summary(crude_lm)$r.squared), 2) ## [1] 0.25 In epidemiological studies, the potential influence of confounders is considered by including important covariates within the final regression model. Let’s go ahead and investigate Environmental Health Question 4: Are maternal age and gestational age considered potential covariates in the relationship between maternal BMI and birth weight? We can do that by adding those variables to the linear model. adjusted_lm = lm(data = full.data, BW ~ BMI + MAge + GA) summary(adjusted_lm) ## ## Call: ## lm(formula = BW ~ BMI + MAge + GA, data = full.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -454.04 -111.24 5.79 116.46 488.41 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2247.995 117.946 19.060 &lt; 2e-16 *** ## BMI 6.237 1.774 3.515 0.000547 *** ## MAge 4.269 1.887 2.263 0.024752 * ## GA 19.612 2.656 7.385 4.28e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 165.5 on 196 degrees of freedom ## Multiple R-squared: 0.2805, Adjusted R-squared: 0.2695 ## F-statistic: 25.47 on 3 and 196 DF, p-value: 5.884e-14 Let’s further visualize these regression modeling results by adding a regression line to the original scatterplot. Before doing so, we’ll use the effect() function from the effects package to make estimated predictions of birth weight values for the crude and adjusted linear models. The crude model only has BMI as the dependent variable, while the adjusted model includes BMI, maternal age, and gestational age as dependent variables. This function creates a table that contains 5 columns: fitted values for BMI (BMI), predictor values (fit), standard errors of the predictions (se), lower confidence limits (lower), and upper confidence limits (upper). An additional column, Model, was added to specify whether the values correspond to the crude or adjusted model. For additional information on visualizing adjusted linear models, see Plotting Adjusted Associations in R. crude_lm_predtable = data.frame(effect(term = &quot;BMI&quot;, mod = crude_lm), Model = &quot;Crude&quot;) adjusted_lm_predtable = data.frame(effect(term = &quot;BMI&quot;, mod = adjusted_lm), Model = &quot;Adjusted&quot;) # Viewing one of the tables crude_lm_predtable ## BMI fit se lower upper Model ## 1 10 3141.284 33.63898 3074.948 3207.621 Crude ## 2 19 3206.159 18.54497 3169.588 3242.730 Crude ## 3 28 3271.034 14.21563 3243.000 3299.067 Crude ## 4 36 3328.700 24.86346 3279.669 3377.732 Crude ## 5 45 3393.575 41.18575 3312.356 3474.794 Crude Now we can plot each linear model and their corresponding 95% confidence intervals (CI). It’s easier to visualize this using ggplot2 instead of ggubr so that’s what we’ll use: options(repr.plot.width=9, repr.plot.height=6) # changing dimensions of the entire figure ggplot(full.data, aes(x = BMI, y = BW)) + geom_point() + # Crude line geom_line(data = crude_lm_predtable, mapping = aes(x = BMI, y = fit, color = Model)) + # Adjusted line geom_line(data = adjusted_lm_predtable, mapping = aes(x = BMI, y = fit, color = Model)) + # Crude 95% CI geom_ribbon(data = crude_lm_predtable, mapping = aes(x = BMI, y = fit, ymin = lower, ymax = upper, fill = Model), alpha = 0.25) + # Adjusted 95% CI geom_ribbon(data = adjusted_lm_predtable, mapping = aes(x = BMI, y = fit, ymin = lower, ymax = upper, fill = Model), alpha = 0.25) Answer to Environmental Health Question 4 With this, we can answer Environmental Health Question #4: Are maternal age and gestational age considered potential covariates in the relationship between maternal BMI and birth weight? Answer: BMI is still significantly associated with BW and the included covariates are also shown to be significantly related to birth weight in this model. However, the addition of gestational age and maternal age did not have much of an impact on modifying the relationship between BMI and birth weight. Logistic Regression To carry out a logistic regression, we need to evaluate one continuous variable (here, we select gestational age, using the GA variable) and one dichotomous variable (here, we select smoking status, using the Smoker variable) to evaluate Environmental Health Question 5: Are there statistically significant differences in gestational age based on whether a subject is a non-smoker or a smoker? Because smoking status is a dichotomous variable, we will use logistic regression to look at this relationship. Let’s first visualize these data using a stacked bar plot for the dichotomous smoker dataset: ggboxplot(data = full.data, x = &quot;Smoker&quot;, y = &quot;GA&quot;) With this visualization, it’s difficult to tell whether or not there are significant differences in maternal education based on smoking status. Let’s now run the statistical analysis, using logistic regression modeling: # Before running the model, &quot;Smoker&quot;, needs to be binarized to 0&#39;s or 1&#39;s for the glm function glm_data = full.data %&gt;% mutate(Smoker = ifelse(Smoker == &quot;NS&quot;, 0,1)) # Use GLM (generalized linear model) and specify the family as binomial # This tells GLM to run a logistic regression log.res = glm(Smoker ~ GA, family = &quot;binomial&quot;, data = glm_data) summary(log.res) # viewing the results ## ## Call: ## glm(formula = Smoker ~ GA, family = &quot;binomial&quot;, data = glm_data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.26669 1.37042 0.924 0.3553 ## GA -0.06764 0.03796 -1.782 0.0747 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 218.10 on 199 degrees of freedom ## Residual deviance: 214.89 on 198 degrees of freedom ## AIC: 218.89 ## ## Number of Fisher Scoring iterations: 4 Similar to the regression modeling analysis, we can also derive confidence intervals: confint(log.res)[&quot;GA&quot;,] ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## -0.14301103 0.00640128 Answer to Environmental Health Question 5 With this, we can answer Environmental Health Question #5: Are there statistically significant differences in maternal education level based on whether they are a non-smoker or a smoker? Answer: Collectively, these results show a non-significant p-value relating gestational age to smoking status. The confidence intervals also overlap across zero. Therefore, these data do not demonstrate a significant association between gestational age and smoking status. Statistical Evaluations of Categorical Data using the Chi-Squared Test and Fisher’s Exact Test Chi-squared test and Fisher’s exact tests are used primarily when evaluating data distributions between two categorical variables. The difference between a Chi-squared test and the Fisher’s exact test surrounds the specific procedure being run. The Chi-squared test is an approximation and is run with larger sample sizes to determine whether there is a statistically significant difference between the expected vs. observed frequencies in one or more categories of a contingency table. The Fisher’s exact test is similar, though is an exact measure that can be run on any sample size, including smaller sample sizes. The number of samples or subjects (n) considered to be sufficiently large enough is subjective, contingent upon the research question being asked, and the experimental design. However, smaller sample sizes can be more permissible if the sample is normally distributed, but generally speaking having n &gt; 30 is a common convention in statistics (Alexander, 2022). For this example, we are interested in evaluating the potential relationship between two categorical variables: smoking status (using the Smoker variable) and categorical BMI group (using the BMIcat variable) to address Environmental Health Question 6: Is there a relationship between smoking status and BMI? To run these categorical statistical tests, let’s first create and view a 2-way contingency table describing the frequencies of observations across the categorical BMI and smoking groups: ContingencyTable &lt;- with(full.data, table(BMIcat, Smoker)) ContingencyTable ## Smoker ## BMIcat NS S ## Normal 43 14 ## Overweight 87 22 ## Underweight 23 11 Now let’s run the Chi-squared test on this table: chisq.test(ContingencyTable) ## ## Pearson&#39;s Chi-squared test ## ## data: ContingencyTable ## X-squared = 2.1849, df = 2, p-value = 0.3354 Note that we can also run the Chi-squared test using the following code, without having to generate the contingency table: chisq.test(full.data$BMIcat, full.data$Smoker) ## ## Pearson&#39;s Chi-squared test ## ## data: full.data$BMIcat and full.data$Smoker ## X-squared = 2.1849, df = 2, p-value = 0.3354 Or: with(full.data, chisq.test(BMIcat, Smoker)) ## ## Pearson&#39;s Chi-squared test ## ## data: BMIcat and Smoker ## X-squared = 2.1849, df = 2, p-value = 0.3354 Answer to Environmental Health Question 6 Note that these all produce the same results. With this, we can answer Environmental Health Question #6: Is there a relationship between smoking status and BMI? Answer: This results in a p-value = 0.34, demonstrating that there is no significant relationship between BMI categories and smoking status. We can also run a Fisher’s Exact Test when considering sample sizes. We won’t run this here due to computing time, but here is some example code for your records: #With small sample sizes, can use Fisher&#39;s Exact Test #fisher.test(full.data$BMI, full.data$Smoker) Concluding Remarks In conclusion, this training module serves as a high-level introduction to basic statistics and visualization methods. Statistical approaches described in this training module include tests for normality, t-test, analysis of variance, regression modeling, chi-squared test, and Fisher’s exact test. Visualization approaches include boxplots, histograms, scatterplots, and regression lines. These methods serve as an important foundation for nearly all studies carried out in environmental health research. Test Your Knowledge If we’re interested in investigating if there are significant differences in birth weight based on maternal education level, which statistical test should you use? Is that relationship considered to be statistically significant and how can we visualize the distributions of these groups? "],["overview-of-experimental-design-and-example-data.html", "4.1 Overview of Experimental Design and Example Data Introduction to Training Module Replicates Orientation to Example Data for Chapter 4 Concluding Remarks", " 4.1 Overview of Experimental Design and Example Data This training module was developed by Elise Hickman, Sarah Miller, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Converting wet lab experimentation data into dry lab analyses facilitates reproducibility and transparency in data analysis. This is helpful for consistency across members of the same research group, review of analyses by collaborators or reviewers, and implementation of similar future analyses. In comparison with analysis workflows that use subscription- or license-based applications, such as Prism or SAS, analysis workflows that leverage open-source programming languages such as R also increase accessibility of analyses. Additionally, scripted analyses minimize the risk for copy-paste error, which can occur when cleaning experimental data, transferring it to an analysis application, and exporting and formatting analysis results. Some of the barriers in converting wet lab experimentation into dry lab analyses include data cleaning, selection and implementation of appropriate statistical tests, and reporting results. This chapter will provide introductory material guiding wet-bench scientists in R analyses, bridging the gap between commonly available R tutorials (which, while helpful, may not provide sufficient level of detail or relevant examples) and intensive data science workflows (which may be too detailed). In this module, we will provide an overview of key experimental design features and terms that will be used throughout this chapter, and we will provide a detailed overview of the example data. In the subsequent modules, we will dive into analyzing the example data. Replicates One of the most important components of selecting an appropriate analysis is first understanding how data should be compared between samples, which often means addressing experimental replicates. There are two main types of replicates that are used in environmental health research: biological replicates and technical replicates. Biological Replicates Biological replicates are the preferred unit of statistical comparison because they represent biologically distinct samples, demonstrating biological variation in the system. What is considered to be a biological replicate can depend on what model system is being used. For example, in studies with human clinical samples or cells from different human donors, the different humans are considered the biological replicates. In studies using animals as model organisms, individual animals are typically considered biological replicates, although this can vary depending on the experimental design. In studies that use cell lines, which are derived from one human or animal and are modified to continuously grow in culture, a biological replicate could be either cells from different passages (different thawed aliquots) grown in completely separate flasks, all experimented with on the same day, or repeating an experiment on the same set of cells (one thawed aliquot) but on separate experimental days, so the cells have grown/replicated between experiments. The final “N” that you report should reflect your biological replicates, or independent experiments. What constitutes an independent experiment or biological replicate is highly field-, lab-, organism-, and endpoint-dependent, so make sure to discuss this within your research group in the experiment planning phase and again before your analysis begins. No matter what you choose, ensure that when you report your results, you are transparent about what your biological replicates are. For example, the below diagram (adapted from BitesizeBio) illustrates different ways of defining replicates in experiments with cell lines: N = 3 cells could be considered technical replicates if the endpoint of interest is very low throughput, such as single cell imaging or analyses. N = 3 cell culture wells is a more common approach to technical replicates and is typically used when one sample is collected from each well, such as in the case of media or cell lysate collection. Note that each well within the Week 1 biological replicate would be considered a technical replicate for Week 1’s experiment. Similarly, each well within the Week 2 biological replicate would be considered a technical replicate for Week 2’s experiment. For more on technical replicates, see the next section. Although N = 3 cell lines is a less common approach to biological replicates, some argue for this approach because each cell line is typically derived from one biological source. In this scenario, each of the cell lines would be unique but would represent the same cell type or lineage (e.g., for respiratory epithelium, A549, 16HBE, and BEAS-2B cell lines). Also note that to perform statistical analyses, an N of at least 3 biological replicates is needed, and an even higher N may be needed for a sufficiently powered study. Although power calculations are outside the scope of this module, we encourage you to use power calculation resources, such as G*Power to assist in selecting an appropriate N for your study. Technical Replicates Technical replicates are repeated measurements on the same sample or biological source, demonstrating the variation underlying protocols, equipment, and sample handling. In environmental health research, there can be technical replicates separately related to either the experimental design or the downstream analyses. Technical replicates related to experimental design refer to the chemical exposure for cell-based (in vitro) experiments, where there may be multiple wells of cells from the same passage or human/mouse exposed to the same treatment. Technical replicates related to downstream analyses refer to the endpoints that are measured after chemical exposure in each sample. To illustrate this, consider an experiment where cells from four unique human donors (D1-D4) are grown in cell culture plates, and then three wells of cells from each donor are exposed to a chemical treatment (Tx) or a vehicle control (Ctrl). The plate layout might look something like this, with technical replicates related to experimental design, i.e. chemical exposure, in the same color: For this experiment, we have four biological replicates (the four donors) and three technical exposure replicates per dose (because three wells from each donor were exposed to each condition). The technical replicates here capture potential unintended variation between wells in cell growth and chemical exposure. Following the exposure of the cells to a chemical of interest, the media is collected from each well and assayed using a plate reader assay for concentrations of a marker of inflammation. For each sample collected (from each well), there are three technical replicates used to measure the concentration of the inflammatory marker. The purpose of these technical replicates is to capture potential unintended well-to-well variation in the plate reader assay. The plate layout might look something like this, with the letter and number in each well of the plate layout representing the well in the exposure plate layout that the media sample being assayed came from: Technical replicates should typically be averaged before performing any statistical analysis. For the experiment described above, we would: Average the technical replicates for the plate reader assay to obtain one value per original cell culture well for inflammatory marker concentration. Then, average the technical replicates for the chemical exposure to obtain one value per biological replicate (donor). This would result in a dataset with eight values (four control and four treatment) for statistical analysis. Number and inclusion of technical replicates The above example is just one approach to experimental design. As mentioned above in the biological replicates section, selection of appropriate biological and technical replicates can vary greatly depending on your model organism, experimental design, assay, and standards in the field. For example, there may be cases where well-to-well variation for certain assays is minimal compared with variation between biological replicates, or when including technical replicates for each donor is experimentally or financially unfeasible, resulting in a lack of technical replicates. Matched Experimental Design Matching (also known as paired or repeated measures) in an experimental design is also a very important concept when selecting the appropriate statistical analysis. In experiments with matched design, multiple measurements are collected from the same biological replicate. This typically provides increased statistical power because changes are observed within each biological replicate relative to its starting point. In environmental health research, this can include study designs such as: Samples were collected from the same individuals, animals, or cell culture wells pre- and post-exposure. Cells from the same biological replicate were exposed to different doses of a chemical. The experimental design described above represents a matched design because cells from the same donor are exposed to both the treatment and the vehicle control. Orientation to Example Data for Chapter 4 In this chapter, we will be using an example dataset derived from an in vitro, or cell culture, experiment. Before diving into analysis of these data in the subsequent modules, we will provide an overview of where these data came from and preview what the input data frames look like. Experimental Design In this experiment, primary human bronchial epithelial cells (HBECs) from sixteen different donors were exposed to the gas acrolein, which is emitted from the combustion of fossil fuels, tobacco, wood, and plastic. Inhalation exposure to acrolein is associated with airway inhalation, and this study aimed to understand how exposure to acrolein changes secretion of markers of inflammation. Prior to experimentation, the HBECs were grown on a permeable membrane support for 24 days with air on one side and liquid media on the other side, allowing them to differentiate into a form that is very similar to what is found in the human body. The cells were then exposed for 2 hours to 0 (filtered air), 0.6, 1, 2, or 4 ppm acrolein, with two technical replicate wells from each donor per dose. Twenty-four hours later, the media was collected, and concentrations of inflammatory markers were measured using an enzyme-linked immunosorbent assay (ELISA). Note that this is a matched experimental design because cells from every donor were exposed to every concentration of acrolein, rather than cells from different donors being exposed to each of the different doses. Starting Data Next, let’s familiarize ourselves with the data that resulted from this experiment. There are two input data files, one that contains cytokine concentration data and one that contains demographic information about the donors: The cytokine data contains information about the cytokine measurements for each of the six proteins measured in the basolateral media for each sample (units = pg/mL), which can be identified by the donor, dose, and replicate columns. The demographic data contains information about the age and sex of each donor. In the subsequent modules, we’ll be using these data to assess whether exposure to acrolein significantly changes secretion of inflammatory markers and whether donor characteristics, such as sex and age, modify these responses. Concluding Remarks This module reviewed important components of experimental design, such as replicates and matching, which are critical for data pre-processing and selecting appropriate statistical tests. Test Your Knowledge Read the following experimental design descriptions. For each description, determine the number of biological replicates (per group), the number of technical replicates, and whether the experimental design is matched. One hundred participants are recruited to a study aiming to determine whether people who use e-cigarettes have different concentrations of inflammatory markers in their airways. Fifty participants are non e-cigarette users and 50 participants are e-cigarette users. After the airway samples are collected, each sample is analyzed with an ELISA, with three measurements taken per sample. Twenty mice are used in a study aiming to understand the effects of particulate matter on cardiovascular health. The mice are randomized such that half of the mice are exposed to filtered air and half are exposed to particulate matter. During the exposures, the mice are continuously monitored for endpoints such as heart rate and heart function. One month later, the mice that were exposed to particulate matter are exposed to filtered air, and the mice that were exposed to filtered air are exposed to particulate matter, with the same cardiovascular endpoints collected. "],["data-import-processing-and-summary-statistics.html", "4.2 Data Import, Processing, and Summary Statistics Introduction to Training Module Data Import Handling Missing Values Averaging Replicates Descriptive Statistics Normality Assessment and Data Transformation Concluding Remarks", " 4.2 Data Import, Processing, and Summary Statistics This training module was developed by Elise Hickman, Alexis Payton, Sarah Miller, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module The first steps in any scripted analysis of wet-bench data include importing the data, cleaning the data to prepare for analyses, and conducting preliminary data exploration steps, such as addressing missing values, calculating summary statistics, and assessing normality. Although less exciting than diving right into the statistical analysis, these steps are crucial in guiding downstream analyses and ensuring accurate results. In this module, we will discuss each of these steps and work through them using an example dataset (introduced in TAME 2.0 Module 4.1 Overview of Experimental Design and Example Data of inflammatory markers secreted by airway epithelial cells after exposure to different concentrations of acrolein. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: What is the mean concentration of each inflammatory biomarker by acrolein concentration? Are our data normally distributed? Data Import First, we need to import our data. Data can be imported into R from many different file formats, including .csv (as demonstrated in previous chapters), .txt, .xlsx, and .pdf. Often, data are formatted in Excel prior to import, and the openxlsx package provides helpful functions that allow the user to import data from Excel, create workbooks for storing results generated in R, and export data from R to Excel workbooks. Below, we will use the read.xlsx() function to import our data directly from Excel. Other useful packages include pdftools (PDF import), tm (text mining of PDFs), and plater (plate reader formatted data import). Workspace Preparation and Data Import Set working directory In preparation, first let’s set our working directory to the folder path that contains our input files: setwd(&quot;/filepath to where your input files are&quot;) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;table1&quot;)) install.packages(&quot;table1&quot;); if (!requireNamespace(&quot;vtable&quot;)) install.packages(&quot;vtable&quot;); # some packages need to be installed through Bioconductor/ BiocManager if (!require(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;pcaMethods&quot;) BiocManager::install(&quot;impute&quot;) BiocManager::install(&quot;imputeLCMD&quot;) Load required packages And load required packages: library(openxlsx) # for importing Excel files library(DT) # for easier viewing of data tables library(tidyverse) # for data cleaning and graphing library(imputeLCMD) # for data imputation with QRILC library(table1) # for summary table library(vtable) # for summary table library(ggpubr) # for making Q-Q plots with ggplot Import example datasets Next, let’s read in our example datasets: biomarker_data &lt;- read.xlsx(&quot;Module4_2_Input/Module4_2_InputData1.xlsx&quot;) demographic_data &lt;- read.xlsx(&quot;Module4_2_Input/Module4_2_InputData2.xlsx&quot;) View example datasets First, let’s preview our example data. Using the datatable() function from the DT package allows us to interactively scroll through our biomarker data. datatable(biomarker_data) We can see that our biomarker data are arranged with samples in rows and sample information and biomarker measurements in the columns. datatable(demographic_data) Our demographic data provide information about the donors that our cells came from, matching to the Donor column in our biomarker data. Handling Missing Values Next, we will investigate whether we have missing values and which variables and donors have missing values. # Calculate the total number of NAs per variable biomarker_data %&gt;% summarise(across(IL1B:VEGF, ~sum(is.na(.)))) ## IL1B IL6 IL8 IL10 TNFa VEGF ## 1 0 0 6 5 0 0 # Calculate the number of missing values per subject biomarker_data %&gt;% group_by(Donor) %&gt;% summarise(across(IL1B:VEGF, ~sum(is.na(.)))) ## # A tibble: 16 × 7 ## Donor IL1B IL6 IL8 IL10 TNFa VEGF ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 D1 0 0 0 0 0 0 ## 2 D10 0 0 0 2 0 0 ## 3 D11 0 0 0 0 0 0 ## 4 D12 0 0 2 0 0 0 ## 5 D13 0 0 0 3 0 0 ## 6 D14 0 0 0 0 0 0 ## 7 D15 0 0 0 0 0 0 ## 8 D16 0 0 0 0 0 0 ## 9 D2 0 0 2 0 0 0 ## 10 D3 0 0 0 0 0 0 ## 11 D4 0 0 0 0 0 0 ## 12 D5 0 0 0 0 0 0 ## 13 D6 0 0 0 0 0 0 ## 14 D7 0 0 0 0 0 0 ## 15 D8 0 0 0 0 0 0 ## 16 D9 0 0 2 0 0 0 Here, we can see that we do have a few missing values. What should we do with these values? Missing Values and Data Imputation Missing values Before deciding what to do about our missing values, it’s important to understand why they are missing. There are a few different types of missing values that could be present in a dataset: Missing completely at random (MCAR): has nothing to do with the experimental unit being studied (e.g., a sample is damaged or lost in the lab) Missing at random (MAR): there may be a systematic difference between missing and measured values, but they can be explained by observed differences in the data or experimental unit Missing not at random (MNAR): data are missing due to factors that are not observed/measured (e.g., measurement for a specific endpoint is below the limit of detection (LOD) of an assay) We know from the researchers who generated this dataset that the values are missing because these specific proteins were below the limit of detection for the assay for certain samples; therefore, our data are missing not at random. This can help us with our choice of imputation method, described below. Imputation Imputation is the assignment of a value to a missing data point by inferring that value from other properties of the dataset or externally defined limits. Whether or not you should impute your data is not a one-size-fits-all approach and may vary depending on your field, experimental design, the type of data, and the type of missing values in your dataset. Two questions you can ask yourself when deciding whether or not to impute data are: Is imputation needed for downstream analyses? Some analyses are not permissive to including NAs or 0s; others are. Will imputing values bias my analyses unnecessarily? If so, consider analyzing subsets of the data that are complete separately. There are many different imputation methods (too many to cover them all in this module); here, we will introduce a few that we use most often. We encourage you to explore these in more depth and to understand typical imputation workflows for your lab, data type, and/or discipline. For variables where imputed values are expected to be generally bound by the existing range of data (e.g., MCAR): missForest For variables with samples below the limit of detection for the assay, such as for mass spectrometry or ELISAs (e.g., MNAR) Replace non-detects with the limit of detection divided by the square root of 2 Quantile Regression Imputation of Left-Censored Data (QRILC) GSimp (can also be used to impute values above a specific threshold) If you do impute missing values, make sure to include both your raw and imputed data, along with detailed information about the imputation method, within your manuscript, supplemental information, and/or GitHub. You can even present summary statistics for both raw and imputed data for additional transparency. Imputation of Our Data Before imputing our data, it is a good idea to implement a background filter that checks to see if a certain percentage of values for each variable are missing. For variables with a very high percentage of missing values, imputation can be unreliable because there is not enough information for the imputation algorithm to reference. The threshold for what this percentage should be can vary by study design and the extent to which your data are subset into groups that may have differing biomarker profiles; however, a common threshold we frequently use is to remove variables with missing data for 25% or more of samples. We can use the following code to calculate the percentage values missing for each endpoint: biomarker_data %&gt;% summarise(across(IL1B:VEGF, ~sum(is.na(.))/nrow(biomarker_data)*100)) ## IL1B IL6 IL8 IL10 TNFa VEGF ## 1 0 0 3.75 3.125 0 0 Here, we can see that only about 3-4% of values are missing for our variables with missing data, so we will proceed to imputation with our dataset as-is. We will impute values using QRILC, which pulls from the left side of the data distribution (the lower values) to impute missing values. We will write a function that will apply QRILC imputation to our dataframe. This function takes a dataframe with missing values as input and returns a dataframe with QRILC imputed values in place of NAs as output. QRILC_imputation = function(df){ # Normalize data before applying QRILC per QRILC documentation ## Select only numeric columns, psuedo log2 transform, and convert to a matrix ### 4 comes from there being 3 metadata columns before the numeric data starts QRILC_prep = df[,4:dim(df)[2]] %&gt;% mutate_all(., function(x) log2(x + 1)) %&gt;% as.matrix() # QRILC imputation imputed_QRILC_object = impute.QRILC(QRILC_prep, tune.sigma = 0.1) QRILC_log2_df = data.frame(imputed_QRILC_object[1]) # Converting back the original scale QRILC_df = QRILC_log2_df %&gt;% mutate_all(., function(x) 2^x - 1) # Adding back in metadata columns QRILC_df = cbind(Donor = df$Donor, Dose = df$Dose, Replicate = df$Replicate, QRILC_df) return(QRILC_df) } Now we can apply the QRILC_imputation() function to our dataframe. We use the function set.seed() to ensure that the QRILC function generates the same numbers each time we run the script. For more on setting seeds, see here. # Set random seed to ensure reproducibility in results set.seed(1104) # Apply function biomarker_data_imp &lt;- QRILC_imputation(biomarker_data) Averaging Replicates The last step we need to take before our data are ready for analysis is averaging the two technical replicates for each donor and dose. We will do this by creating an ID column that represents the donor and dose together and using that column to group and average the data. This results in a dataframe where our rows contain data representing each biological replicate exposed to each of the five concentrations of acrolein. biomarker_data_imp_avg &lt;- biomarker_data_imp %&gt;% # Create an ID column that represents the donor and dose unite(Donor_Dose, Donor, Dose, sep = &quot;_&quot;) %&gt;% # Average replicates with each unique Donor_Dose group_by(Donor_Dose) %&gt;% summarize(across(IL1B:VEGF, mean)) %&gt;% # Round results to the same number of significant figures as the original data mutate(across(IL1B:VEGF, \\(x) round(x, 2))) %&gt;% # Separate back out the Donor_Dose column separate(Donor_Dose, into = c(&quot;Donor&quot;, &quot;Dose&quot;), sep = &quot;_&quot;) # View new dataframe datatable(biomarker_data_imp_avg) Descriptive Statistics Generating descriptive statistics (e.g., mean, median, mode, range, standard deviation) can be helpful for understanding the general distribution of your data and for reporting results either in the main body of a manuscript/report (for small datasets) or in the supplementary material (for larger datasets). There are a number of different approaches that can be used to calculate summary statistics, including functions that are part of base R and that are part of packages. Here, we will demonstrate a few different ways to efficiently calculate descriptive statistics across our dataset. Method #1 - Tidyverse and Basic Functions The mean, or average of data points, is one of the most commonly reported summary statistics and is often reported as mean ± standard deviation to demonstrate the spread in the data. Here, we will make a table of mean ± standard deviation for each of our biomarkers across each of the dose groups using tidyverse functions. # Calculate means biomarker_group_means &lt;- biomarker_data_imp_avg %&gt;% group_by(Dose) %&gt;% summarise(across(IL1B:VEGF, \\(x) mean(x))) # View data datatable(biomarker_group_means) You’ll notice that there are a lot of decimal places in our calculated means, while in our original data, there are only two decimal places. We can add a step to round the data to our above code chunk to produce cleaner results. # Calculate means biomarker_group_means &lt;- biomarker_data_imp_avg %&gt;% group_by(Dose) %&gt;% summarise(across(IL1B:VEGF, \\(x) mean(x))) %&gt;% mutate(across(IL1B:VEGF, \\(x) round(x, 2))) # View data datatable(biomarker_group_means) Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question 1: What is the mean concentration of each inflammatory biomarker by acrolein concentration? Answer: With the above table, we can see the mean concentrations for each of our inflammatory biomarkers by acrolein dose. IL-8 overall has the highest concentrations, followed by VEGF and IL-6. For IL-1\\(\\beta\\), IL-8, TNF-\\(\\alpha\\), and VEGF, it appears that the concentration of the biomarker goes up with increasing dose. We can use very similar code to calculate our standard deviations: # Calculate means biomarker_group_sds &lt;- biomarker_data_imp_avg %&gt;% group_by(Dose) %&gt;% summarise(across(IL1B:VEGF, \\(x) sd(x))) %&gt;% mutate(across(IL1B:VEGF, \\(x) round(x, 1))) # View data datatable(biomarker_group_sds) Now we’ve calculated both the means and standard deviations! However, these are typically presented as mean ± standard deviation. We can merge these dataframes by executing the following steps: Pivot each dataframe to a long format, with each row containing the value for one biomarker at one dose. Create a variable that represents each unique row (combination of Dose and variable). Join the dataframes by row. Unite the two columns with mean and standard deviation, with ± in between them. Pivot the dataframe wider so that the dataframe resembles what we started with for the means and standard deviations. First, we’ll pivot each dataframe to a long format and create a variable that represents each unique row. # Pivot dataframes longer and create variable column for each row biomarker_group_means_long &lt;- pivot_longer(biomarker_group_means, !Dose, names_to = &quot;variable&quot;, values_to = &quot;mean&quot;) %&gt;% unite(Dose_variable, Dose, variable, remove = FALSE) biomarker_group_sds_long &lt;- pivot_longer(biomarker_group_means, !Dose, names_to = &quot;variable&quot;, values_to = &quot;sd&quot;) %&gt;% unite(Dose_variable, Dose, variable, remove = FALSE) # Preview what dataframe looks like datatable(biomarker_group_means_long) Next, we will join the mean and standard deviation datasets. Notice that we are only joining the Dose_variable and sd columns from the standard deviation dataframe to prevent duplicate columns (Dose, variable) from being included. # Merge the dataframes by row biomarker_group_summstats &lt;- left_join(biomarker_group_means_long, biomarker_group_sds_long %&gt;% select(c(Dose_variable, sd)), by = &quot;Dose_variable&quot;) # Preview the new dataframe datatable(biomarker_group_summstats) Then, we can unite the mean and standard deviation columns and add the ± symbol between them by storing that character as a variable and pasting that variable in our paste() function. # Store plus/minus character plusminus &lt;-&quot;\\u00b1&quot; Encoding(plusminus)&lt;-&quot;UTF-8&quot; # Create new column with mean +/- standard deviation biomarker_group_summstats &lt;- biomarker_group_summstats %&gt;% mutate(mean_sd = paste(mean, plusminus, sd, sep = &quot; &quot;)) # Preview the new dataframe datatable(biomarker_group_summstats) Last, we can pivot the dataframe wider to revert it to its original layout, which is easier to read. # Pivot dataframe wider biomarker_group_summstats &lt;- biomarker_group_summstats %&gt;% # Remove columns we don&#39;t need any more select(-c(Dose_variable, mean, sd)) %&gt;% # Pivot wider pivot_wider(id_cols = Dose, names_from = &quot;variable&quot;, values_from = &quot;mean_sd&quot;) # View final dataframe datatable(biomarker_group_summstats) These data are now in a publication-ready format that can be exported to a .txt, .csv., or .xlsx file for sharing. Method #2 - Applying a List of Functions Calculating our mean and standard deviation separately using tidyverse wasn’t too difficult, but what if we want to calculate other descriptive statistics, such as minimum, median, and maximum? We could use the above approach, but we would need to make a separate dataframe for each and then merge them all together. Instead, we can use the map_dfr() function from the purrr package, which is also part of tidyverse. This function takes a list of functions you want to apply to your data and applies these functions over specified columns in the data. Let’s see how it works: # Define summary functions summary_functs &lt;- lst(min, median, mean, max, sd) # Apply functions to data, grouping by dose # .id = &quot;statistic&quot; tells the function to create a column describing which statistic that row is reporting biomarker_descriptive_stats_all &lt;- map_dfr(summary_functs, ~ summarize(biomarker_data_imp_avg %&gt;% group_by(Dose), across(IL1B:VEGF, .x)), .id = &quot;statistic&quot;) # View data datatable(biomarker_descriptive_stats_all) Depending on your final goal, descriptive statistics data can then be extracted from this dataframe and cleaned up or reformatted as needed to create a publication-ready table! Other Methods There are also packages that have been developed for specifically making summary tables, such as table1 and vtable. These packages can create summary tables in HTML format, which appear nicely in R Markdown and can be copied and pasted into Word. Here, we will briefly demonstrate how these packages work, and we encourage you to explore more using the package vignettes! Table1 The table1 package makes summary tables using the function table1(), which takes the columns that you want in the rows of the table on the left side of the first argument, followed by | and then the grouping variable. The output table can be customized in a number of ways, including what summary statistics are output and whether or not statistical comparisons are run between groups (see package vignette for more details). # Get names of all of the columns to include in the table paste(names(biomarker_data_imp_avg %&gt;% select(IL1B:VEGF)), collapse=&quot; + &quot;) ## [1] &quot;IL1B + IL6 + IL8 + IL10 + TNFa + VEGF&quot; # Make the table table1(~ IL1B + IL6 + IL8 + IL10 + TNFa + VEGF | Dose, data = biomarker_data_imp_avg) Vtable The vtable package includes the function st(), which can also be used to make HTML tables (and other output formats; see out argument). For example: # HTML output st(biomarker_data_imp_avg, group = &#39;Dose&#39;) Table 1: Summary Statistics Dose 0 0.6 1 2 4 Variable N Mean SD N Mean SD N Mean SD N Mean SD N Mean SD IL1B 16 9.8 1.9 16 10 1.9 16 11 1.5 16 11 1.6 16 12 2 IL6 16 597 658 16 670 810 16 565 652 16 536 474 16 288 297 IL8 16 21987 7570 16 24703 10768 16 22607 10648 16 41687 17939 16 94439 33535 IL10 16 1.4 1.3 16 0.44 0.56 16 1.2 1.2 16 0.88 0.43 16 1.2 0.78 TNFa 16 3.2 1.8 16 3.1 1.7 16 4.1 2.4 16 5.2 2 16 6.7 2.7 VEGF 16 971 417 16 1023 486 16 1196 621 16 1754 749 16 1617 836 # Dataframe output st(biomarker_data_imp_avg, group = &#39;Dose&#39;, out = &#39;return&#39;) ## Variable N Mean SD N Mean SD N Mean SD N Mean SD N Mean ## 1 Dose 0 0.6 1 2 4 ## 2 IL1B 16 9.8 1.9 16 10 1.9 16 11 1.5 16 11 1.6 16 12 ## 3 IL6 16 597 658 16 670 810 16 565 652 16 536 474 16 288 ## 4 IL8 16 21987 7570 16 24703 10768 16 22607 10648 16 41687 17939 16 94439 ## 5 IL10 16 1.4 1.3 16 0.44 0.56 16 1.2 1.2 16 0.88 0.43 16 1.2 ## 6 TNFa 16 3.2 1.8 16 3.1 1.7 16 4.1 2.4 16 5.2 2 16 6.7 ## 7 VEGF 16 971 417 16 1023 486 16 1196 621 16 1754 749 16 1617 ## SD ## 1 ## 2 2 ## 3 297 ## 4 33535 ## 5 0.78 ## 6 2.7 ## 7 836 Similar to table1, see the package vignette for detailed information about how to customize tables using this package. Normality Assessment and Data Transformation The last step we will take before beginning to test our data for statistical differences between groups (in the next module) is to understand our data’s distribution through normality assessment. This will inform which statistical tests we will perform on our data. For more detail on normality testing, including detailed explanations of each type of normality assessment and explanations of the code underlying the following graphs and tables, see TAME 2.0 Module 3.3 Normality Tests and Data Transformations. We’ll start by looking at histograms of our data for qualitative normality assessment: # Set theme theme_set(theme_bw()) # Pivot data longer to prepare for plotting biomarker_data_imp_avg_long &lt;- biomarker_data_imp_avg %&gt;% pivot_longer(-c(Donor, Dose), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) # Make figure panel of histograms ggplot(biomarker_data_imp_avg_long, aes(value)) + geom_histogram(fill = &quot;gray40&quot;, color = &quot;black&quot;, binwidth = function(x) {(max(x) - min(x))/25}) + facet_wrap(~ variable, scales = &quot;free&quot;, nrow = 2) + labs(y = &quot;# of Observations&quot;, x = &quot;Value&quot;) From these histograms, we can see that IL-1\\(\\beta\\) appears to be normally distributed, while the other endpoints do not appear to be normally distributed. We can also use Q-Q plots to assess normality qualitatively: ggqqplot(biomarker_data_imp_avg_long, x = &quot;value&quot;, facet.by = &quot;variable&quot;, ggtheme = theme_bw(), scales = &quot;free&quot;) With this figure panel, we can see that most of the variables have very noticeable deviations from the reference, suggesting non-normal distributions. To assess normality quantitatively, we can use the Shapiro-Wilk test. Note that the null hypothesis is that the sample distribution is normal, and a significant p-value means the distribution is non-normal. # Apply Shapiro Wilk test to dataframe shapiro_res &lt;- apply(biomarker_data_imp_avg %&gt;% select(IL1B:VEGF), 2, shapiro.test) # Create results dataframe shapiro_res &lt;- do.call(rbind.data.frame, shapiro_res) # Clean dataframe shapiro_res &lt;- shapiro_res %&gt;% ## Add normality conclusion mutate(normal = ifelse(p.value &lt; 0.05, F, T)) %&gt;% ## Remove columns that do not contain informative data select(c(p.value, normal)) # View cleaned up dataframe datatable(shapiro_res) Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question 2: Are our data normally distributed? Answer: The results from the Shapiro-Wilk test demonstrate that the IL-1\\(\\beta\\) data are normally distributed, while the other variables are non-normally distributed. These results support the conclusions we made based on our qualitative assessment above with histograms and Q-Q plots. Log2 Transforming and Re-Assessing Normality Log2 transformation is a common transformation used in environmental health research and can move data closer to a normal distribution. For more on data transformation, see TAME 2.0 Module 3.3 Normality Tests and Data Transformations. We will pseudo-log2 transform our data, which adds a 1 to each value before log2 transformation and ensures that resulting values are positive real numbers. Let’s see if the log2 data are more normally distributed than the raw data. # Apply log2 transformation to data biomarker_data_imp_avg_log2 &lt;- biomarker_data_imp_avg %&gt;% mutate(across(IL1B:VEGF, ~ log2(.x + 1))) Make histogram panel: # Pivot data longer and make figure panel of histograms biomarker_data_imp_avg_log2_long &lt;- biomarker_data_imp_avg_log2 %&gt;% pivot_longer(-c(Donor, Dose), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) # Make histogram panel ggplot(biomarker_data_imp_avg_log2_long, aes(value)) + geom_histogram(fill = &quot;gray40&quot;, color = &quot;black&quot;, binwidth = function(x) {(max(x) - min(x))/25}) + facet_wrap(~ variable, scales = &quot;free&quot;) + labs(y = &quot;# of Observations&quot;, x = &quot;Value&quot;) Make Q-Q plot panel: ggqqplot(biomarker_data_imp_avg_log2_long, x = &quot;value&quot;, facet.by = &quot;variable&quot;, ggtheme = theme_bw(), scales = &quot;free&quot;) Run Shapiro-Wilk test: # Apply Shapiro Wilk test shapiro_res_log2 &lt;- apply(biomarker_data_imp_avg_log2 %&gt;% select(IL1B:VEGF), 2, shapiro.test) # Create results dataframe shapiro_res_log2 &lt;- do.call(rbind.data.frame, shapiro_res_log2) # Clean dataframe shapiro_res_log2 &lt;- shapiro_res_log2 %&gt;% ## Add normality conclusion mutate(normal = ifelse(p.value &lt; 0.05, F, T)) %&gt;% ## Remove columns that do not contain informative data select(c(p.value, normal)) # View cleaned up dataframe shapiro_res_log2 ## p.value normal ## IL1B 0.250821677 TRUE ## IL6 0.001445717 FALSE ## IL8 0.017386740 FALSE ## IL10 0.018935719 FALSE ## TNFa 0.194774045 TRUE ## VEGF 0.047231367 FALSE The histograms and Q-Q plots demonstrate that the log2 data are more normally distributed than the raw data. The results from the Shapiro-Wilk test also demonstrate that the the log2 data are more normally distributed as a whole than the raw data. Overall, the p-values, even for the variables that are still non-normally distributed, are much higher. So, should we proceed with the raw data or the log2 data? This depends on what analyses we plan to do. In general, it is best to keep the data in as close to its raw format as possible, so if all of our analyses are available with a non-parametric test, we could use our raw data. However, some statistical tests do not have a non-parametric equivalent, in which case it would likely be best to use the log2 transformed data. For subsequent modules, we will proceed with the log2 data for consistency; however, choices regarding normality assessment can vary, so be sure to discuss these choices within your research group before proceeding with your analysis. For more on decisions regarding normality, see TAME 2.0 Module 3.3 Normality Tests and Data Transformations. For more on parametric vs. non-parametric tests, see TAME 2.0 Module 4.4 Two Group Comparisons and Visualizations and TAME 2.0 Module 4.5 Multi-Group Comparisons and Visualizations. Concluding Remarks Taken together, this module demonstrates important data processing steps necessary before proceeding with between-group statistical testing, including data import, handling missing values, averaging replicates, generating descriptive statistics tables, and assessing normality. Careful consideration and description of these steps in the methods section of a manuscript or report increases reproducibility of analyses and helps to improve the accuracy and statistical validity of subsequent statistical results. Test Your Knowledge Functional endpoints from these cultures were also measured. These endpoints were: 1) Membrane Permeability (MemPerm), 2) Trans-Epithelial Electrical Resistance (TEER), 3) Ciliary Beat Frequency (CBF), and 4) Expression of Mucin (MUC5AC). Work through the same processes demonstrated in this module using the provided data (“Module4_2_TYKInput.xlsx”) to answer the following questions: How many technical replicates are there for each dose? Are there any missing values? What are the average values for each endpoint by dose? Are the raw data normally distributed? "],["data-import-from-pdf-sources.html", "4.3 Data Import from PDF Sources Introduction to Training Module Importing Data from Many Single PDFs with the Same Formatting Importing Data Stored in PDF Tables Concluding Remarks", " 4.3 Data Import from PDF Sources This training module was developed by Elise Hickman, Alexis Payton, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Most tutorials for R rely on importing .csv, .xlsx, or .txt files, but there are numerous other file formats that can store data, and these file formats can be more difficult to import into R. PDFs can be particularly difficult to interface with in R because they are not formatted with defined rows/columns/cells as is done in Excel or .csv/.txt formatting. In this module, we will demonstrate how to import data from from PDFs into R and format it such that it is amenable for downstream analyses or export as a table. Familiarity with tidyverse, for loops, and functions will make this module much more approachable, so be sure to review TAME 2.0 Modules 2.3 Data Manipulation and Reshaping and 2.4 Improving Coding Efficiencies if you need a refresher. Overview of Example Data To demonstrate import of data from PDFs, we will be leveraging two example datasets, described in more detail in their respective sections later on in the module. PDFs generated by Nanoparticle Tracking Analysis (NTA), a technique used to quantify the size and distribution of particles (such as extracellular vesicles) in a sample. We will be extracting data from an experiment in which epithelial cells were exposed to four different environmental chemicals or a vehicle control, and secreted particles were isolated and characterized using NTA. A PDF containing information about variables collected as part of a study whose samples are part of NIH’s BioLINCC Repository. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Which chemical(s) increase and decrease the concentration of particles secreted by epithelial cells? How many variables total are available to us to request from the study whose data are store in the repository, and what are these variables? Importing Data from Many Single PDFs with the Same Formatting Getting Familiar with the Example Dataset The following example is based on extracting data from PDFs generated by Nanoparticle Tracking Analysis (NTA), a technique used to quantify the size and distribution of particles in a sample. Each PDF file is associated with one sample, and each PDF contains multiple values that we want to extract. Although this is a very specific type of data, keep in mind that this general approach can be applied to any data stored in PDF format - you will just need to make modifications based on the layout of your PDF file! For this example, we will be extracting data from 5 PDFs that are identically formatted but contain information unique to each sample. The samples represent particles isolated from epithelial cell media following an experiment where cells were exposed to four different environmental chemicals (labeled “A”, “B”, “C”, and “D”) or a vehicle control (labeled “Ctrl”). Here is what a full view of one of the PDFs looks like, with values we want to extract highlighted in yellow: Our goal is to extract these values and end up with a dataframe that looks like this, with each sample in a row and each variable in a column: If your files are not already named in a way that reflects unique sample information, such as the date of the experiment or sample ID, update your file names to contain this information before proceeding with the script. Here are the names for the example PDF files: Workspace Preparation and Data Import Installing and loading required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you. We will be using the pdftools and tm packages to extract text from the PDF. And instead of using head() to preview dataframes, we will be using the function datatable() from the DT package. This function produces interactive tables and generates better formatting for viewing dataframes that have long character strings (like the ones we will be viewing in this section). if (!requireNamespace(&quot;pdftools&quot;)) install.packages(&quot;pdftools&quot;) if (!requireNamespace(&quot;tm&quot;)) install.packages(&quot;tm&quot;) if (!requireNamespace(&quot;DT&quot;)) install.packages(&quot;DT&quot;) if (!requireNamespace(&quot;janitor&quot;)) install.packages(&quot;janitor&quot;) Next, load the packages. library(tidyverse) library(pdftools) library(tm) library(DT) library(janitor) Initial data import from PDF files The following code stores the file names of all of the files in your directory that end in .pdf. To ensure that only PDFs of interest are imported, consider making a subfolder within your directory containing only the PDF extraction script file and the PDFs you want to extract data from. pdf_list &lt;- list.files(path = &quot;./Module4_3_Input&quot;, pattern = &quot;488.pdf$&quot;) We can see that each of our file names are now contained in the list. head(pdf_list) ## [1] &quot;20230214_0002_Expt1_A_size_488.pdf&quot; ## [2] &quot;20230214_0006_Expt1_Ctrl_size_488.pdf&quot; ## [3] &quot;20230214_0014_Expt1_C_size_488.pdf&quot; ## [4] &quot;20230214_0023_Expt1_D_size_488.pdf&quot; ## [5] &quot;20230214_0024_Expt1_B_size_488.pdf&quot; Next, we need to make a dataframe to store the extracted data. The PDF Identifier column will store the file name, and the Text column will store extracted text from the PDF. pdf_raw &lt;- data.frame(&quot;PDF Identifier&quot; = c(), &quot;Text&quot; = c()) The following code uses a for loop to loop through each file (as stored in the pdf_list vector) and extract the text from the PDF. Sometimes this code generates duplicates, so we will also remove the duplicates with distinct(). for (i in 1:length(pdf_list)){ # Iterating through each pdf file and separating each line of text document_text = pdf_text(paste(&quot;./Module4_3_Input/&quot;, pdf_list[i], sep = &quot;&quot;)) %&gt;% strsplit(&quot;\\n&quot;) # Saving the name of each PDF file and its text document = data.frame(&quot;PDF Identifier&quot; = gsub(x = pdf_list[i], pattern = &quot;.pdf&quot;, replacement = &quot;&quot;), &quot;Text&quot; = document_text, stringsAsFactors = FALSE) colnames(document) &lt;- c(&quot;PDF Identifier&quot;, &quot;Text&quot;) # Appending the new text data to the dataframe pdf_raw &lt;- rbind(pdf_raw, document) } pdf_raw &lt;- pdf_raw %&gt;% distinct() The new dataframe contains the data from all of the PDFs, with the PDF Identifier column containing the name of the input PDF file that corresponds to the text in the column next to it. datatable(pdf_raw) Extracting Variables of Interest Specific variables of interest can be extracted from the pdf_raw dataframe by filtering the dataframe for rows that contain a specific character string. This character string could be the variable of interest (if that word or set of words is unique and only occurs in that one place in the document) or a character string that occurs in the same line of the PDF as your variable of interest. Examples of both of these approaches are shown below. It is important to note that there can be different numbers of spaces in each row and after each semicolon, which will change the sep argument for each variable. For example, there are a different number of spaces after the semicolon for “Dilution Factor” than there are for “Concentration” (see above PDF screen shot for reference). We will work through an example for the first variable of interest, dilution factor, in detail. First, we can see what the dataframe looks like when we just filter rows based on keeping only rows that contain the string “Dilution Factor” in the text column using the grepl() function. dilution_factor_df &lt;- pdf_raw %&gt;% filter(grepl(&quot;Dilution Factor&quot;, Text)) datatable(dilution_factor_df) The value we are trying to extract is at the end of a long character string. We will want to use the tidyverse function separate() to isolate those values, but we need to know what part of the character string will separate the dilution factor values from the rest of the text. To determine this, we can call just one of the data cells and copy the semicolon and following spaces for use in the separate() function. # Return the value in the first row and second column. dilution_factor_df[1,2] ## [1] &quot; Temperature: 24.64 °C sensed Dilution Factor: 200&quot; Building on top of the previous code, we can now separate the dilution factor value from the rest of the text in the string. The separate() function takes an input data column and separates it into two or more columns based on the character passed to the separation argument. Here, everything before the separation string is discarded by setting the first new column to NA. Everything after the separation string will be stored in a new column called Dilution Factor, The starting Text column is removed by default. dilution_factor_df &lt;- pdf_raw %&gt;% filter(grepl(&quot;Dilution Factor&quot;, Text)) %&gt;% separate(Text, into = c(NA, &quot;Dilution Factor&quot;), sep = &quot;: &quot;) datatable(dilution_factor_df) For the “Original Concentration” variable, we filter rows by the string “pH” because the word concentration is found in multiple locations in the document. concentration_df = pdf_raw %&gt;% filter(grepl(&quot;pH&quot;, Text)) %&gt;% separate(Text, c(NA, &quot;Concentration&quot;), sep = &quot;: &quot;) datatable(concentration_df) With the dilution factor variable, there were no additional characters after the value of interest, but here, “Particles / mL” remains and needs to be removed so that the data can be used in downstream analyses. We can add an additional cleaning step to remove “Particles / mL” from the data and add the units to the column title. sep = \" P\" refers to the space before and first letter of the string to be removed. concentration_df = pdf_raw %&gt;% filter(grepl(&quot;pH&quot;, Text)) %&gt;% separate(Text, c(NA, &quot;Concentration&quot;), sep = &quot;: &quot;) %&gt;% separate(Concentration, c(&quot;Concentration (Particles/ mL)&quot;, NA), sep = &quot; P&quot;) datatable(concentration_df) Next, we want to extract size distribution data from the lower table. Note that the space in the first separate() function comes from the space between the “Number” and “Concentration” column in the string, and the space in the second separate() function comes from the space between the variable name and the number of interest. We can also convert values to numeric since they are currently stored as characters. size_distribution_df = pdf_raw %&gt;% filter(grepl(&quot;X10&quot;, Text)| grepl(&quot;X50 &quot;, Text)| grepl(&quot;X90&quot;, Text) | grepl(&quot;Mean&quot;, Text)| grepl(&quot;StdDev&quot;, Text)) %&gt;% separate(Text, c(&quot;Text&quot;, NA), sep = &quot; &quot;) %&gt;% separate(Text, c(&quot;Text&quot;, &quot;Size&quot;), sep = &quot; &quot;) %&gt;% mutate(Size = as.numeric(Size)) %&gt;% pivot_wider(names_from = Text, values_from = Size) datatable(size_distribution_df) Creating the final dataframe Now that we have created dataframes for all of the variables that we are interested in, we can join them together into one final dataframe. # Make list of all dataframes to include all_variables &lt;- list(dilution_factor_df, concentration_df, size_distribution_df) # Combine dataframes using reduce function. Sometimes, duplicate rows are generated by full_join. full_df = all_variables %&gt;% reduce(full_join, by = &quot;PDF Identifier&quot;) %&gt;% distinct() # View new dataframe datatable(full_df) For easier downstream analysis, the last step is to separate the PDF Identifier column into an informative sample ID that matches up with other experimental data. final_df &lt;- full_df %&gt;% separate(&#39;PDF Identifier&#39;, # Split sample identifier column into new columns, retaining the original column into = c(&quot;Date&quot;, &quot;FileNumber&quot;, &quot;Experiment Number&quot;, &quot;Sample_ID&quot;, &quot;Size&quot;, &quot;Wavelength&quot;), sep = &quot;_&quot;, remove = FALSE) %&gt;% select(-c(FileNumber, Size)) %&gt;% # Remove uninformative columns mutate(across(&#39;Dilution Factor&#39;:&#39;StdDev&#39;, as.numeric)) # Change variables to numeric where appropriate datatable(final_df) Let’s make a graph to help us answer Environmental Health Question 1. theme_set(theme_bw()) data_for_graphing &lt;- final_df %&gt;% clean_names() data_for_graphing$sample_id &lt;- factor(data_for_graphing$sample_id, levels = c(&quot;Ctrl&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;)) ggplot(data_for_graphing, aes(x = sample_id, y = concentration_particles_m_l)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;gray70&quot;, color = &quot;black&quot;) + ylab(&quot;Particle Concentration (Particles/mL)&quot;) + xlab(&quot;Exposure&quot;) With this, we can answer Environmental Health Question #1: Which chemical(s) increase and decrease the concentration of particles secreted by epithelial cells? Answer: Chemicals B and C appear to increase the concentration of secreted particles. However, additional replicates of this experiment are needed to assess statistical significance. Importing Data Stored in PDF Tables The above workflow is useful if you just want to extract a few specific values from PDFs, but isn’t as useful if data are already in a table format in a PDF. The tabulapdf package provides helpful functions for extracting dataframes from tables in PDF format. Getting Familiar with the Example Dataset The following example is based on extracting dataframes from a long PDF containing many individual data tables. This particular PDF came from the NIH’s BioLINCC Repository and details variables that researchers can request from the repository. Variables are part of larger datasets that contain many variables, with each dataset in a separate table. All of the tables are stored in one PDF file, and some of the tables are longer than one page (this will become relevant later on!). Similar to the first PDF workflow, remember that this is a specific example intended to demonstrate how to work through extracting data from PDFs. Modifications will need to be made for differently formatted PDFs. Here is what the first three pages of our 75-page starting PDF look like: If we zoom in a bit more on the first page, we can see that the dataset name is defined in bold above each table. This formatting is consistent throughout the PDF. The zoomed in view also allows us to see the columns and their contents more clearly. Some are more informative than others. The columns we are most interested in are listed below along with a description to guide you through the contents. Num: The number assigned to each variable in the dataset. This numbering restarts with 1 for each table. Variable: The variable name. Type: The type (or class) of the variable, either numeric or character. Label: A description of the variable and values associated with the variable. After extracting the data, we want to end up with a dataframe that contains all of the variables, their corresponding columns, and a column that indicates which dataset the variable is associated with: Workspace Preparation and Data Import Installing and loading required R packages Similar to previous sections, we need to install and load a few packages before proceeding. The tabulapdf package needs to be installed in a specific way as shown below and can sometimes be difficult to install on Macs. If errors are produced, follow the troubleshooting tips outlined in this Stack Overflow solution. # To install all of the packages except for tabulapdf if (!requireNamespace(&quot;stringr&quot;)) install.packages(&quot;stringr&quot;) if (!requireNamespace(&quot;pdftools&quot;)) install.packages(&quot;pdftools&quot;) if (!requireNamespace(&quot;rJava&quot;)) install.packages(&quot;rJava&quot;) # To install tabulapdf if (!require(&quot;remotes&quot;)) { install.packages(&quot;remotes&quot;) } library(remotes) remotes::install_github(c(&quot;ropensci/tabulizerjars&quot;, &quot;ropensci/tabulapdf&quot;), force=TRUE, INSTALL_opts = &quot;--no-multiarch&quot;) ## ## ── R CMD build ────────────────────────────────────────────────────── ## ✔ checking for file &#39;C:\\Users\\Jessie PC\\AppData\\Local\\Temp\\RtmpANHJlo\\remotes49284d455e75\\ropensci-tabulizerjars-d1924e0/DESCRIPTION&#39; ## ─ preparing &#39;tabulizerjars&#39;: ## checking DESCRIPTION meta-information ... checking DESCRIPTION meta-information ... ✔ checking DESCRIPTION meta-information ## ─ checking for LF line-endings in source and make files and shell scripts ## ─ checking for empty or unneeded directories ## ─ building &#39;tabulizerjars_1.0.1.tar.gz&#39; ## ## ## ## ── R CMD build ────────────────────────────────────────────────────── ## checking for file &#39;C:\\Users\\Jessie PC\\AppData\\Local\\Temp\\RtmpANHJlo\\remotes49285b85219\\ropensci-tabulapdf-7325a8d/DESCRIPTION&#39; ... ✔ checking for file &#39;C:\\Users\\Jessie PC\\AppData\\Local\\Temp\\RtmpANHJlo\\remotes49285b85219\\ropensci-tabulapdf-7325a8d/DESCRIPTION&#39; ## ─ preparing &#39;tabulapdf&#39;: ## checking DESCRIPTION meta-information ... checking DESCRIPTION meta-information ... ✔ checking DESCRIPTION meta-information ## ─ checking for LF line-endings in source and make files and shell scripts ## ─ checking for empty or unneeded directories ## Removed empty directory &#39;tabulapdf/docs&#39; ## ─ building &#39;tabulapdf_1.0.5-4.tar.gz&#39; ## ## Load packages: library(tabulapdf) library(tidyverse) library(janitor) library(pdftools) library(stringr) Initial data import from PDF file The extract_tables() function automatically extracts tables from PDFs and stores them as tibbles (a specific tidyverse data structure similar to a dataframe) within a list. One table is extracted per page, even if the table spans multiple pages. This line of code can take a few seconds to run depending on the length of your PDF. tables &lt;- extract_tables(&quot;Module4_3_Input/Module4_3_InputData4.pdf&quot;, output = &quot;tibble&quot;) Glimpsing the first three elements in the tables list, we can see that each list element is a dataframe containing the columns from the PDF tables. glimpse(tables[1:3]) ## List of 3 ## $ : spc_tbl_ [30 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## ..$ Num : num [1:30] 1 NA NA NA 2 NA NA 3 NA NA ... ## ..$ Variable: chr [1:30] &quot;AAQ_1000&quot; NA NA NA ... ## ..$ Type : chr [1:30] &quot;Num&quot; NA NA NA ... ## ..$ Len : num [1:30] 8 NA NA NA 8 NA NA 8 NA NA ... ## ..$ Format : chr [1:30] &quot;2.&quot; NA NA NA ... ## ..$ Informat: chr [1:30] &quot;2.&quot; NA NA NA ... ## ..$ Label : chr [1:30] &quot;In the past 3 days, how much of the time did your asthma keep you from&quot; &quot;doing your usual activities at work, school, or at home? 0=None of the&quot; &quot;time, 1=A little of the time, 2=Some of the time, 3=Most of the time, 4=All&quot; &quot;of the time&quot; ... ## ..- attr(*, &quot;spec&quot;)= ## .. .. cols( ## .. .. Num = col_double(), ## .. .. Variable = col_character(), ## .. .. Type = col_character(), ## .. .. Len = col_double(), ## .. .. Format = col_character(), ## .. .. Informat = col_character(), ## .. .. Label = col_character() ## .. .. ) ## ..- attr(*, &quot;problems&quot;)=&lt;externalptr&gt; ## $ : spc_tbl_ [46 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## ..$ Num : num [1:46] 1 2 3 4 5 6 NA NA 7 NA ... ## ..$ Variable: chr [1:46] &quot;ABP_1000&quot; &quot;ABP_1010&quot; &quot;ABP_1020&quot; &quot;ABP_1030&quot; ... ## ..$ Type : chr [1:46] &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; ... ## ..$ Len : num [1:46] 8 8 8 8 8 8 NA NA 8 NA ... ## ..$ Format : num [1:46] 2 2 2 2 2 2 NA NA 2 NA ... ## ..$ Informat: num [1:46] 2 2 2 2 2 2 NA NA 2 NA ... ## ..$ Label : chr [1:46] &quot;Are you currently retired? 1=Yes,0=No&quot; &quot;Are you retired because of asthma? 1=Yes,0=No&quot; &quot;Are you currently unemployed? 1=Yes,0=No&quot; &quot;Are you unemployed because of asthma? 1=Yes,0=No&quot; ... ## ..- attr(*, &quot;spec&quot;)= ## .. .. cols( ## .. .. Num = col_double(), ## .. .. Variable = col_character(), ## .. .. Type = col_character(), ## .. .. Len = col_double(), ## .. .. Format = col_double(), ## .. .. Informat = col_double(), ## .. .. Label = col_character() ## .. .. ) ## ..- attr(*, &quot;problems&quot;)=&lt;externalptr&gt; ## $ : spc_tbl_ [21 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## ..$ Num : num [1:21] 20 NA NA 21 NA NA 22 NA NA 23 ... ## ..$ Variable: chr [1:21] &quot;ABP_1190&quot; NA NA &quot;ABP_1200&quot; ... ## ..$ Type : chr [1:21] &quot;Num&quot; NA NA &quot;Num&quot; ... ## ..$ Len : num [1:21] 8 NA NA 8 NA NA 8 NA NA 8 ... ## ..$ Format : chr [1:21] &quot;2.&quot; NA NA &quot;2.&quot; ... ## ..$ Informat: chr [1:21] &quot;2.&quot; NA NA &quot;2.&quot; ... ## ..$ Label : chr [1:21] &quot;How much bother is the worry that you will have an asthma attack when&quot; &quot;visiting a new place? 0=No bother at all, 1=Minor irritation, 2=Slight&quot; &quot;bother, 3=Moderate bother, 4=A lot of bother, 5=Makes my life a misery&quot; &quot;How much bother is the worry that you will catch a cold? 0=No bother at&quot; ... ## ..- attr(*, &quot;spec&quot;)= ## .. .. cols( ## .. .. Num = col_double(), ## .. .. Variable = col_character(), ## .. .. Type = col_character(), ## .. .. Len = col_double(), ## .. .. Format = col_character(), ## .. .. Informat = col_character(), ## .. .. Label = col_character() ## .. .. ) ## ..- attr(*, &quot;problems&quot;)=&lt;externalptr&gt; Exploring further, here is how each dataframe is formatted: datatable(tables[[1]]) Notice that, although the dataframe format mirrors the PDF table format, the label column is stored across multiple rows with NAs in the other columns of that row because the text was across multiple lines. In our final dataframe, we will want the entire block of text in one cell. We can also remove the “Len”, “Format”, and “Informat” columns because they are not informative and they are not found in every table. Next, we will walk through how to clean up this table using a series of steps in tidyverse. Cleaning dataframes First, we will select the columns we are interested in and use the fill() function to change the NAs in the “Num” column so that each line of text in the “Label” column has the correct “Num” value in the same row. cleaned_table1 &lt;- data.frame(tables[[1]]) %&gt;% # Extract the first table in the list # Select only the columns of interest select(c(Num, Variable, Type, Label)) %&gt;% # Change the &quot;Num&quot; column to numeric, which is required for the fill function mutate(Num = as.numeric(Num)) %&gt;% # Fill in the NAs in the &quot;Num&quot; column down the column fill(Num, .direction = &quot;down&quot;) datatable(cleaned_table1) We still need to move all of the Label text for each variable into one cell in one row instead of across multiple rows. For this, we can use the unlist() function. Here is a demonstration of how the unlist() function works using just the first variable: cleaned_table1_var1 &lt;- cleaned_table1 %&gt;% # Filter dataframe to just contain rows associated with the first variable filter(Num == 1) %&gt;% # Paste all character strings in the Label column with a space in between them into a new column called &quot;new_label&quot; mutate(new_label = paste(unlist(Label), collapse = &quot; &quot;)) datatable(cleaned_table1_var1) We now have all of the text we want in one cell, but we have duplicate rows that we don’t need. We can get rid of these rows by assigning blank values “NA” and then omitting rows that contain NAs. cleaned_table1_var1 &lt;- cleaned_table1_var1 %&gt;% mutate(across(Variable, na_if, &quot;&quot;)) %&gt;% na.omit() datatable(cleaned_table1_var1) We need to apply this code to the whole dataframe and not just one variable, so we can add group_by(Num) to our cleaning workflow, followed by the code we just applied to our filtered dataframe. cleaned_table1 &lt;- data.frame(tables[[1]]) %&gt;% # Extract the first table in the list # Select only the columns of interest select(c(Num, Variable, Type, Label)) %&gt;% # Change the &quot;Num&quot; column to numeric, which is required for the fill function mutate(Num = as.numeric(Num)) %&gt;% # Fill in the NAs in the &quot;Num&quot; column down the column fill(Num, .direction = &quot;down&quot;) %&gt;% # Group by variable number group_by(Num) %&gt;% # Unlist the text replace the text in the &quot;Label&quot; column with the unlisted text mutate(Label = paste(unlist(Label), collapse =&quot; &quot;)) %&gt;% # Make blanks in the &quot;Variable&quot; column into NAs mutate(across(Variable, na_if, &quot;&quot;)) %&gt;% # Remove rows with NAs na.omit() datatable(cleaned_table1) Ultimately, we need to clean up each dataframe in the list the same way, and we need all of the dataframes to be in one dataframe, instead of in a list. There are a couple of different ways to do this. Both rely on the code shown above for cleaning up each dataframe. Option #1 uses a for loop, while Option #2 uses application of a function on the list of dataframes. Both result in the same ending dataframe! Option #1 # Create a dataframe for storing variables variables &lt;- data.frame() # Make a for loop to format each dataframe and add it to the variables for (i in 1:length(tables)) { table &lt;- data.frame(tables[[i]]) %&gt;% select(c(Num, Variable, Type, Label)) %&gt;% mutate(Num = as.numeric(Num)) %&gt;% fill(Num, .direction = &quot;down&quot;) %&gt;% group_by(Num) %&gt;% mutate(Label = paste(unlist(Label), collapse =&quot; &quot;)) %&gt;% mutate(across(Variable, na_if, &quot;&quot;)) %&gt;% na.omit() variables &lt;- bind_rows(variables, table) } # View resulting dataframe datatable(variables) Option #2 # Write a function that applies all of the cleaning steps to an dataframe (output = cleaned dataframe) clean_tables &lt;- function(data) { data &lt;- data %&gt;% select(c(Num, Variable, Type, Label)) %&gt;% mutate(Num = as.numeric(Num)) %&gt;% fill(Num, .direction = &quot;down&quot;) %&gt;% group_by(Num) %&gt;% mutate(Label = paste(unlist(Label), collapse =&quot; &quot;)) %&gt;% mutate(across(Variable, na_if, &quot;&quot;)) %&gt;% na.omit() return(data) } # Apply the function over each table in the list of tables tables_clean &lt;- lapply(X = tables, FUN = clean_tables) # Unlist the dataframes and combine them into one dataframe tables_clean_unlisted &lt;- do.call(rbind, tables_clean) # View resulting dataframe datatable(tables_clean_unlisted) Adding Dataset Names We now have a dataframe with all of the information from the PDFs contained in one long table. However, now we need to add back in the label on top of each table. We can’t do this with the tabulapdf package because the name isn’t stored in the table. But we can use the pdftools package for this! First, we will read in the pdf using the PDF tools package. This results in a vector containing a long character string for each page of the PDF. Notice a few features of these character strings: Each line is separated by \\n Elements [1] and [2] of the vector contain the text “dataset Name:”, while element [3] does not because the third page was a continuation of the table from the second page and therefore did not have a table title. table_names &lt;- pdf_text(&quot;Module4_3_Input/Module4_3_InputData4.pdf&quot;) head(table_names[1:3]) ## [1] &quot; 10:13 Tuesday, February 11, 2020 1\\n\\n\\nData Set Name: aaaq.sas7bdat\\n\\nNum Variable Type Len Format Informat Label\\n 1 AAQ_1000 Num 8 2. 2. In the past 3 days, how much of the time did your asthma keep you from\\n doing your usual activities at work, school, or at home? 0=None of the\\n time, 1=A little of the time, 2=Some of the time, 3=Most of the time, 4=All\\n of the time\\n 2 AAQ_1010 Num 8 2. 2. During the past 3 days, how often have you had asthma symptoms? 0=Not\\n at all, 1=Once per day, 2=2-3 times per day, 3=4-5 times per day, 4=6 or\\n more times per day\\n 3 AAQ_1020 Num 8 2. 2. During the past 3 days, how often have you used your rescue inhaler or\\n nebulizer medication ? 0=Not at all, 1=Once per day, 2=2-3 times per day,\\n 3=4-5 times per day, 4=6 or more times per day\\n 4 AAQ_1030 Num 8 2. 2. During the past 3 days, how many total times did your asthma symptoms\\n wake you up from sleep? 0=Not at all, 1=1 time in the last 3 days, 2=2-3\\n times in the last 3 days, 3=4-5 times in the last 3 days, 4=&gt;=6 times in the\\n last 3 days\\n 5 AAQ_1040 Num 8 2. 2. How would you rate the amount of impairment you have experienced due\\n to your asthma in the past 3 days? 0=No impairment, 1=Mild impairment,\\n 2=Moderate impairment, 3=Severe impairment, 4=Very severe impairment\\n 6 AAQ_1050 Num 8 2. 2. How stressed or frightened were you by your asthma symptoms in the past\\n 3 days? 0=Not at all, 1=Mildly, 2=Moderately, 3=Severely, 4=Very\\n severely\\n 7 AAQ_1060 Num 8 2. 2. Why do you think your asthma was worse in the past 3 days compared to\\n what is normal for you? 0=I have not been worse over the past 3 days. My\\n asthma symptoms have been usual., 1=Common cold, 2=Allergies,\\n 3=Pollution or chemical irritant, 4=Too little asth\\n 8 VNUM_C Char 3 $3. $3. Visit Number (character)\\n 9 VNUM Num 8 Visit Number (numeric)\\n 10 VDATE Num 8 Number of days from Visit 1 to this visit\\n 11 RAND_ID Char 6 Randomized Master ID\\n 12 ENROLL_TYPE Char 15 Enrollment Type (Screen Fail, Randomized, Healthy Control)\\n 13 ENROLL_ORDER Num 8 Enrollment Order Number\\n&quot; ## [2] &quot; 10:13 Tuesday, February 11, 2020 2\\n\\n\\n\\nData Set Name: abp.sas7bdat\\n\\nNum Variable Type Len Format Informat Label\\n 1 ABP_1000 Num 8 2. 2. Are you currently retired? 1=Yes,0=No\\n 2 ABP_1010 Num 8 2. 2. Are you retired because of asthma? 1=Yes,0=No\\n 3 ABP_1020 Num 8 2. 2. Are you currently unemployed? 1=Yes,0=No\\n 4 ABP_1030 Num 8 2. 2. Are you unemployed because of asthma? 1=Yes,0=No\\n 5 ABP_1040 Num 8 2. 2. Do you get paid to do work? 1=Yes,0=No\\n 6 ABP_1050 Num 8 2. 2. How much does your asthma bother you at your paid work? 0=No bother\\n at all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother, 4=A lot of\\n bother, 5=Makes my life a misery\\n 7 ABP_1060 Num 8 2. 2. Overall, how much does your asthma bother you when you do jobs\\n around the house? 0=No bother at all, 1=Minor irritation, 2=Slight bother,\\n 3=Moderate bother, 4=A lot of bother, 5=Makes my life a misery,\\n 0=None of these really apply to me\\n 8 ABP_1070 Num 8 2. 2. Overall, how much does your asthma bother your social life? 0=No bother\\n at all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother, 4=A lot of\\n bother, 5=Makes my life a misery\\n 9 ABP_1080 Num 8 2. 2. Overall, how much does your asthma bother your personal life? 0=No\\n bother at all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother,\\n 4=A lot of bother, 5=Makes my life a misery, 0=None of these really\\n apply to me\\n 10 ABP_1090 Num 8 2. 2. Are you involved in leisure activities, such as: walking for pleasure,\\n sports, exercise, travelling, taking vacations? 1=Yes,0=No\\n 11 ABP_1100 Num 8 2. 2. When involved in leisure activities, how much does your asthma bother\\n you? 0=No bother at all, 1=Minor irritation, 2=Slight bother, 3=Moderate\\n bother, 4=A lot of bother, 5=Makes my life a misery\\n 12 ABP_1110 Num 8 2. 2. Would you say that you can&#39;t do some of these sorts of things because of\\n asthma? 1=Yes,0=No\\n 13 ABP_1120 Num 8 2. 2. How much does your asthma bother you when you sleep? 0=No bother at\\n all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother, 4=A lot of\\n bother, 5=Makes my life a misery\\n 14 ABP_1130 Num 8 2. 2. How much does the cost of your asthma medicines bother you? 0=No\\n bother at all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother,\\n 4=A lot of bother, 5=Makes my life a misery\\n 15 ABP_1140 Num 8 2. 2. Do you get free prescriptions? 1=Yes,0=No\\n 16 ABP_1150 Num 8 2. 2. How much does the inconvenience or embarrassment of taking your\\n asthma medicines bother you? 0=No bother at all, 1=Minor irritation,\\n 2=Slight bother, 3=Moderate bother, 4=A lot of bother, 5=Makes my life\\n a misery\\n 17 ABP_1160 Num 8 2. 2. How much do coughs and colds bother you? 0=No bother at all, 1=Minor\\n irritation, 2=Slight bother, 3=Moderate bother, 4=A lot of bother,\\n 5=Makes my life a misery, 0=None of these really apply to me\\n 18 ABP_1170 Num 8 2. 2. Feeling upset is also a bother. Does your asthma make you feel anxious,\\n depressed, tired, or helpless? 1=Yes,0=No\\n 19 ABP_1180 Num 8 2. 2. Feeling upset is also a bother. Does your asthma make you feel anxious,\\n depressed, tired, or helpless? 0=No bother at all, 1=Minor irritation,\\n 2=Slight bother, 3=Moderate bother, 4=A lot of bother, 5=Makes my life\\n a misery\\n&quot; ## [3] &quot; 10:13 Tuesday, February 11, 2020 3\\n\\n\\nNum Variable Type Len Format Informat Label\\n 20 ABP_1190 Num 8 2. 2. How much bother is the worry that you will have an asthma attack when\\n visiting a new place? 0=No bother at all, 1=Minor irritation, 2=Slight\\n bother, 3=Moderate bother, 4=A lot of bother, 5=Makes my life a misery\\n 21 ABP_1200 Num 8 2. 2. How much bother is the worry that you will catch a cold? 0=No bother at\\n all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother, 4=A lot of\\n bother, 5=Makes my life a misery\\n 22 ABP_1210 Num 8 2. 2. How much bother is the worry that you will let others down? 0=No bother\\n at all, 1=Minor irritation, 2=Slight bother, 3=Moderate bother, 4=A lot of\\n bother, 5=Makes my life a misery\\n 23 ABP_1220 Num 8 2. 2. How much bother is the worry that your health may get worse in the\\n future? 0=No bother at all, 1=Minor irritation, 2=Slight bother,\\n 3=Moderate bother, 4=A lot of bother, 5=Makes my life a misery\\n 24 ABP_1230 Num 8 2. 2. How much bother is the worry that you won&#39;t be able to cope with an\\n asthma attack? 0=No bother at all, 1=Minor irritation, 2=Slight bother,\\n 3=Moderate bother, 4=A lot of bother, 5=Makes my life a misery\\n 25 VNUM_C Char 3 $3. $3. Visit Number (character)\\n 26 VNUM Num 8 Visit Number (numeric)\\n 27 VDATE Num 8 Number of days from Visit 1 to this visit\\n 28 RAND_ID Char 6 Randomized Master ID\\n 29 ENROLL_TYPE Char 15 Enrollment Type (Screen Fail, Randomized, Healthy Control)\\n 30 ENROLL_ORDER Num 8 Enrollment Order Number\\n&quot; Similar to the table cleaning section, we will work through an example of extracting the text of interest from one of these character vectors, then apply the same code to all of the character vectors. First, we will select just the first element in the vector and make it into a dataframe. # Create dataframe dataset_name_df_var1 &lt;- data.frame(strsplit(table_names[1], &quot;\\n&quot;)) # Clean column name colnames(dataset_name_df_var1) &lt;- c(&quot;Text&quot;) # View dataframe datatable(dataset_name_df_var1) Next, we will extract the dataset name using the same approach used in extracting values from the nanoparticle tracking example above and assign the name to a variable. We filter by the string “Data Set Name” because this is the start of the text string in the row where our dataset name is stored and is the same across all of our datasets. # Create dataframe dataset_name_df_var1 &lt;- dataset_name_df_var1 %&gt;% filter(grepl(&quot;Data Set Name&quot;, dataset_name_df_var1$Text)) %&gt;% separate(Text, into = c(NA, &quot;dataset&quot;), sep = &quot;Data Set Name: &quot;) # Assign variable dataset_name_var1 &lt;- dataset_name_df_var1[1,1] # View variable name dataset_name_var1 ## [1] &quot;aaaq.sas7bdat&quot; Now that we have the dataset name stored as a variable, we can create a dataframe that will correspond to the rows in our variables dataframe. The challenge is that each dataset contains a different number of variables! We can determine how many rows each dataset contains by returning to our variables dataframe and calculating the number of rows associated with each dataset. The following code splits the variables dataframe into a list of dataframes by each occurrence of 1 in the “Num” column (when the numbering restarts for a new dataset). # Calculate the number of rows associated with each dataset for reference dataset_list &lt;- split(variables, cumsum(variables$Num == 1)) glimpse(dataset_list[1:3]) ## List of 3 ## $ 1:&#39;data.frame&#39;: 13 obs. of 4 variables: ## ..$ Num : num [1:13] 1 2 3 4 5 6 7 8 9 10 ... ## ..$ Variable: chr [1:13] &quot;AAQ_1000&quot; &quot;AAQ_1010&quot; &quot;AAQ_1020&quot; &quot;AAQ_1030&quot; ... ## ..$ Type : chr [1:13] &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; ... ## ..$ Label : chr [1:13] &quot;In the past 3 days, how much of the time did your asthma keep you from doing your usual activities at work, sch&quot;| __truncated__ &quot;During the past 3 days, how often have you had asthma symptoms? 0=Not at all, 1=Once per day, 2=2-3 times per d&quot;| __truncated__ &quot;During the past 3 days, how often have you used your rescue inhaler or nebulizer medication ? 0=Not at all, 1=O&quot;| __truncated__ &quot;During the past 3 days, how many total times did your asthma symptoms wake you up from sleep? 0=Not at all, 1=1&quot;| __truncated__ ... ## $ 2:&#39;data.frame&#39;: 30 obs. of 4 variables: ## ..$ Num : num [1:30] 1 2 3 4 5 6 7 8 9 10 ... ## ..$ Variable: chr [1:30] &quot;ABP_1000&quot; &quot;ABP_1010&quot; &quot;ABP_1020&quot; &quot;ABP_1030&quot; ... ## ..$ Type : chr [1:30] &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; ... ## ..$ Label : chr [1:30] &quot;Are you currently retired? 1=Yes,0=No&quot; &quot;Are you retired because of asthma? 1=Yes,0=No&quot; &quot;Are you currently unemployed? 1=Yes,0=No&quot; &quot;Are you unemployed because of asthma? 1=Yes,0=No&quot; ... ## $ 3:&#39;data.frame&#39;: 11 obs. of 4 variables: ## ..$ Num : num [1:11] 1 2 3 4 5 6 7 8 9 10 ... ## ..$ Variable: chr [1:11] &quot;ACT_1&quot; &quot;ACT_2&quot; &quot;ACT_3&quot; &quot;ACT_4&quot; ... ## ..$ Type : chr [1:11] &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; &quot;Num&quot; ... ## ..$ Label : chr [1:11] &quot;In the past 4 weeks, how much of the time did your asthma keep you from getting as much done at work, school or&quot;| __truncated__ &quot;During the past 4 weeks, how often have your had shortness of breath? 1=All of the time, 2=Most of the time, 3=&quot;| __truncated__ &quot;During the past 4 weeks, how often did your asthma symptoms wake you up at night or earlier than usual in the m&quot;| __truncated__ &quot;During the past 4 weeks, how often have you used your rescue inhaler or nebulizer medication? 1=All of the time&quot;| __truncated__ ... The number of rows in each list is the number of variables in that dataset. We can use this value in creating our dataframe of dataset names. # Store the number of rows in a variable n_rows = nrow(data.frame(dataset_list[1])) # Repeat the dataset name for the number of variables there are dataset_name_var1 = data.frame(&quot;dataset_name&quot; = rep(dataset_name_var1, times = n_rows)) # View data farme datatable(dataset_name_var1) We now have a dataframe that can be joined with our variables dataframe for the first table. We can apply this approach to each table in our original PDF using a for loop. # Make dataframe to store dataset names dataset_names &lt;- data.frame() # Create list of datasets dataset_list &lt;- split(variables, cumsum(variables$Num == 1)) # Remove elements from the table_names vector that do not contain the string &quot;Data Set Name&quot; table_names_filtered &lt;- stringr::str_subset(table_names, &#39;Data Set Name&#39;) # Populate dataset_names dataframe for (i in 1:length(table_names_filtered)) { # Get dataset name dataset_name_df &lt;- data.frame(strsplit(table_names_filtered[i], &quot;\\n&quot;)) base::colnames(dataset_name_df) &lt;- c(&quot;Text&quot;) dataset_name_df &lt;- dataset_name_df %&gt;% filter(grepl(&quot;Data Set Name&quot;, dataset_name_df$Text)) %&gt;% separate(Text, into = c(NA, &quot;dataset&quot;), sep = &quot;Data Set Name: &quot;) dataset_name &lt;- dataset_name_df[1,1] # Determine number of variables in that dataset data_set &lt;- data.frame(dataset_list[i]) n_rows = nrow(data_set) # Repeat the dataset name for the number of variables there are dataset_name = data.frame(&quot;Data Set Name&quot; = rep(dataset_name, times = n_rows)) # Bind to dataframe dataset_names &lt;- bind_rows(dataset_names, dataset_name) } # Rename column colnames(dataset_names) &lt;- c(&quot;Data Set Name&quot;) # View datatable(dataset_names) Combining Dataset Names and Variable Information Last, we will merge together the dataframe containing dataset names and variable information. # Merge together final_variable_df &lt;- cbind(dataset_names, variables) %&gt;% rename(&quot;Variable Description&quot; = &quot;Label&quot;, &quot;Variable Number Within Dataset&quot; = &quot;Num&quot;) %&gt;% clean_names() datatable(final_variable_df) We can also determine how many total variables we have, all of which are accessible via the table we just generated. # Total number of variables nrow(final_variable_df) ## [1] 1190 # Total number of variables With this, we can answer Environmental Health Question #2: How many variables total are available to us to request from the study whose data are stored in the repository, and what are these variables? Answer: There are 1190 variable available to us. We can browse through the variables, including the sub-table they were from, the type of variable they are, and how they were derived using the table we generated. Concluding Remarks This training module provides example case studies demonstrating how to import PDF data into R and clean it so that it is more useful and accessible for analyses. The approaches demonstrated in this module, though specific to our specific example data, can be adapted to many different types of PDF data. Test Your Knowledge Using the same input files that we used in part 1, “Importing Data from Many Single PDFs with the Same Formatting”, found in the Module4_3_TYKInput folder, extract the remaining variables of interest (Original Concentration and Positions Removed) from the PDFs and summarize them in one dataframe. "],["two-group-comparisons-and-visualizations.html", "4.4 Two Group Comparisons and Visualizations Introduction to Training Module Overview of Two Group Statistical Tests Statistical vs. Biological Significance Unpaired Test Example Paired Test Example Visualizing Results Concluding Remarks", " 4.4 Two Group Comparisons and Visualizations This training module was developed by Elise Hickman, Alexis Payton, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Two group statistical comparisons, in which we want to know whether the means between two different groups are significantly different, are some of the most common statistical tests in environmental health research and even biomedical research as a field. In this training module, we will demonstrate how to run two group statistical comparisons and how to present publication-quality figures and tables of these results. We will continue to use the same example dataset as used in this chapter’s previous modules, which represents concentrations of inflammatory biomarkers secreted by airway epithelial cells after exposure to different concentrations of acrolein. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Are there significant differences in inflammatory biomarker concentrations between cells from male and female donors at baseline? Are there significant differences in inflammatory biomarker concentrations between cells exposed to 0 and 4 ppm acrolein? Workspace Preparation and Data Import Here, we will import the processed data that we generated at the end of TAME 2.0 Module 4.2 Data Import, Processing, and Summary Statistics. These data, along with the associated demographic data, were introduced in TAME 2.0 Module 4.1 Overview of Experimental Design and Example Data. These data represent log2 concentrations of inflammatory biomarkers secreted by airway epithelial cells after exposure to four different concentrations of acrolein (plus filtered air as a control). We will also load packages that will be needed for the analysis, including previously introduced packages such as openxlsx, tidyverse, DT, and ggpubr, and additional packages relevant to statistical analysis and graphing that will be discussed in greater detail below. # Load packages library(openxlsx) library(tidyverse) library(DT) library(rstatix) library(ggpubr) # Import data biomarker_data &lt;- read.xlsx(&quot;Module4_4_Input/Module4_4_InputData1.xlsx&quot;) demographic_data &lt;- read.xlsx(&quot;Module4_4_Input/Module4_4_InputData2.xlsx&quot;) # View data datatable(biomarker_data) datatable(demographic_data) Overview of Two Group Statistical Tests Before applying statistical tests to our data, let’s first review common two group statistical tests, their underlying assumptions, and variations on these tests. Common Tests The two most common two group statistical tests are the… T-test (also known as the student’s t-test) and the Wilcoxon test (also known as the Wilcox test, Wilcoxon test, or Mann Whitney test) Both of these tests are testing the null hypothesis that the means of the two populations (groups) are the same; the alternative hypothesis is that they are not the same. A significant p-value means that we can reject the null hypothesis that the means of the two groups are the same. Whether or not a p-value meets criteria for significance is experiment-specific, though commonly implemented p-value filters for significance include p&lt;0.05 and p&lt;0.01. P-values can also be called alpha values, and they indicate the probability of a type I error, or false positive, where the null hypothesis is rejected despite it actually being true. On the other hand, a type II error, or false negative, occurs when the null hypothesis is not rejected when it actually should have been. Assumptions The main difference between these two tests is in the assumption about the underlying distribution of the data. T-tests assume that the data are pulled from a normal distribution, while Wilcoxon tests do not assume that the data are pulled from a normal distribution. Therefore, it is most appropriate to use a t-test when data are, in general, normally distributed and a Wilcoxon test when data are not normally distributed. Additional assumptions underlying t-tests and Wilcoxon test are: The dependent variable is continuous or ordinal (discrete, ordered values). The data is collected from a representative, random sample. T-tests also assume that: The standard deviations of the two groups are approximately equal (also called homogeneity of variance). When to Use a Parametric vs Non-Parametric Test? Deciding whether to use a parametric or non-parametric test isn’t a one size fits all approach, and the decision should be made holistically for each dataset. Typically, parametric tests should be used when the data are normally distributed, continuous, random sampled, without extreme outliers, and representative of independent samples or participants. A non-parametric test can be used when the sample size (n) is small, outliers are present in the dataset, and/or the data are not normally distributed. This decision matters more when dealing with smaller sample sizes (n&lt;10) as smaller sample sizes are more prone to being skewed, and parametric tests are more sensitive to outliers. Therefore, when dealing with a smaller n, it might be best to perform a data transformation as discussed in TAME 2.0 Module 3.3 Normality Testing &amp; Data Transformations and then perform a parametric test if more parametric assumptions are able to be met, or to use non-parametric tests. For larger sample sizes (n&gt;50), outliers can potentially be removed and the dataset can be retested for assumptions. Lastly, what’s considered “small” or “large” in regards to sample size can be subjective and should be taken into consideration within the context of the experiment. Variations Unequal Variance: When the assumption of homogeneity of variance is not met, a Welch’s t-test is generally preferred over a student’s t-test. This can be implemented easily by setting var.equal = FALSE as an argument to the function executing the t-test (e.g., t.test(), t_test()). For more on testing homogeneity of variance in R, see here. Paired vs Unpaired: Variations on the t-test and Wilcoxon test are used when the experimental design is paired (also called repeated measures or matching). This occurs when there are different treatments, exposures, or time points collected from the same biological/experimental unit. For example, cells from the same donor or passage number exposed to different concentrations of a chemical represents a paired design. Matched/paired experiments have increased power to detect significant differences because samples can be compared back to their own controls. One vs Two-Sided: A one-sided test evaluates the hypothesis that the mean of the treatment group significantly differs in a specific direction from the control. A two-sided test evaluates the hypothesis that the mean of the treatment group significantly differs from the control but does not specify a direction for that change. A two-sided test is the preferred approach and the default in R because, typically, either direction of change is possible and represents an informative finding. However, one-sided tests may be appropriate if an effect can only possibly occur in one direction. This can be implemented by setting alternative = \"one.sided\" within the statistical testing function. Which test should I choose? We provide the following flowchart to help guide your choice of statistical test to compare two groups: Statistical vs. Biological Significance Another important topic to discuss before proceeding to statistical testing is the true meaning of statistical significance. Statistical significance simply means that it is unlikely that the patterns being observed are due to random chance. However, just because an effect is statistically significant does not mean that it is biologically significant (i.e., has notable biological consequences). Often, there also needs to be a sufficient magnitude of effect (also called effect size) for the effects on a system to be meaningful. Although a p-value &lt; 0.05 is often considered the threshold for significance, this is just a standard threshold set to a generally “acceptable” amount of error (5%). What about a p-value of 0.058 with a very large biological effect? Accounting for effect size is also why filters such as log2 fold change are often applied alongside p-value filters in -omics based analysis. In discussions of effect size, the population size is also a consideration - a small percentage increase in a very large population can represent tens of thousands of individuals (or more). Another consideration is that we frequently do not know what magnitude of biological effect should be considered “significant.” These discussions can get complicated very quickly, and here we do not propose to have a solution to these thought experiments; rather, we recommend considering both statistical and biological significance when interpreting data. And, as stated in other sections of TAME, transparent reporting of statistical results will aid the audience in interpreting the data through their preferred perspectives. Unpaired Test Example We will start by performing a statistical test to determine whether there are significant differences in biomarker concentrations between male and female donors at baseline (0 ppm exposure). Previously we determined that the majority of our data was non-normally distributed (see TAME 2.0 Module 4.2 Data Import, Processing, and Summary Statistics), so we’ll skip testing for that assumption in this module. Based on those results, we will use the Wilcoxon test to determine if there are significant differences between groups. The Wilcoxon test does not assume homogeneity of variance, so we do not need to test for that prior to applying the test. This is an unpaired analysis because samples collected from the cells derived from male and female donor cells are different sets of cells (i.e., independent from each other). Thus, the specific statistical test applied will be the Wilcoxon Rank Sum test. First, we will filter our dataframe to only data representing the control (0 ppm) exposure: biomarker_data_malevsfemale &lt;- biomarker_data %&gt;% filter(Dose == &quot;0&quot;) Next, we need to add the demographic data to our dataframe: biomarker_data_malevsfemale &lt;- biomarker_data_malevsfemale %&gt;% left_join(demographic_data %&gt;% select(Donor, Sex), by = &quot;Donor&quot;) Here is what our data look like now: datatable(biomarker_data_malevsfemale) We can demonstrate the basic anatomy of the Wilcoxon test function wilcox.test() by running the function on just one variable. wilcox.test(IL1B ~ Sex, data = biomarker_data_malevsfemale) ## ## Wilcoxon rank sum exact test ## ## data: IL1B by Sex ## W = 29, p-value = 0.8371 ## alternative hypothesis: true location shift is not equal to 0 The p-value of 0.8371 indicates that males and females do not have significantly different concentrations of IL-1\\(\\beta\\). The wilcox.test() function is part of the pre-loaded package stats. The package rstatix provides identical statistical tests to stats but in a pipe-friendly (tidyverse-friendly) format, and these functions output results as dataframes rather than the text displayed above. biomarker_data_malevsfemale %&gt;% wilcox_test(IL1B ~ Sex) ## # A tibble: 1 × 7 ## .y. group1 group2 n1 n2 statistic p ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 IL1B F M 7 9 29 0.837 Here, we can see the exact same results as with the wilcox.test() function. For the rest of this module, we’ll proceed with using the rstatix version of statistical testing functions. Although it is simple to run the Wilcoxon test with the code above, it’s impractical for a large number of endpoints and doesn’t store the results in an organized way. Instead, we can run the Wilcoxon test over every variable of interest using a for loop. There are also other ways you could approach this, such as a function applied over a list. This for loop runs the Wilcoxon test on each endpoint, stores the results in a dataframe, and then binds together the results dataframes for each variable of interest. Note that you could easily change wilcox_test() to t_test() and add additional arguments to modify the way the statistical test is run. # Create a vector with the names of the variables you want to run the test on endpoints &lt;- colnames(biomarker_data_malevsfemale %&gt;% select(IL1B:VEGF)) # Create dataframe to store results sex_wilcoxres &lt;- data.frame() # Run for loop for (i in 1:length(endpoints)) { # Assign a name to the endpoint variable. endpoint &lt;- endpoints[i] # Run wilcox test and store in results dataframe. res_df &lt;- biomarker_data_malevsfemale %&gt;% wilcox_test(as.formula(paste0(endpoint, &quot;~ Sex&quot;, sep = &quot;&quot;))) # Bind results from this test with other tests in this loop sex_wilcoxres &lt;- rbind(sex_wilcoxres, res_df) } # View results sex_wilcoxres ## # A tibble: 6 × 7 ## .y. group1 group2 n1 n2 statistic p ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 IL1B F M 7 9 29 0.837 ## 2 IL6 F M 7 9 28.5 0.791 ## 3 IL8 F M 7 9 34 0.832 ## 4 IL10 F M 7 9 43 0.252 ## 5 TNFa F M 7 9 39 0.458 ## 6 VEGF F M 7 9 30 0.916 With this, we can answer Environmental Health Question #1: Are there significant differences in inflammatory biomarker concentrations between cells from male and female donors at baseline? Answer: There are not any significant differences in concentrations of any of our biomarkers between male and female donors at baseline. Adjusting for Multiple Hypothesis Testing Above, we compared concentrations between males and females for six different endpoints or variables. Each time we run a comparison (with a p-value threshold of &lt; 0.05), we are accepting that there is a 5% chance that a significant result will actually be due to random chance and that we are rejecting the null hypothesis when it is actually true (type I error). Since we are testing six different hypotheses simultaneously, what is the probability then of observing at least one significant result due just to chance? \\[\\mathbb{P}({\\rm At Least One Significant Result}) = 1 - \\mathbb{P}({\\rm NoSignificantResults}) = 1 - (1 - 0.05)^{6} = 0.26\\] Here, we can see that we have a 26% chance of observing at least one significant result, even if all the tests are actually not significant. This chance increases as our number of endpoints increases; therefore, adjusting for multiple hypothesis testing becomes even more important with larger datasets. Many methods exist for adjusting for multiple hypothesis testing, with some of the most popular including Bonferroni, False Discovery Rate (FDR), and Benjamini-Hochberg (BH). However, opinions about when and how to adjust for multiple hypothesis testing can vary and also depend on the question you are trying to answer. For example, when there are a low number of variables (e.g., &lt; 10), it’s often not necessary to adjust for multiple hypothesis testing, and when there are many variables (e.g., 100s to 1000s), it is necessary, but what about for an intermediate number of comparisons? Whether or not to apply multiple hypothesis test correction also depends on whether each endpoint is of interest on its own or whether the analysis seeks to make general statements about all of the endpoints together and on whether reducing type I or type II error is most important in the analysis. For this analysis, we will not adjust for multiple hypothesis testing due to our relatively low number of variables. For more on multiple hypothesis testing, check out the following publications: Mohieddin J; Naser AP. “Why, When and How to Adjust Your P Values?”. Cell Journal (Yakhteh), 20, 4, 2018, 604-607. doi: 10.22074/cellj.2019.5992 PUBMID: 30124010 Feise, R.J. Do multiple outcome measures require p-value adjustment?. BMC Med Res Methodol 2, 8 (2002). https://doi.org/10.1186/1471-2288-2-8 PUBMID: 12069695 Paired Test Example To demonstrate an example of a paired two group test, we can also determine whether exposure to 4 ppm acrolein significantly changes biomarker concentrations. This is now a paired design because each donor’s cells were exposed to both 0 and 4 ppm acrolein. To prepare the data, we will filter the dataframe to only include 0 and 4 ppm: biomarker_data_0vs4 &lt;- biomarker_data %&gt;% filter(Dose == &quot;0&quot; | Dose == &quot;4&quot;) Let’s view the dataframe. Note how the measurements for each donor are next to each other - this an important element of the default handling of the paired analysis in R. The dataframe should have the donors in the same order for the 0 and 4 ppm data. datatable(biomarker_data_0vs4) We can now run the same type of loop that we ran before, changing the independent variable in the formula to ~ Dose and adding paired = TRUE to the wilcox_test() function. # Create a vector with the names of the variables you want to run the test on endpoints &lt;- colnames(biomarker_data_0vs4 %&gt;% select(IL1B:VEGF)) # Create dataframe to store results dose_wilcoxres &lt;- data.frame() # Run for loop for (i in 1:length(endpoints)) { # Assign a name to the endpoint variable. endpoint &lt;- endpoints[i] # Run wilcox test and store in results dataframe. res_df &lt;- biomarker_data_0vs4 %&gt;% wilcox_test(as.formula(paste0(endpoint, &quot;~ Dose&quot;, sep = &quot;&quot;)), paired = TRUE) # Bind results from this test with other tests in this loop dose_wilcoxres &lt;- rbind(dose_wilcoxres, res_df) } # View results dose_wilcoxres ## # A tibble: 6 × 7 ## .y. group1 group2 n1 n2 statistic p ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 IL1B 0 4 16 16 18 0.00763 ## 2 IL6 0 4 16 16 114 0.0155 ## 3 IL8 0 4 16 16 0 0.0000305 ## 4 IL10 0 4 16 16 65 0.9 ## 5 TNFa 0 4 16 16 5 0.000305 ## 6 VEGF 0 4 16 16 28 0.0386 Although this dataframe contains useful information about our statistical test, such as the groups being compared, the sample size (n) of each group, and the test statistic, what we really want (and what would likely be shared in supplemental material), is a more simplified version of these results in table format and more detailed information (n, specific statistical test, groups being compared) in the table legend. We can clean up the results using the following code to make clearer column names and ensure that the p-values are formatted consistently. dose_wilcoxres &lt;- dose_wilcoxres %&gt;% select(c(.y., p)) %&gt;% mutate(p = format(p, digits = 3, scientific = TRUE)) %&gt;% rename(&quot;Variable&quot; = &quot;.y.&quot;, &quot;P-Value&quot; = &quot;p&quot;) datatable(dose_wilcoxres) With this, we can answer Environmental Health Question #2: Are there significant differences in inflammatory biomarker concentrations between cells exposed to 0 and 4 ppm acrolein? Answer: Yes, there are significant differences in IL-1\\(\\beta\\), IL-6, IL-8, TNF-\\(\\alpha\\), and VEGF concentrations between cells exposed to 0 and 4 ppm acrolein. Visualizing Results Now, let’s visualize our results using ggplot2. For an introduction to ggplot2 visualizations, see TAME 2.0 Modules 3.1 Data Visualizations and 3.2 Improving Data Visualizations, as well as the extensive online documentation available for ggplot2. Single Plots We will start by making a very basic box and whisker plot of the IL-1\\(\\beta\\) data with individual data points overlaid. It is best practice to show all data points, allowing the reader to view the whole spread of the data, which can be obscured by plots such as bar plots with mean and standard error. # Setting theme for plot theme_set(theme_bw()) # Making plot ggplot(biomarker_data_0vs4, aes(x = Dose, y = IL1B)) + geom_boxplot() + geom_jitter(position = position_jitter(0.15)) We could add statistical markings to denote significance to this graph manually in PowerPoint or Adobe Illustrator, but there are actually R packages that act as extensions to ggplot2 and will do this for you! Two of our favorites are ggpubr and ggsignif. Here is an example using ggpubr: ggplot(biomarker_data_0vs4, aes(x = Dose, y = IL1B)) + geom_boxplot() + geom_jitter(position = position_jitter(0.15)) + # Adding a p value from a paired Wilcoxon test stat_compare_means(method = &quot;wilcox.test&quot;, paired = TRUE) We can further clean up our figure by modifying elements of the plot’s theme, including the font sizes, axis range, colors, and the way that the statistical results are presented. Perfecting figures can be time consuming but ultimately worth it, because clear figures aid greatly in presenting a coherent story that is understandable to readers/listeners. ggplot(biomarker_data_0vs4, aes(x = Dose, y = IL1B)) + # outlier.shape = NA removes outliers geom_boxplot(aes(fill = Dose), outlier.shape = NA) + # Changing box plot colors scale_fill_manual(values = c(&quot;#BFBFBF&quot;, &quot;#EE2B2B&quot;)) + geom_jitter(size = 3, position = position_jitter(0.15)) + # Adding a p value from a paired Wilcoxon test stat_compare_means(method = &quot;wilcox.test&quot;, paired = TRUE, # Changing the value to asterisks and moving to the middle of the plot label = &quot;p.signif&quot;, label.x = 1.5, label.y = 4.5, size = 12) + ylim(2.5, 5) + # Changing y axis label labs(y = &quot;Log2(IL-1\\u03B2 (pg/mL))&quot;) + # Removing legend theme(legend.position = &quot;none&quot;, axis.title = element_text(color = &quot;black&quot;, size = 15), axis.title.x = element_text(vjust = -0.75), axis.title.y = element_text(vjust = 2), axis.text = element_text(color = &quot;black&quot;, size = 12)) Multiple plots Making one plot was relatively straightforward, but to graph all of our endpoints, we would either need to repeat that code chunk for each individual biomarker or write a function to create similar plots given a specific biomarker as input. Then, we would need to stitch together the individual plots in external software or using a package such as patchwork (which is a great package if you need to combine individual figures from different sources or different size ratios!). While these are workable solutions and would get us to the same place, ggplot2 actually contains a function - facet_wrap() - that can be used to graph multiple endpoints from the same groups in one figure panel, which takes care of a lot of the work for us! To prepare our data for facet plotting, first we will pivot it longer: biomarker_data_0vs4_long &lt;- biomarker_data_0vs4 %&gt;% pivot_longer(-c(Donor, Dose), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) datatable(biomarker_data_0vs4_long) Then, we can use similar code to what we used to make our single graph, with a few modifications to plot multiple panels simultaneously and adjust the style of the plot. Although it is beyond the scope of this module to explain the mechanics of each line of code, here are a few specific things to note about the code below that may be helpful when constructing similar plots: To create the plot with all six endpoints instead of just one, we: Changed input dataframe from wide to long format Changed y = from one specific endpoint to value Added the facet_wrap() argument ~ variable tells the function to make an individual plot for each variable nrow = 2 tells the function to put the plots into two rows scales = \"free_y\" tells the function to allow each individual graph to have a unique y-scale that best shows all of the data on that graph labeller feeds the edited (more stylistically correct) names for each panel to the function To ensure that the statistical results appear cleanly, within stat_compare_means(), we: Added hide.ns = TRUE so that only significant results are shown Added label.x.npc = \"center\" and hjust = 0.5 to ensure that asterisks are centered on the plot and that the text is center justified To add padding along the y axis, allowing space for significance asterisks, we added scale_y_continuous(expand = expansion(mult = c(0.1, 0.4))) # Create clean labels for the graph titles new_labels &lt;- c(&quot;IL10&quot; = &quot;IL-10&quot;, &quot;IL1B&quot; = &quot;IL-1\\u03B2 &quot;, &quot;IL6&quot; = &quot;IL-6&quot;, &quot;IL8&quot; = &quot;IL-8&quot;, &quot;TNFa&quot; = &quot;TNF-\\u03b1&quot;, &quot;VEGF&quot; = &quot;VEGF&quot;) # Make graph ggplot(biomarker_data_0vs4_long, aes(x = Dose, y = value)) + # outlier.shape = NA removes outliers geom_boxplot(aes(fill = Dose), outlier.shape = NA) + # Changing box plot colors scale_fill_manual(values = c(&quot;#BFBFBF&quot;, &quot;#EE2B2B&quot;)) + geom_jitter(size = 1.5, position = position_jitter(0.15)) + # Adding a p value from a paired Wilcoxon test stat_compare_means(method = &quot;wilcox.test&quot;, paired = TRUE, # Changing the value to asterisks and moving to the middle of the plot label = &quot;p.signif&quot;, size = 10, hide.ns = TRUE, label.x.npc = &quot;center&quot;, hjust = 0.5) + # Adding padding y axis scale_y_continuous(expand = expansion(mult = c(0.1, 0.4))) + # Changing y axis label ylab(expression(Log[2]*&quot;(Concentration (pg/ml))&quot;)) + # Faceting by each biomarker facet_wrap(~ variable, nrow = 2, scales = &quot;free_y&quot;, labeller = labeller(variable = new_labels)) + # Removing legend theme(legend.position = &quot;none&quot;, axis.title = element_text(color = &quot;black&quot;, size = 12), axis.title.x = element_text(vjust = -0.75), axis.title.y = element_text(vjust = 2), axis.text = element_text(color = &quot;black&quot;, size = 10), strip.text = element_text(size = 12, face = &quot;bold&quot;)) An appropriate title for this figure could be: “Figure X. Exposure to 4 ppm acrolein increases inflammatory biomarker secretion in primary human bronchial epithelial cells. Groups were compared using the Wilcoxon signed rank test. * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001, **** p &lt; 0.0001, n = 16 per group (paired).” Concluding Remarks In this module, we introduced two group statistical tests, which are some of the most common statistical tests applied in biomedical research. We applied these tests to our example dataset and demonstrated how to produce publication-quality tables and figures of our results. Implementing a workflow such as this enables efficient analysis of wet-bench generated data and customization of output figures and tables suited to your personal preferences. Test Your Knowledge Functional endpoints from these cultures were also measured. These endpoints were: 1) Membrane Permeability (MemPerm), 2) Trans-Epithelial Electrical Resistance (TEER), 3) Ciliary Beat Frequency (CBF), and 4) Expression of Mucin (MUC5AC). These data were already processed and tested for normality (see Test Your Knowledge for TAME 2.0 Module 4.2 Data Import, Processing, and Summary Statistics), with results indicating that two of the endpoints are normally distributed and two non-normally distributed. Due to the relatively low n of this dataset, we therefore recommend using non-parametric statistical tests. Use the same processes demonstrated in this module and the provided data (“Module4_4_TYKInput1.xlsx” (functional data) and “Module4_4_TYKInput2.xlsx” (demographic data)), run analyses and make publication-quality figures and tables to answer the following questions to determine: Are there significant differences in functional endpoints between cells from male and female donors at baseline? Are there significant differences in functional endpoints between cells exposed to 0 and 4 ppm acrolein? Go ahead and use non-parametric tests for these analyses. "],["multi-group-and-multi-variable-comparisons-and-visualizations.html", "4.5 Multi-Group and Multi-Variable Comparisons and Visualizations Introduction to Training Module Overview of Multi-Group Statistical Tests Multi-Group Analysis Example Visualization of Multi-Group Statistical Results Concluding Remarks", " 4.5 Multi-Group and Multi-Variable Comparisons and Visualizations This training module was developed by Elise Hickman, Alexis Payton, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module In the previous module, we covered how to apply two-group statistical testing, one of the most basic types of statistical tests. In this module, we will build on the concepts introduced previously to apply statistical testing to datasets with more than two groups, which are also very common in environmental health research. We will review common multi-group overall effects tests and post-hoc tests, and we will demonstrate how to apply these tests and how to graph the results using the same example dataset as in previous modules in this chapter, which represents concentrations of inflammatory biomarkers secreted by airway epithelial cells after exposure to different concentrations of acrolein. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Are there significant differences in inflammatory biomarker concentrations between different doses of acrolein? Do TNF-\\(\\alpha\\) concentrations significantly increase with increasing dose of acrolein? Workspace Preparation and Data Import Here, we will import the processed data that we generated at the end of TAME 2.0 Module 4.2, introduced in TAME 2.0 Module 4.1 Overview of Experimental Design and Example Data and the associated demographic data. These data represent log2 concentrations of inflammatory biomarkers secreted by airway epithelial cells after exposure to four different concentrations of acrolein (plus filtered air as a control). We will also load packages that will be needed for the analysis, including previously introduced packages such as openxlsx, tidyverse, DT, ggpubr, and rstatix. Cleaning the global environment rm(list=ls()) Loading R packages required for this session library(openxlsx) library(tidyverse) library(DT) library(rstatix) library(ggpubr) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example dataset biomarker_data &lt;- read.xlsx(&quot;Module4_5_Input/Module4_5_InputData1.xlsx&quot;) demographic_data &lt;- read.xlsx(&quot;Module4_5_Input/Module4_5_InputData2.xlsx&quot;) # View data datatable(biomarker_data) datatable(demographic_data) Overview of Multi-Group Statistical Tests Before applying statistical tests to our data, let’s first review the mechanics of multi-group statistical tests, including overall effects tests and post-hoc tests. Overall Effects Tests The first step for multi-group statistical testing is to run an overall effects test. The null hypothesis for the overall effects test is that there are no differences among group means. A significant p-value rejects the null hypothesis that the groups are drawn from populations with the same mean and indicates that at least one group differs significantly from the overall mean. Similar to two-group statistical testing, choice of the specific overall statistical test to run depends on whether the data are normally or non-normally distributed and whether the experimental design is paired: Importantly, overall effects tests return one p-value regardless of the number of groups being compared. To determine which pairwise comparisons are significant, post-hoc testing is needed. Post-Hoc Testing If significance is obtained with an overall effects test, we can use post-hoc testing to determine which specific pairs of groups are significantly different from each other. Just as with two group statistical tests and overall effects multi-group statistical tests, choosing the appropriate post-hoc test depends on the data’s normality and whether the experimental design is paired: Note that the above diagram represents commonly selected post-hoc tests; others may also be appropriate depending on your specific experimental design. As with other aspects of the analysis, be sure to report which post-hoc test(s) you performed! Correcting for Multiple Hypothesis Testing Correcting for multiple hypothesis testing is important for both the overall effects test (if you are running it over many endpoints) and post-hoc tests; however, it is particularly important for post-hoc tests. This is because even an analysis of a relatively small number of experimental groups results in quite a few pairwise comparisons. Comparing each of our five dose groups to each other in our example data, there are 10 separate statistical tests being performed! Therefore, it is generally advisable to adjust pairwise post-hoc testing p-values. The Tukey’s HSD function within rstatix does this automatically, while pairwise t-tests, pairwise Wilcoxon tests, and Dunn’s test do not. P-value adjustment can be added to their respective rstatix functions using the p.adjust.method = argument. When applying a post-hoc test, you may choose to compare every group to every other group, or you may only be interested in significant differences between specific groups (e.g., treatment groups vs. a control). This choice will be governed by your hypothesis. Statistical testing functions will typically default to comparing all groups to each other, but the comparisons can be defined using the comparisons = argument if you want to restrict the test to specific comparisons. It is important to decide at the beginning of your analysis which comparisons are relevant to your hypothesis because the number of pairwise tests performed in the post-hoc analysis will influence how much the resulting p-values will be adjusted for multiple hypothesis testing. Which test should I choose? Use the following flowchart to help guide your choice of statistical test to compare multiple groups: Multi-Group Analysis Example To determine whether there are significant differences across all of our doses, the Friedman test is the most appropriate due to our matched experimental design and non-normally distributed data. The friedman_test() function is part of the rstatix package. This package also has many other helpful functions for statistical tests that are pipe/tidyverse friendly. To demonstrate how this test works, we will first perform the test on one variable: biomarker_data %&gt;% friedman_test(IL1B ~ Dose | Donor) ## # A tibble: 1 × 6 ## .y. n statistic df p method ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 IL1B 16 12.5 4 0.0140 Friedman test A p-value of 0.01 indicates that we can reject the null hypothesis that all of our data are drawn from groups that have equivalent means. Now, we can run a for loop similar to our two-group comparisons in TAME 2.0 Module 4.4 Two Group Comparisons and Visualizations to determine the overall p-value for each endpoint: # Create a vector with the names of the variables you want to run the test on endpoints &lt;- colnames(biomarker_data %&gt;% select(IL1B:VEGF)) # Create data frame to store results dose_friedmanres &lt;- data.frame() # Run for loop for (i in 1:length(endpoints)) { # Assign a name to the endpoint variable. endpoint &lt;- endpoints[i] # Run wilcox test and store in results data frame. res &lt;- biomarker_data %&gt;% friedman_test(as.formula(paste0(endpoint, &quot;~ Dose | Donor&quot;, sep = &quot;&quot;))) %&gt;% select(c(.y., p)) dose_friedmanres &lt;- rbind(dose_friedmanres, res) } # View results datatable(dose_friedmanres) These results demonstrate that all of our endpoints have significant overall differences across doses (p &lt; 0.05). To determine which pairwise comparisons are significant, we next need to apply a post-hoc test. We will apply a pairwise, paired Wilcoxon test due to our experimental design and data distribution, with the Benjamini-Hochberg (BH) correction for multiple testing: dose_wilcox_posthoc_IL1B &lt;- biomarker_data %&gt;% pairwise_wilcox_test(IL1B ~ Dose, paired = TRUE, p.adjust.method = &quot;BH&quot;) dose_wilcox_posthoc_IL1B ## # A tibble: 10 × 9 ## .y. group1 group2 n1 n2 statistic p p.adj p.adj.signif ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 IL1B 0 0.6 16 16 17 0.006 0.025 * ## 2 IL1B 0 1 16 16 11 0.002 0.017 * ## 3 IL1B 0 2 16 16 21 0.013 0.033 * ## 4 IL1B 0 4 16 16 18 0.008 0.025 * ## 5 IL1B 0.6 1 16 16 38 0.13 0.186 ns ## 6 IL1B 0.6 2 16 16 31 0.058 0.096 ns ## 7 IL1B 0.6 4 16 16 25 0.025 0.05 * ## 8 IL1B 1 2 16 16 43 0.211 0.234 ns ## 9 IL1B 1 4 16 16 40 0.159 0.199 ns ## 10 IL1B 2 4 16 16 62 0.782 0.782 ns Here, we can now see whether there are statistically significant differences in IL-1\\(\\beta\\) secretion between each of our doses. To generate pairwise comparison results for each of our inflammatory biomarkers, we can run a for loop similar to the one we ran for our overall test: # Create a vector with the names of the variables you want to run the test on endpoints &lt;- colnames(biomarker_data %&gt;% select(IL1B:VEGF)) # Create data frame to store results dose_wilcox_posthoc &lt;- data.frame() # Run for loop for (i in 1:length(endpoints)) { # Assign a name to the endpoint variable. endpoint &lt;- endpoints[i] # Run wilcox test and store in results data frame. res &lt;- biomarker_data %&gt;% pairwise_wilcox_test(as.formula(paste0(endpoint, &quot;~ Dose&quot;, sep = &quot;&quot;)), paired = TRUE, p.adjust.method = &quot;BH&quot;) dose_wilcox_posthoc &lt;- rbind(dose_wilcox_posthoc, res) } # View results datatable(dose_wilcox_posthoc) We now have a dataframe storing all of our pairwise comparison results. However, this is a lot to scroll through, making it hard to interpret. We can generate a publication-quality table by manipulating the table and joining it with the overall test data. dose_results_cleaned &lt;- dose_wilcox_posthoc %&gt;% unite(comparison, group1, group2, sep = &quot; vs. &quot;) %&gt;% select(c(.y., comparison, p.adj)) %&gt;% pivot_wider(id_cols = &quot;.y.&quot;, names_from = &quot;comparison&quot;, values_from = &quot;p.adj&quot;) %&gt;% left_join(dose_friedmanres, by = &quot;.y.&quot;) %&gt;% relocate(p, .after = &quot;.y.&quot;) %&gt;% rename(&quot;Variable&quot; = &quot;.y.&quot;, &quot;Overall&quot; = &quot;p&quot;) %&gt;% mutate(across(&#39;Overall&#39;:&#39;2 vs. 4&#39;, \\(x) format(x, scientific = TRUE, digits = 3))) datatable(dose_results_cleaned) To more easily see overall significance patterns, we could also make the same table but with significance stars instead of p-values by keeping the p.adjust.signif column instead of the p.adj column in our post-hoc test results dataframe: dose_results_cleaned_2 &lt;- dose_wilcox_posthoc %&gt;% unite(comparison, group1, group2, sep = &quot; vs. &quot;) %&gt;% select(c(.y., comparison, p.adj.signif)) %&gt;% pivot_wider(id_cols = &quot;.y.&quot;, names_from = &quot;comparison&quot;, values_from = &quot;p.adj.signif&quot;) %&gt;% left_join(dose_friedmanres, by = &quot;.y.&quot;) %&gt;% relocate(p, .after = &quot;.y.&quot;) %&gt;% rename(&quot;Variable&quot; = &quot;.y.&quot;, &quot;Overall&quot; = &quot;p&quot;) %&gt;% mutate(across(&#39;Overall&#39;:&#39;2 vs. 4&#39;, \\(x) format(x, scientific = TRUE, digits = 3))) datatable(dose_results_cleaned_2) Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question #1 : Are there significant differences in inflammatory biomarker concentrations between different doses of acrolein? Answer: Yes, there are significant differences in inflammatory biomarker concentrations between different doses of acrolein. The overall p-values for all biomarkers are significant. Within each biomarker, at least one pairwise comparison was significant between doses, with a majority of these significant comparisons being with the highest dose (4 ppm). Visualization of Multi-Group Statistical Results The statistical results we generated are a lot to digest in table format, so it can be helpful to graph the results. As our statistical testing becomes more complicated, so does the code used to generate results. The ggpubr package can perform statistical testing and overlay the results onto graphs for a specific set of tests, such as overall effects tests and unpaired t-tests or Wilcoxon tests. However, for tests that aren’t available by default, the package also contains the helpful stat_pvalue_manual() function that can be added to plots. This is what we will need to use to add the results of our pairwise, paired Wilcoxon test with BH correction, as there is no option for BH correction within the default function we might otherwise use (stat_compare_means()). We will first work through an example of this using one of our endpoints, and then we will demonstrate how to apply it to facet plotting. Single Plot We first need to format our existing statistical results so that they match the format that the function needs as input. Specifically, the dataframe needs to contain the following columns: group1 and group2: the groups being compared A column containing the results you want displayed (p, p.adj, or p.adj.signif typically) y.position, which tells the function where to plot the significance markers Our results dataframe for IL-1\\(\\beta\\) already contains our groups and p-values: datatable(dose_wilcox_posthoc_IL1B) We can add the position columns using the function add_xy_position(): dose_wilcox_posthoc_IL1B &lt;- dose_wilcox_posthoc_IL1B %&gt;% add_xy_position(x = &quot;Dose&quot;, step.increase = 2) datatable(dose_wilcox_posthoc_IL1B) Now, we are ready to make a graph of our results. We will use stat_friedman_test() to add our overall p-value and stat_pvalue_manual() to add our pairwise values. # Set graphing theme theme_set(theme_bw()) # Make plot ggplot(biomarker_data, aes(x = Dose, y = IL1B)) + geom_boxplot(aes(fill = Dose), outlier.shape = NA) + scale_fill_manual(values = c(&quot;#BFBFBF&quot;, &quot;#D5A298&quot;, &quot;#E38273&quot;, &quot;#EB5F4E&quot;, &quot;#EE2B2B&quot;)) + geom_jitter(size = 3, position = position_jitter(0.15)) + stat_friedman_test(wid = &quot;Donor&quot;, p.adjust.method = &quot;none&quot;, label = &quot;p = {p.format}&quot;, label.x.npc = &quot;left&quot;, label.y = 9.5, hjust = 0.5, size = 6) + stat_pvalue_manual(dose_wilcox_posthoc_IL1B, label = &quot;p.adj.signif&quot;, size = 12, hide.ns = TRUE) + ylim(2.5, 10) + labs(y = &quot;Log2(IL-1\\u03B2 (pg/mL))&quot;, x = &quot;Acrolein (ppm)&quot;) + theme(legend.position = &quot;none&quot;, axis.title = element_text(color = &quot;black&quot;, size = 15), axis.title.x = element_text(vjust = -0.75), axis.title.y = element_text(vjust = 2), axis.text = element_text(color = &quot;black&quot;, size = 12)) However, to make room for all of our annotations, our data become compressed, and it makes it difficult to see our data. Although presentation of statistical results is largely a matter of personal preference, we could clean up this plot by making our annotations appear on top of the bars, with indication in the figure legend that the comparison is with a specific dose. We will do this by: Filtering our results to those that are significant. Changing the symbol for comparisons that are not to the 0 dose. Layering this text onto the plot with geom_text() rather than stat_pvalue_manual(). First, let’s filter our results to significant results and change the symbol for comparisons that are not to the 0 dose to a caret (^) instead of stars. We can do this by creating a new column called label that keeps the existing label if group1 is 0, and if not, changes the label to a caret of the same length. We then use the summarize function to paste the labels for each of the groups together, resulting in a final dataframe containing our annotations for our plot. dose_wilcox_posthoc_IL1B_2 &lt;- dose_wilcox_posthoc_IL1B %&gt;% # Filter results to those that are significant filter(p.adj &lt;= 0.05) %&gt;% # Make new symbol mutate(label = ifelse(group1 == &quot;0&quot;, p.adj.signif, strrep(&quot;^&quot;, nchar(p.adj.signif)))) %&gt;% # Select only the columns we need select(c(group1, group2, label)) %&gt;% # Combine symbols for the same group group_by(group2) %&gt;% summarise(label = paste(label, collapse=&quot; &quot;)) %&gt;% # Remove duplicate row distinct(group2, .keep_all = TRUE) %&gt;% # Rename group2 to dose rename(&quot;Dose&quot; = &quot;group2&quot;) dose_wilcox_posthoc_IL1B_2 ## # A tibble: 4 × 2 ## Dose label ## &lt;chr&gt; &lt;chr&gt; ## 1 0.6 * ## 2 1 * ## 3 2 * ## 4 4 * ^ Then, we can use the same code as for our previous plot, but instead of using stat_pvalue_manual(), we will use geom_text() in combination with the dataframe we just created. ggplot(biomarker_data, aes(x = Dose, y = IL1B)) + geom_boxplot(aes(fill = Dose), outlier.shape = NA) + scale_fill_manual(values = c(&quot;#BFBFBF&quot;, &quot;#D5A298&quot;, &quot;#E38273&quot;, &quot;#EB5F4E&quot;, &quot;#EE2B2B&quot;)) + geom_jitter(size = 3, position = position_jitter(0.15)) + stat_friedman_test(wid = &quot;Donor&quot;, p.adjust.method = &quot;none&quot;, label = &quot;p = {p.format}&quot;, label.x.npc = &quot;left&quot;, label.y = 4.85, hjust = 0.5, size = 6) + geom_text(data = dose_wilcox_posthoc_IL1B_2, aes(x = Dose, y = 4.5, label = paste0(label)), size = 10, hjust = 0.5) + ylim(2.5, 5) + labs(y = &quot;Log2(IL-1\\u03B2 (pg/mL))&quot;, x = &quot;Acrolein (ppm)&quot;) + theme(legend.position = &quot;none&quot;, axis.title = element_text(color = &quot;black&quot;, size = 15), axis.title.x = element_text(vjust = -0.75), axis.title.y = element_text(vjust = 2), axis.text = element_text(color = &quot;black&quot;, size = 12)) An appropriate title for this figure could be: “Figure X. Exposure to 0.6-4 ppm acrolein increases IL-1\\(\\beta\\) secretion in primary human bronchial epithelial cells. Groups were compared using the Friedman test to obtain overall p-value and Wilcoxon signed rank test for post-hoc testing. * p &lt; 0.05 in comparison with 0 ppm, ^ p &lt; 0.05 in comparison with 0.6 ppm, n = 16 per group (paired).” Faceted Plot Ideally, we would extend this sort of graphical approach to our faceted plot showing all of our endpoints. However, there are quite a few statistically significant comparisons to graph, including comparisons that are significant between different pairs of doses (not just back to the control). While we could attempt to graph all of them, ultimately, this will lead to a cluttered figure panel. When thinking about how to simplify our plots, some options are: Instead of using the number of symbols to represent p-values, we could use a single symbol to represent any comparison with a p-value with at least p &lt; 0.05, and that symbol could be different depending on which group the significance is in comparison to. Symbols can be difficult to parse in R, so we could use letters or even the group names above the column of interest. For example, if the concentration of an endpoint at 2 ppm was significant in comparison with both 0 and 0.6 ppm, we could annotate “0, 0.6” above the 2 ppm column, or we could choose a letter (“a, b”) or symbol (“*, ^“) to convey these results. If the pattern is the same across many of the endpoints measured, we could graph a subset of the endpoints with the most notable data trends or the most biological meaning for the main body of the manuscript, with data for additional endpoints referred to in the text and shown in the supplemental figures or tables. If most of the significant comparisons are back to the control group, we could choose to only show comparisons with the control group, with textual description of the other significant comparisons and indication that those specific p-values can be viewed in the supplemental table of results. Which approach you decide to take (or maybe another approach altogether) is a matter of both personal preference and your specific study goals. You may also decide that it is important to you to show all significant comparisons, which will require more careful formatting of the plots to ensure that all text and annotations are legible. For this module, we will proceed with option #3 because many of our comparisons to the control dose (0) are significant, and we have enough groups that there likely will not be space to annotate all of them above our data. We will take similar steps here that we did when constructing our single endpoint graph, with a couple of small differences. Specifically, we need to: Create a dataframe of labels/annotations as we did above, but now filtered to only significant comparisons with the 0 group. Add to the label/annotation dataframe what we want the y position for each of the labels to be, which will be different for each endpoint. First, let’s create our annotations dataframe. We will start with the results dataframe from our posthoc testing: datatable(dose_wilcox_posthoc) dose_wilcox_posthoc_forgraph &lt;- dose_wilcox_posthoc %&gt;% filter(p.adj &lt;= 0.05) %&gt;% # Filter for only comparisons to 0 filter(group1 == &quot;0&quot;) %&gt;% # Rename columns rename(&quot;variable&quot; = &quot;.y.&quot;, &quot;Dose&quot; = &quot;group2&quot;) datatable(dose_wilcox_posthoc_forgraph) The Dose column will be used to tell ggplot2 where to place the annotations on the x axis, but we need to also specify where to add the annotations on the y axis. This will be different for each variable because each variable is on a different scale. We can approach this by computing the maximum value of each variable, then increasing that by 20% to add some space on top of the points. sig_labs_y &lt;- biomarker_data %&gt;% summarise(across(IL1B:VEGF, \\(x) max(x))) %&gt;% t() %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;variable&quot;) %&gt;% rename(&quot;y_pos&quot; = &quot;V1&quot;) %&gt;% mutate(y_pos = y_pos*1.2) sig_labs_y ## variable y_pos ## 1 IL1B 4.967953 ## 2 IL6 14.074658 ## 3 IL8 20.752105 ## 4 IL10 3.224473 ## 5 TNFa 4.478723 ## 6 VEGF 14.148027 Then, we can join these data to our labeling dataframe to complete what we need to make the annotations. dose_wilcox_posthoc_forgraph &lt;- dose_wilcox_posthoc_forgraph %&gt;% left_join(sig_labs_y, by = &quot;variable&quot;) Now, it’s time to graph! Keep in mind that although the plotting script can get long and unweildy, each line is just a new instruction to ggplot about a formatting element or an additional layer to add to the graph. # Pivot data longer biomarker_data_long &lt;- biomarker_data %&gt;% pivot_longer(-c(Donor, Dose), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) # Create clean labels for the graph titles new_labels &lt;- c(&quot;IL10&quot; = &quot;IL-10&quot;, &quot;IL1B&quot; = &quot;IL-1\\u03B2 &quot;, &quot;IL6&quot; = &quot;IL-6&quot;, &quot;IL8&quot; = &quot;IL-8&quot;, &quot;TNFa&quot; = &quot;TNF-\\u03b1&quot;, &quot;VEGF&quot; = &quot;VEGF&quot;) # Make graph ggplot(biomarker_data_long, aes(x = Dose, y = value)) + # outlier.shape = NA removes outliers geom_boxplot(aes(fill = Dose), outlier.shape = NA) + # Changing box plot colors scale_fill_manual(values = c(&quot;#BFBFBF&quot;, &quot;#D5A298&quot;, &quot;#E38273&quot;, &quot;#EB5F4E&quot;, &quot;#EE2B2B&quot;)) + geom_jitter(size = 1.5, position = position_jitter(0.15)) + # Adding a p value from Friedman test stat_friedman_test(wid = &quot;Donor&quot;, p.adjust.method = &quot;none&quot;, label = &quot;p = {p.format}&quot;, label.x.npc = &quot;left&quot;, vjust = -3.5, hjust = 0.1, size = 3.5) + # Add label geom_text(data = dose_wilcox_posthoc_forgraph, aes(x = Dose, y = y_pos, label = p.adj.signif, size = 5, hjust = 0.5)) + # Adding padding y axis scale_y_continuous(expand = expansion(mult = c(0.1, 0.6))) + # Changing y axis label ylab(expression(Log[2]*&quot;(Concentration (pg/ml))&quot;)) + # Changing x axis label xlab(&quot;Acrolein (ppm)&quot;) + # Faceting by each biomarker facet_wrap(~ variable, nrow = 2, scales = &quot;free_y&quot;, labeller = labeller(variable = new_labels)) + # Removing legend theme(legend.position = &quot;none&quot;, axis.title = element_text(color = &quot;black&quot;, size = 12), axis.title.x = element_text(vjust = -0.75), axis.title.y = element_text(vjust = 2), axis.text = element_text(color = &quot;black&quot;, size = 10), strip.text = element_text(size = 12, face = &quot;bold&quot;)) An appropriate title for this figure could be: “Figure X. Exposure to acrolein increases secretion of proinflammatory biomarkers in primary human bronchial epithelial cells. Groups were compared using the Friedman test to obtain overall p-value and Wilcoxon signed rank test for post-hoc testing. * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001, **** p &lt; 0.0001 for comparison with control. For additional significant comparisons, see Supplemental Table X. n = 16 per group (paired).” Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2 : Do TNF-\\(\\alpha\\) concentrations significantly increase with increasing dose of acrolein? Answer: Yes, TNF-\\(\\alpha\\) concentrations significantly increase with increasing dose of acrolein, which we were able to visualize, along with other mediators, in our facet plot. Concluding Remarks In this module, we introduced common multi-group statistical tests, including both overall effects tests and post-hoc testing. We applied these tests to our example dataset and demonstrated how to produce publication-quality tables and figures of our results. Implementing a workflow such as this enables efficient analysis of wet-bench generated data and customization of output figures and tables suited to your personal preferences. Additional Resources STHDA: How to Add P-Values and Significance Levels to ggplots using ggpubr Adding p-values with ggprism Overview of ggsignif Test Your Knowledge Functional endpoints from these cultures were also measured. These endpoints were: 1) Membrane Permeability (MemPerm), 2) Trans-Epithelial Electrical Resistance (TEER), 3) Ciliary Beat Frequency (CBF), and 4) Expression of Mucin (MUC5AC). These data were already processed and tested for normality (see Test Your Knowledge for TAME 2.0 Module 4.2 Data Import, Processing, and Summary Statistics), with results indicating that two of the endpoints are normally distributed and two non-normally distributed. Use the same processes demonstrated in this module and the provided data (“Module4_5_TYKInput.xlsx” (functional data)) to run analyses and make a publication-quality figure panel and table to answer the following question: Are there significant differences in functional endpoints between cells treated with different concentrations of acrolein? For an extra challenge, try also making your faceted plot in the style of option #1 above, with different symbols, letters, or group names above columns to indicate which group that column in significant in comparison with. "],["advanced-multi-group-comparisons.html", "4.6 Advanced Multi-Group Comparisons Introduction to Training Module Advanced Multi-Group Comparisons Two-way ANOVA Example ANCOVA Example Concluding Remarks", " 4.6 Advanced Multi-Group Comparisons This training module was developed by Elise Hickman, Alexis Payton, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module In the previous module, we covered how to apply multi-group statistical testing, in which we tested for significant differences in endpoints across different values for one independent variable. In this module, we will build on the concepts introduced previously to test for significant differences in endpoints while considering two or more independent variables. We will review relevant statistical approaches and demonstrate how to apply these tests using the same example dataset as in previous modules in this chapter. As a reminder, this dataset includes concentrations of inflammatory biomarkers secreted by airway epithelial cells after exposure to different concentrations of acrolein. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Are there significant differences in inflammatory biomarker concentrations between sex and different doses of acrolein? Are there significant differences in inflammatory biomarker concentrations across different doses of acrolein after controlling for sex and age? Workspace Preparation and Data Import Here, we will import the processed data that we generated at the end of TAME 2.0 Module 4.2, introduced in TAME 2.0 Module 4.1 Overview of Experimental Design and Example Data and associated demographic data. These data represent log2 concentrations of inflammatory biomarkers secreted by airway epithelial cells after exposure to four different concentrations of acrolein (plus filtered air as a control). We will also load packages that will be needed for the analysis, including previously introduced packages such as openxlsx, tidyverse, DT, ggpubr, and rstatix. Cleaning the global environment rm(list=ls()) Loading R packages required for this session library(openxlsx) library(tidyverse) library(DT) library(rstatix) library(ggpubr) library(multcomp) library(pander) theme_set(theme_bw()) # Set graphing theme Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example dataset biomarker_data &lt;- read.xlsx(&quot;Module4_6_Input/Module4_6_InputData1.xlsx&quot;) demographic_data &lt;- read.xlsx(&quot;Module4_6_Input/Module4_6_InputData2.xlsx&quot;) # View data datatable(biomarker_data) datatable(demographic_data) Advanced Multi-Group Comparisons Two-way ANOVA The first test that we’ll introduce is a two-way ANOVA. This test involves testing for mean differences in a continuous dependent variable across two categorical independent variables. (As a refresher, a one-way ANOVA uses a single independent variable to compare mean differences between groups.) Subjects or samples can be matched based upon their between-group factors (i.e., exposure duration) and/or their within-group factors (i.e., batch effects). Models that include both between-group and within-group factors are known as mixed two-way ANOVAs. Like other parametric tests, two-way ANOVAs assume: Homogeneity of variance Independent observations Normal distribution ANCOVA An Analysis of Covariances (ANCOVA) tests for mean differences in a continuous dependent variable and at least one categorical independent variable. It also includes another variable, known as a covariate, that needs to be controlled or adjusted for to more accurately capture the relationship between the independent and dependent variables. Potential covariates can include either between-group factors like exposure duration and/or within-group factors like batch effects or sex. Note that if the dataset has a smaller sample size, stratification of the dataset based on that covariate is another option to determine its effects rather than adjusting for it using an ANCOVA. ANCOVAs have the same assumptions listed above. Note: It is possible to run two-way ANCOVA models, where the model contains two independent variables and at least one covariate to be adjusted for. Two-way ANOVA Example Our first environmental health question can be answered using a two-way ANOVA. We can test three different null hypotheses using this test: There is no difference in average biomarker concentrations based on sex. There is no difference in average biomarker concentrations based on dose. The effect of sex on average biomarker concentration does not depend on the effect of dose and vice versa. The first step would be to check that the assumptions (independence, homogeneity of variance, and normal distribution) have been met, but this was done previously in TAME 2.0 Module 4.4 Two Group Comparisions and Visualizations. To run our two-way ANOVA, we will use the anova_test() function from the rstatix package. This function allows us to define subject identifiers for matching between-subject factor variables (such as sex - factors that differ between subjects) and within-subject factors (such as dose - factors that are measured within each subject). Since we have both between- and within- subject factors, we will specifically be running a two-way mixed ANOVA. First, we need to add our demographic data to our biomarker data so that these variables can be incorporated into the analysis. Also, we need to convert Dose into a factor to specify the levels. biomarker_data &lt;- biomarker_data %&gt;% left_join(demographic_data, by = &quot;Donor&quot;) %&gt;% mutate(Dose = factor(Dose, levels = c(&quot;0&quot;, &quot;0.6&quot;, &quot;1&quot;, &quot;2&quot;, &quot;4&quot;))) # viewing data datatable(biomarker_data) Then, we can demonstrate how to run the two-way ANOVA and what the results look like by running the test on just one of our variables (IL-1\\(\\beta\\)). get_anova_table(anova_test(data = biomarker_data, dv = IL1B, wid = Donor, between = Sex, within = Dose)) ## ANOVA Table (type III tests) ## ## Effect DFn DFd F p p&lt;.05 ges ## 1 Sex 1.00 14.0 0.014 0.909 0.000488 ## 2 Dose 2.42 33.9 6.632 0.002 * 0.192000 ## 3 Sex:Dose 2.42 33.9 2.799 0.065 0.091000 The column names are described below: Effect: the name of the variable tested DFn: degrees of freedom in the numerator Dfd: degrees of freedom in the denominator F: F distribution test p: p-value p&lt;.05: denotes whether the p-value is significant ges: generalized effect size Based on the table above, there are significant differences in IL-1\\(\\beta\\) concentrations based on dose (p-value = 0.02). There are no significant differences in IL-1\\(\\beta\\) between the sexes nor are there significant differences in IL-1\\(\\beta\\) with an interaction between sex and dose. Similar to previous modules, we now want to apply our two-way ANOVA to each of our variables of interest. To do this, we can use a for loop that will: Loop through each column in the data and apply the test to each column. Pull out statistics we are interested in (for example, p-value) and bind the results from each column together into a results dataframe. # Create a vector with the names of the variables you want to run the test on endpoints &lt;- colnames(biomarker_data %&gt;% dplyr::select(IL1B:VEGF)) # Create data frame to store results twoway_aov_res &lt;- data.frame(Factor = c(&quot;Dose&quot;, &quot;Sex&quot;, &quot;Sex:Dose&quot;)) # Run for loop for (i in 1:length(endpoints)) { # Assign a name to the endpoint variable endpoint &lt;- endpoints[i] # Run two-way mixed ANOVA and store results in res_aov res_aov &lt;- anova_test(data = biomarker_data, dv = paste0(endpoint), wid = Donor, between = Sex, within = Dose) # Extract the results we are interested in (from the ANOVA table) res_df &lt;- data.frame(get_anova_table(res_aov)) %&gt;% dplyr::select(c(Effect, p)) %&gt;% rename(&quot;Factor&quot; = &quot;Effect&quot;) # Rename columns in the results dataframe so that the output is more nicely formatted names(res_df)[names(res_df) == &#39;p&#39;] &lt;- noquote(paste0(endpoint)) # Bind the results to the results dataframe twoway_aov_res &lt;- merge(twoway_aov_res, res_df, by = &quot;Factor&quot;, all.y = TRUE) } # View results datatable(twoway_aov_res) An appropriate title for this table could be: “Figure X. Statistical test results for differences in cytokine concentrations. A two-way ANOVA was performed using sex and dose as independent variables to test for statistical differences in concentration across 6 cytokines.” From this table, dose is the only variable with significant differences in concentrations in all 6 biomarkers (p-value &lt; 0.05). Although we know that dose has significant differences overall, an ANOVA test doesn’t tell us which doses of acrolein differ from each other or the directionality of each biomarker’s change in concentration after exposure to each dose. Therefore, we need to use a post-hoc test. One common post-hoc test following a one-way or two-way ANOVA is a Tukey’s HSD. However, there is no way to pass the output of the anova_test() function to the TukeyHSD() function. A good alternative is a pairwise t-test with a Bonferroni correction. Our data are paired in that there are repeated measures (doses) on each subject. # Create data frame to store results twoway_aov_pairedt &lt;- data.frame(Comparison = c(&quot;0_0.6&quot;, &quot;0_1&quot;, &quot;0_2&quot;, &quot;0_4&quot;, &quot;0.6_1&quot;, &quot;0.6_2&quot;, &quot;0.6_4&quot;, &quot;1_2&quot;, &quot;1_4&quot;, &quot;2_4&quot;)) # Run for loop for (i in 1:length(endpoints)) { # Assign a name to the endpoint variable. endpoint &lt;- endpoints[i] # Run pairwise t-tests res_df &lt;- biomarker_data %&gt;% pairwise_t_test(as.formula(paste0(paste0(endpoint), &quot;~&quot;, &quot;Dose&quot;, sep = &quot;&quot;)), paired = TRUE, p.adjust.method = &quot;bonferroni&quot;) %&gt;% unite(Comparison, group1, group2, sep = &quot;_&quot;, remove = FALSE) %&gt;% dplyr::select(Comparison, p.adj) # Rename columns in the results data frame so that the output is more nicely formatted. names(res_df)[names(res_df) == &#39;p.adj&#39;] &lt;- noquote(paste0(endpoint)) # Bind the results to the results data frame. twoway_aov_pairedt &lt;- merge(twoway_aov_pairedt, res_df, by = &quot;Comparison&quot;, all.y = TRUE) } # View results datatable(twoway_aov_pairedt) An appropriate title for this table could be: “Figure X. Post hoc testing for differences in cytokine concentrations. Paired t-tests were run as a post hoc test using dose as an independent variable to test for statistical differences in concentration across 6 cytokines.” Note that this table and the two-way ANOVA table would likely be put into supplemental material for a publication. Before including this table in supplemental material, it would be best to clean it up (make the two comparison groups more clear, round all results to the same number of decimals) as demonstrated in TAME 2.0 Module 4.5 Multi-Group Comparisons and Visualizations. Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question #1: Are there significant differences in inflammatory biomarker concentrations between sex and different doses of acrolein? Answer: Based on the two-way ANOVA and post-hoc t-tests, there are only significant differences in cytokine concentrations based on dose (p adj &lt; 0.05). All biomarkers, with the exception of IL-6, had at least 1 significantly different concentration when comparing doses. Visualizing Two-Way ANOVA Results Since our overall p-values associated with dose were significant for a number of mediators, we will proceed with creating our final figures with our endpoints by dose, showing the overall two-way ANOVA p-value and the pairwise comparisons from our post hoc paired pairwise t-tests. To facilitate plotting in a faceted panel, we’ll first pivot our biomarker_data dataframe longer. biomarker_data_long &lt;- biomarker_data %&gt;% dplyr::select(-c(Age_yr, Sex)) %&gt;% pivot_longer(-c(Donor, Dose), names_to = &quot;Variable&quot;, values_to = &quot;Value&quot;) datatable(biomarker_data_long) Then, we will create an annotation dataframe for adding our overall two-way ANOVA p-values. This dataframe needs to contain a column for our variables (to match with our variable column in our biomarker_data_long dataframe) and the p-value for annotation. We can extract these from our two_way_aov_res dataframe generated above. overall_dose_pvals &lt;- twoway_aov_res %&gt;% # Transpose dataframe column_to_rownames(&quot;Factor&quot;) %&gt;% t() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% # Keep only the dose results and rename them to p-value dplyr::select(c(Variable, Dose)) %&gt;% rename(`P Value` = Dose) datatable(overall_dose_pvals) We now have our p-values for each biomarker. Next, we’ll make a column where our p-values are formatted with “p =” for annotation on the graph. overall_dose_pvals &lt;- overall_dose_pvals %&gt;% mutate(`P Value` = formatC(`P Value`, format = &quot;e&quot;, digits = 2), label = paste(&quot;p = &quot;, `P Value`, sep = &quot;&quot;)) datatable(overall_dose_pvals) Finally, we’ll add a column indicating where to add the labels on the y-axis. This will be different for each variable because each variable is on a different scale. We can approach this by computing the maximum value of each variable, then increasing that by 10% to add some space on top of the points. sig_labs_y &lt;- biomarker_data %&gt;% summarise(across(IL1B:VEGF, \\(x) max(x))) %&gt;% t() %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% rename(&quot;y_pos&quot; = &quot;V1&quot;) %&gt;% # moving the significance asterisks higher on the y axis mutate(y_pos = y_pos * 1.1) sig_labs_y ## Variable y_pos ## 1 IL1B 4.553957 ## 2 IL6 12.901770 ## 3 IL8 19.022763 ## 4 IL10 2.955767 ## 5 TNFa 4.105496 ## 6 VEGF 12.969025 overall_dose_pvals &lt;- overall_dose_pvals %&gt;% left_join(sig_labs_y, by = &quot;Variable&quot;) datatable(overall_dose_pvals) Now, we’ll use the biomarker_data dataframe to plot our individual points and boxplots (similar to the plotting demonstrated in previous TAME Chapter 4 modules) and our overall_dose_pvals dataframe to add our p value annotation. # Create clean labels for the graph titles new_labels &lt;- c(&quot;IL10&quot; = &quot;IL-10&quot;, &quot;IL1B&quot; = &quot;IL-1\\u03B2 &quot;, &quot;IL6&quot; = &quot;IL-6&quot;, &quot;IL8&quot; = &quot;IL-8&quot;, &quot;TNFa&quot; = &quot;TNF-\\u03b1&quot;, &quot;VEGF&quot; = &quot;VEGF&quot;) # Make graph ggplot(biomarker_data_long, aes(x = Dose, y = Value)) + # outlier.shape = NA removes outliers geom_boxplot(aes(fill = Dose), outlier.shape = NA) + geom_jitter(size = 1.5, position = position_jitter(0.15), alpha = 0.7) + # Add label geom_text(data = overall_dose_pvals, aes(x = 1.3, y = y_pos, label = label, size = 5)) + # Adding padding y axis scale_y_continuous(expand = expansion(mult = c(0.1, 0.1))) + # Faceting by each biomarker facet_wrap(~ Variable, nrow = 2, scales = &quot;free_y&quot;, labeller = labeller(variable = new_labels)) + theme(legend.position = &quot;none&quot;, # Removing legend axis.title = element_text(face = &quot;bold&quot;, size = rel(1.3)), axis.title.x = element_text(vjust = -0.75), axis.title.y = element_text(vjust = 2), axis.text = element_text(color = &quot;black&quot;, size = 10), strip.text = element_text(size = 12, face = &quot;bold&quot;)) + # Changing axes labels labs(x = &quot;Acrolein (ppm)&quot;, y = expression(bold(Log[2]*&quot;(Concentration (pg/ml))&quot;))) It’s a bit more difficult to add the pairwise t test results to the boxplots comparing each treatment group to each other as was done similarly in TAME 2.0 Module 4.5 Multi-Group Comparisons and Visualizations, so that addition to the figure was omitted here. ANCOVA Example In the following ANCOVA example, we’ll still investigate potential differences in cytokine concentrations as result of varying doses of acrolein. However, this time we’ll adjust for sex and age to answer our second environmental health question: Are there significant differences in inflammatory biomarker concentrations across different doses of acrolein after controlling for sex and age?. Let’s first demonstrate how to run an ANCOVA and what the results look like by running the test on just one of our variables (IL-1\\(\\beta\\)). The Anova() function was specifically designed to run type II or III ANOVA tests, which have different approaches to dealing with interactions terms and unbalanced datasets. For more information on Type I, II, III ANOVA tests, check out Anova – Type I/II/III SS explained. For the purposes of this example just know that isn’t much of a difference between the type I, II, or III results. anova_test = aov(IL1B ~ Dose + Sex + Age_yr, data = biomarker_data) type3_anova = Anova(anova_test, type = &#39;III&#39;) type3_anova ## Anova Table (Type III tests) ## ## Response: IL1B ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 56.720 1 1092.9386 &lt; 2.2e-16 *** ## Dose 0.762 4 3.6697 0.008862 ** ## Sex 0.002 1 0.0318 0.858954 ## Age_yr 0.000 1 0.0000 0.994833 ## Residuals 3.788 73 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Based on the table above, there are significant differences in IL-1\\(\\beta\\) concentrations in dose after adjusting for sex and age (p-value = 0.009). Now we’ll run ANCOVA tests across all of our biomarkers. # Create data frame to store results ancova_res = data.frame() # Add row names to data frame so that it will be able to add ANCOVA results rownames &lt;- c(&quot;(Intercept)&quot;, &quot;Dose&quot;, &quot;Sex&quot;, &quot;Age_yr&quot;) ancova_res &lt;- data.frame(cbind(rownames)) # Assign row names ancova_res &lt;- data.frame(ancova_res[, -1], row.names = ancova_res$rownames) # Perform ANCOVA over all columns for (i in 3:8) { fit = aov(as.formula(paste0(names(biomarker_data)[i], &quot;~ Dose + Sex + Age_yr&quot;, sep = &quot;&quot;)), biomarker_data) res &lt;- data.frame(car::Anova(fit, type = &quot;III&quot;)) res &lt;- subset(res, select = Pr..F.) names(res)[names(res) == &#39;Pr..F.&#39;] &lt;- noquote(paste0(names(biomarker_data[i]))) ancova_res &lt;- transform(merge(ancova_res, res, by = 0), row.names = Row.names, Row.names = NULL) } # Transpose for easy viewing, keep columns of interest, and apply BH adjustment ancova_res &lt;- data.frame(t(ancova_res)) %&gt;% dplyr::select(Dose) %&gt;% mutate(across(everything(), \\(x) format(p.adjust(x, &quot;BH&quot;), scientific = TRUE))) # View results datatable(ancova_res) Looking at the table above, there are statistically differences in all cytokine concentrations with the exception of IL-6 based on dose (p adj &lt; 0.05). To determine what doses were significantly different from one another we’ll need to run Tukey’s post hoc tests. # Create results data frame with a column showing the comparisons (extracted from single run vs for loop) tukey_res &lt;- data.frame(Comparison = c(&quot;0.6 - 0&quot;, &quot;1 - 0&quot;, &quot;2 - 0&quot;, &quot;4 - 0&quot;, &quot;1 - 0.6&quot;, &quot;2 - 0.6&quot;, &quot;4 - 0.6&quot;, &quot;2 - 1&quot;, &quot;4 - 1&quot;, &quot;4 - 2&quot;)) # Perform Tukey&#39;s test for (i in 3:8) { # need to run ANCOVA first fit = aov(as.formula(paste0(names(biomarker_data)[i], &quot;~ Dose + Sex + Age_yr&quot;, sep = &quot;&quot;)), biomarker_data) # Tukey&#39;s posthoc &lt;- summary(glht(fit, linfct = mcp(Dose = &quot;Tukey&quot;)), test = adjusted(&quot;BH&quot;)) res &lt;- summary(posthoc)$test # Formatting the df with the Tukey&#39;s values res_df &lt;- data.frame(cbind (res$coefficients, res$sigma, res$tstat, res$pvalues)) colnames(res_df) &lt;- c(&quot;Estimate&quot;, &quot;Std.Error&quot;, &quot;t.value&quot;, &quot;Pr(&gt;|t|)&quot;) res_df &lt;- round(res_df[4],4) names(res_df)[names(res_df) == &#39;Pr(&gt;|t|)&#39;] &lt;- noquote(paste0(names(biomarker_data[i]))) res_df &lt;- res_df %&gt;% rownames_to_column(&quot;Comparison&quot;) tukey_res &lt;- left_join(tukey_res, res_df, by = &quot;Comparison&quot;) } datatable(tukey_res) Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: Are there significant differences in inflammatory biomarker concentrations across different doses of acrolein after controlling for sex and age? Answer: Based on the ANCOVA tests, there are significant differences resulting from various doses of acrolein (p adj &lt; 0.05) across all cytokine concentrations with the exception of IL-6. All biomarkers, with the exception of IL-6, had at least 1 significantly different biomarker concentration when comparing doses. Visualizing ANCOVA Results Before graphing these results, we first need to think about which ones we want to display. For simplicity’s sake, we will demonstrate graphing only comparisons that are with the control(“0”) group and that are significant. To do this, we’ll: Separate our Comparison column into a group1 and group2 column. Filter to comparisons including only the 0 group. Pivot the dataframe longer, to match the format of our data used as input for facet plotting. Filter to only p-values that are less that 0.05. tukey_res_forgraph &lt;- tukey_res %&gt;% separate(Comparison, into = c(&quot;group1&quot;, &quot;group2&quot;), sep = &quot; - &quot;) %&gt;% filter(group2 == &quot;0&quot;) %&gt;% dplyr::select(-group2) %&gt;% pivot_longer(!group1, names_to = &quot;Variable&quot;, values_to = &quot;P Value&quot;) %&gt;% filter(`P Value` &lt; 0.05) %&gt;% # rounding the p values to 4 digits for readability mutate(`P Value` = round(`P Value`, 4)) datatable(tukey_res_forgraph) Next, we can take a few steps to add columns to the dataframe that will aid in graphing: Add a column for significance stars. Add a column to indicate the y position for the significance annotation (similar to the above example with the two-way ANOVA). # Add column for significance stars tukey_res_forgraph &lt;- tukey_res_forgraph %&gt;% mutate(p.signif = ifelse(`P Value` &lt; 0.0001, &quot;****&quot;, ifelse(`P Value` &lt; 0.001, &quot;***&quot;, ifelse(`P Value` &lt; 0.01, &quot;**&quot;, ifelse(`P Value` &lt; 0.05, &quot;*&quot;, NA))))) # Calculate y positions to plot significance stars sig_labs_y_tukey &lt;- biomarker_data %&gt;% summarise(across(IL1B:VEGF, \\(x) max(x))) %&gt;% t() %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% rename(&quot;y_pos&quot; = &quot;V1&quot;) %&gt;% mutate(y_pos = y_pos * 1.15) sig_labs_y_tukey ## Variable y_pos ## 1 IL1B 4.760955 ## 2 IL6 13.488214 ## 3 IL8 19.887434 ## 4 IL10 3.090120 ## 5 TNFa 4.292110 ## 6 VEGF 13.558526 # Join y positions to tukey_res tukey_res_forgraph &lt;- tukey_res_forgraph %&gt;% left_join(sig_labs_y_tukey, by = &quot;Variable&quot;) %&gt;% rename(&quot;Dose&quot; = &quot;group1&quot;) datatable(tukey_res_forgraph) We also need to prepare our overall p-values from our ANCOVA for display: ancova_res_forgraphing &lt;- ancova_res %&gt;% rename(`P Value` = Dose) %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% left_join(sig_labs_y, by = &quot;Variable&quot;) %&gt;% mutate(`P Value` = formatC(as.numeric(`P Value`), format = &quot;e&quot;, digits = 2), label = paste(&quot;p = &quot;, `P Value`, sep = &quot;&quot;)) Now, we are ready to make our graph! We will use similar code to the above, this time adding in our significance stars over specific columns. # Make graph ggplot(biomarker_data_long, aes(x = Dose, y = Value)) + # outlier.shape = NA removes outliers geom_boxplot(aes(fill = Dose), outlier.shape = NA) + # Changing box plot colors scale_fill_manual(values = c(&quot;#BFBFBF&quot;, &quot;#D5A298&quot;, &quot;#E38273&quot;, &quot;#EB5F4E&quot;, &quot;#EE2B2B&quot;)) + geom_jitter(size = 1.5, position = position_jitter(0.15), alpha = 0.7) + # Add overall ANCOVA label geom_text(data = ancova_res_forgraphing, aes(x = 1.3, y = y_pos * 1.15, label = label, size = 10)) + # Add tukey annotation geom_text(data = tukey_res_forgraph, aes(x = Dose, y = y_pos, label = p.signif, size = 10, hjust = 0.5)) + # Faceting by each biomarker facet_wrap(~ Variable, nrow = 2, scales = &quot;free_y&quot;, labeller = labeller(Variable = new_labels)) + # Removing legend theme(legend.position = &quot;none&quot;, axis.title = element_text(face = &quot;bold&quot;, size = rel(1.5)), axis.title.x = element_text(vjust = -0.75), axis.title.y = element_text(vjust = 2), axis.text = element_text(color = &quot;black&quot;, size = 10), strip.text = element_text(size = 12, face = &quot;bold&quot;)) + # Changing axes labels labs(x = &quot;Acrolein (ppm)&quot;, y = expression(bold(Log[2]*&quot;(Concentration (pg/ml))&quot;))) An appropriate title for this figure could be: “Figure X. Acrolein exposure increases inflammatory cytokine secretion in most primary human bronchial epithelial cells. Overall p-values from ANCOVA tests adjusting for age and sex are in the left-hand corner. Tukey’s post hoc tests were subsequently run and significant Benjamini-Hochberg adjusted p-values are denoted with asterisks compared to the control (0ppm) dose only. p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001, **** p &lt; 0.0001, n = 16 per group.” Concluding Remarks In this module, we introduced advanced multi-group comparisons using two-way ANOVA and ANCOVA tests. These overall effect tests along with post-hoc testing were used on an example dataset to provide a basis for publication-ready tables and figures to present these results. This training module provides code and text for advanced multi-group comparisons necessary to answer more complex research questions. Additional Resources Two-Way ANOVA Repeated Measure ANOVA in R ANCOVA Example Nonparametric ANOVA RDocumentation Nonparametric ANCOVA RDocumentation Test Your Knowledge Functional endpoints from these cultures were also measured. These endpoints were: 1) Membrane Permeability (MemPerm), 2) Trans-Epithelial Electrical Resistance (TEER), 3) Ciliary Beat Frequency (CBF), and 4) Expression of Mucin (MUC5AC). These data were already processed and tested for normality (see Test Your Knowledge for TAME 2.0 Module 4.2 Data Import, Processing, and Summary Statistics), with results indicating that two of the endpoints are normally distributed and two non-normally distributed. Using the data found in “Module4_5_TYKInput.xlsx”, answer the following research question: Are there significant differences in functional endpoints based on doses of acrolein and sex after adjusting for age? To streamline the analysis, we’ll only include doses of acrolein at 0, 1, and 4ppm. Hint: You’ll need to run a two-way ANCOVA. Given that some of the assumptions for parametric tests (i.e., normality and homogeneity of variance) and the size of the data is on the smaller side, we likely wouldn’t run a parametric test. However, we’ll do so here just to illustrate an example of how to run a two-way ANCOVA. "],["introduction-to-artificial-intelligence-machine-learning-and-predictive-modeling-for-environmental-health.html", "5.1 Introduction to Artificial Intelligence, Machine Learning, and Predictive Modeling for Environmental Health Introduction to Training Module General Historical Context and Taxonomy of Modern AI/ML Application of Machine Learning in Environmental Health Science Concluding Remarks", " 5.1 Introduction to Artificial Intelligence, Machine Learning, and Predictive Modeling for Environmental Health This training module was developed by David M. Reif, with contributions from Elise Hickman, Alexis Payton, and Julia E. Rager All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Artificial intelligence (AI), machine learning (ML), and predictive modeling are becoming increasingly popular buzzwords both in the public domain and within research fields, including environmental health. Within environmental health, these computational techniques are implemented to integrate large, high dimensional datasets (e.g., chemical, biological, clinical/medical, model estimates, etc) to better understand links between environmental exposures and biological responses. In this training module, we will: Provide general historical context and taxonomy of modern AI/ML Provide an overview of the intersection between environmental health science ML through discussing… Why there is a need for ML in environmental health science The differences between ML and traditional statistical methods Predictive modeling in the context of environmental health science Additional applications of ML in environmental health science Training Module’s Environmental Health Question This training module was specifically developed to answer the following environmental health question: How and why are artificial intelligence, machine learning, and predictive modeling used in environmental health research? General Historical Context and Taxonomy of Modern AI/ML Before diving in to the applications of AI and ML in environmental health, let’s first establish what these term mean and how they are related. Note that the definitions surrounding AI and ML can be subjective, however the purpose of this module is not to get caught up in semantics, but to broadly understand how AI and ML can be applied to environmental health research. Artificial Intelligence (AI) encompasses computer systems that perform tasks typically associated with human cognition and intelligence. AI is found in our everyday lives, for instance, within face recognition, internet search queries, email spam detection, smart home devices, auto-navigation, and digital assistants. Machine Learning (ML) can be thought of as a subset of AI and describes a computer system that iteratively learns and improves from that experience autonomously. Below is a high level taxonomy of AI. It’s not meant to be an exhaustive depiction of all AI techniques but a simple visualization of how some of these methodologies are nested within each other. Note: AI can be categorized in different ways and may deviate from what is illustrated below. Advantages of AI and ML include the automation of repetitive tasks, complex problem solving, and reducing human error. However, disadvantages include learning from biased datasets or patterns that are reflected in the decisions of AI/ML and the potential limited interpretability of algorithms created by AI/ML. Check out the following resources for… Further explanation on differences in Artificial Intelligence vs. Machine Learning Other subsets of AI that fall outside of the scope of these modules in Types of Artificial Intelligence Additional discussion on the utility of ML approaches for high-dimensional data common in environmental health research in Payton et. al It is important to understand the methodological “roots” of current methods. Otherwise, it seems like every approach is novel! AI and ML methods have been around since the mid- to late- 1900s and continue to evolve in the present day. The earliest conceptual roots for these approaches can be traced from antiquity; however, it is generally thought that the field was named “artificial intelligence” at the “Dartmouth Workshop” in 1956, led by John McCarthy and others. The following schematic demonstrates the general taxonomy (categories, sub-fields, and specific methods) of modern AI and ML: A Brief Detour to Discuss ChatGPT ChatGPT (Chat Generative Pre-trained Transformer) is a publicly available chatbot developed by OpenAI. It was released in November of 2022 and quickly gained popularity due to its accessibility and ability to have human-like conversations with the user across almost any imaginable topic. Language Models (LLMs), including large language models like GPT-3 (a predecessor to ChatGPT), generally fall under the “Connectionist AI” category, which use deep learning techniques and are considered a subset of artificial neural networks. They fall under the deep learning subset due to their use of deep neural networks with many layers, allowing them to learn from large amounts of data and find intricate patterns. LLMs are trained to predict the probability of a word given its context in a dataset (a form of next-word prediction), which is a machine learning methodology. It’s notable that they use architectures like Transformer Networks, which are known for their efficiency in handling sequential data, making them a go-to choice for natural language processing (NLP) tasks. The use of attention mechanisms in these architectures allows the model to focus on different parts of the input sequence when producing an output sequence, offering a substantial improvement in performance for many natural language processing tasks. The role of ChatGPT and similar tools in the environmental health research space is still being explored. Although ChatGPT has the potential to streamline certain parts of the research process, such as text and language polishing, synthesizing existing information, and suggesting custom coding solutions, it is not an intellectual replacement for the expertise and diverse viewpoints of scientists and must be used transparently and with caution. Application of Machine Learning in Environmental Health Science For the rest of this module and chapter, we will focus on machine learning (ML). Generally speaking, ML is considered to encompass the study of computer algorithms that can improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence (AI), discussed broadly above. Why do we need machine learning in environmental health science? There are many avenues to incorporate ML into environmental health research, all aimed at better identifying patterns amongst large datasets spanning medical health records, clinical data, exposure monitoring data, chemistry profiles, and the rapidly expanding realm of biological response data including multiple -omics endpoints. One well-known problem that can be better addressed by incorporating ML is the ‘too many chemicals, too little data’ problem. To detail, there are thousands of chemicals in commerce today. Testing these chemicals one by one for toxicity using comprehensive animal screening experiments would take decades and is not feasible financially. Current efforts to address this problem include using cell-based high throughput screening to efficiently determine biological responses to a variety of chemical exposures and treatment conditions. These screening efforts result in increasing amounts of data, which can be gathered to start building big databases. When many of these datasets and databases are combined, including diversity across different types of screening platforms, technologies, cell types, species, and other experimental variables, the associated dimensionality of the data gets “big.” This presents a problem because these data are diverse and high dimensional (the number of features or endpoints exceeds the number of observations/chemicals). To appropriately analyze and model these data, new approaches beyond traditional statistical methods are needed. Machine Learning vs. Traditional Statistical Methods There is plenty of debate as to where the line(s) between ML and traditional statistics should be drawn. In our opinion, a perfect delineation is not necessary for our purposes. Rather, we will focus on the usual goals/intent of each to help us understand the distinction for environmental health research. Traditional statistics may be able to handle 1:1 or 1:many comparisons of singular quantities (e.g., activity concentrations for two chemicals). However, once the modeling becomes more complex or exploratory, assumptions of most traditional methods will be violated. Furthermore, statistics draws population inferences from a sample, while AI/ML finds generalizable predictive patterns (Bzdok et al 2018). This is particularly helpful in predictive toxicology, in which we leverage high dimensional data to obtain generalizable forecasts for the effects of chemicals on biological systems. This image shows graphical abstractions of how a “problem” is solved using: Traditional statistics ((A) logistic regression and (B) linear regression), OR Machine learning ((C) support vector machines, (D) artificial neural networks, and (E) decision trees) Predictive Modeling in the Context of Environmental Health Science In the previous section, we briefly mentioned predictive toxicology. We often think of predictions as having a forward-time component (i.e. What will happen next?) … what about “prediction” in a different sense as applied to toxicology? Our working definition is that predictive toxicology describes a multidisciplinary approach to chemical toxicity evaluation that more efficiently uses animal test results, when needed, and leverages expanding non-animal test methods to forecast the effects of a chemical on biological systems. Examples of the questions we can answer using predictive toxicology include: Can we more efficiently design animal studies and analyze data from shorter assays using fewer animals to predict long-term health outcomes? Can this suite of in vitro assays predict what would happen in an organism? Can we use diverse, high dimensional data to cluster chemicals into predicted activity classes? Similar logic applies to the field of exposure science. What about “prediction” applied to exposure science? Our working definition is that predictive exposure science describes a multidisciplinary approach to chemical exposure evaluations that more efficiently uses biomonitoring, chemical inventory, and other exposure science-relevant databases to forecast exposure rates in target populations. For example: Can we use existing biomonitoring data from NHANES to predict exposure rates for chemicals that have yet to be measured in target populations? (see ExpoCast program, e.g., Wambagh et al 2014) Can I use chemical product use inventory data to predict the likelihood of a chemical being present in a certain consumer product? (e.g., Phillips et al 2018) There are many different types of ML methods that we can employ in predictive toxicology and exposure science, depending on the data type / purpose of data analysis. A recent review written together with Erin Baker’s lab provides a high-level overview on some of the types of ML methods and challenges to address when analyzing multi-omic data (including chemical signature data). Answer to Environmental Health Question With this, we can now answer our Environmental Health Question: How and why are machine learning, predictive modeling, and artificial intelligence used in environmental health research? Answer: Machine learning, a subcategory of artificial intelligence, can be used in environmental health science to better understand patterns between chemical exposure and biological response in complex, high dimensional datasets. These datasets are often generated as part of efforts to screen many chemicals efficiently. Predictive modeling, which can include machine learning approaches, leverages these data to forecast the effects of a chemical on biological systems. Additional Applications of Machine Learning in Environmental Health Science In addition to the predictive toxicology questions above, ML can also be applied in the analysis of complex, high dimensional data in observational clinical (human subjects) studies in environmental health, such as: Do subjects cluster by chemical exposure? Are there similarities between subjects that cluster together for chemical exposure, suggesting underlying factors relevant to chemical exposure? Are biological signatures in different exposure groups different enough overall that ML can predict which group a subject belongs to based on their signature? Concluding Remarks In conclusion, this training module provides an overview of the field of AI and ML and discusses applications of these tools in environmental health science through predictive modeling. These methods represent common tools that are used in high dimensional data analyses within the field of environmental health sciences. In the following modules, we will provide specific examples detailing how to apply both supervised and unsupervised machine learning methods to environmental health questions and how to interpret the results of these analyses. For a review article on ML, see: Odenkirk MT, Reif DM, Baker ES. Multiomic Big Data Analysis Challenges: Increasing Confidence in the Interpretation of Artificial Intelligence Assessments. Anal Chem. 2021 Jun 8;93(22):7763-7773. PMID: 34029068 For additional case studies that leverage more advanced ML techniques, see the following recent publications that also address environmental health questions from our research groups, with bracketed tags at the end of each citation denoting ML methods used in that study: Clark J, Avula V, Ring C, Eaves LA, Howard T, Santos HP, Smeester L, Bangma JT, O’Shea TM, Fry RC, Rager JE. Comparing the Predictivity of Human Placental Gene, microRNA, and CpG Methylation Signatures in Relation to Perinatal Outcomes. Toxicol Sci. 2021 Sep 28;183(2):269-284. PMID: 34255065 [hierarchical clustering, principal component analysis, random forest] Green AJ, Mohlenkamp MJ, Das J, Chaudhari M, Truong L, Tanguay RL, Reif DM. Leveraging high-throughput screening data, deep neural networks, and conditional generative adversarial networks to advance predictive toxicology. PLoS Comput Biol. 2021 Jul 2;17(7):e1009135. PMID: 3421407 [conditional generative adversarial network, deep neural network, support vector machine, random forest, multilayer perceptron] To KT, Truong L, Edwards S, Tanguay RL, Reif DM. Multivariate modeling of engineered nanomaterial features associated with developmental toxicity. NanoImpact. 2019 Apr;16:10.1016. PMID: 32133425 [random forest] Ring C, Sipes NS, Hsieh JH, Carberry C, Koval LE, Klaren WD, Harris MA, Auerbach SS, Rager JE. Predictive modeling of biological responses in the rat liver using in vitro Tox21 bioactivity: Benefits from high-throughput toxicokinetics. Comput Toxicol. 2021 May;18:100166. PMID: 34013136 [random forest] Hickman E, Payton A, Duffney P, Wells H, Ceppe AS, Brocke S, Bailey A, Rebuli ME, Robinette C, Ring B, Rager JE, Alexis NE, Jaspers I. Biomarkers of Airway Immune Homeostasis Differ Significantly with Generation of E-Cigarettes. Am J Respir Crit Care Med. 2022 Nov 15; 206(10):1248-1258. PMID: 35731626 [hierarchical clustering, quadratic discriminant analysis, multinomial logistic regression] Perryman AN, Kim H-YH, Payton A, Rager JE, McNell EE, Rebuli ME, et al. (2023) Plasma sterols and vitamin D are correlates and predictors of ozone-induced inflammation in the lung: A pilot study. PLoS ONE 18(5): e0285721. PMID: 37186612 [random forest, support vector machine, k nearest neighbor] Payton AD, Perryman AN, Hoffman JR, Avula V, Wells H, Robinette C, Alexis NE, Jaspers I, Rager JE, Rebuli ME. Cytokine signature clusters as a tool to compare changes associated with tobacco product use in upper and lower airway samples. American Journal of Physiology-Lung Cellular and Molecular Physiology 2022 322:5, L722-L736. PMID: 35318855 [k-means clustering, principal component analysis] "],["supervised-machine-learning.html", "5.2 Supervised Machine Learning Introduction to Training Module Types of Machine Learning Types of Supervised Machine Learning Algorithms Training Supervised Machine Learning Models Assessing Classification-Based Model Performance Introduction to Activity and Example Dataset Testing for Differences in Predictor Variables across the Outcome Classes Predicting iAs Detection with a Random Forest (RF) Model Class Imbalance Concluding Remarks", " 5.2 Supervised Machine Learning This training module was developed by Alexis Payton, Oyemwenosa N. Avenbuan, Lauren E. Koval, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Machine learning is a field that has been around for decades but has exploded in popularity and utility in recent years due to the proliferation of big and/or high dimensional data. Machine learning has the ability to sift through and learn from large volumes of data and use that knowledge to solve problems. The challenges of high dimensional data as they pertain to environmental health and the applications of machine learning to mitigate some of those challenges are discussed further in Payton et. al. In this module, we will introduce different types of machine learning and then focus in on supervised machine learning, including how to train and assess supervised machine learning models. We will then analyze an example dataset with supervised machine learning highlighting an example with random forest modeling. Types of Machine Learning Within the field of machine learning, there are many different types of algorithms that can be leveraged to address environmental health research questions. The two broad categories of machine learning frequently applied to environmental health research are: (1) supervised machine learning and (2) unsupervised machine learning. Supervised machine learning involves training a model using a labeled dataset, where each independent or predictor variable is associated with a dependent variable with a known outcome. This allows the model to learn how to predict the labeled outcome on data it hasn’t “seen” before based on the patterns and relationships it previously identified in the data. For example, supervised machine learning has been used for cancer prediction and prognosis based on variables like tumor size, stage, and age (Lynch et. al, Asadi et. al). Supervised machine learning includes: Classification: Using algorithms to classify a categorical outcome (ie. plant species, disease status, etc.) Regression: Using algorithms to predict a continuous outcome (ie. gene expression, chemical concentration, etc.) Soni, D. (2018, March 22). Supervised vs. Unsupervised Learning. Towards Data Science; Towards Data Science. https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d Unsupervised machine learning, on the other hand, involves using models to find patterns or associations between variables in a dataset that lacks a known or labeled outcome. For example, unsupervised machine learning has been used to identify new patterns across genes that are co-expressed, informing potential biological pathways mediating human disease (Botía et. al, Pagnuco et. al). Langs, G., Röhrich, S., Hofmanninger, J., Prayer, F., Pan, J., Herold, C., &amp; Prosch, H. (2018). Machine learning: from radiomics to discovery and routine. Der Radiologe, 58(S1), 1–6. PMID: 34013136. Figure regenerated here in alignment with its published Creative Commons Attribution 4.0 International License. Overall, the distinction between supervised and unsupervised learning is an important concept in machine learning, as it can inform the choice of algorithms and techniques used to analyze and make predictions from data. It is worth noting that there are also other types of machine learning, such as semi-supervised learning, reinforcement learning, and deep learning, though we will not further discuss these topics in this module. Types of Supervised Machine Learning Algorithms Although this module’s example will focus on a random forest model in the coding example below, other commonly used algorithms for supervised machine learning include: K-Nearest Neighbors (KNN): Uses distance to classify a data point in the test set based upon the most common class of neighboring data points from the training set. For more information on KNN, see K-Nearest Neighbor. Support Vector Machine (SVM): Creates a decision boundary line (hyperplane) in n-dimensional space to separate the data into each class so that when new data is presented, they can be easily categorized. For more information on SVM, see Support Vector Machine. Random Forest (RF): Uses a multitude of decision trees trained on a subset of different samples from the training set and the resulting classification of a data point in the test set is aggregated from all the decision trees. A decision tree is a hierarchical model that depicts decisions from predictors and their resulting outcomes. It starts with a root node, which represents an initial test from a single predictor. The root node splits into subsequent decision nodes that test another feature. These decision nodes can either feed into more decision nodes or leaf nodes that represent the predicted class label. A branch or a sub-tree refers to a subsection of an entire decision tree. Here is an example decision tree with potential variables and decisions informing a college basketball player’s likelihood of being drafted to the NBA: While decision trees are highly interpretable, they are prone to overfitting, thus they may not always generalize well to data outside of the training set. To address this, random forests are comprised of many different decision trees. Each tree is trained on a subset of the samples in the training data, selected with replacement, and a randomly selected set of predictor variables. For a dataset with p predictors, it is common to test \\(\\sqrt{p}\\), \\(\\frac{p}{2}\\), and p predictors to see which gives the best results. This process decorrelates the trees. For a classification problem, majority vote of the decision trees determines the final class for a prediction. This process loses interpretability inherent to individual trees, but reduces the risk of overfitting. For more information on RF and decision trees, check out Random Forest and Decision Trees. Note: One algorithm is not inherently better than the others with each having their respective advantages and disadvantages. Each algorithm’s predictive ability will be largely dependent on the size of the dataset, the distribution of the data points, and the scenario. Training Supervised Machine Learning Models In supervised machine learning, algorithms need to be trained before they can be used to predict on new data. This involves selecting a smaller portion of the dataset to train the model so it will learn how to predict the outcome as accurately as possible. The process of training an algorithm is essential for enabling the model to learn and improve over time, allowing it to make more accurate predictions and better adapt to new and changing circumstances. Ultimately, the quality and relevance of the training data will have a significant impact on the effectiveness of a machine learning model. Common partitions of the full dataset used to train and test a supervised machine learning model are the following: Training Set: a subset of the data that the algorithm “sees” and uses to identify patterns. Validation Set: a subset of the training set that is used to evaluate the model’s fit in an unbiased way allowing us to fine-tune its parameters and optimize performance. Test Set: a subset of data that is used to evaluate the final model’s fit based on the training and validation sets. This provides an objective assessment of the model’s ability to generalize new data. It is common to split the dataset into a training set that contains 60% of the data and the test set that contains 40% of the data, though other common splits include 70% training / 30% test and 80% training / 20% test. It is important to note that the test set should only be examined after the algorithm has been trained using the training/validation sets. Using the test set during the development process can lead to overfitting, where the model performs well on the test data but poorly on new data. The ideal algorithm is generalizable or flexible enough to accurately predict unseen data. This is known as the bias-variance tradeoff. For further information on the bias-variance tradeoff, see Understanding the Bias-Variance Tradeoff. Cross Validation Finally, we will discuss cross validation, which is an approach used during training to expose the model to more patterns in the data and aid in model evaluation. For example, if a model is trained and tested on a 60:40 split, our model’s accuracy will likely be influenced by where this 60:40 split occurs in the dataset. This will likely bias the data and reduce the algorithm’s ability to predict accurately for data not in the training set. Overall, cross validation (CV) is implemented to fine tune a model’s parameters and improve prediction accuracy and ability to generalize. Although there are a number of cross validation approaches, we will specifically highlight k-fold cross validation. k-fold cross validation works by splitting the samples in the training dataset into k equally sized folds or groups. For example, if we implement 5-fold CV, we start by… Splitting the training data into 5 groups, or “folds”. Five iterations of training/testing are then run where each of the 5 folds serves as the test data once and as part of the training set four times, as seen in the figure below. To measure predictive ability of each of the parameters tested, like the number of features to include, values like accuracy and specificity are calculated for each iteration. The parameters that optimize performance are selected for the final model which will be evaluated against the test set not used in training. Check out these resources for additional information on Cross Validation in Machine Learning and Cross Validation Pros &amp; Cons. Assessing Classification-Based Model Performance Evaluation metrics from a confusion matrix are often used to determine the best model during training and measure model performance during testing for classification-based supervised machine learning models. A confusion matrix consists of a table that displays the numbers of how often the algorithm correctly and incorrectly predicted the outcome. Let’s imagine you’re interested in predicting whether or not a player will be drafted to the National Basketball Association (NBA) based on a dataset that contains variables regarding a player’s assists, points, height etc. Let’s say that this dataset contains information on 253 players with 114 that were actually drafted and 139 that weren’t drafted. The confusion matrix below shows a model’s results where a player that is drafted is the “positive” class and a player that is not drafted is the “negative” class. Helpful confusion matrix terminology: True positive (TP): the number of correctly classified “positive” data points (i.e., the number of correctly classified players to be drafted) True negative (TN): the number of correctly classified “negative” data points (i.e., the number of correctly classified players to be not drafted) False positive (FP): the number of incorrectly classified “positive” data points (i.e., the number of players not drafted incorrectly classified as draft picks) False negative (FN): the number of incorrectly classified “negative” data points (i.e., the number of draft picks incorrectly classified as players not drafted) Some of the metrics that can be obtained from a confusion matrix are listed below: Overall Accuracy: indicates how often the model makes a correct prediction relative to the total number of predictions made and is typically used to assess overall model performance (\\(\\frac{TP+TN}{TP+TN+FP+FN}\\)). Sensitivity or Recall: evaluates how well the model was able to predict the “positive” class. It is calculated as the ratio of correctly classified true positives to the total number of positive cases (\\(\\frac{TP}{TP+FN}\\)). Specificity: evaluates how well the model was able to predict the “negative” class. It is calculated as the ratio of correctly classified true negatives to total number of negatives cases (\\(\\frac{TN}{TN+FP}\\)). Balanced Accuracy: is the mean of sensitivity and specificity and is often used in the case of a class imbalance to gauge how well the model can correctly predict values for both classes (\\(\\frac{sensitivity+specificity}{2}\\)). Positive Predictive Value (PPV) or Precision: evaluates how accurate predictions of the “positive” class are. It is calculated as the ratio of correctly classified true positives to total number of predicted positives (\\(\\frac{TP}{TP+FN}\\)). Negative Predictive Value (NPV): evaluates how accurate predictions of the “negative” class are. It is calculated as the ratio of correctly classified true negatives to total number of predicted negatives (\\(\\frac{TN}{TN+FP}\\)). For the above metrics, values fall between 0 and 1. Instances of 0 indicate that the model was not able to classify any data points correctly, and instances of 1 indicate that the model was able to classify all test data correctly. Although subjective, an overall accuracy of at least 0.7 is considered respectable (Barkved, 2022). Furthermore, a variety of additional metrics exist for evaluating model performance for classification problems (24 Evaluation Metrics for Binary Classification (And When to Use Them)). Selecting a metric for evaluating model performance varies by situation and is dependent not only on the individual dataset, but also the question being answered. Note: For multi-class classification (more than two labeled outcomes to be predicted), the same metrics are often used, but are obtained in a slightly different way. Regression based supervised machine learning models use loss functions to evaluate model performance. For more information regarding confusion matrices and loss functions for regression-based models, see: Additional Confusion Matrix Metrics Precision vs. Recall or Specificity vs. Sensitivity Loss Functions for Machine Learning Regression Introduction to Activity and Example Dataset In this activity, we will analyze an example dataset to see whether we can use environmental monitoring data to predict areas of contamination using random forest (RF). This example model will leverage a dataset of well water variables that span geospatial location, sampling date, and well water attributes, with the goal of predicting whether detectable levels of inorganic arsenic (iAs) are present. This dataset was obtained through the sampling of 713 private wells across North Carolina through the University of North Carolina Superfund Research Program (UNC-SRP) using an analytical method that was capable of detecting levels of iAs greater than 5ppm. As demonstrated through the script below, the algorithm will first be trained and tested, and then resulting model performance will be assessed using the previously detailed confusion matrix and related performance metrics. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Which well water variables, spanning various geospatial locations, sampling dates, and well water attributes, significantly differ between samples containing detectable levels of iAs vs samples that are not contaminated/ non-detectable? How can we train a random forest (RF) model to predict whether a well might be contaminated with iAs? With this RF model, can we predict if iAs will be detected based on well water information? How could this RF model be improved upon, acknowledging that there is class imbalance? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;readxl&quot;)) install.packages(&quot;readxl&quot;); if (!requireNamespace(&quot;lubridate&quot;)) install.packages(&quot;lubridate&quot;); if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); if (!requireNamespace(&quot;gtsummary&quot;)) install.packages(&quot;gtsummary&quot;); if (!requireNamespace(&quot;flextable&quot;)) install.packages(&quot;flextable&quot;); if (!requireNamespace(&quot;caret&quot;)) install.packages(&quot;caret&quot;); if (!requireNamespace(&quot;randomForest&quot;)) install.packages(&quot;randomForest&quot;); Loading R packages required for this session library(readxl); library(lubridate); library(tidyverse); library(gtsummary); library(flextable); library(caret); library(randomForest); Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example dataset # Load the data arsenic_data &lt;- data.frame(read_xlsx(&quot;Module5_2_Input/Module5_2_InputData.xlsx&quot;)) # View the top of the dataset head(arsenic_data) ## Well_ID Water_Sample_Date Casing_Depth Well_Depth Static_Water_Depth ## 1 W_1 9/24/12 52 165 41 ## 2 W_2 12/17/15 40 445 42 ## 3 W_3 2/2/15 45 160 40 ## 4 W_4 10/22/12 42 440 57 ## 5 W_5 1/3/11 48 120 42 ## 6 W_6 12/15/15 60 280 32 ## Flow_Rate pH Detect_Concentration ## 1 60.0 7.7 ND ## 2 2.0 7.3 ND ## 3 40.0 7.4 ND ## 4 1.5 8.0 D ## 5 25.0 7.1 ND ## 6 10.0 8.2 D The columns in this dataset are described below: Well_ID: Unique id for each well (This is the sample identifier and not a predictive feature) Water_Sample_Date: Date that the well was sampled Casing_Depth: Depth of the casing of the well (ft) Well_Depth: Depth of the well (ft) Static_Water_Depth: Static water depth in the well (ft) Flow_Rate: Well flow rate (gallons per minute) pH: pH of water sample Detect_Concentration: Binary identifier (either non-detect “ND” or detect “D”) if iAs concentration detected in water sample Changing Data Types First, Detect_Concentration needs to be converted from a character to a factor so that Random Forest knows that the non-detect class is the baseline or “negative” class, while the detect class will be the “positive” class. Water_Sample_Date will be converted from a character to a date type using the mdy() function from the lubridate package. This is done so that the model understands this column contains dates. arsenic_data &lt;- arsenic_data %&gt;% # Converting `Detect_Concentration` from a character to a factor mutate(Detect_Concentration = relevel(factor(Detect_Concentration), ref = &quot;ND&quot;), # Converting water sample date from a character to a date type Water_Sample_Date = mdy(Water_Sample_Date)) %&gt;% # Removing tax id and only keeping the predictor and outcome variables in the dataset # This allows us to put the entire dataframe as is into RF select(-Well_ID) # Look at the top of the revised dataset head(arsenic_data) ## Water_Sample_Date Casing_Depth Well_Depth Static_Water_Depth Flow_Rate pH ## 1 2012-09-24 52 165 41 60.0 7.7 ## 2 2015-12-17 40 445 42 2.0 7.3 ## 3 2015-02-02 45 160 40 40.0 7.4 ## 4 2012-10-22 42 440 57 1.5 8.0 ## 5 2011-01-03 48 120 42 25.0 7.1 ## 6 2015-12-15 60 280 32 10.0 8.2 ## Detect_Concentration ## 1 ND ## 2 ND ## 3 ND ## 4 D ## 5 ND ## 6 D Testing for Differences in Predictor Variables across the Outcome Classes It is useful to run summary statistics on the variables that will be used as predictors in the algorithm to see if there are differences in distributions between the outcomes classes (either non-detect or detect in this case). Typically, greater significance often leads to better predictivity for a certain variable, since the model is better able to separate the classes. We’ll use the tbl_summary() function from the gtsummary package. Note, this may only be practical with smaller datasets or for a subset of predictors if there are many. For more information on the tbl_summary() function, check out this helpful Tutorial. arsenic_data %&gt;% # Displaying the mean and standard deviation in parentheses for all continuous variables tbl_summary(by = Detect_Concentration, statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;)) %&gt;% # Adding a column that displays the total number of samples for each variable. This will be 713 for all variables since we have no missing data add_n() %&gt;% # Adding a column that displays the p-value from an anova test add_p(test = list(all_continuous() ~ &quot;aov&quot;)) %&gt;% as_flex_table() %&gt;% bold(bold = TRUE, part = &quot;header&quot;) ## The following errors were returned during `as_flex_table()`: ## ✖ For variable `Casing_Depth` (`Detect_Concentration`) and &quot;p.value&quot; ## statistic: The package &quot;cardx&quot; (&gt;= 0.2.1) is required. ## ✖ For variable `Flow_Rate` (`Detect_Concentration`) and &quot;p.value&quot; ## statistic: The package &quot;cardx&quot; (&gt;= 0.2.1) is required. ## ✖ For variable `Static_Water_Depth` (`Detect_Concentration`) and ## &quot;p.value&quot; statistic: The package &quot;cardx&quot; (&gt;= 0.2.1) is required. ## ✖ For variable `Water_Sample_Date` (`Detect_Concentration`) and ## &quot;p.value&quot; statistic: The package &quot;cardx&quot; (&gt;= 0.2.1) is required. ## ✖ For variable `Well_Depth` (`Detect_Concentration`) and &quot;p.value&quot; ## statistic: The package &quot;cardx&quot; (&gt;= 0.2.1) is required. ## ✖ For variable `pH` (`Detect_Concentration`) and &quot;p.value&quot; ## statistic: The package &quot;cardx&quot; (&gt;= 0.2.1) is required. .cl-95b6f7f2{}.cl-95b19cee{font-family:'Arial';font-size:11pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-95b19cef{font-family:'Arial';font-size:6.6pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;position: relative;bottom:3.3pt;}.cl-95b19cf8{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-95b19cf9{font-family:'Arial';font-size:6.6pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;position: relative;bottom:3.3pt;}.cl-95b3a372{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2pt;padding-top:2pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-95b3a37c{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2pt;padding-top:2pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-95b3a37d{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-95b3a386{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-95b3a387{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-95b3b844{width:1.703in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b84e{width:0.54in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b84f{width:2.528in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b850{width:0.82in;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b858{width:1.703in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b859{width:0.54in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b85a{width:2.528in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b862{width:0.82in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b863{width:1.703in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b864{width:0.54in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b865{width:2.528in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b86c{width:0.82in;background-color:transparent;vertical-align: top;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b86d{width:1.703in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b86e{width:0.54in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b86f{width:2.528in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-95b3b870{width:0.82in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}CharacteristicNND N = 5151D N = 1981p-valueWater_Sample_Date7132013-06-05 (979.174260670888)2013-03-05 (957.843005291701)Casing_Depth71374 (33)55 (23)Well_Depth713301 (144)334 (128)Static_Water_Depth71335 (12)36 (13)Flow_Rate71325 (33)14 (16)pH7137.45 (0.55)7.82 (0.40)1Mean (SD) Note that N refers to the total sample number; ND refers to the samples that contained non-detectable levels of iAs; and D refers to the samples that contained detectable levels of iAs. Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question #1: Which well water variables, spanning various geospatial locations, sampling dates, and well water attributes, significantly differ between samples containing detectable levels of iAs vs samples that are not contaminated/ non-detect? Answer: All of the evaluated descriptor variables are significantly different, with p&lt;0.05 between detect and non-detect iAs samples, with the exception of the sample date and the static water depth. With these findings, we feel comfortable moving forward with these well water descriptive variables as predictors in our model. Setting up Cross Validation At this point, we can move forward with training and testing a RF model aimed at predicting whether or not detectable levels of iAs are present in well water samples. We’ll take a glance at the distribution of Detect_Concentration between the two classes. # Set seed for reproducibility set.seed(17) # Establish a list of indices that will used to identify our training and testing data with a 60-40 split tt_indices &lt;- createDataPartition(y = arsenic_data$Detect_Concentration, p = 0.6, list = FALSE) # Use indices to make our training and testing datasets and view the number of Ds and NDs iAs_train &lt;- arsenic_data[tt_indices,] table(iAs_train$Detect_Concentration) ## ## ND D ## 309 119 iAs_test &lt;- arsenic_data[-tt_indices,] table(iAs_test$Detect_Concentration) ## ## ND D ## 206 79 We can see that there are notably more non-detects (ND) than detects (D) in both our training and testing sets. This is something important to consider when evaluating our model’s performance. Now we can set up our cross validation and train our model. We will be using the trainControl() function from the caret package for this task. It is one of the most commonly used libraries for supervised machine learning in R and can be leveraged for a variety algorithms including RF, SVM, KNN, and others. This model will be trained with 5-fold cross validation. Additionally, we will test 2, 3, and 6 predictors through the mtry parameter. See the caret documentation here. # Establish the parameters for our cross validation with 5 folds control &lt;- trainControl(method = &#39;cv&#39;, number = 5, search = &#39;grid&#39;, classProbs = TRUE) # Establish grid of predictors to test in our model as part of hyperparameter tuning p &lt;- ncol(arsenic_data) - 1 # p is the total number of predictors in the dataset tunegrid_rf &lt;- expand.grid(mtry = c(floor(sqrt(p)), p/2, p)) # We will test sqrt(p), p/2, and p predictors (2,3,&amp; 6 predictors, respectively) to see which performs best Predicting iAs Detection with a Random Forest (RF) Model # Look at the column names in training dataset colnames(iAs_train) ## [1] &quot;Water_Sample_Date&quot; &quot;Casing_Depth&quot; &quot;Well_Depth&quot; ## [4] &quot;Static_Water_Depth&quot; &quot;Flow_Rate&quot; &quot;pH&quot; ## [7] &quot;Detect_Concentration&quot; # Train model rf_train &lt;- train(x = iAs_train[,1:6], # Our predictor variables are in columns 1-6 of the dataframe y = iAs_train[,7], # Our outcome variable is in column 7 of the dataframe trControl = control, # Specify the cross-validation parameters we defined above method = &#39;rf&#39;, # Specify we want to train a Random Forest importance = TRUE, # This parameter calculates the variable importance for RF models specifically which can help with downstream analyses tuneGrid = tunegrid_rf, # Specify the number of predictors we want to test as defined above metric = &quot;Accuracy&quot;, ) # Specify what evaluation metric we want to use to decide which model is the best # Look at the results of training rf_train ## Random Forest ## ## 428 samples ## 6 predictor ## 2 classes: &#39;ND&#39;, &#39;D&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 344, 342, 342, 342, 342 ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.7548726 0.3329518 ## 3 0.7361019 0.2891243 ## 6 0.7501661 0.3257679 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 2. # Save the best model from our training. The best performing model is determined by the number of predictor variables we tested that resulted in the highest accuracy during the cross validation step. rf_final &lt;- rf_train$finalModel # View confusion matrix for best model rf_final ## ## Call: ## randomForest(x = x, y = y, mtry = param$mtry, importance = TRUE) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 24.77% ## Confusion matrix: ## ND D class.error ## ND 275 34 0.1100324 ## D 72 47 0.6050420 Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: How can we train a random forest (RF) model to predict whether a well might be contaminated with iAs? Answer: As is standard practice with supervised ML, we split our full dataset into a training dataset and a test dataset using a 60-40 split. Using the caret package, we implemented 5-fold cross validation to train a RF while also testing different numbers of predictors to see which optimized performance. The model that resulted in the greatest accuracy was selected as the final model. Now we can see how well our model does on data it hasn’t seen before by applying it to our testing data. # Use our best model to predict the classes for our test data. We need to make sure we remove the column of Ds/NDs from our test data. rf_res &lt;- predict(rf_final, iAs_test %&gt;% select(!Detect_Concentration)) # View a confusion matrix of the results and gauge model performance # Be sure to include the &#39;positive&#39; parameter to specify the correct positive class confusionMatrix(rf_res, iAs_test$Detect_Concentration, positive = &quot;D&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction ND D ## ND 178 47 ## D 28 32 ## ## Accuracy : 0.7368 ## 95% CI : (0.6817, 0.787) ## No Information Rate : 0.7228 ## P-Value [Acc &gt; NIR] : 0.32445 ## ## Kappa : 0.2907 ## ## Mcnemar&#39;s Test P-Value : 0.03767 ## ## Sensitivity : 0.4051 ## Specificity : 0.8641 ## Pos Pred Value : 0.5333 ## Neg Pred Value : 0.7911 ## Prevalence : 0.2772 ## Detection Rate : 0.1123 ## Detection Prevalence : 0.2105 ## Balanced Accuracy : 0.6346 ## ## &#39;Positive&#39; Class : D ## Answer to Environmental Health Question 3 With this, we can answer Environmental Health Question #3: With this RF model, can we predict if iAs will be detected based on well water information? Answer: We can use this model to predict if iAs can be detected in well water given that an overall accuracy of ~0.72 is decent, however we should consider other metrics that may influence how good we feel about this model depending on what is important to the question we are trying to answer. For example, the model did a good job at predicting non-detect data based on a sensitivity of ~0.85 and a NPV ~0.78, but struggled at predicting detect data based on a specificity of ~0.39 and a PPV of ~0.50. Additionally, the balanced accuracy of ~0.62 further emphasizes the difference in predictive ability of the model for non-detects and detects. If it is highly important to us that detects are classified correctly, we may want to improve this model before implementing it. Class Imbalance It is worth noting this discrepancy in predictive capabilities for detects vs. non-detects makes sense due to the observed class imbalance in our training data. There were notably more non-detects than detects in the training set, so the model was exposed to more of these data points and struggles to distinguish unique characteristics of detects when compared to non-detects. Additionally, we told the training algorithm to prioritize selecting a final model based on its overall accuracy. In the instances of a heavy class imbalance, it is common for a high accuracy to be achieved as the more prevalent class is predicted more often, though this doesn’t give the full picture of the model’s predictive capabilities. For example, if you consider a dog/cat case with a set of 90 dogs and 10 cats, a model could achieve 90% accuracy by predicting dog every time, which isn’t at all helpful in predicting cats. This is particularly important, because for toxicology related datasets, the “positive” class often represents the class with greater public health risk/ interest but can have less data. For example, when you classify subjects based upon whether or not they have asthma based on gene expression data. Asthmatics would likely be the “positive” class, but given that asthmatics are less prevalent than non-asthmatics in the general population, they would likely represent the minority class too. To address this issue, a few methods can be considered. Full implementation of these approaches is beyond the scope of this module, but relevant resources for further exploration are given. Synthetic Minority Oversampling Technique (SMOTE)- increases the number of minority classes in the training data, thereby reducing the class imbalance by synthetically generating additional samples derived from the existing minority class samples. SMOTE Oversampling &amp; Tutorial On How To Implement In Python And R How to Use SMOTE for Imbalanced Data in R (With Example) Adjusting the loss function- Loss functions in machine learning quantify the penalty for a bad prediction. They can be adjusted to where the minority class is penalized more forcing the model to learn to make fewer mistakes when predicting the minority class. Alternative Performance Metrics- When training the model, alternative metrics to overall accuracy may yield a more robust model capable of better predicting the minority class. Example alternatives may include balanced accuracy or an F1-score. The caret package further allows for custom, user-defined metrics to be evaluated during training by specifying the summaryFunction parameter in the trainControl() function, as seen below, in addition to the defaultSummary() and twoClassSummary() functions. In the example code below, we’re creating a function (f1) that will calculate the F1 score and find the optimal model with the highest F1 score as opposed to the highest accuracy as we did above. install.packages(&quot;MLmetrics&quot;) library(MLmetrics) f1 &lt;- function(data, lev = NULL, model = NULL) { # Creating a function to calculate the F1 score f1_val &lt;- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1]) c(F1 = f1_val) } # 5 fold CV ctrl &lt;- trainControl( method = &quot;cv&quot;, number = 5, classProbs = TRUE, summaryFunction = f1 ) # Training the RF model mod &lt;- train(x = X, y = Y, trControl = ctrl, method = &quot;rf&quot;, tuneGrid = tunegrid_rf, importance = TRUE, # Basing the best model performance off of the F1 score within 5 CV metric = &quot;F1&quot;) For more in-depth information and additional ways to address class imbalance check out How to Deal with Imbalanced Data in Classification. Answer to Environmental Health Question 4 With this, we can answer Environmental Health Question #4: How could this RF model be improved upon, acknowledging that there is class imbalance? Answer: We can implement SMOTE to increase the number of training data points for the minority class thereby reducing the class imbalance. In conjunction with using SMOTE, another approach includes selecting an alternative performance metric during training that does a better job taking the existing class imbalance into consideration, such as balanced accuracy or an F1-score, improves our predictive ability for the minority class. Concluding Remarks In conclusion, this training module has provided an introduction to supervised machine learning using classification techniques in R. Machine learning is a powerful tool that can help researchers gain new insights and improve models to analyze complex datasets faster and in a more comprehensive way. The example we’ve explored demonstrates the utility of supervised machine learning models on an environmentally relevant dataset. Additional Resources To learn more check out the following resources: IBM - What is Machine Learning Curate List of AI and Machine Learning Resources Introduction to Machine Learning in R Machine Learning by Mueller, J. P. (2021). Machine learning for dummies. John Wiley &amp; Sons. Test Your Knowledge Using the “Module5_2TYKInput.xlsx”, use RF to determine if well water data can be accurate predictors of Manganese detection. The data is structured similarly to the “Module5_2_InputData.xlsx” used in this module, however it now includes 4 additional features: Longitude: Longitude of address (decimal degrees) Latitude: Latitude of address (decimal degrees) Stream_Distance: Euclidean distance to the nearest stream (feet) Elevation: Surface elevation of the sample location (feet) "],["supervised-machine-learning-model-interpretation.html", "5.3 Supervised Machine Learning Model Interpretation Introduction to Training Module Variable Importance Decision Boundary Introduction to Example Dataset and Activity Predicting iAs Detection with a Random Forest (RF) Model Variable Importance Plot Decision Boundary Plot Decision Boundary Plot Incorporating SMOTE Concluding Remarks", " 5.3 Supervised Machine Learning Model Interpretation This training module was developed by Alexis Payton, Lauren E. Koval, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Supervised machine learning (ML) represents a subset of ML methods wherein the outcome variable is known or assigned prior to training a model to be able to predict said outcome. As we discussed in previous modules, ML methods are advantageous in that they easily incorporate a multitude of potential predictor variables, which allows these models to more closely consider real-world, complex environmental health scenarios and offer new insights through a more holistic consideration of available data inputs. However, one disadvantage of ML is that it is often not as easily interpretable as traditional statistics (e.g., regression based methods with defined beta coefficients for each input predictor variable). With this limitation in mind, there are methods and concepts that can be applied to supervised ML algorithms to aid in the understanding of their predictions including variable (feature) importance and decision boundaries, which we will cover in this module. We will also include example visualization techniques of these methods, representing important aspects contributing to model interpretability, since visualizing helps convey concepts faster and across a broader target audience. In addition, this module addresses methods to communicate these findings in a paper so that a wider span of readers can understand overall take-home points. As with other data analyses, we advise to focus just as much on the why components of a study’s research question(s) as opposed to only focusing on the what or how. To elaborate, we explain through this module that it is not as important to explain all the intricacies of how a model works and how its parameters were tuned; rather, it is more important to focus on why a particular model was selected and how it will be leveraged to answer your research questions. This can all be a bit subjective and requires expertise within your research field. As a first step, let’s first learn about some model interpretation methodologies highlighting Variable Importance and Decision Boundaries as important examples relevant to environmental health research. Then, this training module will further describe approaches to summarize these methods and communicate supervised ML findings to a broader audience. Variable Importance When a supervised ML algorithm makes predictions, it relies more heavily on some variables than others. How much a variable contributes to classifying data is known as variable (feature) importance. Often times, this is thought of as the impact on overall model performance if a variable were to be removed from the model. There are many methods that are used to measure feature importance, including… SHapley Additive exPlanations (SHAP): based on game theory where each variable is considered a “player” where we’re seeking to determine each player’s contribution to the outcome of a “game” or overall model performance. It divides the model performance metric amongst all the variables, so that the sum of the shapley values for all the predictors is equal to the overall model performance. For more information on SHAP, see A Novel Approach to Feature Importance. Mean decrease gini (gini impurity): quantifies the improvement of predictivity with the addition of each predictor in a decision tree, which is then averaged over all the decision trees tested. The higher the value the greater the importance on the algorithm. This metric can easily be extracted from classification-based models, including random forest (RF) classifications, which is what we will focus on in this module. Note for RF regression-based models, node purity can be extracted as a measure of feature importance. For more information, please see the following resources regarding Feature Importance and Mean Decrease Gini. Decision Boundary Another concept that is pertinent to a model’s interpretability is understanding a decision boundary and how visualizing it can further aid in understanding how the model classifies new data points. A decision boundary is a line (or a hyperplane) that seeks to separate the training data by class. This line can be linear or non-linear and is formed in n-dimensional space. To clarify, although support vector machine (SVM) specifically uses decision boundaries to classify training data and make predictions on test data, decision boundaries can still be drawn for other algorithms. A decision boundary can be visualized to convey how well an algorithm is able to classify an outcome based on the data given. It is important to note that most ML models make use of datasets that contain three or more predictors, and it is difficult to visualize a plot in more than three dimensions. Therefore, the number of features and which features to plot need to be narrowed down to two variables. For this reason, the resulting visualization is not a true representation of the decision boundary from the initial model using all predictors, since the visualization only relies on prediction results from two variables. Nevertheless, decision boundary plots can be powerful visualizations to determine thresholds between the outcome classes. When choosing variables for decision boundary plots, features that have the most influence on the model are often selected, but that is not always the case. Sometimes predictors are selected based upon the environmental health implications relevant to the research question. For example in Perryman et. al, lung response following ozone exposure was investigated by sampling derivatives of cholesterol biosynthesis in human subjects. In this paper, these sterol metabolites were used to predict whether a subject would be classified as having a lung response that was considered non-responsive or responsive. A decision boundary plot was made using two predictors: Cholesterol, given that it had the highest variable importance and Vitamin D, given its synthesis can be affected by ozone despite it having a lower variable importance in the paper’s models. Figure 5. Decision boundary plot for SVM model predicting lung response class. Cholesterol and 25-hydroxyvitamin D were used as predictors visualizing responder status [non-responders(green) and responders (yellow)] and disease status [non-asthmatics (triangles) and asthmatics (circles)]. The shaded regions are the model’s prediction of a subject’s lung response class at a given cholesterol and 25-hydroxyvitamin D concentration. Takeaways from this decision boundary plot: Subjects with more lung inflammation (“responders”) after ozone exposure tended to have higher Vitamin D levels (&gt; 35pmol/mL) and lower Cholesterol levels (&lt; 675nmol/mL). These “responder” subjects were more likely to be non-asthmatics. Introduction to Example Dataset and Activity In the previous module, we investigated whether a classification-based RF model using well water variables would be accurate predictors of inorganic arsenic (iAs) contamination. While it is helpful to know if certain variables are able to be used to construct a model that accurately predict detectability, from a public health standpoint, it is also helpful to know which of those features contribute the most to a model’s accuracy. Therefore, if we can identify the features that are associated with having lower arsenic detection, we can use that information to inform policies when new wells are constructed. In addition to identifying variables with the greatest importance to the algorithm, it is also pertinent to understand the ranges of when a well is more or less likely to have arsenic detected. For example, are wells with a lower flow rate more likely to have arsenic detected? In this module, this will be addressed by extracting variable importance from the same algorithm and plotting it. The two features with the highest variable importance will be identified and used to construct a decision boundary plot to determine how features are associated with iAs detection. The data to be used in this module was described and referenced previously in TAME 2.0 Module 5.2 Supervised Machine Learning. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: After plotting variable importance from highest to lowest, which two predictors have the highest variable importance on the predictive accuracy of iAs detection from a RF algorithm? Using the two features with the highest variable importance, under what conditions are we more likely to predict detectable iAs in wells based on a decision boundary plot? How do the decision boundaries shift after incorporating SMOTE to address class imbalance? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;readxl&quot;)) install.packages(&quot;readxl&quot;); if (!requireNamespace(&quot;lubridate&quot;)) install.packages(&quot;lubridate&quot;); if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); if (!requireNamespace(&quot;caret&quot;)) install.packages(&quot;caret&quot;); if (!requireNamespace(&quot;randomForest&quot;)) install.packages(&quot;randomForest&quot;); if (!requireNamespace(&quot;themis&quot;)) install.packages(&quot;themis&quot;); Loading R packages required for this session library(readxl) library(lubridate) library(tidyverse) library(caret) library(randomForest) library(e1071) library(ggsci) library(themis) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example dataset # Load the data arsenic_data &lt;- data.frame(read_excel(&quot;Module5_3_Input/Module5_3_InputData.xlsx&quot;)) # View the top of the dataset head(arsenic_data) ## Well_ID Water_Sample_Date Casing_Depth Well_Depth Static_Water_Depth ## 1 W_1 9/24/12 52 165 41 ## 2 W_2 12/17/15 40 445 42 ## 3 W_3 2/2/15 45 160 40 ## 4 W_4 10/22/12 42 440 57 ## 5 W_5 1/3/11 48 120 42 ## 6 W_6 12/15/15 60 280 32 ## Flow_Rate pH Detect_Concentration ## 1 60.0 7.7 ND ## 2 2.0 7.3 ND ## 3 40.0 7.4 ND ## 4 1.5 8.0 D ## 5 25.0 7.1 ND ## 6 10.0 8.2 D Changing Data Types First, Detect_Concentration needs to be converted from a character to a factor so that Random Forest knows that the non-detect class is the baseline or “negative” class, while the detect class will be the “positive” class. Water_Sample_Date will be converted from a character to a date type using the mdy() function from the lubridate package. This is done so that the model understands this column contains dates. arsenic_data &lt;- arsenic_data %&gt;% # Converting `Detect_Concentration` from a character to a factor mutate(Detect_Concentration = relevel(factor(Detect_Concentration), ref = &quot;ND&quot;), # Converting water sample date from a character to a date type Water_Sample_Date = mdy(Water_Sample_Date)) %&gt;% # Removing well id and only keeping the predictor and outcome variables in the dataset # This allows us to put the entire dataframe as is into RF select(-Well_ID) # View the top of the current dataset head(arsenic_data) ## Water_Sample_Date Casing_Depth Well_Depth Static_Water_Depth Flow_Rate pH ## 1 2012-09-24 52 165 41 60.0 7.7 ## 2 2015-12-17 40 445 42 2.0 7.3 ## 3 2015-02-02 45 160 40 40.0 7.4 ## 4 2012-10-22 42 440 57 1.5 8.0 ## 5 2011-01-03 48 120 42 25.0 7.1 ## 6 2015-12-15 60 280 32 10.0 8.2 ## Detect_Concentration ## 1 ND ## 2 ND ## 3 ND ## 4 D ## 5 ND ## 6 D Setting up Cross Validation Note that the code below is different than the code presented in the previous module, TAME 2.0 Module 5.2 Supervised Machine Learning. Both coding methods are valid and produce comparable results, however we wanted to present another way to run k-fold cross validation and random forest. In 5-fold cross validation (CV), there are 5 equally-sized folds (ideally!). This means that 80% of the original dataset is split into the 4 folds that comprise the training set and the remaining 20% in the last fold is reserved for the test set. Previously, the trainControl() function was used for CV. This time we’ll use the createFolds() function also from the caret package. # Setting seed for reproducibility set.seed(12) # 5-fold cross validation arsenic_index = createFolds(arsenic_data$Detect_Concentration, k = 5) # Seeing if about 20% of the records are in the testing set kfold1 = arsenic_index[[1]] length(kfold1)/nrow(arsenic_data) ## [1] 0.1991585 # Creating vectors for parameters to be tuned ntree_values = c(50, 250, 500) # number of decision trees p = dim(arsenic_data)[2] - 1 # number of predictor variables in the dataset mtry_values = c(sqrt(p), p/2, p) # number of predictors to be used in the model Predicting iAs Detection with a Random Forest (RF) Model Notice that in the code below we are choosing the final RF model to be the one with the lowest out of bag (OOB) error. In the previous module, the final model was chosen based on the highest accuracy, however this is a similar approach here given that OOB error = 1 - Accuracy. # Setting the seed again so the predictions are consistent set.seed(12) # Creating an empty dataframe to save the confusion matrix metrics and variable importance metrics = data.frame() variable_importance_df = data.frame() # Iterating through the cross validation folds for (i in 1:length(arsenic_index)){ # Training data data_train = arsenic_data[-arsenic_index[[i]],] # Test data data_test = arsenic_data[arsenic_index[[i]],] # Creating empty lists and dataframes to store errors reg_rf_pred_tune = list() rf_OOB_errors = list() rf_error_df = data.frame() # Tuning parameters: using ntree and mtry values to determine which combination yields the smallest OOB error # from the validation datasets for (j in 1:length(ntree_values)){ for (k in 1:length(mtry_values)){ # Running RF to tune parameters reg_rf_pred_tune[[k]] = randomForest(Detect_Concentration ~ ., data = data_train, ntree = ntree_values[j], mtry = mtry_values[k]) # Obtaining the OOB error rf_OOB_errors[[k]] = data.frame(&quot;Tree Number&quot; = ntree_values[j], &quot;Variable Number&quot; = mtry_values[k], &quot;OOB_errors&quot; = reg_rf_pred_tune[[k]]$err.rate[ntree_values[j],1]) # Storing the values in a dataframe rf_error_df = rbind(rf_error_df, rf_OOB_errors[[k]]) } } # Finding the lowest OOB error from the 5 folds using best number of predictors at split best_oob_errors &lt;- which(rf_error_df$OOB_errors == min(rf_error_df$OOB_errors)) # Now running RF on the entire training set with the tuned parameters # This will be done 5 times for each fold reg_rf &lt;- randomForest(Detect_Concentration ~ ., data = data_train, ntree = rf_error_df$Tree.Number[min(best_oob_errors)], mtry = rf_error_df$Variable.Number[min(best_oob_errors)]) # Predicting on test set and adding the predicted values as an additional column to the test data data_test$Pred_Detect_Concentration = predict(reg_rf, newdata = data_test, type = &quot;response&quot;) matrix = confusionMatrix(data = data_test$Pred_Detect_Concentration, reference = data_test$Detect_Concentration, positive = &quot;D&quot;) # Extracting accuracy, sens, spec, PPV, NPV and adding to the dataframe to take mean later matrix_values = data.frame(t(c(matrix$byClass[11])), t(c(matrix$byClass[1:4]))) metrics = rbind(metrics, matrix_values) # Extracting variable importance variable_importance_values = data.frame(importance(reg_rf)) %&gt;% rownames_to_column(var = &quot;Predictor&quot;) variable_importance_df = rbind(variable_importance_df, variable_importance_values) } # Taking average across the 5 folds metrics = metrics %&gt;% summarise(`Balanced Accuracy` = mean(Balanced.Accuracy), Sensitivity = mean(Sensitivity), Specificity = mean(Specificity), PPV = mean(Pos.Pred.Value), NPV = mean(Neg.Pred.Value)) variable_importance_df = variable_importance_df %&gt;% group_by(Predictor) %&gt;% summarise(MeanDecreaseGini = mean(MeanDecreaseGini)) %&gt;% # Sorting from highest to lowest arrange(-MeanDecreaseGini) The confusion matrix results from the previous module are shown below. Now let’s double check that when using this new method, our results are still comparable. # First comparing results to the previous module round(metrics, 2) ## Balanced Accuracy Sensitivity Specificity PPV NPV ## 1 0.64 0.41 0.87 0.55 0.79 They are! Now we’ll take a look at the model’s variable importance. variable_importance_df ## # A tibble: 6 × 2 ## Predictor MeanDecreaseGini ## &lt;chr&gt; &lt;dbl&gt; ## 1 Casing_Depth 50.6 ## 2 pH 42.3 ## 3 Water_Sample_Date 37.6 ## 4 Flow_Rate 33.5 ## 5 Well_Depth 32.2 ## 6 Static_Water_Depth 31.7 Although we have the results we need, let’s take it a step further and plot the data. Reformatting the dataframe for plotting First, the dataframe will be transformed so that the figure is more legible. Specifically, spaces will be added between the variables, and the Predictor column will be put into a factor to rearrange the order of the variables from lowest to highest mean decrease gini. For additional information on tricks like this to make visualizations easier to read, see TAME 2.0 Module 3.2 Improving Data Visualizations. # Adding spaces between the variables that need the space modified_variable_importance_df = variable_importance_df %&gt;% mutate(Predictor = gsub(&quot;_&quot;, &quot; &quot;, Predictor)) # Saving the order of the variables from lowest to highest mean decrease gini by putting into a factor predictor_order = rev(modified_variable_importance_df$Predictor) modified_variable_importance_df$Predictor = factor(modified_variable_importance_df$Predictor, levels = predictor_order) head(modified_variable_importance_df) ## # A tibble: 6 × 2 ## Predictor MeanDecreaseGini ## &lt;fct&gt; &lt;dbl&gt; ## 1 Casing Depth 50.6 ## 2 pH 42.3 ## 3 Water Sample Date 37.6 ## 4 Flow Rate 33.5 ## 5 Well Depth 32.2 ## 6 Static Water Depth 31.7 Variable Importance Plot ggplot(data = modified_variable_importance_df , aes(x = MeanDecreaseGini, y = Predictor, size = 2)) + geom_point() + theme_light() + theme(axis.line = element_line(color = &quot;black&quot;), #making x and y axes black axis.text = element_text(size = 12), #changing size of x axis labels axis.title = element_text(face = &quot;bold&quot;, size = rel(1.7)), #changes axis titles legend.title = element_text(face = &#39;bold&#39;, size = 14), #changes legend title legend.text = element_text(size = 12), #changes legend text strip.text.x = element_text(size = 15, face = &quot;bold&quot;), #changes size of facet x axis strip.text.y = element_text(size = 15, face = &quot;bold&quot;)) + #changes size of facet y axis labs(x = &#39;Variable Importance&#39;, y = &#39;Predictor&#39;) + #changing axis labels guides(size = &quot;none&quot;)#removing size legend An appropriate title for this figure could be: “Figure X. Variable importance from random forest models predicting iAs detection. Variable importance is derived from mean decrease gini values extracted from random forest models. Features are listed on the y axis from greatest (top) to least (bottom) mean decrease gini.” Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question #1: After plotting variable importance from highest to lowest, which two predictors have the highest variable importance on the predictive accuracy of iAs detection from a RF algorithm? Answer: From the variable importance dataframe and plot, we can see that casing depth and pH had the greatest impact on RF followed by water sample date, flow rate, static water depth, and well depth in descending order. Since casing depth and pH have been identified as the predictors with the highest variable importance, they will be prioritized as the two predictors included in the decision boundary plot example below. Decision Boundary Calculation First, models will be trained using only casing depth and pH as variables. Since, the decision boundary plot will be used for visualization purposes, and a 2-D figure can only plot two variables, we will not worry about tuning the parameters as was previously done. In this module, we’re creating a decision boundary based on a random forest model, however we’ll also explore what decision boundaries look like for other algorithms including support vector machine (SVM), and k nearest neighbor (KNN), logistic regression. Each supervised ML method has its advantages and performance is dependent upon the situation and the dataset. Therefore, it is common to see multiple models used to predict an outcome of interest in a publication. Let’s create additional boundary plots still using casing depth and pH, but this time we will use logistic regression, SVM, and KNN as comparisons to RF. # Creating a dataframe with variables based on the highest predictors highest_pred_data = data.frame(arsenic_data[,c(&quot;Casing_Depth&quot;, &quot;pH&quot;, &quot;Detect_Concentration&quot;)]) # Training RF rf_detect_arsenic = randomForest(Detect_Concentration~., data = highest_pred_data) # Logistic regression lr_detect_arsenic = glm(Detect_Concentration~., data = highest_pred_data, family = binomial(link = &#39;logit&#39;)) # SVM with a radial kernel (hyperplane) svm_detect_arsenic = svm(Detect_Concentration~., data = highest_pred_data, kernel = &quot;radial&quot;) # KNN knn_detect_arsenic = knn3(Detect_Concentration~., data = highest_pred_data) # specifying 2 classes From these predictions, decision boundaries will be calculated. This will be done by predicting Detect_Concentration between a grid of values - specifically the minimum and maximum of the two predictors (casing depth and pH). A non-linear line will be drawn on the plot to separate the two classes. get_grid_df &lt;- function(classification_model, data, resolution = 100, predict_type) { # This function predicts the outcome (Detect_Concentration) at evenly spaced data points using the two variables (pH and casing depth) # to create a decision boundary between the outcome classes (detect and non-detect samples). # :parameters: a classification-based supervised machine learning model, dataset containing the predictors and outcome variable, # specifies the number of data points to make between the minimum and maximum predictor values, prediction type # :output: a grid of values for both predictors and their corresponding predicted outcome class # Grabbing only the predictor data predictor_data &lt;- data[,1:2] # Creating a dataframe that contains the min and max for both features min_max_df &lt;- sapply(predictor_data, range, na.rm = TRUE) # Creating a vector of evenly spaced points between the min and max for the first variable (casing depth) variable1_vector &lt;- seq(min_max_df[1,1], min_max_df[2,1], length.out = resolution) # Creating a vector of evenly spaced points between the min and max for the second variable (pH) variable2_vector &lt;- seq(min_max_df[1,2], min_max_df[2,2], length.out = resolution) # Creating a dataframe of grid values by combining the two vectors grid_df &lt;- data.frame(cbind(rep(variable1_vector, each = resolution), rep(variable2_vector, time = resolution))) colnames(grid_df) &lt;- colnames(min_max_df) # Predicting class label based on all the predictor pairs of data grid_df$Pred_Class = predict(classification_model, grid_df, type = predict_type) return(grid_df) } # calling function # RF grid_df_rf = get_grid_df(rf_detect_arsenic, highest_pred_data, predict_type = &quot;class&quot;) %&gt;% # Adding in a column that indicates the model so all the dataframes can be combined mutate(Model = &quot;A. Random Forest&quot;) # SVM with a radial kernel (hyperplane) grid_df_svm = get_grid_df(svm_detect_arsenic, highest_pred_data, predict_type = &quot;class&quot;) %&gt;% mutate(Model = &quot;B. Support Vector Machine&quot;) # KNN grid_df_knn = get_grid_df(knn_detect_arsenic, highest_pred_data, predict_type = &quot;class&quot;) %&gt;% mutate(Model = &quot;C. K Nearest Neighbor&quot;) # Logistic regression grid_df_lr = get_grid_df(lr_detect_arsenic, highest_pred_data, predict_type = &quot;response&quot;) %&gt;% # First specifying the cutoff point for logistic regression predictions # If the response is &gt;= 0.5 it will be classified as a detect prediction mutate(Pred_Class = relevel(factor(ifelse(Pred_Class &gt;= 0.5, &quot;D&quot;, &quot;ND&quot;)), ref = &quot;ND&quot;), Model = &quot;D. Logistic Regression&quot;) # Creating 1 dataframe grid_df = rbind(grid_df_rf, grid_df_lr, grid_df_svm, grid_df_knn) # Viewing the dataframe to be plotted head(grid_df) ## Casing_Depth pH Pred_Class Model ## 1 27 5.400000 ND A. Random Forest ## 2 27 5.433333 ND A. Random Forest ## 3 27 5.466667 ND A. Random Forest ## 4 27 5.500000 ND A. Random Forest ## 5 27 5.533333 ND A. Random Forest ## 6 27 5.566667 ND A. Random Forest Decision Boundary Plot Now let’s plot the grid of predictions with the sampled data. # choosing palette from package ggsci_colors = pal_npg()(5) ggplot() + geom_point(data = arsenic_data, aes(x = pH, y = Casing_Depth, color = Detect_Concentration), position = position_jitter(w = 0.1, h = 0.1), size = 4, alpha = 0.8) + geom_contour(data = grid_df, aes(x = pH, y = Casing_Depth, z = as.numeric(Pred_Class == &quot;D&quot;)), color = &quot;black&quot;, breaks = 0.5) + # adds contour line geom_point(data = grid_df, aes(x = pH, y = Casing_Depth, color = Pred_Class), size = 0.1) + # shades plot xlim(5.9, NA) + # changes the limits of the x axis facet_wrap(~Model, scales = &#39;free&#39;) + theme_light() + theme(axis.line = element_line(color = &quot;black&quot;), #making x and y axes black axis.text = element_text(size = 10), #changing size of x axis labels axis.title = element_text(face = &quot;bold&quot;, size = rel(1.7)), #changes axis titles legend.title = element_text(face = &#39;bold&#39;, size = 12), #changes legend title legend.text = element_text(size = 12), #changes legend text legend.position = &quot;bottom&quot;, # move legend to top left corner legend.background = element_rect(color = &#39;black&#39;, fill = &#39;white&#39;, linetype = &#39;solid&#39;), # changes legend background strip.text = element_text(size = 15, face = &quot;bold&quot;)) + #changes size of facet x axis labs(y = &#39;Casing Depth (ft)&#39;) + #changing axis labels scale_color_manual(name = &quot;Arsenic Detection&quot;, # renaming the legend values = ggsci_colors[c(4,5)], labels = c(&#39;Non-Detect&#39;,&#39;Detect&#39;)) # renaming the classes Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: Using the two features with the highest variable importance, under what conditions are we more likely to predict detectable iAs in wells based on a decision boundary plot? Answer: There is some overlap between detect and non-detect iAs samples; however, it is evident that wells with detectable levels of iAs were more likely to have lower (&lt;80 ft) casing depths and a more basic pH (&gt; 7) based on RF and KNN models. It seems like SVM and logistic regression could have potentially captured a greater “detect” region indicating that the models likely struggled to predict “detect” values. In the next section, SMOTE will be used to see if these decision boundaries can be improved. Decision Boundary Plot Incorporating SMOTE Here, we will create a decision boundary plot still using casing depth and pH, but this time we will make our dataset more balance to see how improve model performance visually. The Synthetic Minority Oversampling Technique (SMOTE) was introduced in TAME 2.0 Module 5.2 Supervised Machine Learning and will be used to make the dataset more balanced by oversampling the minority class (detect values) and undersampling the majority class (non-detect values). Starting by training each model: # Using SMOTE first to balance classes balanced_highest_pred_data = smotenc(highest_pred_data, &quot;Detect_Concentration&quot;) # Training RF rf_detect_arsenic = randomForest(Detect_Concentration~., data = balanced_highest_pred_data) # Logistic regression lr_detect_arsenic = glm(Detect_Concentration~., data = balanced_highest_pred_data, family = binomial(link = &#39;logit&#39;)) # SVM with a radial kernel (hyperplane) svm_detect_arsenic = svm(Detect_Concentration~., data = balanced_highest_pred_data, kernel = &quot;radial&quot;) # KNN knn_detect_arsenic = knn3(Detect_Concentration~., data = balanced_highest_pred_data) # specifying 2 classes Now calling the get_grid_df() function we created above to create a grid of predictions. # Calling function # RF balanced_grid_df_rf = get_grid_df(rf_detect_arsenic, balanced_highest_pred_data, predict_type = &quot;class&quot;) %&gt;% # Adding in a column that indicates the model so all the dataframes can be combined mutate(Model = &quot;A. Random Forest&quot;) # SVM with a radial kernel (hyperplane) balanced_grid_df_svm = get_grid_df(svm_detect_arsenic, balanced_highest_pred_data, predict_type = &quot;class&quot;) %&gt;% mutate(Model = &quot;B. Support Vector Machine&quot;) # KNN balanced_grid_df_knn = get_grid_df(knn_detect_arsenic, balanced_highest_pred_data, predict_type = &quot;class&quot;) %&gt;% mutate(Model = &quot;C. K Nearest Neighbor&quot;) # Logistic regression balanced_grid_df_lr = get_grid_df(lr_detect_arsenic, balanced_highest_pred_data, predict_type = &quot;response&quot;) %&gt;% # First specifying the cutoff point for logistic regression predictions # If the response is &gt;= 0.5 it will be classified as a detect prediction mutate(Pred_Class = relevel(factor(ifelse(Pred_Class &gt;= 0.5, &quot;D&quot;, &quot;ND&quot;)), ref = &quot;ND&quot;), Model = &quot;D. Logistic Regression&quot;) # Creating 1 dataframe balanced_grid_df = rbind(balanced_grid_df_rf, balanced_grid_df_lr, balanced_grid_df_svm, balanced_grid_df_knn) # Viewing the dataframe to be plotted head(balanced_grid_df) ## Casing_Depth pH Pred_Class Model ## 1 27 5.400000 ND A. Random Forest ## 2 27 5.433333 ND A. Random Forest ## 3 27 5.466667 ND A. Random Forest ## 4 27 5.500000 ND A. Random Forest ## 5 27 5.533333 ND A. Random Forest ## 6 27 5.566667 ND A. Random Forest # choosing palette from package ggsci_colors = pal_npg()(5) ggplot() + geom_point(data = arsenic_data, aes(x = pH, y = Casing_Depth, color = Detect_Concentration), position = position_jitter(w = 0.1, h = 0.1), size = 4, alpha = 0.8) + geom_contour(data = balanced_grid_df, aes(x = pH, y = Casing_Depth, z = as.numeric(Pred_Class == &quot;D&quot;)), color = &quot;black&quot;, breaks = 0.5) + # adds contour line geom_point(data = balanced_grid_df, aes(x = pH, y = Casing_Depth, color = Pred_Class), size = 0.1) + # shades plot xlim(5.9, NA) + # changes the limits of the x axis facet_wrap(~Model, scales = &#39;free&#39;) + theme_light() + theme(axis.line = element_line(color = &quot;black&quot;), #making x and y axes black axis.text = element_text(size = 10), #changing size of x axis labels axis.title = element_text(face = &quot;bold&quot;, size = rel(1.7)), #changes axis titles legend.title = element_text(face = &#39;bold&#39;, size = 12), #changes legend title legend.text = element_text(size = 12), #changes legend text legend.position = &quot;bottom&quot;, # move legend to top left corner legend.background = element_rect(color = &#39;black&#39;, fill = &#39;white&#39;, linetype = &#39;solid&#39;), # changes legend background strip.text = element_text(size = 15, face = &quot;bold&quot;)) + #changes size of facet x axis labs(y = &#39;Casing Depth (ft)&#39;) + #changing axis labels scale_color_manual(name = &quot;Arsenic Detection&quot;, # renaming the legend values = ggsci_colors[c(4,5)], labels = c(&#39;Non-Detect&#39;,&#39;Detect&#39;)) # renaming the classes An appropriate title for this figure could be: “Figure X. Decision boundary plots from supervised machine learning models predicting iAs detection. The top two predictors on model performance, casing depth and pH, were used to visualize arsenic detection [non-detect (red) and detect (blue)]. The shaded regions represent prediction of a well’s detection class based on varying casing depth and pH values using (A) Random Forest, (B) Support Vector Machine, (C) K Nearest Neighbor, and (D) Logistic Regression. Answer to Environmental Health Question 3 With this, we can answer Environmental Health Question #3: How do the decision boundaries shift after incorporating SMOTE to address class imbalance? Answer: It is still evident that wells with detectable levels of iAs were more likely to have lower (&lt;80 ft) casing depths and a more basic pH (&gt; 7). However, we see the greatest shifts in the decision boundaries of SVM and logistic regression with both models now predicting greater regions to detectable iAs levels. Concluding Remarks In conclusion, this training module provided methodologies to aid in the interpretation of supervised ML with variable importance and decision boundary plots. Variable importance helps quantify the impact of each feature’s importance on an algorithm’s predictivity. The most important or environmentally-relevant predictors can be selected in a decision boundary plot to further understand and visualize the features impact on the model’s classification. Additional Resources Christoph Molnar. (2019, August 27). Interpretable Machine Learning. Github.io. https://christophm.github.io/interpretable-ml-book/ Variable Importance Decision Boundary Test Your Knowledge Using the “Module5_2_TYKInput.xlsx”, use RF to determine if well water data can be accurate predictors of manganese detection as was done in the previous module. However, this time, incorporate SMOTE in the model. Feel free to use either the trainControl() or createFolds() function for CV. Extract the variable importance for each predictor on a RF model. What two features have the highest variable importance? Hint: Regardless of the cross validation function you choose, run SMOTE on the training dataset only to create a more balanced training set while the test set will remain unchanged. Using casing depth and the feature with the highest variable importance, construct a decision boundary plot. Under what conditions are a well more likely to predict detectable manganese levels based on a decision boundary plot? "],["unsupervised-machine-learning-part-1-k-means-clustering-pca.html", "5.4 Unsupervised Machine Learning Part 1: K-Means Clustering &amp; PCA Introduction to Training Module Introduction to Unsupervised Machine Learning Introduction to Example Data Identifying Clusters of Chemicals through K-Means Principal Component Analysis (PCA) Incorporating K-Means into PCA for Predictive Modeling Concluding Remarks", " 5.4 Unsupervised Machine Learning Part 1: K-Means Clustering &amp; PCA This training module was developed by David M. Reif with contributions from Alexis Payton, Lauren E. Koval, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module To reiterate what has been discussed in the previous module, machine learning is a field that has great utility in environmental health sciences, often to investigate high-dimensional datasets. The two main classifications of machine learning discussed throughout the TAME Toolkit are supervised and unsupervised machine learning, though additional classifications exist. Previously, we discussed artificial intelligence and supervised machine learning in TAME 2.0 Module 5.1 Introduction to Machine Learning &amp; Artificial Intelligence, TAME 2.0 Module 5.2 Supervised Machine Learning, and TAME 2.0 Module 5.3 Supervised Machine Learning Model Interpretation. In this module, we’ll cover background information on unsupervised machine learning and then work through a scripted example of an unsupervised machine learning analysis. Introduction to Unsupervised Machine Learning Unsupervised machine learning, as opposed to supervised machine learning, involves training a model on a dataset lacking ground truths or response variables. In this regard, unsupervised approaches are often used to identify underlying patterns amongst data in a more unbiased manner. This can provide the analyst with insights into the data that may not otherwise be apparent. Unsupervised machine learning has been used for understanding differences in gene expression patterns of breast cancer patients (Jezequel et. al, 2015) and evaluating metabolomic signatures of patients with and without cystic fibrosis (Laguna et. al, 2015). Note: Unsupervised machine learning is used for exploratory purposes, and just because it can find relationships between data points, that doesn’t necessarily mean that those relationships have merit, are indicative of causal relationships, or have direct biological implications. Rather, these methods can be used to find new patterns that can also inform future studies testing direct relationships. Langs, G., Röhrich, S., Hofmanninger, J., Prayer, F., Pan, J., Herold, C., &amp; Prosch, H. (2018). Machine learning: from radiomics to discovery and routine. Der Radiologe, 58(S1), 1–6. PMID: 34013136. Figure regenerated here in alignment with its published Creative Commons Attribution 4.0 International License Unsupervised machine learning includes: Clustering: Involves grouping elements in a dataset such that the elements in the same group are more similar to each other than to the elements in the other groups. Exclusive (K-means) Overlapping Hierarchical Probabilistic Dimensionality reduction: Focuses on taking high-dimensional data and transforming it into a lower-dimensional space that has fewer features while preserving important information inherent to the original dataset. This is useful because reducing the number of features makes the data easier to visualize while trying to maintain the initial integrity of the dataset. Principal Component Analysis (PCA) Singular Value Decomposition (SVD) t-Distributed Stochastic Neighbor Embedding (t-SNE) Uniform Manifold Approximation and Projection (UMAP) Partial Least Squares-Discriminant Analysis (PLS-DA) In this module, we’ll focus on methods for K-means clustering and Principal Component Analysis described in more detail in the following sections. In the next module, TAME 2.0 Module 5.5 Unsupervised Machine Learning II: Hierarchical Clustering, we’ll focus on hierarchical clustering. For further information on types of unsupervised machine learning, check out Unsupervised Learning. K-Means Clustering K-means is a common clustering algorithm used to partition quantitative data. This algorithm works by first randomly selecting a pre-specified number of clusters, k, across the data space with each cluster having a data centroid. When using a standard Euclidean distance metric, the distance is calculated from an observation to each centroid, then the observation is assigned to the cluster of the closest centroid. After all observations have been assigned to one of the k clusters, the average of all observations in a cluster is calculated, and the centroid for the cluster is moved to the location of the mean. The process then repeats, with the distance computed between the observations and the updated centroids. Observations may be reassigned to the same cluster or moved to a different cluster if it is closer to another centroid. These iterations continue until there are no longer changes between cluster assignments for observations, resulting in the final cluster assignments that are then carried forward for analysis/interpretation. Helpful resources on k-means clustering include the following: The Elements of Statistical Learning &amp; Towards Data Science. Principal Component Analysis (PCA) Principal Component Analysis, or PCA, is a dimensionality-reduction technique used to transform high-dimensional data into a lower dimensional space while trying to preserve as much of the variability in the original data as possible. PCA has strong foundations in linear algebra, so background knowledge of eigenvalues and eigenvectors is extremely useful. Though the mathematics of PCA is beyond the scope of this module, a variety of more in-depth resources on PCA exist including this Towards Data Science Blog, and this Sartorius Blog. At a higher level, important concepts in PCA include: PCA partitions variance in a dataset into linearly uncorrelated principal components (PCs), which are weighted combinations of the original features. Each PC (starting from the first one) summarizes a decreasing percentage of variance. Every instance (e.g. chemical) in the original dataset has a “weight” or score” on each PC. Any combination of PCs can be compared to summarize relationships amongst the instances (e.g. chemicals), but typically it’s the first two eigenvectors that capture a majority of the variance. Introduction to Example Data In this activity, we are going to analyze an example dataset of physicochemical property information for chemicals spanning per- and polyfluoroalkyl substances (PFAS) and statins. PFAS represent a ubiquitous and pervasive class of man-made industrial chemicals that are commonly used in food packaging, commercial household products such as Teflon, cleaning products, and flame retardants. PFAS are recognized as highly stable compounds that, upon entering the environment, can persist for many years and act as harmful sources of exposure. Statins represent a class of lipid-lowering compounds that are commonly used as pharmaceutical treatments for patients at risk of cardiovascular disease. Because of their common use amongst patients, statins can also end up in water and wastewater effluent, making them environmentally relevant as well. This example analysis was designed to evaluate the chemical space of these diverse compounds and to illustrate the utility of unsupervised machine learning methods to differentiate chemical class and make associations between chemical groupings that can inform a variety of environmental and toxicological applications. The two types of machine learning methods that will be employed are k-means and PCA (as described in the introduction). Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Can we differentiate between PFAS and statin chemical classes when considering just the raw physicochemical property variables without applying unsupervised machine learning techniques? If substances are able to be clustered, what are some of the physicochemical properties that seem to be driving chemical clustering patterns derived through k-means? How do the data compare when physicochemical properties are reduced using PCA? Upon reducing the data through PCA, which physicochemical property contributes the most towards informing data variance captured in the primary principal component? If we did not have information telling us which chemical belonged to which class, could we use PCA and k-means to inform whether a chemical is more similar to a PFAS or a statin? What kinds of applications/endpoints can be better understood and/or predicted because of these derived chemical groupings? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;factoextra&quot;)) install.packages(&quot;factoextra&quot;); if (!requireNamespace(&quot;pheatmap&quot;)) install.packages(&quot;pheatmap&quot;); if (!requireNamespace(&quot;cowplot&quot;)) install.packages(&quot;cowplot&quot;); Loading required R packages library(tidyverse) library(factoextra) library(pheatmap) #used to make heatmaps library(cowplot) Getting help with packages and functions ?tidyverse # Package documentation for tidyverse ?kmeans # Package documentation for kmeans (a part of the standard stats R package, automatically uploaded) ?prcomp # Package documentation for deriving principal components within a PCA (a part of the standard stats R package, automatically uploaded) ?pheatmap # Package documentation for pheatmap Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Loading the Example Dataset Let’s start by loading the datasets needed for this training module. We are going to use a dataset of substances that have a diverse chemical space of PFAS and statin compounds. This list of chemicals will be uploaded alongside physicochemical property data. The chemical lists for ‘PFAS’ and ‘Statins’ were obtained from the EPA’s Computational Toxicology Dashboard Chemical Lists. The physicochemical properties were obtained by uploading these lists into the National Toxicology Program’s Integrated Chemical Environment (ICE). dat &lt;- read.csv(&quot;Module5_4_Input/Module5_4_InputData.csv&quot;, fileEncoding = &quot;UTF-8-BOM&quot;) Data Viewing Starting with the overall dimensions: dim(dat) ## [1] 144 14 Then looking at the first four rows and five columns of data: dat[1:4,1:5] ## List Substance.Name CASRN ## 1 PFAS Perfluoro-2-(trifluoromethyl)propanesulphonic acid 93762-09-5 ## 2 PFAS Potassium perfluoroheptanesulfonate 60270-55-5 ## 3 PFAS Bis(2-hydroxyethyl)ammonium perfluoroheptanesulfonate 70225-15-9 ## 4 PFAS Potassium perfluoro-p-ethylcyclohexanesulfonate 335-24-0 ## DTXSID Molecular.Weight ## 1 DTXSID90239569 300.100 ## 2 DTXSID9069392 488.212 ## 3 DTXSID60880946 555.258 ## 4 DTXSID50880117 500.223 Note that the first column, List, designates the following two larger chemical classes: unique(dat$List) ## [1] &quot;PFAS&quot; &quot;Statins&quot; Let’s lastly view all of the column headers: colnames(dat) ## [1] &quot;List&quot; ## [2] &quot;Substance.Name&quot; ## [3] &quot;CASRN&quot; ## [4] &quot;DTXSID&quot; ## [5] &quot;Molecular.Weight&quot; ## [6] &quot;OPERA..Boiling.Point&quot; ## [7] &quot;OPERA..Henry.s.Law.Constant&quot; ## [8] &quot;OPERA..Melting.Point&quot; ## [9] &quot;OPERA..Negative.Log.of.Acid.Dissociation.Constant&quot; ## [10] &quot;OPERA..Octanol.Air.Partition.Coefficient&quot; ## [11] &quot;OPERA..Octanol.Water.Distribution.Coefficient&quot; ## [12] &quot;OPERA..Octanol.Water.Partition.Coefficient&quot; ## [13] &quot;OPERA..Vapor.Pressure&quot; ## [14] &quot;OPERA..Water.Solubility&quot; In the data file, the first four columns represent chemical identifier information. All remaining columns represent different physicochemical properties derived from OPERA via Integrated Chemical Environment (ICE). Because the original titles of these physicochemical properties contained commas and spaces, R automatically converted these into periods. Hence, titles like OPERA..Boiling.Point. For ease of downstream data analyses, let’s create a more focused dataframe option containing only one chemical identifier (CASRN) as row names and then just the physicochemical property columns. # Creating a new dataframe that contains the physiocochemical properties chemical_prop_df &lt;- dat[,5:ncol(dat)] rownames(chemical_prop_df) &lt;- dat$CASRN Now explore this data subset: dim(chemical_prop_df) # overall dimensions ## [1] 144 10 chemical_prop_df[1:4,1:5] # viewing the first four rows and five columns ## Molecular.Weight OPERA..Boiling.Point OPERA..Henry.s.Law.Constant ## 93762-09-5 300.100 213.095 -3.60 ## 60270-55-5 488.212 223.097 -9.75 ## 70225-15-9 555.258 223.097 -9.75 ## 335-24-0 500.223 220.578 -7.56 ## OPERA..Melting.Point ## 93762-09-5 96.455 ## 60270-55-5 273.228 ## 70225-15-9 182.152 ## 335-24-0 231.827 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant ## 93762-09-5 0.175 ## 60270-55-5 -1.810 ## 70225-15-9 1.000 ## 335-24-0 1.000 colnames(chemical_prop_df) ## [1] &quot;Molecular.Weight&quot; ## [2] &quot;OPERA..Boiling.Point&quot; ## [3] &quot;OPERA..Henry.s.Law.Constant&quot; ## [4] &quot;OPERA..Melting.Point&quot; ## [5] &quot;OPERA..Negative.Log.of.Acid.Dissociation.Constant&quot; ## [6] &quot;OPERA..Octanol.Air.Partition.Coefficient&quot; ## [7] &quot;OPERA..Octanol.Water.Distribution.Coefficient&quot; ## [8] &quot;OPERA..Octanol.Water.Partition.Coefficient&quot; ## [9] &quot;OPERA..Vapor.Pressure&quot; ## [10] &quot;OPERA..Water.Solubility&quot; Evaluating the Original Physicochemical Properties across Substances Let’s first plot two physicochemical properties to determine if and how substances group together without any fancy data reduction or other machine learning techniques. This will answer Environmental Health Question #1: Can we differentiate between PFAS and statin chemical classes when considering just the raw physicochemical property variables without applying unsupervised machine learning techniques? Let’s put molecular weight (Molecular.Weight) as one axis and boiling point (OPERA..Boiling.Point) on the other. We’ll also color by the chemical classes using the List column from the original dataframe. ggplot(chemical_prop_df[,1:2], aes(x = Molecular.Weight, y = OPERA..Boiling.Point, color = dat$List)) + geom_point(size = 2) + theme_bw() + ggtitle(&#39;Version A: Bivariate Plot of Two Original Physchem Variables&#39;) + xlab(&quot;Molecular Weight&quot;) + ylab(&quot;Boiling Point&quot;) Let’s plot two other physicochemical property variables, Henry’s Law constant (OPERA..Henry.s.Law.Constant) and melting point (OPERA..Melting.Point), to see if the same separation of chemical classes is apparent. ggplot(chemical_prop_df[,3:4], aes(x = OPERA..Henry.s.Law.Constant, y = OPERA..Melting.Point, color = dat$List)) + geom_point(size = 2) + theme_bw() + ggtitle(&#39;Version B: Bivariate Plot of Two Other Original Physchem Variables&#39;) + xlab(&quot;OPERA..Henry.s.Law.Constant&quot;) + ylab(&quot;OPERA..Melting.Point&quot;) Answer to Environmental Health Question 1 With these, we can answer Environmental Health Question #1: Can we differentiate between PFAS and statin chemical classes when considering just the raw physicochemical property variables without applying machine learning techniques? Answer: Only in part. From the first plot, we can see that PFAS tend to have lower molecular weight ranges in comparison to the statins, though other property variables clearly overlap in ranges of values making the groupings not entirely clear. Identifying Clusters of Chemicals through K-Means Let’s turn our attention to Environmental Health Question #2: If substances are able to be clustered, what are some of the physicochemical properties that seem to be driving chemical clustering patterns derived through k-means? This will be done deriving clusters of chemicals based on ALL underlying physicochemical property data using k-means clustering. For this example, let’s coerce the k-means algorithms to calculate 2 distinct clusters (based on their corresponding mean centered values). Here, we choose to derive two distinct clusters, because we are ultimately going to see if we can use this information to predict each chemical’s classification into two distinct chemical classes (i.e., PFAS vs statins). Note that we can derive more clusters using similar code depending on the question being addressed. We can give a name to this variable to easily provide the number of clusters in the next lines of code, num.centers: num.centers &lt;- 2 Here we derive chemical clusters using k-means: clusters &lt;- kmeans(chemical_prop_df, # input dataframe centers = num.centers, # number of cluster centers to calculate iter.max = 1000, # the maximum number of iterations allowed nstart = 50) # the number of rows used as the random set for the initial centers (during the first iteration) The resulting property values that were derived as the final cluster centers can be pulled using: clusters$centers ## Molecular.Weight OPERA..Boiling.Point OPERA..Henry.s.Law.Constant ## 1 690.1443 233.0402 -9.589444 ## 2 395.0716 281.4445 -8.655185 ## OPERA..Melting.Point OPERA..Negative.Log.of.Acid.Dissociation.Constant ## 1 183.7980 0.01658333 ## 2 157.5036 1.33226852 ## OPERA..Octanol.Air.Partition.Coefficient ## 1 5.940861 ## 2 6.629556 ## OPERA..Octanol.Water.Distribution.Coefficient ## 1 -2.541750 ## 2 -1.271315 ## OPERA..Octanol.Water.Partition.Coefficient OPERA..Vapor.Pressure ## 1 4.000639 -5.538889 ## 2 3.010302 -6.762009 ## OPERA..Water.Solubility ## 1 -3.760222 ## 2 -3.450750 Let’s add the cluster assignments to the physicochemical data and create a new dataframe, which can then be used in a heatmap visualization to see how these physicochemical data distributions clustered according to k-means. These cluster assignments can be pulled from the cluster list output, where chemicals are designated to each cluster with either a 1 or 2. You can view these using: clusters$cluster ## 93762-09-5 60270-55-5 70225-15-9 335-24-0 647-29-0 68259-12-1 ## 2 2 1 2 2 1 ## 68259-09-6 68259-07-4 60453-92-1 357-31-3 441296-91-9 749786-16-1 ## 2 2 2 2 1 1 ## 93762-10-8 135524-36-6 93894-55-4 34642-43-8 2706-91-4 791563-89-8 ## 2 2 1 2 2 1 ## 742-73-4 29420-49-3 3871-99-6 29359-39-5 3872-25-1 126105-34-8 ## 2 2 2 1 2 1 ## 630402-22-1 2274731-07-4 98789-57-2 85963-79-7 375-73-5 108427-53-8 ## 2 1 1 2 2 2 ## 4021-47-0 117806-54-9 67906-42-7 68555-66-8 92982-03-1 375-92-8 ## 2 2 1 2 1 2 ## 175905-36-9 102061-82-5 134615-58-0 174675-49-1 79780-39-5 91036-71-4 ## 2 2 1 1 1 1 ## 70225-17-1 6401-03-2 374-58-3 646-83-3 86525-30-6 3916-24-3 ## 2 2 2 2 1 2 ## 42409-05-2 474511-07-4 2795-39-3 45187-15-3 82382-12-5 79963-95-4 ## 2 1 1 2 2 2 ## 45298-90-6 134615-57-9 927670-12-0 2806-15-7 70225-14-8 131651-65-5 ## 2 2 2 1 1 2 ## 343629-46-9 144797-51-3 29081-56-9 80988-54-1 1379460-39-5 343629-43-6 ## 1 2 2 2 1 1 ## 146689-46-5 29457-72-5 355-46-4 3107-18-4 70259-86-8 1036375-28-6 ## 2 2 2 2 2 2 ## 70225-18-2 70225-16-0 84224-48-6 507453-86-3 40365-28-4 110676-15-8 ## 2 2 2 1 1 2 ## 70259-85-7 2106-55-0 1997344-07-6 423-41-6 115416-68-7 17202-41-4 ## 2 2 2 2 2 1 ## 93894-73-6 134615-56-8 134615-59-1 68259-08-5 68259-10-9 374-62-9 ## 1 2 1 2 2 2 ## 68555-67-9 2806-16-8 36913-91-4 85187-17-3 803688-15-5 55120-77-9 ## 2 1 1 1 2 2 ## 335-77-3 141263-54-9 95465-60-4 130200-44-1 144535-22-8 130468-11-0 ## 1 2 2 2 2 2 ## 93957-54-1 126059-69-6 153463-20-8 154417-69-3 147511-69-1 141263-69-6 ## 2 2 2 2 2 2 ## 77517-29-4 80799-31-1 73390-02-0 503-49-1 117678-63-4 145599-86-6 ## 2 2 2 2 2 2 ## 147098-20-2 85798-96-5 120551-59-9 13552-81-3 90761-31-2 79691-18-2 ## 1 2 2 1 2 2 ## 73573-88-3 114801-27-3 151106-12-6 129443-92-1 134523-03-8 122254-45-9 ## 2 2 2 2 1 2 ## 75330-75-5 137023-81-5 136320-61-1 87770-13-6 85551-06-0 144501-27-9 ## 2 2 2 2 2 2 ## 159014-70-7 153321-50-7 133983-25-2 78366-44-6 148750-02-1 79902-63-9 ## 2 2 2 2 2 2 ## 120185-34-4 120171-12-2 141267-47-2 94061-80-0 141240-46-2 81093-37-0 ## 2 2 2 2 2 2 Because these results are listed in the exact same order as the inputted dataframe, we can simply add these assignments to the chemical_prop_df dataframe. dat_wclusters &lt;- cbind(chemical_prop_df,clusters$cluster) colnames(dat_wclusters)[11] &lt;- &quot;Cluster&quot; # renaming this new column &quot;Custer&quot; dat_wclusters &lt;- dat_wclusters[order(dat_wclusters$Cluster),] # sorting data by cluster assignments To generate a heatmap, we need to first create a separate dataframe for the cluster assignments, ordered in the same way as the physicochemical data: hm_cluster &lt;- data.frame(dat_wclusters$Cluster, row.names = row.names(dat_wclusters)) # creating the dataframe colnames(hm_cluster) &lt;- &quot;Cluster&quot; # reassigning the column name hm_cluster$Cluster &lt;- as.factor(hm_cluster$Cluster) # coercing the cluster numbers into factor variables, to make the heatmap prettier head(hm_cluster) # viewing this new cluster assignment dataframe ## Cluster ## 70225-15-9 1 ## 68259-12-1 1 ## 441296-91-9 1 ## 749786-16-1 1 ## 93894-55-4 1 ## 791563-89-8 1 We’re going to go ahead and clean up the physiocochemical property names to make the heatmap a bit tidier. clean_names1 = gsub(&quot;OPERA..&quot;, &quot;&quot;, colnames(dat_wclusters)) # &quot;\\\\.&quot; denotes a period clean_names2 = gsub(&quot;\\\\.&quot;, &quot; &quot;, clean_names1) # Reassigning the cleaner names back to the df colnames(dat_wclusters) = clean_names2 # Going back to add in the apostrophe in &quot;Henry&#39;s Law Constant&quot; colnames(dat_wclusters)[3] = &quot;Henry&#39;s Law Constant&quot; Then we can call this dataframe (data_wclusters) into the following heatmap visualization code leveraging the pheatmap() function. This function was designed specifically to enable clustered heatmap visualizations. Check out pheatmap Documenation for additional information. Heatmap Visualization of the Resulting K-Means Clusters pheatmap(dat_wclusters[,1:10], cluster_rows = FALSE, cluster_cols = FALSE, # no further clustering, for simplicity scale = &quot;column&quot;, # scaling the data to make differences across chemicals more apparent annotation_row = hm_cluster, # calling the cluster assignment dataframe as a separate color bar annotation_names_row = FALSE, # adding removing the annotation name (&quot;Cluster&quot;) from the x axis angle_col = 45, fontsize_col = 7, fontsize_row = 3, # adjusting size/ orientation of axes labels cellheight = 3, cellwidth = 25, # setting height and width for cells border_color = FALSE # specify no border surrounding the cells ) An appropriate title for this figure could be: “Figure X. Heatmap of physicochemical properties with k-means cluster assignments. Shown are the relative values for each physicochemical property labeled on the x axis. Individual chemical names are listed on the y axis. The chemicals are grouped based on their k-means cluster assignment as denoted by the color bar on the left.” Notice that the pheatmap() function does not add axes or legend titles. Adding those can provide clarity, however those can be added to the figure after exporting from R in MS Powerpoint or Adobe. Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: What are some of the physicochemical properties that seem to be driving chemical clustering patterns derived through k-means? Answer: Properties with values that show obvious differences between resulting clusters including molecular weight, boiling point, negative log of acid dissociation constant, octanol air partition coefficient, and octanol water distribution coefficient. Principal Component Analysis (PCA) Next, we will run through some example analyses applying the common data reduction technique of PCA. We’ll start by determining how much of the variance is able to be captured within the first two principal components to answer Environmental Health Question #3: How do the data compare when physicochemical properties are reduced using PCA? We can calculate the principal components across ALL physicochemical data across all chemicals using the prcomp() function. Always make sure your data is centered and scaled prior to running to PCA, since it’s sensitive to variables having different scales. my.pca &lt;- prcomp(chemical_prop_df, # input dataframe of physchem data scale = TRUE, center = TRUE) We can see how much of the variance was able to be captured in each of the eigenvectors or dimensions using a scree plot. fviz_eig(my.pca, addlabels = TRUE) We can also calculate these values and pull them into a dataframe for future use. For example, to pull the percentage of variance explained by each principal component, we can run the following calculations, where first eigenvalues (eigs) are calculated and then used to calculate percent of variance per principal component: eigs &lt;- my.pca$sdev^2 Comp.stats &lt;- data.frame(eigs, eigs/sum(eigs), row.names = names(eigs)) colnames(Comp.stats) &lt;- c(&quot;Eigen_Values&quot;, &quot;Percent_of_Variance&quot;) head(Comp.stats) ## Eigen_Values Percent_of_Variance ## 1 4.0925925 0.40925925 ## 2 2.3840022 0.23840022 ## 3 1.5415308 0.15415308 ## 4 0.5873057 0.05873057 ## 5 0.4769190 0.04769190 ## 6 0.3659181 0.03659181 Answer to Environmental Health Question 3 With this, we can answer Environmental Health Question #3: How do the data compare when physicochemical properties are reduced using PCA? Answer: Principal Component 1 captures ~41% of the variance and Principal Component 2 captures ~24% across all physicochemical property values across all chemicals. These two components together describe ~65% of data. Next, we’ll use PCA to answer Environmental Health Question #4: Upon reducing the data through PCA, which physicochemical property contributes the most towards informing data variance captured in the primary principal component (Comp.1)? Here are the resulting scores for each chemical’s contribution towards each principal component (shown here as components PC1-PC10). head(my.pca$x) ## PC1 PC2 PC3 PC4 PC5 ## 93762-09-5 -2.0354310 -1.4824239 1.29328367 0.04865893 0.25305471 ## 60270-55-5 -1.2249015 2.2857092 -0.24261251 -0.40140814 0.62781108 ## 70225-15-9 -1.0944361 1.3915069 -0.03340358 -0.90391749 0.05735985 ## 335-24-0 -1.1334897 1.0675553 0.14299978 -1.08713268 0.21172964 ## 647-29-0 -0.4830620 0.1259827 -1.16147937 1.11383217 -0.29571642 ## 68259-12-1 -0.3264762 0.2369031 -1.31984895 0.47512052 -1.17555774 ## PC6 PC7 PC8 PC9 PC10 ## 93762-09-5 -0.67755281 -0.14546495 1.25520979 0.230936852 0.14075496 ## 60270-55-5 -1.01515519 -0.11316504 -0.34335060 -0.383682123 0.09849160 ## 70225-15-9 0.14334979 0.38355763 0.01717491 -0.006702116 -0.02715723 ## 335-24-0 -0.93947357 -0.22877517 0.11457280 -0.107720120 0.17701035 ## 647-29-0 0.19174355 -0.18228398 -0.48013543 0.074967840 0.22750496 ## 68259-12-1 0.05910151 0.01399123 0.03790455 0.043309249 -0.18032083 And the resulting loading factors of each property’s contribution towards each principal component. my.pca$rotation ## PC1 PC2 ## Molecular.Weight 0.09825313 0.108454961 ## OPERA..Boiling.Point 0.46350428 0.029650863 ## OPERA..Henry.s.Law.Constant -0.17856542 -0.502116638 ## OPERA..Melting.Point 0.20645719 0.474473487 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant 0.32172963 -0.119465105 ## OPERA..Octanol.Air.Partition.Coefficient 0.45329804 -0.008918089 ## OPERA..Octanol.Water.Distribution.Coefficient 0.33026147 -0.437053508 ## OPERA..Octanol.Water.Partition.Coefficient 0.16152270 -0.342905461 ## OPERA..Vapor.Pressure -0.35205065 -0.349618523 ## OPERA..Water.Solubility -0.36456684 0.254809066 ## PC3 PC4 ## Molecular.Weight -0.679740448 -0.48432419 ## OPERA..Boiling.Point 0.199365885 0.03108544 ## OPERA..Henry.s.Law.Constant 0.179876660 -0.27695374 ## OPERA..Melting.Point -0.214857945 -0.09449999 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant 0.286239544 -0.58268278 ## OPERA..Octanol.Air.Partition.Coefficient 0.132157694 0.04820475 ## OPERA..Octanol.Water.Distribution.Coefficient -0.005904541 0.15115855 ## OPERA..Octanol.Water.Partition.Coefficient -0.466833374 0.48460961 ## OPERA..Vapor.Pressure -0.195008167 -0.24988717 ## OPERA..Water.Solubility 0.253557563 0.12980079 ## PC5 PC6 ## Molecular.Weight 0.17351578 0.357367954 ## OPERA..Boiling.Point 0.22224554 -0.018507534 ## OPERA..Henry.s.Law.Constant 0.30566003 -0.470666686 ## OPERA..Melting.Point -0.08063905 -0.686723561 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant -0.66998767 0.029248038 ## OPERA..Octanol.Air.Partition.Coefficient 0.20778895 0.205757894 ## OPERA..Octanol.Water.Distribution.Coefficient 0.06378821 0.169005123 ## OPERA..Octanol.Water.Partition.Coefficient -0.48515616 -0.162279640 ## OPERA..Vapor.Pressure -0.06782464 -0.007031883 ## OPERA..Water.Solubility -0.29062543 0.284060941 ## PC7 PC8 ## Molecular.Weight -0.11763362 0.32938640 ## OPERA..Boiling.Point -0.12503355 0.09718690 ## OPERA..Henry.s.Law.Constant -0.21138163 0.44526650 ## OPERA..Melting.Point -0.34342931 -0.10233816 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant 0.09083446 0.03113686 ## OPERA..Octanol.Air.Partition.Coefficient -0.44434707 -0.29734602 ## OPERA..Octanol.Water.Distribution.Coefficient -0.22039378 0.05052653 ## OPERA..Octanol.Water.Partition.Coefficient -0.08082351 0.17794120 ## OPERA..Vapor.Pressure -0.34958097 -0.65367959 ## OPERA..Water.Solubility -0.65183980 0.34989539 ## PC9 PC10 ## Molecular.Weight 0.03295675 -0.02698233 ## OPERA..Boiling.Point 0.03336277 -0.81709497 ## OPERA..Henry.s.Law.Constant 0.19706729 0.10099077 ## OPERA..Melting.Point -0.24532148 0.10229774 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant 0.02576652 0.03380215 ## OPERA..Octanol.Air.Partition.Coefficient 0.49672303 0.39565984 ## OPERA..Octanol.Water.Distribution.Coefficient -0.74417871 0.19866516 ## OPERA..Octanol.Water.Partition.Coefficient 0.30648450 -0.09852386 ## OPERA..Vapor.Pressure -0.03202641 -0.31333091 ## OPERA..Water.Solubility -0.05123040 -0.07696401 Answer to Environmental Health Question 4 With these results, we can answer Environmental Health Question #4: Upon reducing the data through PCA, which physicochemical property contributes the most towards informing data variance captured in the primary principal component (Comp.1)? Answer: Boiling point contributes the most towards principal component 1, as it has the largest magnitude (0.464). Visualizing PCA Results Let’s turn our attention to Environmental Health Question #5: If we did not have information telling us which chemical belonged to which class, could we use PCA and k-means to inform whether a chemical is more similar to a PFAS or a statin? We can start by answering this question by visualizing the first two principal components and coloring each chemical according to class (i.e. PFAS vs statins). ggplot(data.frame(my.pca$x), aes(x = PC1, y = PC2, color = dat$List)) + geom_point(size = 2) + theme_bw() + ggtitle(&#39;Version C: PCA Plot of the First 2 PCs, colored by Chemical Class&#39;) + # it&#39;s good practice to put the percentage of the variance captured in the axes titles xlab(&quot;Principal Component 1 (40.9%)&quot;) + ylab(&quot;Principal Component 2 (23.8%)&quot;) Answer to Environmental Health Question 5 With this, we can answer Environmental Health Question #5: If we did not have information telling us which chemical belonged to which class, could we use PCA and k-means to inform whether a chemical is more similar to a PFAS or a statin? Answer: Data become more compressed and variables reduce across principal components capturing the majority of the variance from the original dataset (~65%). This results in improved data visualizations, where all dimensions of the physiochemical dataset are compressed and captured across the displayed components. In addition, the figure above shows a clear separation between PFAS and statin chemical when visualizing the reduced dataset. Incorporating K-Means into PCA for Predictive Modeling We can also identify cluster-based trends within data that are reduced after running PCA. This example analysis does so, expanding upon the previously generated PCA results. Estimate K-Means Clusters from PCA Results Let’s first run code similar to the previous k-means analysis and associated parameters, though instead here we will use data reduced values from the PCA analysis. Specifically, clusters across PCA “scores” values will be derived, where scores represent the relative amount each chemical contributed to each principal component. clusters_PCA &lt;- kmeans(my.pca$x, centers = num.centers, iter.max = 1000, nstart = 50) The resulting PCA score values that were derived as the final cluster centers can be pulled using: clusters_PCA$centers ## PC1 PC2 PC3 PC4 PC5 PC6 ## 1 2.612553 -0.3054641 0.6219575 -0.008818739 0.18490367 0.05041185 ## 2 -1.187524 0.1388473 -0.2827080 0.004008518 -0.08404712 -0.02291448 ## PC7 PC8 PC9 PC10 ## 1 -0.03417184 -0.05270363 -0.018330999 -0.011904302 ## 2 0.01553265 0.02395620 0.008332272 0.005411046 Viewing the final cluster assignment per chemical: head(cbind(rownames(chemical_prop_df),clusters_PCA$cluster)) ## [,1] [,2] ## 93762-09-5 &quot;93762-09-5&quot; &quot;2&quot; ## 60270-55-5 &quot;60270-55-5&quot; &quot;2&quot; ## 70225-15-9 &quot;70225-15-9&quot; &quot;2&quot; ## 335-24-0 &quot;335-24-0&quot; &quot;2&quot; ## 647-29-0 &quot;647-29-0&quot; &quot;2&quot; ## 68259-12-1 &quot;68259-12-1&quot; &quot;2&quot; Visualizing K-Means Clusters from PCA Results Let’s now view, again, the results of the main PCA focusing on the first two principal components; though this time let’s color each chemical according to k-means cluster. ggplot(data.frame(my.pca$x), aes(x = PC1, y = PC2, color = as.factor(clusters_PCA$cluster))) + geom_point(size = 2) + theme_bw() + ggtitle(&#39;Version D: PCA Plot of the First 2 PCs, colored by k-means Clustering&#39;) + # it&#39;s good practice to put the percentage of the variance capture in the axes titles xlab(&quot;Principal Component 1 (40.9%)&quot;) + ylab(&quot;Principal Component 2 (23.8%)&quot;) Let’s put these two PCA plots side by side to compare them more easily. We’ll also tidy up the figures a bit so they’re closer to publication-ready. # PCA plot colored by chemical class pcaplot1 = ggplot(data.frame(my.pca$x), aes(x = PC1, y = PC2, color = dat$List)) + geom_point(size = 2) + theme_light() + theme(axis.text = element_text(size = 9), # changing size of axis labels axis.title = element_text(face = &quot;bold&quot;, size = rel(1.3)), # changes axis titles legend.title = element_text(face = &#39;bold&#39;, size = 10), # changes legend title legend.text = element_text(size = 9)) + # changes legend text labs(x = &#39;Principal Component 1 (40.9%)&#39;, y = &#39;Principal Component 2 (23.8%)&#39;, color = &quot;Chemical Class&quot;) # changing axis labels # PCA Plot by k means clusters pcaplot2 = ggplot(data.frame(my.pca$x), aes(x = PC1, y = PC2, color = as.factor(clusters_PCA$cluster))) + geom_point(size = 2) + theme_light() + theme(axis.text = element_text(size = 9), # changing size of axis labels axis.title = element_text(face = &quot;bold&quot;, size = rel(1.3)), # changes axis titles legend.text = element_text(size = 9)) + # changes legend text labs(x = &#39;Principal Component 1 (40.9%)&#39;, y = &#39;Principal Component 2 (23.8%)&#39;, color = expression(bold(bolditalic(K)-Means~Cluster))) # changing axis labels # Creating 1 figure plot_grid(pcaplot1, pcaplot2, # Adding labels, changing size their size and position labels = &quot;AUTO&quot;, label_size = 15, label_x = 0.03) An appropriate title for this figure could be: “Figure X. Principal Component Analysis (PCA) plots highlight similarities between chemical class and k-means clusters. These PCA plots are based on physiochemical properties and compare (A) chemical class categories and the (B) K-means derived cluster assignments.” Answer to Environmental Health Question 6 With this we can answer Environmental Health Question #6: What kinds of applications/endpoints can be better understood and/or predicted because of these derived chemical groupings? Answer: With these well-informed chemical groupings, we can now better understand the variables that attribute to the chemical classifications. We can also use this information to better understand data trends and predict environmental fate and transport for these chemicals. The reduced variables derived through PCA, and/or k-means clustering patterns can also be used as input variables to predict toxicological outcomes. Concluding Remarks In conclusion, this training module provides an example exercise on organizing physicochemical data and analyzing trends within these data to determine chemical groupings. Results are compared from those produced using just the original data vs. clustered data from k-means vs. reduced data from PCA. These methods represent common tools that are used in high dimensional data analyses within the field of environmental health sciences. Additional Resources Detailed study of Principal Component Analysis Practical Guide to Cluster Analysis in R Test Your Knowledge In this training module, we presented an unsupervised machine learning example that was based on defining k-means clusters based on chemical class where k = 2. Often times, analyses are conducted to explore potential clustering relationships without a preexisting idea of what k or the number of clusters should be. In this test your knowledge section, we’ll go through an example like that. Using the accompanying flame retardant and pesticide physicochemical property variables found in the file (“Module5_4_TYKInput.csv”), answer the following questions: What are some of the physicochemical properties that seem to be driving chemical clustering patterns derived through k-means? Upon reducing the data through PCA, which physicochemical property contributes the most towards informing data variance captured in the primary principal component? If we did not have information telling us which chemical belonged to which class, could we use PCA and k-means to accurately predict whether a chemical is a PFAS or a statin? "],["unsupervised-machine-learning-part-2-additional-clustering-applications.html", "5.5 Unsupervised Machine Learning Part 2: Additional Clustering Applications Introduction to Training Module K-Means Clustering Hierarchical Clustering Introduction to Example Data Hierarchical Clustering Clustering Plot Hierarchical Clustering Visualization Variable Contributions Concluding Remarks", " 5.5 Unsupervised Machine Learning Part 2: Additional Clustering Applications This training module was developed by Alexis Payton, Lauren E. Koval, David M. Reif, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module The previous module TAME 2.0 Module 5.4 Unsupervised Machine Learning, served as introduction to unsupervised machine learning (ML). Unsupervised ML involves training a model on a dataset lacking ground truths or response variables. However, in the previous module, the number of clusters was selected based on prior information (i.e., chemical class), but what if you’re in a situation where you don’t know how many clusters to investigate a priori? This commonly occurs, particularly in the field of environmental health research in instances when investigators want to take a more unbiased view of their data and/or do not have information that can be used to inform the optimal number of clusters to select. In these instances,unsupervised ML techniques can be very helpful, and in this module, we’ll explore the following concepts to further understand unsupervised ML: K-Means and hierarchical clustering Deriving the optimal number of clusters Visualizing clusters through a PCA-based plot, dendrograms, and heatmaps Determining each variable’s contribution to the clusters K-Means Clustering As mentioned in the previous module, K-means is a common clustering algorithm used to partition quantitative data. This algorithm works by first, randomly selecting a pre-specified number of clusters, k, across the data space, with each cluster having a data centroid. When using a standard Euclidean distance metric, the distance is calculated from an observation to each centroid, then the observation is assigned to the cluster of the closest centroid. After all observations have been assigned to one of the k clusters, the average of all observations in a cluster is calculated, and the centroid for the cluster is moved to the location of the mean. The process then repeats, with the distance computed between the observations and the updated centroids. Observations may be reassigned to the same cluster or moved to a different cluster if it is closer to another centroid. These iterations continue until there are no longer changes between cluster assignments for observations, resulting in the final cluster assignments that are then carried forward for analysis/interpretation. Helpful resources on k-means clustering include the following: The Elements of Statistical Learning &amp; Towards Data Science. Hierarchical Clustering Hierarchical clustering groups objects into clusters by repetitively joining similar observations until there is one large cluster (aka agglomerative or bottom-up) or repetitively splitting one large cluster until each observation stands alone (aka divisive or top-down). Regardless of whether agglomerative or divisive hierarchical clustering is used, the results can be visually represented in a tree-like figure called a dendrogram. The dendrogram below is based on the USArrests dataset available in R. The datset contains statistics on violent crimes rates (murder, assault, and rape) per capita (per 100,000 residents) for each state in the United States in 1973. For more information on the USArrests dataset, check out its associated RDocumentation. An appropriate title for this figure could be: “Figure X. Hierarchical clustering of states based on violent crime. The dendogram uses violent crime data including murder, assault, and rape rates per 100,000 residents for each state in 1973.” Takeaways from this dendogram: The 50 states can be grouped into 4 clusters based on violent crime statistics from 1973 The dendogram can only show us clusters of states but not the data trends that led to the clustering patterns that we see. Yes, it is useful to know what states have similar violent crime patterns overall, but it is important to pinpoint the variables (ie. murder, assault, and rape) that are responsible for the clustering patterns we’re seeing. This idea will be explored later in the module with an environmentally-relevant dataset. Going back to hiearchical clustering, during the repetitive splitting or joining of observations, the similarity between existing clusters is calculated after each iteration. This value informs the formation of subsequent clusters. Different methods, or linkage functions, can be considered when calculating this similarity, particularly for agglomerative clustering which is often the preferred approach. Some example methods include: Complete Linkage: the maximum distance between two data points located in separate clusters. Single Linkage: the minimum distance between two data points located in separate clusters. Average Linkage: the average pairwise distance between all pairs of data points in separate clusters. Centroid Linkage: the distance between the centroids or centers of each cluster. Ward Linkage: seeks to minimize the variance between clusters. Each method has its advantages and disadvantages and more information on all distance calculations between clusters can be found at the following resource: Hierarchical Clustering. Deriving the Optimal Number of Clusters Before clustering can be performed, the function needs to be informed of the number of clusters to group the objects into. In the previous module, an example was explored to see if k-means clustering would group the chemicals similarly to their chemical class (either a PFAS or statin). Therefore, we told the k-means function to cluster into 2 groups. In situations where there is little to no prior knowledge regarding the “correct” number of clusters to specify, methods exist for deriving the optimal number of clusters. Three common methods to find the optimal k, or number of clusters, for both k-means and hierarchical clustering include: the elbow method, silhouette method, and the gap statistic method. These techniques help us in determining the optimal k using visual inspection. Elbow Method: uses a plot of the within cluster sum of squares (WCSS) on the y axis and different values of k on the x axis. The location where we no longer observe a significant reduction in WCSS, or where an “elbow” can be seen, is the optimal k value. As we can see, after a certain point, having more clusters does not lead to a significant reduction in WCSS. Looking at the figures above, the elbow point is much clearer in the first plot versus the second, however, elbow curves from real-world datasets typically resemble the second figure. This is why it’s recommended to consider more than one method to determine the optimal number of clusters. Silhouette Method: uses a plot of the average silhouette width (score) on the y axis and different values of k on the x axis. The silhouette score is measure of each object’s similarity to its own cluster and how dissimilar it is to other clusters. The location where the average silhouette width is maximized is the optimal k value. Based on the figure above, the optimal number of clusters is 2 using the silhouette method. Gap Statistic Method: uses a plot of the gap statistic on the y axis and different values of k on the x axis. The gap statistic evaluates the intracluster variation in comparison to expected values derived from a Monte Carlo generated, null reference data distribution for varying values of k. The optimal number of clusters is the smallest value where the gap statistic of k is greater than or equal to the gap statistic of k+1 minus the standard deviation of k+1. More details can be found here. Based on the figure above, the optimal number of clusters is 2 using the gap statistic method. For additional information and code on all three methods, check out Determining the Optimal Number of Clusters: 3 Must Know Methods. It is also worth mentioning that while these methods are useful, further interpreting the result in the context of your problem can be beneficial, for example, checking whether clusters make biological sense when working with a genomic dataset. Introduction to Example Data We will apply these techniques using an example dataset from a previously published study where 22 cytokine concentrations were derived from 44 subjects with varying smoking statuses (14 non-smokers, 17 e-cigarette users, and 13 cigarette smokers) from 4 different sampling regions in the body. These samples were derived from nasal lavage fluid (NLF), nasal epithelieum fluid (NELF), sputum, and serum as pictured below. Samples were taken from different regions in the body to compare cytokine expression in the upper respiratory tract, lower respiratory tract, and systemic circulation. A research question that we had was “Does cytokine expression change based on a subject’s smoking habits? If so, does cigarette smoke or e-cigarette vapor induce cytokine suppression or proliferation?” Traditionally these questions would have been answered by analyzing each biomarker individually using a two-group comparison test like a t test (which we completed in this study). However, biomarkers do not work in isolation in the body, suggesting that individual biomarker statistical approaches may not capture the full biological responses occurring. Therefore we used a clustering approach to group cytokines as an attempt to more closely simulate interactions that occur in vivo. From there, statistical tests were run to assess the effects of smoking status on each cluster. For the purposes of this training exercise, we will focus solely on the nasal epithelieum lining fluid, or NELF, samples. In addition, we’ll use k-means and hierarchical clustering to compare how cytokines cluster at baseline. Full methods are further described in the publication below: Payton AD, Perryman AN, Hoffman JR, Avula V, Wells H, Robinette C, Alexis NE, Jaspers I, Rager JE, Rebuli ME. Cytokine signature clusters as a tool to compare changes associated with tobacco product use in upper and lower airway samples. American Journal of Physiology-Lung Cellular and Molecular Physiology 2022 322:5, L722-L736. PMID: 35318855 Let’s read in and view the dataset we’ll be working with. Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;vegan&quot;)) install.packages(&quot;vegan&quot;); if (!requireNamespace(&quot;ggrepel&quot;)) install.packages(&quot;ggrepel&quot;); if (!requireNamespace(&quot;dendextend&quot;)) install.packages(&quot;dendextend&quot;); if (!requireNamespace(&quot;ggsci&quot;)) install.packages(&quot;ggsci&quot;); if (!requireNamespace(&quot;FactoMineR&quot;)) install.packages(&quot;FactoMineR&quot;); Loading required R packages library(readxl) library(factoextra) library(FactoMineR) library(tidyverse) library(vegan) library(ggrepel) library(reshape2) library(pheatmap) library(ggsci) suppressPackageStartupMessages(library(dendextend)) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example dataset Then let’s read in our example dataset. As mentioned in the introduction, this example dataset contains cytokine concentrations derived from 44 subjects. Let’s import and view these data: # Reading in file cytokines_df &lt;- data.frame(read_excel(&quot;Module5_5_Input/Module5_5_InputData.xlsx&quot;, sheet = 2)) # Viewing data head(cytokines_df) ## Original_Identifier Group SubjectNo SubjectID Compartment Protein Conc ## 1 E_C_F_002 NS 1 NS_1 NELF IFNg 17.642316 ## 2 E_C_F_002 NS 1 NS_1 NELF IL10 2.873724 ## 3 E_C_F_002 NS 1 NS_1 NELF IL12p70 1.625272 ## 4 E_C_F_002 NS 1 NS_1 NELF IL13 36.117692 ## 5 E_C_F_002 NS 1 NS_1 NELF IL1b 104.409217 ## 6 E_C_F_002 NS 1 NS_1 NELF IL6 21.159536 ## Conc_pslog2 ## 1 4.220509 ## 2 1.953721 ## 3 1.392467 ## 4 5.214035 ## 5 6.719857 ## 6 4.469856 These data contain the following information: Original_Identifier: initial identifier given to each subject by our wet bench colleagues Group: denotes the smoking status of the subject (“NS” = “non-smoker”, “Ecig” = “E-cigarette user”, “CS” = “cigarette smoker”) SubjectNo: ordinal subject number assigned to each subject after the dataset was wrangled (1-44) SubjectID: unique subject identifier that combines the group and subject number Compartment: region of the body from which the sample was taken (“NLF” = “nasal lavage fluid sample”, “NELF” = “nasal epithelieum lining fluid sample”, “Sputum” = “induced sputum sample”, “Serum” = “blood serum sample”) Protein: cytokine name Conc: concentration (pg/mL) Conc_pslog2: psuedo-log2 concentration Now that the data has been read in, we can start by asking some initial questions about the data. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: What is the optimal number of clusters the cytokines can be grouped into that were derived from nasal epithelium fluid (NELF) in non-smokers using k-means clustering? After selecting a cluster number, which cytokines were assigned to each k-means cluster? What is the optimal number of clusters the cytokines can be grouped into that were derived from nasal epithelium fluid (NELF) in non-smokers using hierarchical clustering? How do the hierarchical cluster assignments compare to the k-means cluster assignments? Which cytokines have the greatest contributions to the first two eigenvectors? To answer the first environmental health question, let’s start by filtering to include only NELF derived samples and non-smokers. baseline_df &lt;- cytokines_df %&gt;% filter(Group == &quot;NS&quot;, Compartment == &quot;NELF&quot;) head(baseline_df) ## Original_Identifier Group SubjectNo SubjectID Compartment Protein Conc ## 1 E_C_F_002 NS 1 NS_1 NELF IFNg 17.642316 ## 2 E_C_F_002 NS 1 NS_1 NELF IL10 2.873724 ## 3 E_C_F_002 NS 1 NS_1 NELF IL12p70 1.625272 ## 4 E_C_F_002 NS 1 NS_1 NELF IL13 36.117692 ## 5 E_C_F_002 NS 1 NS_1 NELF IL1b 104.409217 ## 6 E_C_F_002 NS 1 NS_1 NELF IL6 21.159536 ## Conc_pslog2 ## 1 4.220509 ## 2 1.953721 ## 3 1.392467 ## 4 5.214035 ## 5 6.719857 ## 6 4.469856 The functions we use will require us to cast the data wider. We will accomplish this using the dcast() function from the reshape2 package. wider_baseline_df &lt;- reshape2::dcast(baseline_df, Protein ~ SubjectID, value.var = &quot;Conc_pslog2&quot;) %&gt;% column_to_rownames(&quot;Protein&quot;) head(wider_baseline_df) ## NS_1 NS_10 NS_11 NS_12 NS_13 NS_14 ## Eotaxin 7.215729 8.016193 6.264252 6.938344 5.152683 7.937892 ## Eotaxin3 4.396783 4.006488 5.289509 3.597945 3.051951 3.063025 ## Fractalkine 13.584554 13.679882 12.952220 13.464652 12.110742 12.785656 ## I309 0.000000 0.000000 0.000000 0.000000 0.000000 1.617372 ## IFNg 4.220509 1.990018 2.656691 3.439682 2.318340 3.439850 ## IL10 1.953721 1.145162 1.348623 1.801954 1.176498 1.800412 ## NS_2 NS_3 NS_4 NS_5 NS_6 NS_7 ## Eotaxin 9.773407 6.12290913 5.758602 4.19868540 6.832578 7.998467 ## Eotaxin3 9.022390 2.50161364 2.721195 2.21084320 3.820379 5.549164 ## Fractalkine 14.961872 12.71538464 12.057105 10.38711624 12.917180 14.636190 ## I309 1.103140 0.00000000 0.000000 0.00000000 0.000000 0.000000 ## IFNg 4.394705 0.00000000 2.638213 0.04830196 1.914092 3.251501 ## IL10 2.652120 0.08945805 2.020155 0.84649523 1.119272 2.007941 ## NS_8 NS_9 ## Eotaxin 9.242464 7.127746 ## Eotaxin3 13.724108 3.553097 ## Fractalkine 14.047382 13.146599 ## I309 0.000000 0.000000 ## IFNg 4.104820 2.940248 ## IL10 2.576814 1.970627 Now we can derive clusters using the fviz_nbclust() function to determine the optimal k based on suggestions from the elbow, silhouette, and gap statistic methods. We can use this code for both k-means and hierarchical clustering by changing the FUNcluster parameter. Lets start with k-means:. # Elbow method fviz_nbclust(wider_baseline_df, FUNcluster = kmeans, method = &quot;wss&quot;) + labs(subtitle = &quot;Elbow method&quot;) # Silhouette method fviz_nbclust(wider_baseline_df, FUNcluster = kmeans, method = &quot;silhouette&quot;) + labs(subtitle = &quot;Silhouette method&quot;) # Gap statistic method fviz_nbclust(wider_baseline_df, FUNcluster = kmeans, method = &quot;gap_stat&quot;) + labs(subtitle = &quot;Gap Statisitc method&quot;) The elbow method is suggesting 2 or 3 clusters, the silhouette method is suggesting 2, and the gap statistic method is suggesting 1. Since each of these methods is recommending different k values, we can go ahead and run k-means to visualize the clusters and test those different k’s. K-means clusters will be visualized using the fviz_cluster() function. # Choosing to iterate through 2 or 3 clusters using i as our iterator for (i in 2:3){ # nstart = number of random starting partitions, it&#39;s recommended for nstart &gt; 1 cluster_k &lt;- kmeans(wider_baseline_df, centers = i, nstart = 25) cluster_plot &lt;- fviz_cluster(cluster_k, data = wider_baseline_df) + ggtitle(paste0(&quot;k = &quot;, i)) print(cluster_plot) } Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question #1: What is the optimal number of clusters the cytokines can be grouped into that were derived from nasal epithelium fluid (NELF) in non-smokers using k-means clustering? Answer: 2 or 3 clusters can be justified here, based on using the elbow or silhouette method or if k-means happens to group cytokines together that were implicated in similar biological pathways. In the final paper, we moved forward with 3 clusters, because it was justifiable from the methods and provided more granularity in the clusters. The final cluster assignments can easily be obtained using the kmeans() function from the stats package. cluster_kmeans_3 &lt;- kmeans(wider_baseline_df, centers = 3, nstart = 25) cluster_kmeans_df &lt;- data.frame(cluster_kmeans_3$cluster) %&gt;% rownames_to_column(&quot;Cytokine&quot;) %&gt;% rename(`K-Means Cluster` = cluster_kmeans_3.cluster) %&gt;% # Ordering the dataframe for easier comparison arrange(`K-Means Cluster`) cluster_kmeans_df ## Cytokine K-Means Cluster ## 1 Fractalkine 1 ## 2 IL8 1 ## 3 IP10 1 ## 4 MIG 1 ## 5 Eotaxin 2 ## 6 Eotaxin3 2 ## 7 IL13 2 ## 8 IL17B 2 ## 9 IL1b 2 ## 10 IL6 2 ## 11 ITAC 2 ## 12 MCP1 2 ## 13 MIP1a 2 ## 14 MIP1b 2 ## 15 TARC 2 ## 16 I309 3 ## 17 IFNg 3 ## 18 IL10 3 ## 19 IL12p70 3 ## 20 IL17D 3 ## 21 IL4 3 ## 22 TNFa 3 Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: After selecting a cluster number, which cytokines were assigned to each k-means cluster? Answer: After choosing the number of clusters to be 3, the cluster assignments are as follows: Hierarchical Clustering Next, we’ll turn our attention to answering environmental health questions 3 and 4: What is the optimal number of clusters the cytokines can be grouped into that were derived from nasal epithelium fluid (NELF) in non-smokers using hierarchical clustering? How do the hierarchical cluster assignments compare to the k-means cluster assignments? Just as we used the elbow method, silhouette profile, and gap statistic to determine the optimal number of clusters for k-means, we can leverage the same approaches for hierarchical by changing the FUNcluster parameter. # Elbow method fviz_nbclust(wider_baseline_df, FUNcluster = hcut, method = &quot;wss&quot;) + labs(subtitle = &quot;Elbow method&quot;) # Silhouette method fviz_nbclust(wider_baseline_df, FUNcluster = hcut, method = &quot;silhouette&quot;) + labs(subtitle = &quot;Silhouette method&quot;) # Gap statistic method fviz_nbclust(wider_baseline_df, FUNcluster = hcut, method = &quot;gap_stat&quot;) + labs(subtitle = &quot;Gap Statisitc method&quot;) We can see the results are quite similar with 2-3 clusters appearing optimal. Answer to Environmental Health Question 3 With this, we can answer Environmental Health Question #3: What is the optimal number of clusters the cytokines can be grouped into that were derived from nasal epithelium fluid (NELF) in non-smokers using hierarchical clustering? Answer: Again, 2 or 3 clusters can be justified here, but for the same reasons mentioned for the first environmental health question, we landed on 3 clusters. Now we can perform the clustering and visualize and extract the results. We’ll start by using the dist() function to calculate the euclidean distance between the clusters followed by the hclust() function to obtain the hierarchical clustering assignments. # Viewing the wider dataframe we&#39;ll be working with head(wider_baseline_df) ## NS_1 NS_10 NS_11 NS_12 NS_13 NS_14 ## Eotaxin 7.215729 8.016193 6.264252 6.938344 5.152683 7.937892 ## Eotaxin3 4.396783 4.006488 5.289509 3.597945 3.051951 3.063025 ## Fractalkine 13.584554 13.679882 12.952220 13.464652 12.110742 12.785656 ## I309 0.000000 0.000000 0.000000 0.000000 0.000000 1.617372 ## IFNg 4.220509 1.990018 2.656691 3.439682 2.318340 3.439850 ## IL10 1.953721 1.145162 1.348623 1.801954 1.176498 1.800412 ## NS_2 NS_3 NS_4 NS_5 NS_6 NS_7 ## Eotaxin 9.773407 6.12290913 5.758602 4.19868540 6.832578 7.998467 ## Eotaxin3 9.022390 2.50161364 2.721195 2.21084320 3.820379 5.549164 ## Fractalkine 14.961872 12.71538464 12.057105 10.38711624 12.917180 14.636190 ## I309 1.103140 0.00000000 0.000000 0.00000000 0.000000 0.000000 ## IFNg 4.394705 0.00000000 2.638213 0.04830196 1.914092 3.251501 ## IL10 2.652120 0.08945805 2.020155 0.84649523 1.119272 2.007941 ## NS_8 NS_9 ## Eotaxin 9.242464 7.127746 ## Eotaxin3 13.724108 3.553097 ## Fractalkine 14.047382 13.146599 ## I309 0.000000 0.000000 ## IFNg 4.104820 2.940248 ## IL10 2.576814 1.970627 # First scaling data with each subject (down columns) scaled_df &lt;- data.frame(apply(wider_baseline_df, 2, scale)) rownames(scaled_df) = rownames(wider_baseline_df) head(scaled_df) ## NS_1 NS_10 NS_11 NS_12 NS_13 NS_14 ## Eotaxin 0.1563938 0.5913460 0.20280481 0.3812918 0.04218372 0.3889166 ## Eotaxin3 -0.4836295 -0.3751558 -0.03031899 -0.4135856 -0.51644074 -0.7745030 ## Fractalkine 1.6023933 1.9565250 1.80232784 1.9342844 1.89246390 1.5458678 ## I309 -1.4818900 -1.3408820 -1.29538062 -1.2697485 -1.32801259 -1.1195175 ## IFNg -0.5236512 -0.8612069 -0.65999509 -0.4512458 -0.71152186 -0.6845711 ## IL10 -1.0383105 -1.0648515 -0.97283813 -0.8409577 -1.01515934 -1.0758338 ## NS_2 NS_3 NS_4 NS_5 NS_6 NS_7 ## Eotaxin 0.5142129 0.5748344 0.3352251 0.1297382 0.6605088 0.4509596 ## Eotaxin3 0.3434700 -0.3390521 -0.4516462 -0.3710044 -0.1481961 -0.1213017 ## Fractalkine 1.6938060 2.2385415 1.9669167 1.6886200 2.2940816 2.0018134 ## I309 -1.4569648 -0.9703707 -1.1565995 -0.9279215 -1.1738781 -1.4178220 ## IFNg -0.7086302 -0.9703707 -0.4731435 -0.9157541 -0.6599894 -0.6581332 ## IL10 -1.1048056 -0.9477947 -0.6332576 -0.7146872 -0.8733797 -0.9486817 ## NS_8 NS_9 ## Eotaxin 0.5504372 0.6258899 ## Eotaxin3 1.4999690 -0.3063521 ## Fractalkine 1.5684615 2.1955622 ## I309 -1.4077759 -1.2329735 ## IFNg -0.5380823 -0.4661787 ## IL10 -0.8618228 -0.7190487 The dist() function is initially used to calculate the Euclidean distance between each cytokine. Next, the hclust() function is used to run the hierarchical clustering analysis using the complete method by default. The method can be changed in the function using the method parameter. # Calculating euclidean dist dist_matrix &lt;- dist(scaled_df, method = &#39;euclidean&#39;) # Hierarchical clustering cytokines_hc &lt;- hclust(dist_matrix) Now we can generate a dendrogram to help us evaluate the results using the fviz_dend() function from the factoextra package. We use k=3 to be consistent with the k-means analysis. fviz_dend(cytokines_hc, k = 3, # Specifying k cex = 0.85, # Label size palette = &quot;futurama&quot;, # Color palette see ?ggpubr::ggpar rect = TRUE, rect_fill = TRUE, # Add rectangle around groups horiz = TRUE, # Changes the orientation of the dendogram rect_border = &quot;futurama&quot;, # Rectangle color labels_track_height = 0.8 # Changes the room for labels ) We can also extract those cluster assignments using the cutree() function from the stats package. hc_assignments_df &lt;- data.frame(cutree(cytokines_hc, k = 3)) %&gt;% rownames_to_column(&quot;Cytokine&quot;) %&gt;% rename(`Hierarchical Cluster` = cutree.cytokines_hc..k...3.) %&gt;% # Ordering the dataframe for easier comparison arrange(`Hierarchical Cluster`) # Combining the dataframes to compare the cluster assignments from each approach comp &lt;- full_join(cluster_kmeans_df, hc_assignments_df, by = &quot;Cytokine&quot;) comp ## Cytokine K-Means Cluster Hierarchical Cluster ## 1 Fractalkine 1 2 ## 2 IL8 1 2 ## 3 IP10 1 2 ## 4 MIG 1 2 ## 5 Eotaxin 2 1 ## 6 Eotaxin3 2 1 ## 7 IL13 2 1 ## 8 IL17B 2 1 ## 9 IL1b 2 1 ## 10 IL6 2 1 ## 11 ITAC 2 1 ## 12 MCP1 2 1 ## 13 MIP1a 2 1 ## 14 MIP1b 2 1 ## 15 TARC 2 1 ## 16 I309 3 3 ## 17 IFNg 3 3 ## 18 IL10 3 3 ## 19 IL12p70 3 3 ## 20 IL17D 3 3 ## 21 IL4 3 3 ## 22 TNFa 3 3 For additional resources on running hierarchical clustering in R, see Visualizing Clustering Dendrogram in R and Hiearchical Clustering on Principal Components. Answer to Environmental Health Question 4 With this, we can answer Environmental Health Question #4: How do the hierarchical cluster assignments compare to the k-means cluster assignments? Answer: Though this may not always be the case, in this instance, we see that k-means and hierarchical clustering with k=3 clusters yield the same groupings despite the clusters being presented in a different order. Clustering Plot One additional way to visualize clustering is to plot the first two principal components on the axes and color the data points based on their corresponding cluster. This visualization can be used for both k-means and hierarchical clustering using the fviz_cluster() function. This figure is essentially a PCA plot with shapes drawn around each cluster to make them distinct from each other. fviz_cluster(cluster_kmeans_3, data = wider_baseline_df) Rather than using the fviz_cluster() function as shown in the figure above, we’ll extract the data to recreate the sample figure using ggplot(). For the manuscript this was necessary, since it was important to facet the plots for each compartment (i.e., NLF, NELF, sputum, and serum). For a single plot, this data extraction isn’t required, and the figure above can be further customized within the fviz_cluster() function. However, we’ll go through the steps of obtaining the indices to recreate the same polygons in ggplot() directly. K-means actually uses principal component analysis (PCA) to reduce a dataset’s dimensionality prior to obtaining the cluster assignments and plotting those clusters. Therefore, to obtain the coordinates of each cytokine within their respective clusters, PCA will need to be run first. # First running PCA pca_cytokine &lt;- prcomp(wider_baseline_df, scale = TRUE, center = TRUE) # Only need PC1 and PC2 for plotting, so selecting the first two columns baseline_scores_df &lt;- data.frame(scores(pca_cytokine)[,1:2]) baseline_scores_df$Cluster &lt;- cluster_kmeans_3$cluster baseline_scores_df$Protein &lt;- rownames(baseline_scores_df) # Changing cluster to a character for plotting baseline_scores_df$Cluster = as.character(baseline_scores_df$Cluster) head(baseline_scores_df) ## PC1 PC2 Cluster Protein ## Eotaxin -1.4837936 -0.37325290 2 Eotaxin ## Eotaxin3 0.7505718 -1.04251406 2 Eotaxin3 ## Fractalkine -7.0500961 -0.49034208 1 Fractalkine ## I309 4.6869896 0.42263944 3 I309 ## IFNg 2.4935464 -0.21910385 3 IFNg ## IL10 3.4361668 -0.04689913 3 IL10 Within each cluster, the chull() function is used to compute the indices of the points on the convex hull. These are needed for ggplot() to create the polygon shapes of each cluster. # hull values for cluster 1 cluster_1 &lt;- baseline_scores_df[baseline_scores_df$Cluster == 1, ][chull(baseline_scores_df %&gt;% filter(Cluster == 1)),] # hull values for cluster 2 cluster_2 &lt;- baseline_scores_df[baseline_scores_df$Cluster == 2, ][chull(baseline_scores_df %&gt;% filter(Cluster == 2)),] # hull values for cluster 3 cluster_3 &lt;- baseline_scores_df[baseline_scores_df$Cluster == 3, ][chull(baseline_scores_df %&gt;% filter(Cluster == 3)),] all_hulls_baseline &lt;- rbind(cluster_1, cluster_2, cluster_3) # Changing cluster to a character for plotting all_hulls_baseline$Cluster = as.character(all_hulls_baseline$Cluster) head(all_hulls_baseline) ## PC1 PC2 Cluster Protein ## MIG -4.7353654 -0.2498662 1 MIG ## IL8 -7.7200078 -0.8317172 1 IL8 ## IP10 -3.8660925 4.2481655 1 IP10 ## Eotaxin3 0.7505718 -1.0425141 2 Eotaxin3 ## MIP1b -0.8699488 -0.8834532 2 MIP1b ## MCP1 -2.4837364 -0.3652235 2 MCP1 Now plotting the clusters using ggplot(). ggplot() + geom_point(data = baseline_scores_df, aes(x = PC1, y = PC2, color = Cluster, shape = Cluster), size = 4) + # Adding cytokine names geom_text_repel(data = baseline_scores_df, aes(x = PC1, y = PC2, color = Cluster, label = Protein), show.legend = FALSE, size = 4.5) + # Creating polygon shapes of the clusters geom_polygon(data = all_hulls_baseline, aes(x = PC1, y = PC2, group = as.factor(Cluster), fill = Cluster, color = Cluster), alpha = 0.25, show.legend = FALSE) + theme_light() + theme(axis.text.x = element_text(vjust = 0.5), #rotating x labels/ moving x labels slightly to the left axis.line = element_line(colour=&quot;black&quot;), #making x and y axes black axis.text = element_text(size = 13), #changing size of x axis labels axis.title = element_text(face = &quot;bold&quot;, size = rel(1.7)), #changes axis titles legend.title = element_text(face = &#39;bold&#39;, size = 17), #changes legend title legend.text = element_text(size = 14), #changes legend text legend.position = &#39;bottom&#39;, # moving the legend to the bottom legend.background = element_rect(colour = &#39;black&#39;, fill = &#39;white&#39;, linetype = &#39;solid&#39;), #changes the legend background strip.text.x = element_text(size = 18, face = &quot;bold&quot;), #changes size of facet x axis strip.text.y = element_text(size = 18, face = &quot;bold&quot;)) + #changes size of facet y axis xlab(&#39;Dimension 1 (85.1%)&#39;) + ylab(&#39;Dimension 2 (7.7%)&#39;) + #changing axis labels # Using colors from the startrek palette from ggsci scale_color_startrek(name = &#39;Cluster&#39;) + scale_fill_startrek(name = &#39;Cluster&#39;) An appropriate title for this figure could be: “Figure X. K-means clusters of cytokines at baseline. Cytokines samples are derived from nasal epithelium (NELF) samples in 14 non-smoking subjects. Cytokine concentration values were transformed using a data reduction technique known as Principal Component Analysis (PCA). The first two eigenvectors plotted on the axes were able to capture a majority of the variance across all samples from the original dataset.” Takeaways from this clustering plot: PCA was able to capture almost all (~93%) of the variance from the original dataset The 22 cytokines were able to be clustered into 3 distinct clusters using k-means Hierarchical Clustering Visualization We can also build a heatmap using the pheatmap() function that has the capability to display hierarchical clustering dendrograms. To do so, we’ll need to go back and use the wider_baseline_df dataframe. pheatmap(wider_baseline_df, cluster_cols = FALSE, # hierarchical clustering of cytokines scale = &quot;column&quot;, # scaling the data to make differences across cytokines more apparent cutree_row = 3, # adds a space between the 3 largest clusters display_numbers = TRUE, number_color = &quot;black&quot;, fontsize = 12, # adding average concentration values angle_col = 45, fontsize_col = 12, fontsize_row = 12, # adjusting size/ orientation of axes labels cellheight = 17, cellwidth = 30 # setting height and width for cells ) An appropriate title for this figure could be: “Figure X. Hierarchical clustering of cytokines at baseline. Cytokines samples are derived from nasal epithelium (NELF) samples in 14 non-smoking subjects. The heatmap visualizes psuedo log2 cytokine concentrations that were scaled within each subject.” It may be helpful to add axes titles like “Subject ID” for the x axis, “Cytokine” for the y axis, and “Scaled pslog2 Concentration” for the legend after exporting from R. The pheatmap() function does not have the functionality to add those titles. Nevertheless, let’s identify some key takeaways from this heatmap: The 22 cytokines were able to be clustered into 3 distinct clusters using hierarchical clustering These clusters are based on cytokine concentration levels with the first cluster having the highest expression, the second cluster having the lowest expression, and the last cluster having average expression Variable Contributions To answer our final environmental health question: Which cytokines have the greatest contributions to the first two eigenvectors, we’ll use the fviz_contrib() function that plots the percentage of each variable’s contribution to the principal component(s). It also displays a red dashed line, and variables that fall above are considered to have significant contributions to those principal components. For a refresher on PCA and variable contributions, see the previous module, TAME 2.0 Module 5.4 Unsupervised Machine Learning. # kmeans contributions fviz_contrib(pca_cytokine, choice = &quot;ind&quot;, addlabels = TRUE, axes = 1:2) # specifies to show contribution percentages for first 2 PCs An appropriate title for this figure could be: “Figure X. Cytokine contributions to principal components. The bar chart displays each cytokine’s contribution to the first two eigenvectors in descending order from left to right. The red dashed line represents the expected contribution of each cytokine if all inputs were uniform, therefore the seven cytokines that fall above this reference line are considered to have significant contributions to the first two principal components.” Answer to Environmental Health Question 5 With this, we can answer Environmental Health Question #5: Which cytokines have the greatest contributions to the first two eigenvectors? Answer: The cytokines that have significant contributions to the first two principal components include IL-8, Fractalkine, IP-10, IL-4, MIG, I309, and IL-12p70. Concluding Remarks In this module, we explored scenarios where clustering would be appropriate but lack contextual details informing the number of clusters that should be considered, thus resulting in the need to derive such a number. In addition, methodology for k-means and hierarchical clustering was presented, along with corresponding visualizations. Lastly, variable contributions to the eigenvectors were introduced as a means to determine the most influential variables on the principal components’ composition. Additional Resources K-Means Cluster Analysis K-Means Clustering in R Hierarchical Clustering in R Test Your Knowledge Using the same dataset, answer the questions below. Determine the optimal number of k-means clusters of cytokines derived from the nasal epithelieum lining fluid of e-cigarette users. How do those clusters compare to the ones that were derived at baseline (in non-smokers)? Which cytokines have the greatest contributions to the first two eigenvectors? "],["descriptive-cohort-analyses.html", "6.1 Descriptive Cohort Analyses Introduction to Training Module Participant Exploration Chemical Detection Filtering Outlier Identification Summary Statistics Tables Demographics Table Concluding Remarks", " 6.1 Descriptive Cohort Analyses This training module was developed by Elise Hickman, Kyle Roell, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Human cohort datasets are very commonly analyzed and integrated in environmental health research. Commone research study designs that incorporate human data include clinical, epidemiological, biomonitoring, and/or biomarker study designs. These datasets represent metrics of health and exposure collected from human participants at one or many points in time. Although these datasets can lend themselves to highly complex analyses, it is important to first explore the basic dataset properties to understand data missingness, filter data appropriately, generate demographic tables and summary statistics, and identify outliers. In this module, we will work through these common steps with an example dataset and discuss additional considerations when working with human cohort datasets. Our example data are derived from a study in which chemical exposure profiles were collected using silicone wristbands. Silicone wristbands are an affordable and minimally invasive method for sampling personal chemical exposure profiles. This exposure monitoring technique has been described through previous publications (see TAME 2.0 Module 3.3 Normality Tests and Data Transformations). The example workflow can also apply to other study designs, including biomonitoring and biomarker studies, which require careful consideration of chemical or biological marker detection filters, transparent reporting of descriptive statistics, and demographics tables. Training Module’s Environmental Health Questions What proportion of participants wore their wristbands for all seven days? How many chemicals were detected in at least 20% of participants? What are the demographics of the study participants? Workspace Preparation and Data Import # Load packages library(tidyverse) # for data organization and manipulation library(janitor) # for data cleaning library(openxlsx) # for reading in and writing out files library(DT) # for displaying tables library(table1) # for making tables library(patchwork) # for graphing library(purrr) # for summary stats library(factoextra) # for PCA outlier detection library(table1) # for making demographics table # Make sure select is calling the correct function select &lt;- dplyr::select # Set graphing theme theme_set(theme_bw()) First, we will import our raw chemical data and preview it. wrist_data &lt;- read.xlsx(&quot;Module6_1_Input/Module6_1_InputData1.xlsx&quot;) %&gt;% mutate(across(everything(), \\(x) as.numeric(x))) datatable(wrist_data[ , 1:6]) In this study, 97 participants wore silicone wristbands for one week, and chemical concentrations on the wristbands were measured with gas chromatography mass spectrometry. This dataframe consists of a column with a unique identifier for each participant (S_ID), a column describing the number of days that participant wore the wristband (Ndays), and subsequent columns containing the amount of each chemical detected (nanograms of chemical per gram of wristband). The chemical columns are labeled with the chemical class first (e.g., alkyl OPE, or alkyl organophosphate ester), followed by and underscore and the chemical name (e.g., 2IPPDPP). This dataset contains 110 different chemicals categorized into 8 chemical classes (listed below with their abbreviations): Brominated diphenyl ether (BDE) Brominated flame retardant (BFR) Organophosphate ester (OPE) Polycyclic aromatic hydrocarbon (PAH) Polychlorinated biphenyl (PCB) Pesticide (Pest) Phthalate (Phthal) Alkyl organophosphate ester (alkylOPE) Through the data exploration and cleaning process, we will aim to: Understand participant behaviors Filter out chemicals with low detection Generate a supplemental table containing chemical detection information and summary statistics such as minimum, mean, median, and maximum Identify participant outliers Generate a demographics table Although these steps are somewhat specific to our example dataset, similar steps can be taken with other datasets. We recommend thinking through the structure of your data and outlining data exploration and cleaning steps prior to starting your analysis. This process can be somewhat time-consuming and tedious but is important to ensure that your data are well-suited for downstream analyses. In addition, these steps should be included in any resulting manuscript as part of the narrative relating to the study cohort and data cleaning. Participant Exploration We can use tidyverse functions to quickly tabulate how many days participants wore the wristbands. wrist_data %&gt;% # Count number of participants for each number of days dplyr::count(Ndays) %&gt;% # Calculate proportion of partipants for each number of days mutate(prop = prop.table(n)) %&gt;% # Arrange the table from highest to lowest number of days arrange(-Ndays) %&gt;% # Round the proportion column to two decimal places mutate(across(prop, \\(x) round(x, 2))) ## Ndays n prop ## 1 7 83 0.86 ## 2 6 6 0.06 ## 3 5 3 0.03 ## 4 3 1 0.01 ## 5 2 2 0.02 ## 6 1 2 0.02 Answer to Environmental Health Question 1 With this, we can now answer Environmental Health Question #1: What proportion of participants wore their wristbands for all seven days? Answer: 86% of participants wore their wristbands for all seven days. Because a few participants did not wear their wristbands for all seven days, it will be important to further explore whether there are outlier participants and to normalize the chemical concentrations by number of days the wristband was worn. We can first assess whether any participants have a particularly low or high number of chemicals detected relative to the other participants. We’ll prepare the data for graphing by creating a dataframe containing information about how many chemicals were detected per participant. wrist_det_by_participant &lt;- wrist_data %&gt;% # Remove Ndays column because we don&#39;t need it for this step select(-Ndays) %&gt;% # Move S_ID to rownames so it doesn&#39;t interfere with count column_to_rownames(&quot;S_ID&quot;) %&gt;% # Create a new column for number of chemicals detected mutate(n_det = rowSums(!is.na(.))) %&gt;% # Clean dataframe rownames_to_column(&quot;S_ID&quot;) %&gt;% select(c(S_ID, n_det)) datatable(wrist_det_by_participant) Then, we can make our histogram: det_per_participant_graph &lt;- ggplot(wrist_det_by_participant, aes(x = n_det)) + geom_histogram(color = &quot;black&quot;, fill = &quot;gray60&quot;, alpha = 0.7, binwidth = 2) + ggtitle(&quot;Distribution of Number of Chemicals Detected Per Participant&quot;) + ylab(&quot;Number of Participants&quot;) + xlab(&quot;Number of Chemicals Detected&quot;) + scale_x_continuous(breaks = seq(0, 70, by = 10), limits = c(0, 70), expand = c(0.025, 0.025)) + scale_y_continuous(breaks = seq(0, 15, by = 5), limits = c(0, 15), expand = c(0, 0)) + theme(plot.title = element_text(hjust = 0.5, size = 16), axis.title.x = element_text(margin = ggplot2::margin(t = 10), size = 13), axis.title.y = element_text(margin = ggplot2::margin(r = 10), size = 13), axis.text = element_text(size = 12)) det_per_participant_graph From this histogram, we can see that the number of chemicals detected per participant ranges from about 30-65 chemicals, with no participants standing out as being well above or below the distribution. Chemical Detection Filtering Next, we want to apply a chemical detection filter to remove chemicals from the dataset with very low detection. To start, let’s make a dataframe summarizing the percentage of participants in which each chemical was detected and graph this distribution using a histogram. # Create dataframe where n_detected is the sum of the rows where there are not NA values chemical_counts &lt;- data.frame(n_detected = colSums(!is.na(wrist_data %&gt;% select(-c(S_ID, Ndays))))) %&gt;% # Move rownames to a column rownames_to_column(&quot;class_chemical&quot;) %&gt;% # Add n_undetected and percentage detected and undetected columns mutate(n_undetected = nrow(wrist_data) - n_detected, perc_detected = n_detected/nrow(wrist_data)*100, perc_undetected = n_undetected/nrow(wrist_data)*100) %&gt;% # Round percentages to two decimal places mutate(across(c(perc_detected, perc_undetected), \\(x) round(x, 2))) # View dataframe datatable(chemical_counts) det_per_chemical_graph &lt;- ggplot(chemical_counts, aes(x = perc_detected)) + geom_histogram(color = &quot;black&quot;, fill = &quot;gray60&quot;, alpha = 0.7, binwidth = 1) + scale_x_continuous(breaks = seq(0, 100, by = 10), expand = c(0.025, 0.025)) + scale_y_continuous(breaks = seq(0, 25, by = 5), limits = c(0, 25), expand = c(0, 0)) + ggtitle(&quot;Distribution of Percentage Chemical Detection&quot;) + ylab(&quot;Number of Chemicals&quot;) + xlab(&quot;Percentage of Detection Across All Participants&quot;) + theme(plot.title = element_text(hjust = 0.5), axis.title.x = element_text(margin = ggplot2::margin(t = 10)), axis.title.y = element_text(margin = ggplot2::margin(r = 10))) det_per_chemical_graph From this histogram, we can see that many of the chemicals fall in the &lt; 15% or &gt; 90% detection range, with the others distributed evenly between 20 and 90% detection. How we choose to filter our data in part depends on the goals of our analysis. For example, if we only want to keep chemicals detected for almost all of the participants, we could set our threshold at 90% detection: # Add annotation column chemical_counts &lt;- chemical_counts %&gt;% mutate(det_filter_90 = ifelse(perc_detected &gt; 90, &quot;Yes&quot;, &quot;No&quot;)) # How many chemicals pass this filter? nrow(chemical_counts %&gt;% filter(det_filter_90 == &quot;Yes&quot;)) ## [1] 34 # Make graph det_per_chemical_graph_90 &lt;- ggplot(chemical_counts, aes(x = perc_detected, fill = det_filter_90)) + geom_histogram(color = &quot;black&quot;, alpha = 0.7, binwidth = 1) + scale_fill_manual(values = c(&quot;gray87&quot;, &quot;gray32&quot;), guide = &quot;none&quot;) + geom_segment(aes(x = 90, y = 0, xend = 90, yend = 25), color = &quot;firebrick&quot;, linetype = 2) + scale_x_continuous(breaks = seq(0, 100, by = 10), expand = c(0.025, 0.025)) + scale_y_continuous(breaks = seq(0, 25, by = 5), limits = c(0, 25), expand = c(0, 0)) + ggtitle(&quot;Distribution of Percentage Chemical Detection&quot;) + ylab(&quot;Number of Chemicals&quot;) + xlab(&quot;Percentage of Detection Across All Participants&quot;) + theme(plot.title = element_text(hjust = 0.5, size = 16), axis.title.x = element_text(margin = ggplot2::margin(t = 10), size = 13), axis.title.y = element_text(margin = ggplot2::margin(r = 10), size = 13), axis.text = element_text(size = 12)) det_per_chemical_graph_90 However, this only keeps 34 chemicals in our dataset, which is a significant proportion of all of the chemicals measured. We could also consider setting the filter at 20% detection to maximize inclusion of as many chemicals as possible. # Add annotation column chemical_counts &lt;- chemical_counts %&gt;% mutate(det_filter_20 = ifelse(perc_detected &gt; 20, &quot;Yes&quot;, &quot;No&quot;)) # How many chemicals pass this filter? nrow(chemical_counts %&gt;% filter(det_filter_20 == &quot;Yes&quot;)) ## [1] 62 # Make graph det_per_chemical_graph_20 &lt;- ggplot(chemical_counts, aes(x = perc_detected, fill = det_filter_20)) + geom_histogram(color = &quot;black&quot;, alpha = 0.7, binwidth = 1) + scale_fill_manual(values = c(&quot;gray87&quot;, &quot;gray32&quot;), guide = &quot;none&quot;) + geom_segment(aes(x = 20, y = 0, xend = 20, yend = 25), color = &quot;firebrick&quot;, linetype = 2) + scale_x_continuous(breaks = seq(0, 100, by = 10), expand = c(0.025, 0.025)) + scale_y_continuous(breaks = seq(0, 25, by = 5), limits = c(0, 25), expand = c(0, 0)) + ggtitle(&quot;Distribution of Percentage Chemical Detection&quot;) + ylab(&quot;Number of Chemicals&quot;) + xlab(&quot;Percentage of Detection Across All Participants&quot;) + theme(plot.title = element_text(hjust = 0.5, size = 16), axis.title.x = element_text(margin = ggplot2::margin(t = 10), size = 13), axis.title.y = element_text(margin = ggplot2::margin(r = 10), size = 13), axis.text = element_text(size = 12)) det_per_chemical_graph_20 Answer to Environmental Health Question 2 With this, we can now answer Environmental Health Question #2: How many chemicals were detected in at least 20% of participants? Answer: 62 chemicals were detected in at least 20% of participants. We’ll use the 20% detection filter for downstream analyses to maximize inclusion of data for our study. Note that selection of data filters is highly project- and goal- dependent, so be sure to take into consideration typical workflows for your type of data, study, or lab group. # Create vector of chemicals to keep chemicals_20perc &lt;- chemical_counts %&gt;% filter(perc_detected &gt; 20) %&gt;% pull(class_chemical) # Filter dataframe wrist_data_filtered &lt;- wrist_data %&gt;% column_to_rownames(&quot;S_ID&quot;) %&gt;% dplyr::select(all_of(chemicals_20perc)) We can also summarize chemical detection vs. non-detection by chemical class to understand the number of chemicals in each class that were 1) detected in any participant or 2) detected in more than 20% of participants. chemical_count_byclass &lt;- chemical_counts %&gt;% separate(class_chemical, into = c(&quot;class&quot;, NA), remove = FALSE, sep = &quot;_&quot;) %&gt;% group_by(class) %&gt;% summarise(n_chemicals = n(), n_chemicals_det = sum(n_detected &gt; 0), n_chemicals_det_20perc = sum(perc_detected &gt;= 20)) %&gt;% bind_rows(summarise(., across(where(is.numeric), sum), across(where(is.character), ~&#39;Total&#39;))) datatable(chemical_count_byclass) From these data, we can see that, of the 62 chemicals retained by our detection filter, some classes were retained more than others. For example, of the 8 of the 10 phthalates (80%) were retained by the 20% detection filter, while only 2 of the 11 PCBs (18%) were retained. Outlier Identification Next, we will check to see if any participants are outliers based on the entire chemical signature for each participant using principal component analysis (PCA). Prior to checking for outliers, a few final data cleaning steps are required, which are beyond the scope of this specific module, though we encourage participants to research these methods as they are important in general data pre-processing. These data cleaning steps were: Imputing missing values. Calculating time-weighted average values by dividing each value by the number of days the participant wore the wristband. Assessing normality of data with and without log2 transformation. Here, we’ll read in the fully cleaned and processed data, which contains data for all 97 participants and the 62 chemicals that passed the detection filter (imputed, time-weighted). We will also apply log2 transformation to move the data closer to a normal distribution. For more on these steps, see TAME 2.0 Module 3.3 Normality Tests and Data Transformations and TAME 2.0 Module 4.2 Data Import, Processing, and Summary Statistics. wrist_data_cleaned &lt;- read.xlsx(&quot;Module6_1_Input/Module6_1_InputData2.xlsx&quot;) %&gt;% column_to_rownames(&quot;S_ID&quot;) %&gt;% mutate(across(everything(), \\(x) log2(x+1))) datatable(wrist_data_cleaned[ 1:6]) First, let’s run PCA and plot our data. # Prepare dataframe wrist_data_cleaned_scaled &lt;- wrist_data_cleaned %&gt;% scale() %&gt;% data.frame() # Run PCA pca &lt;- prcomp(wrist_data_cleaned_scaled) # Visualize PCA pca_chemplot &lt;- fviz_pca_ind(pca, label = &quot;none&quot;, pointsize = 3) + theme(axis.title = element_text(face = &quot;bold&quot;, size = rel(1.1)), panel.border = element_rect(fill = NA, color = &quot;black&quot;, linewidth = 0.3), panel.grid.minor = element_blank(), panel.grid.major = element_blank(), plot.title = element_text(hjust = 0.5), legend.position = &quot;none&quot;) pca_chemplot By visual inspection, it looks like there may be some outliers, so we can use a formula to detect outliers. One standard way to detect outliers is the criterion of being “more than 6 standard deviations away from the mean” (Source). We can apply this approach to our data by first creating a scoring function to detect PCA outliers based on whether or not that participant passed a certain standard deviation cutoff. # Create a scoring function to detect PCA sample outliers. The input is the PCA results data frame and the number of standard deviations for the cutoff. The output is outlier names. outlier_detection = function(pca_df, sd){ # getting scores scores = pca_df$x # identifying samples that are &gt; 6 standard deviations away from the mean outlier_indices = apply(scores, 2, function(x) which( abs(x - mean(x)) &gt; (sd * sd(x)) )) %&gt;% Reduce(union, .) # getting sample names outliers = rownames(scores)[outlier_indices] return(outliers) } # Call function with different standard deviation cutoffs outliers_6 &lt;- outlier_detection(pca, 6) outliers_5 &lt;- outlier_detection(pca, 5) outliers_4 &lt;- outlier_detection(pca, 4) outliers_3 &lt;- outlier_detection(pca, 3) # Summary data frame outlier_summary &lt;- data.frame(sd_cutoff = c(6, 5, 4, 3), n_outliers = c(length(outliers_6), length(outliers_5), length(outliers_4), length(outliers_3))) outlier_summary ## sd_cutoff n_outliers ## 1 6 0 ## 2 5 1 ## 3 4 5 ## 4 3 28 From these results, we see that there are no outliers that are &gt; 6 standard deviations from the mean, so we will proceed with the dataset without filtering any participants out. Summary Statistics Tables Now that we have explored our dataset and finished processing the data, we can make a summary table that includes descriptive statistics (minimum, mean, median, maximum) for each of our chemicals. This table would go into supplementary material when the project is submitted for publication. It is a good idea to make this table using both the raw data and the cleaned data (imputed and normalized by time-weighted average) because different readers may have different interests in the data. For example, they may want to see the raw data so that they can understand chemical detection versus non-detection and absolute minimums or maximums of detection. Or, they may want to use the cleaned data for their own analyses. This table can also include information about whether or not the chemical passed our 20% detection filter. There are many ways to generate summary statistics tables in R. Here, we will demonstrate a method using the map_dfr() function, which takes a list of functions and applies them across columns of the data. The summary statistics are then placed in rows, with each column representing a variable. # Define summary functions summary_functs &lt;- lst(min, median, mean, max) # Apply summary functions to raw data summarystats_raw &lt;- map_dfr(summary_functs, ~ summarise(wrist_data, across(3:ncol(wrist_data), .x, na.rm = TRUE)), .id = &quot;statistic&quot;) # View data datatable(summarystats_raw[, 1:6]) Through a few cleaning steps, we can transpose and format these data so that they are publication-quality. summarystats_raw &lt;- summarystats_raw %&gt;% # Transpose dataframe and return to dataframe class t() %&gt;% as.data.frame() %&gt;% # Make the first row the column names row_to_names(1) %&gt;% # Remove rows with NAs (those where data are completely missing) na.omit() %&gt;% # Move chemical identifier to a column rownames_to_column(&quot;class_chemical&quot;) %&gt;% # Round data mutate(across(min:max, as.numeric)) %&gt;% mutate(across(where(is.numeric), round, 2)) %&gt;% # Add a suffix to column titles so we know that these came from the raw data rename_with(~paste0(., &quot;_raw&quot;), min:max) datatable(summarystats_raw) We can apply the same steps to the cleaned data. summarystats_cleaned &lt;- map_dfr(summary_functs, ~ summarise(wrist_data_cleaned, across(1:ncol(wrist_data_cleaned), .x, na.rm = TRUE)), .id = &quot;statistic&quot;) %&gt;% t() %&gt;% as.data.frame() %&gt;% row_to_names(1) %&gt;% na.omit() %&gt;% rownames_to_column(&quot;class_chemical&quot;) %&gt;% mutate(across(min:max, as.numeric)) %&gt;% mutate(across(where(is.numeric), round, 2)) %&gt;% rename_with(~paste0(., &quot;_cleaned&quot;), min:max) datatable(summarystats_cleaned) Finally, we will merge the data from our chemical_counts dataframe (which contains detection information for all of our chemicals) with our summary statistics dataframes. summarystats_final &lt;- chemical_counts %&gt;% # Remove 90% detection filter column select(-det_filter_90) %&gt;% # Add raw summary stats left_join(summarystats_raw, by = &quot;class_chemical&quot;) %&gt;% # Add cleaned summary stats left_join(summarystats_cleaned, by = &quot;class_chemical&quot;) datatable(summarystats_final, width = 600) Demographics Table Another important element of any analysis of human data is the demographics table. The demographics table provides key information about the study participants and can help inform downstream analyses, such as exploration of the impact of covariates on the endpoint of interest. There are many different ways to make demographics tables in R. Here, we will demonstrate making a demographics table with the table1 package. For more on this package, including making tables with multiple groups and testing for statistical differences in demographics between groups, see the table1 vignette here. First, we’ll read in and view our demographic data: demo_data &lt;- read.xlsx(&quot;Module6_1_Input/Module6_1_InputData3.xlsx&quot;) datatable(demo_data) Then, we can create new labels for our variables so that they are more nicely formatted and more intuitive for display in the table. # Create new labels for the demographics table label(demo_data$mat_age_birth) &lt;- &quot;Age at Childbirth&quot; label(demo_data$pc_sex) &lt;- &quot;Sex&quot; label(demo_data$pc_gender) &lt;- &quot;Gender&quot; label(demo_data$pc_latino_hispanic) &lt;- &quot;Latino or Hispanic&quot; label(demo_data$pc_race_cleaned) &lt;- &quot;Race&quot; label(demo_data$pc_ed) &lt;- &quot;Educational Attainment&quot; Our demographics data also had “F” for female in the sex column. We can change this to “Female” so that the demographics table is more readable. demo_data &lt;- demo_data %&gt;% mutate(pc_sex = dplyr::recode(pc_sex, &quot;F&quot; = &quot;Female&quot;)) label(demo_data$pc_sex) &lt;- &quot;Sex&quot; Now, let’s make the table. The first argument in the formula is all of the columns you want to include in the table, followed by the input dataframe. table1(~ mat_age_birth + pc_sex + pc_gender + pc_latino_hispanic + pc_race_cleaned + pc_ed, data = demo_data) Overall(N=97) Age at Childbirth Mean (SD) 31.0 (5.54) Median [Min, Max] 31.1 [19.0, 46.0] Sex Female 97 (100%) Gender Woman 97 (100%) Latino or Hispanic No 82 (84.5%) Yes 15 (15.5%) Race Biracial/Multiracial 5 (5.2%) Black 19 (19.6%) Other 11 (11.3%) White 62 (63.9%) Educational Attainment Associate Degree 6 (6.2%) Four-Year Degree 36 (37.1%) High School or GED 28 (28.9%) Master's Degree 15 (15.5%) Professional Degree or PhD 12 (12.4%) There are a couple of steps we could take to clean up the table: Change the rendering for our continuous variable (age) to just mean (SD). Order educational attainment so that it progresses from least to most education. We can change the rendering for our continuous variable by defining our own rendering function (as demonstrated in the package’s vignette). # Create function for custom table so that Mean (SD) is shown for continuous variables my.render.cont &lt;- function(x) { with(stats.apply.rounding(stats.default(x), digits=2), c(&quot;&quot;, &quot;Mean (SD)&quot;=sprintf(&quot;%s (&amp;plusmn; %s)&quot;, MEAN, SD))) } We can order the education attainment by changing it to a factor and defining the levels. demo_data &lt;- demo_data %&gt;% mutate(pc_ed = factor(pc_ed, levels = c(&quot;High School or GED&quot;, &quot;Associate Degree&quot;, &quot;Four-Year Degree&quot;, &quot;Master&#39;s Degree&quot;, &quot;Professional Degree or PhD&quot;))) label(demo_data$pc_ed) &lt;- &quot;Educational Attainment&quot; Then, we can make our final table. table1(~ mat_age_birth + pc_sex + pc_gender + pc_latino_hispanic + pc_race_cleaned + pc_ed, data = demo_data, render.continuous = my.render.cont) Overall(N=97) Age at Childbirth Mean (SD) 31 (&plusmn; 5.5) Sex Female 97 (100%) Gender Woman 97 (100%) Latino or Hispanic No 82 (84.5%) Yes 15 (15.5%) Race Biracial/Multiracial 5 (5.2%) Black 19 (19.6%) Other 11 (11.3%) White 62 (63.9%) Educational Attainment High School or GED 28 (28.9%) Associate Degree 6 (6.2%) Four-Year Degree 36 (37.1%) Master's Degree 15 (15.5%) Professional Degree or PhD 12 (12.4%) Answer to Environmental Health Question 3 With this, we can now answer Environmental Health Question #3: What are the demographics of the study participants? Answer: The study participants were all females who identified as women and were, on average, 31 years old when they gave birth. Participants were mostly non-latino/non-hispanic and White. Participants were spread across educational attainment levels, with the smallest education attainment group being those with an associate degree and the largest being those with a four-year degree. Concluding Remarks In conclusion, this training module serves as an introduction to human cohort data exploration and preliminary analysis, including data filtering, summary statistics, and multivariate outlier detection. These methods are an important step at the beginning of human cohort analyses, and the concepts introduced in this module can be applied to a wide variety of datasets. Test Your Knowledge Using a more expanded demographics file (“Module6_1_TYKInput.xlsx”), create a demographics table with: The two new variables (home location and home type) included The table split by which site the participant visited Variable names and values presented in a publication-quality format (first letters capitalized, spaces between words, no underscores) "],["omics-and-system-biology-transcriptomic-applications.html", "6.2 -Omics and System Biology: Transcriptomic Applications Introduction to Training Module Introduction to the Field of “-Omics” Introduction to Transcriptomics Formatting Data for Downstream Statistics Transcriptomics Data QA/QC Controlling for Sources of Sample Heterogeneity Identifying Genes that are Significantly Differentially Expressed by Environmental Exposure Conditions (e.g., Biomass Smoke Exposure) Visualizing Statistical Results using MA Plots Visualizing Statistical Results using Volcano Plots Interpretting Findings at the Systems Level through Pathway Enrichment Analysis Concluding Remarks", " 6.2 -Omics and System Biology: Transcriptomic Applications This training module was developed by Lauren Koval, Dr. Kyle Roell, and Dr. Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module This training module incorporates the highly relevant example of RNA sequencing to evaluate the impacts of environmental exposures on cellular responses and general human health. RNA sequencing is the most common method that is currently implemented to measure the transcriptome. Results from an RNA sequencing platform are often summarized as count data, representing the number of relative times a gene (or other annotated portion of the genome) was ‘read’ in a given sample. For more details surrounding the methodological underpinnings of RNA sequencing, see the following recent review: Stark R, Grzelak M, Hadfield J. RNA sequencing: the teenage years. Nat Rev Genet. 2019 Nov;20(11):631-656. doi: 10.1038/s41576-019-0150-2. Epub 2019 Jul 24. PMID: 31341269. In this training module, we guide participants through an example RNA sequencing analysis. Here, we analyze RNA sequencing data collected in a toxicology study evaluating the effects of biomass smoke exposure, representing wildfire-relevant exposure conditions. This study has been previously been described in the following publications: Rager JE, Clark J, Eaves LA, Avula V, Niehoff NM, Kim YH, Jaspers I, Gilmour MI. Mixtures modeling identifies chemical inducers versus repressors of toxicity associated with wildfire smoke. Sci Total Environ. 2021 Jun 25;775:145759. doi: 10.1016/j.scitotenv.2021.145759. Epub 2021 Feb 10. PMID: 33611182. Kim YH, Warren SH, Krantz QT, King C, Jaskot R, Preston WT, George BJ, Hays MD, Landis MS, Higuchi M, DeMarini DM, Gilmour MI. Mutagenicity and Lung Toxicity of Smoldering vs. Flaming Emissions from Various Biomass Fuels: Implications for Health Effects from Wildland Fires. Environ Health Perspect. 2018 Jan 24;126(1):017011. doi: 10.1289/EHP2200. PMID: 29373863. Here, we specifically analyze mRNA sequencing profiles collected in mouse lung tissues. These mice were exposed to two different biomass burn scenarios: smoldering pine needles and flaming pine needles, representing certain wildfire smoke exposure scenarios that can occur. The goal of these analyses is to identify which genes demonstrate altered expression in response to these wildfire-relevant exposures, and identify which biological pathways these genes influence to evaluate findings at the systems biology level. This training module begins by guiding users through the loading, viewing, and formatting of the example transcriptomics datasets and associated metadata. Methods to carry out quality assurance (QA) / quality control (QC) of the transcriptomics data are then described, which are advantageous to ensure high quality data are included in the final statistical analysis. Because these transcriptomic data were derived from bulk lung tissue samples, consisting of mixed cell populations that could have shifted in response to exposures, data are then adjusted for potential sources of heterogeneity using the R package RUVseq. Statistical models are then implemented to identify genes that were significantly differentially expressed between exposed vs unexposed samples. Models are implemented using algorithms within the commonly implemented R package DESeq2. This package is very convenient, well written, and widely used. The main advantage of this package is that is allows you to perform differential expression analyses and easily obtain various statistics and results with minimal script development on the user-end. After obtaining results from differential gene expression analyses, we visualize these results using both MA and volcano plots. Finally, we carry out a systems level analysis through pathway enrichment using the R package PIANO to identify which biological pathways were altered in response to these wildfire-relevant exposure scenarios. Introduction to the Field of “-Omics” The field of “-omics” has rapidly evolved since its inception in the mid-1990’s, initiated from information obtained through sequencing of the human genome (see the Human Genome Project) as well as the advent of high-content technologies. High-content technologies have allowed the rapid and economical assessment of genome-wide, or ‘omics’-based, endpoints. Traditional molecular biology techniques typically evaluate the function(s) of individual genes and gene products. Omics-based methods, on the other hand, utilize non-targeted methods to identify many to all genes or gene products in a given environmental/biological sample. These non-targeted approaches allow for the unbiased investigation of potentially unknown or understudied molecular mediators involved in regulating cell health and disease. These molecular profiles have the potential of being altered in response to toxicant exposures and/or during disease initiation/progression. To further understand the molecular consequences of -omics-based alterations, molecules can be overlaid onto molecular networks to uncover biological pathways and molecular functions that are perturbed at the systems biology level. An overview of these generally methods, starting with high-content technologies and ending of systems biology, is provided in the below figure (created with BioRender.com). A helpful introduction to the field of -omics in relation to environmental health, as well as methods used to relate -omic-level alterations to systems biology, is provided in the following book chapter: Rager JE, Fry RC. Systems Biology and Environmental Exposures. Chpt 4 of ‘Network Biology’ edited by WenJun Zhang. 2013. ISBN: 978-1-62618-941-3. Nova Science Publishers, Inc. Available at: https://www.novapublishers.com/wp-content/uploads/2019/07/978-1-62618-942-3_ch4.pdf. An additional helpful resource describing computational methods that can be used in systems level analyses is the following book chapter: Meisner M, Reif DM. Computational Methods Used in Systems Biology. Chpt 5 of ‘Systems Biology in Toxicology and Environmental Health’ edited by Fry RC. 2015: 85-115. ISBN 9780128015643. Academic Press. Available at: https://www.sciencedirect.com/science/article/pii/B9780128015643000055. Parallel to human genomics/epigenomics-based research is the newer “-omics” topic of the exposome. The exposome was originally conceptualized as ’all life-course environmental exposures (including lifestyle factors), from the prenatal period onwards (Wild et al. 2005). Since then, this concept has received much attention and additional associated definitions. We like to think of the exposome as including anything in ones environment that may impact the overall health of an individual, excluding the individual’s genome/epigenome. Common elements evaluated as part of the exposome include environmental exposures, such as chemicals and other substances that may impart toxicity. Additional potential stressors include lifestyle factors, socioeconomic factors, infectious agents, therapeutics, and other stressors that may be altered internally (e.g., microbiome). A helpful review of this research field is provided as the following publication: Wild CP. The exposome: from concept to utility. Int J Epidemiol. 2012 Feb;41(1):24-32. doi: 10.1093/ije/dyr236. Epub 2012 Jan 31. PMID: 22296988. Introduction to Transcriptomics One of the most widely evaluated -omics endpoints is messenger RNA (mRNA) expression (also termed gene expression). As a reminder, mRNA molecules are a major type of RNA produced as the “middle step” in the Central Dogma Theory, which describes how genetic DNA is first transcribed into RNA and then translated into protein. Protein molecules are ultimately the major regulators of cellular processes and overall health. Therefore, any perturbations to this process (including changes to mRNA expression levels) can have tremendous consequences on overall cell function and health. A visualization of these steps in the Central Dogma theory are included below. mRNA expression can be evaluated in a high-throughout/high-content manner, across the genome, and is referred to as the transcriptome when doing so. Transcriptomics can be measured using a variety of technologies, including high-density nucleic acid arrays (e.g., DNA microarrays or GeneChip arrays), high-throughput PCR technologies, or RNA sequencing technologies. These methods are used to obtain relative measures of genes that are being expressed or transcribed from DNA by measuring the abundance of mRNA molecules. Results of these methods are often termed as providing gene expression signatures or ‘transcriptomes’ of a sample under evaluation. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: What two types of data are commonly needed in the analysis of transcriptomics data? When preparing transcriptomics data for statistical analyses, what are three common data filtering steps that are completed during the data QA/QC process? When identifying potential sample outliers in a typical transcriptomics dataset, what two types of approaches are commonly employed to identify samples with outlying data distributions? What is an approach that analysts can use when evaluating transcriptomic data from tissues of mixed cellular composition to aid in controlling for sources of sample heterogeneity? How many genes showed significant differential expression associated with flaming pine needles exposure in the mouse lung, based on a statistical filter of a multiple test corrected p-value (padj) &lt; 0.05? How many genes showed significant differential expression associated with smoldering pine needles exposure in the mouse lung, based on a statistical filter of a multiple test corrected p-value (padj) &lt; 0.05? How many genes showed significant differential expression associated with lipopolysaccharide (LPS) exposure in the mouse lung, based on a statistical filter of a multiple test corrected p-value (padj) &lt; 0.05? What biological pathways are disrupted in association with flaming pine needles exposure in the lung, identified through systems level analyses? Workspace Preparation and Data Import Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); if (!requireNamespace(&quot;BiocManager&quot;)) BiocManager::install(&quot;BiocManager&quot;); if (!requireNamespace(&quot;DESeq2&quot;)) BiocManager::install(&quot;DESeq2&quot;); if (!requireNamespace(&quot;edgeR&quot;)) BiocManager::install(&quot;edgeR&quot;); if (!requireNamespace(&quot;RUVSeq&quot;)) BiocManager::install(&quot;RUVSeq&quot;); if (!requireNamespace(&quot;janitor&quot;)) install.packages(&quot;janitor&quot;); if (!requireNamespace(&quot;pheatmap&quot;)) install.packages(&quot;pheatmap&quot;); if (!requireNamespace(&quot;factoextra&quot;)) install.packages(&quot;factoextra&quot;); if (!requireNamespace(&quot;RColorBrewer&quot;)) install.packages(&quot;RColorBrewer&quot;); if (!requireNamespace(&quot;data.table&quot;)) install.packages(&quot;data.table&quot;); if (!requireNamespace(&quot;EnhancedVolcano&quot;)) BiocManager::install(&quot;EnhancedVolcano&quot;); if (!requireNamespace(&quot;piano&quot;)) BiocManager::install(&quot;piano&quot;); Loading R packages required for this session library(tidyverse) library(DESeq2) library(edgeR) library(RUVSeq) library(janitor) library(factoextra) library(pheatmap) library(data.table) library(RColorBrewer) library(EnhancedVolcano) library(piano) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Loading the Example Transcriptomic Dataset and Metadata First, let’s read in the transcriptional signature data, previously summarized as number of sequence reads per gene (also simply referred to as ‘count data’) and its associated metadata file: # Read in the count data countdata &lt;- read.csv(file = &#39;Module6_2_Input/Module6_2_InputData1_GeneCounts.csv&#39;, check.names = FALSE) # Read in the metadata (describing information on each sample) sampleinfo &lt;- read.csv(file = &quot;Module6_2_Input/Module6_2_InputData2_SampleInfo.csv&quot;, check.names = FALSE) Data Viewing Let’s see how many rows and columns of data are present in the countdata dataframe dim(countdata) ## [1] 30146 23 Let’s also view the column headers colnames(countdata) ## [1] &quot;Gene&quot; &quot;Plate1-m13-RNA&quot; &quot;Plate2-m14-RNA&quot; &quot;Plate2-m15-RNA&quot; ## [5] &quot;Plate2-m16-RNA&quot; &quot;Plate2-m17-RNA&quot; &quot;Plate2-m18-RNA&quot; &quot;Plate1-m49-RNA&quot; ## [9] &quot;Plate1-m50-RNA&quot; &quot;Plate1-m51-RNA&quot; &quot;Plate1-m52-RNA&quot; &quot;Plate1-m53-RNA&quot; ## [13] &quot;Plate1-m54-RNA&quot; &quot;Plate1-m31-RNA&quot; &quot;Plate1-m32-RNA&quot; &quot;Plate1-m33-RNA&quot; ## [17] &quot;Plate1-m34-RNA&quot; &quot;Plate1-m35-RNA&quot; &quot;Plate1-m36-RNA&quot; &quot;Plate1-m67-RNA&quot; ## [21] &quot;Plate1-m68-RNA&quot; &quot;Plate1-m69-RNA&quot; &quot;Plate1-m70-RNA&quot; And finally let’s view the top few rows of data head(countdata) ## Gene Plate1-m13-RNA Plate2-m14-RNA Plate2-m15-RNA Plate2-m16-RNA ## 1 Bcat1_29039 0 0 0 0 ## 2 Laptm5_29040 0 2 0 1 ## 3 Skp2_29041 0 0 0 0 ## 4 Nme1_29042 765 1041 1028 856 ## 5 Ctsd_29043 25 30 38 24 ## 6 Camk1_29044 443 445 433 628 ## Plate2-m17-RNA Plate2-m18-RNA Plate1-m49-RNA Plate1-m50-RNA Plate1-m51-RNA ## 1 0 0 0 0 0 ## 2 2 1 0 1 2 ## 3 0 0 0 0 0 ## 4 888 967 816 569 854 ## 5 17 19 38 38 34 ## 6 497 684 574 71 356 ## Plate1-m52-RNA Plate1-m53-RNA Plate1-m54-RNA Plate1-m31-RNA Plate1-m32-RNA ## 1 0 2 0 0 3 ## 2 2 7 1 2 1 ## 3 0 0 0 0 0 ## 4 736 887 930 563 776 ## 5 30 24 23 29 21 ## 6 403 179 302 172 487 ## Plate1-m33-RNA Plate1-m34-RNA Plate1-m35-RNA Plate1-m36-RNA Plate1-m67-RNA ## 1 0 0 0 0 1 ## 2 5 0 0 0 1 ## 3 0 0 0 0 0 ## 4 604 1008 724 632 950 ## 5 27 26 18 22 33 ## 6 487 429 599 683 147 ## Plate1-m68-RNA Plate1-m69-RNA Plate1-m70-RNA ## 1 0 0 0 ## 2 3 2 2 ## 3 0 0 0 ## 4 782 879 860 ## 5 19 12 14 ## 6 265 335 454 Together, this dataframe contains information across 30146 mRNA identifiers, that are labeled according to “Gene name” followed by an underscore and probe number assigned by the platform used in this analysis, BioSpyder TempoSeq Technologies. A total of 23 columns are included in this dataframe, the first of which represents the gene identifier, followed by gene count data across 22 samples. Let’s also see what the metadata dataframe looks like dim(sampleinfo) ## [1] 22 9 Let’s also view the column headers colnames(sampleinfo) ## [1] &quot;SampleID_BioSpyderCountFile&quot; &quot;PlateBatch&quot; ## [3] &quot;MouseID&quot; &quot;NumericID&quot; ## [5] &quot;Treatment&quot; &quot;ID&quot; ## [7] &quot;Timepoint&quot; &quot;Tissue&quot; ## [9] &quot;Group&quot; And finally let’s view the top few rows of data head(sampleinfo) ## SampleID_BioSpyderCountFile PlateBatch MouseID NumericID Treatment ## 1 Plate1-m13-RNA Plate1 M13 13 PineNeedlesSmolder ## 2 Plate2-m14-RNA Plate2 M14 14 PineNeedlesSmolder ## 3 Plate2-m15-RNA Plate2 M15 15 PineNeedlesSmolder ## 4 Plate2-m16-RNA Plate2 M16 16 PineNeedlesSmolder ## 5 Plate2-m17-RNA Plate2 M17 17 PineNeedlesSmolder ## 6 Plate2-m18-RNA Plate2 M18 18 PineNeedlesSmolder ## ID Timepoint Tissue Group ## 1 M13_PineNeedlesSmolder 4h Lung PineNeedlesSmolder_4h_Lung ## 2 M14_PineNeedlesSmolder 4h Lung PineNeedlesSmolder_4h_Lung ## 3 M15_PineNeedlesSmolder 4h Lung PineNeedlesSmolder_4h_Lung ## 4 M16_PineNeedlesSmolder 4h Lung PineNeedlesSmolder_4h_Lung ## 5 M17_PineNeedlesSmolder 4h Lung PineNeedlesSmolder_4h_Lung ## 6 M18_PineNeedlesSmolder 4h Lung PineNeedlesSmolder_4h_Lung Together, this dataframe contains information across the 22 total samples, that are labeled according to “SampleID_BioSpyderCountFile” header. These identifiers match those used as column headers in the countdata dataframe. A total of 9 columns are included in this dataframe, including the following: ‘SampleID_BioSpyderCountFile’: The unique sample identifers (total n=22) ‘PlateBatch’: The plate number that was used in the generation of these data. ‘MouseID’: The unique identifier, that starts with “M” followed by a number, for each mouse used in this study ‘NumericID’: The unique numeric identifier for each mouse. ‘Treatment’: The type of exposure condition that each mouse was administered. These include smoldering pine needles, flaming pine needles, vehicle control (saline), and positive inflammation control (LPS, or lipopolysaccharide) ‘ID’: Another form of identifier that combines the mouse identifier with the exposure condition ‘Timepoint’: The timepoint at which samples were collected (here, all 4h post-exposure) ‘Tissue’: The type of tissue that was collected and analyzed (here, all lung tissue) ‘Group’: The higher level identifier that groups samples together based on exposure condition, timepoint, and tissue One common QC/preparation step that is helpful when organizing transcriptomics data is to check for potential duplicate mRNA IDs in the countdata. # Visualize this data quickly by viewing top left corner, to check where ID column is located: countdata[1:3,1:5] ## Gene Plate1-m13-RNA Plate2-m14-RNA Plate2-m15-RNA Plate2-m16-RNA ## 1 Bcat1_29039 0 0 0 0 ## 2 Laptm5_29040 0 2 0 1 ## 3 Skp2_29041 0 0 0 0 # Then check for duplicates within column 1 (where the ID column is located): Dups &lt;- duplicated(countdata[,1]) summary(Dups) ## Mode FALSE ## logical 30146 In this case, because all potential duplicate checks turn up “FALSE”, these data do not contain duplicate mRNA identifiers in its current organized format. Answer to Environmental Health Question 1 With this, we can now answer Environmental Health Question #1: What two types of data are commonly needed in the analysis of transcriptomics data? Answer: A file containing the raw -omics signatures are needed (in this case, the count data summarized per gene acquired from RNA sequencing technologies), and a file containing the associated metadata describing the actual samples, where they were derived from, what they represent, etc, is needed. Formatting Data for Downstream Statistics Most of the statistical analyses included in this training module will be carried out using the DESeq2 pipeline. This package requires that the count data and sample information data be formatted in a certain manner, which will expedite the downstream coding needed to carry out the statistics. Here, we will walk users through these initial formatting steps. DESeq2 first requires a ‘coldata’ dataframe, which includes the sample information (i.e., metadata). Let’s create this new dataframe based on the original ‘sampleinfo’ dataframe: coldata &lt;- sampleinfo DESeq2 also requires a ‘countdata’ dataframe, which we’ve previously created; however, this dataframe requires some minor formatting before it can be used as input for downstream script. First, the gene identifiers need to be converted into row names: countdata &lt;- countdata %&gt;% column_to_rownames(&quot;Gene&quot;) Then, the column names need to be edited. Let’s remind ourselves what the column names are currently: colnames(countdata) ## [1] &quot;Plate1-m13-RNA&quot; &quot;Plate2-m14-RNA&quot; &quot;Plate2-m15-RNA&quot; &quot;Plate2-m16-RNA&quot; ## [5] &quot;Plate2-m17-RNA&quot; &quot;Plate2-m18-RNA&quot; &quot;Plate1-m49-RNA&quot; &quot;Plate1-m50-RNA&quot; ## [9] &quot;Plate1-m51-RNA&quot; &quot;Plate1-m52-RNA&quot; &quot;Plate1-m53-RNA&quot; &quot;Plate1-m54-RNA&quot; ## [13] &quot;Plate1-m31-RNA&quot; &quot;Plate1-m32-RNA&quot; &quot;Plate1-m33-RNA&quot; &quot;Plate1-m34-RNA&quot; ## [17] &quot;Plate1-m35-RNA&quot; &quot;Plate1-m36-RNA&quot; &quot;Plate1-m67-RNA&quot; &quot;Plate1-m68-RNA&quot; ## [21] &quot;Plate1-m69-RNA&quot; &quot;Plate1-m70-RNA&quot; These column identifiers need to be converted into more intuitive sample IDs, that also indicate treatment. This information can be found in the coldata dataframe. Specifically, information in the column labeled ‘SampleID_BioSpyderCountFile’ will be helpful for these purposes. To replace these original column identifiers with these more helpful sample identifiers, let’s first make sure the order of the countdata columns are in the same order as the coldata column of ‘SampleID_BioSpyderCountFile’: countdata &lt;- setcolorder(countdata, as.character(coldata$SampleID_BioSpyderCountFile)) Now, we can rename the column names within the countdata dataframe with these more helpful identifiers, since both dataframes are now arranged in the same order: colnames(countdata) &lt;- coldata$ID # Rename the countdata column names with the treatment IDs. colnames(countdata) # Viewing these new column names ## [1] &quot;M13_PineNeedlesSmolder&quot; &quot;M14_PineNeedlesSmolder&quot; &quot;M15_PineNeedlesSmolder&quot; ## [4] &quot;M16_PineNeedlesSmolder&quot; &quot;M17_PineNeedlesSmolder&quot; &quot;M18_PineNeedlesSmolder&quot; ## [7] &quot;M49_PineNeedlesFlame&quot; &quot;M50_PineNeedlesFlame&quot; &quot;M51_PineNeedlesFlame&quot; ## [10] &quot;M52_PineNeedlesFlame&quot; &quot;M53_PineNeedlesFlame&quot; &quot;M54_PineNeedlesFlame&quot; ## [13] &quot;M31_Saline&quot; &quot;M32_Saline&quot; &quot;M33_Saline&quot; ## [16] &quot;M34_Saline&quot; &quot;M35_Saline&quot; &quot;M36_Saline&quot; ## [19] &quot;M67_LPS&quot; &quot;M68_LPS&quot; &quot;M69_LPS&quot; ## [22] &quot;M70_LPS&quot; These new column identifiers look much better, and can better inform downstream statistical analysis script. Remember that these identifiers indicate that these are mouse samples (“M”), with unique numbers, followed by an underscore and the exposure condition. When relabeling dataframes, it’s always important to triple check any of these major edits. For example, here, let’s double check that the same samples appear in the same order between the two working dataframes required for dowstream DESeq2 code: setequal(as.character(coldata$ID), colnames(countdata)) ## [1] TRUE identical(as.character(coldata$ID), colnames(countdata)) ## [1] TRUE Transcriptomics Data QA/QC After preparing your transcriptomic data and sample information dataframes for statistical analyses, it is very important to carry out QA/QC on your organized datasets, prior to including all samples and all genes in the actual statistical model. It is critical to only include high quality data that inform underlying biology of exposure responses/disease etiology, rather than data that may contribute noise to the overall data distributions. Some common QA/QC steps and associated data pre-filters carried out in transcriptomics analyses are detailed below. Background Filter It is very common to perform a background filter step when preparing transcriptomic data for statistical analyses. The goal of this step is to remove genes that are very lowly expressed across the majority of samples, and thus are referred to as universally lowly expressed. Signals from these genes can mute the overall signals that may be identified in -omics analyses. The specific threshold that you may want to apply as the background filter to your dataset will depend on the distribution of your dataset and analysis goal(s). For this example, we apply a background threshold, to remove genes that are lowly expressed across the majority of samples, specifically defined as genes that have expression levels across at least 20% of the samples that are less than (or equal to) the median expression of all genes across all samples. This will result in including only genes that are expressed above background, that have expression levels in at least 20% of samples that are greater than the overall median expression. Script to apply this filter is detailed below: # First count the total number of samples, and save it as a value in the global environment nsamp &lt;- ncol(countdata) # Then, calculate the median expression level across all genes and all samples, and save it as a value total_median &lt;- median(as.matrix(countdata)) # We need to temporarily add back in the Gene column to the countdata so we can filter for genes that pass the background filter countdata &lt;- countdata %&gt;% rownames_to_column(&quot;Gene&quot;) # Then we can apply a set of filters and organization steps (using the tidyverse) to result in a list of genes that have an expression greater than the total median in at least 20% of the samples genes_above_background &lt;- countdata %&gt;% # Start from the &#39;countdata&#39; dataframe pivot_longer(cols=!Gene, names_to = &quot;sampleID&quot;, values_to=&quot;expression&quot;) %&gt;% # Melt the data so that we have three columns: gene, exposure condition, and expression counts mutate(above_median=ifelse(expression&gt;total_median,1,0)) %&gt;% # Add a column that indicates whether the expression of a gene for the corresponding exposure condition is above (1) or not above (0) the median of all count data group_by(Gene) %&gt;% # Group the dataframe by the gene summarize(total_above_median=sum(above_median)) %&gt;% # For each gene, count the number of exposure conditions where the expression was greater than the median of all count data filter(total_above_median&gt;=.2*nsamp) %&gt;% # Filter for genes that have expression above the median in at least 20% of the samples select(Gene) # Select just the genes that pass the filter # Then filter the original &#39;countdata&#39; dataframe for only the genes above background. countdata &lt;- left_join(genes_above_background, countdata, by=&quot;Gene&quot;) Here, the ‘countdata’ dataframe went from having 30,146 rows of data (representing genes) to 16,664 rows of data (representing genes with expression levels that passed this background filter) Sample Filtering Another common QA/QC check is to evaluate whether there are any samples that did not produce adequate RNA material to be measured using the technology employed. Thus, a sample filter can be applied to remove samples that have inadequate data. Here, we demonstrate this filter by checking to see whether there were any samples that resulted in mRNA expression values of zero across all genes. If any sample demonstrates this issue, it should be removed prior to any statistical analysis. Note, there are other filter cut-offs you can use depending on your specific study. Below is example script that checks for the presence of samples that meet the above criteria: # Transpose filtered &#39;countdata&#39;, while keeping data in dataframe format, to allow for script that easily sums the total expression levels per sample countdata_T &lt;- countdata %&gt;% pivot_longer(cols=!Gene, names_to=&quot;sampleID&quot;,values_to=&quot;expression&quot;) %&gt;% pivot_wider(names_from=Gene, values_from=expression) # Then add in a column to the transposed countdata dataframe that sums expression across all genes for each exposure condition countdata_T$rowsum &lt;- rowSums(countdata_T[2:ncol(countdata_T)]) # Remove samples that have no expression. All samples have some expression in this example, so all samples are retained. countdata_T &lt;- countdata_T %&gt;% filter(rowsum!=0) # Take the count data filtered for correct samples, remove the &#39;rowsums&#39; column countdata_T &lt;- countdata_T %&gt;% select(!rowsum) # Then, transpose it back to the correct format for analysis countdata &lt;- countdata_T %&gt;% pivot_longer(cols=!sampleID, names_to = &quot;Gene&quot;,values_to=&quot;expression&quot;) %&gt;% pivot_wider(names_from = sampleID, values_from = &quot;expression&quot;) Identifying &amp; Removing Sample Outliers Prior to final statistical analysis, raw transcriptomic data are commonly evaluated for the presence of potential sample outliers. Outliers can result from experimental error, technical error/measurement error, and/or huge sources of variation in biology. For many analyses, it is beneficial to remove such outliers to enhance computational abilities to identify biologically meaningful signals across data. Here, we present two methods to check for the presence of sample outliers: 1. Principal component analysis (PCA) can be used to identify potential outliers in a dataset through visualization of summary-level values illustrating reduced representations of the entire dataset. Note that a more detailed description of PCA is provided in Training Module 2.2. Here, PCA is run on the raw count data and further analyzed using scree plots, assessing principal components (PCs), and visualized using biplots displaying the first two principal components as a scatter plot. 2. Hierarchical clustering is another approach that can be used to identify potential outliers. Hierarchical clustering aims to cluster data based on a similarity measure, defined by the function and/or specified by the user. There are several R packages and functions that will run hierarchical clustering, but it is often helpful visually to do this in conjuction with a heatmap. Here, we use the package pheatmap (introduced in Training Module 1.4) with hierarchical clustering across samples to identify potential outliers. Let’s start by using PCA to identify potential outliers, while providing a visualization of potential sources of variation across the dataset. First we need to move the Gene column back to the rownames so our dataframe is numeric and we can run the PCA script countdata &lt;- countdata %&gt;% column_to_rownames(&quot;Gene&quot;) # Let&#39;s remind ourselves what these data look like countdata[1:10,1:5] #viewing first 10 rows and 5 columns ## M13_PineNeedlesSmolder M14_PineNeedlesSmolder ## 0610009B22Rik_56046 123 118 ## 0610010F05Rik_69119 1378 1511 ## 0610010F05Rik_74637 83 55 ## 0610010K14Rik_31619 462 580 ## 0610010K14Rik_31873 560 814 ## 0610010K14Rik_68949 207 182 ## 0610012G03Rik_58446 243 299 ## 0610030E20Rik_65200 152 105 ## 0610040J01Rik_55628 125 106 ## 1110004F10Rik_77756 324 377 ## M15_PineNeedlesSmolder M16_PineNeedlesSmolder ## 0610009B22Rik_56046 110 167 ## 0610010F05Rik_69119 1534 936 ## 0610010F05Rik_74637 57 13 ## 0610010K14Rik_31619 625 560 ## 0610010K14Rik_31873 680 799 ## 0610010K14Rik_68949 104 202 ## 0610012G03Rik_58446 294 327 ## 0610030E20Rik_65200 92 87 ## 0610040J01Rik_55628 127 107 ## 1110004F10Rik_77756 356 233 ## M17_PineNeedlesSmolder ## 0610009B22Rik_56046 147 ## 0610010F05Rik_69119 1301 ## 0610010F05Rik_74637 54 ## 0610010K14Rik_31619 604 ## 0610010K14Rik_31873 707 ## 0610010K14Rik_68949 166 ## 0610012G03Rik_58446 262 ## 0610030E20Rik_65200 150 ## 0610040J01Rik_55628 93 ## 1110004F10Rik_77756 276 Then we can calculate principal components using transposed count data pca &lt;- prcomp(t(countdata)) And visualize the percent variation captured by each principal component (PC) with a scree plot. # We can generate a scree plot that shows the eigenvalues of each component, indicating how much of the total variation is captured by each component fviz_eig(pca) This scree plot indicates that nearly all variation is explained in PC1 and PC2, so we are comfortable with viewing these first two PCs when evaluating whether or not potential outliers exist in this dataset. Further visualization of how these transcriptomic data appear through PCA can be produced through a scatter plot showing the data reduced values per sample: # Calculate the percent variation captured by each PC pca_percent &lt;- round(100*pca$sdev^2/sum(pca$sdev^2),1) # Make dataframe for PCA plot generation using first two components and the sample name pca_df &lt;- data.frame(PC1 = pca$x[,1], PC2 = pca$x[,2], Sample=colnames(countdata)) # Organize dataframe so we can color our points by the exposure condition pca_df &lt;- pca_df %&gt;% separate(Sample, into = c(&quot;mouse_num&quot;, &quot;expo_cond&quot;), sep=&quot;_&quot;) # Plot PC1 and PC2 for each sample and color the point by the exposure condition ggplot(pca_df, aes(PC1,PC2, color = expo_cond))+ geom_hline(yintercept = 0, size=0.3)+ geom_vline(xintercept = 0, size=0.3)+ geom_point(size=3) + geom_text(aes(label=mouse_num), vjust =-1, size=4)+ labs(x=paste0(&quot;PC1 (&quot;,pca_percent[1],&quot;%)&quot;), y=paste0(&quot;PC2 (&quot;,pca_percent[2],&quot;%)&quot;))+ ggtitle(&quot;PCA for 4h Lung Pine Needles &amp; Control Exposure Conditions&quot;) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this ## warning was generated. With this plot, we can see that samples do not demonstrate obvious groupings, where certain samples group far apart from others. Therefore, our PCA analysis indicates that there are unlikely any sample outliers in this dataset. Now lets implement hierarchical clustering to identify potential outliers First we need to create a dataframe of our transposed ‘countdata’ such that samples are rows and genes are columns to input into the clustering algorithm. countdata_for_clustering &lt;- t(countdata) countdata_for_clustering[1:5,1:10] # Viewing what this transposed dataframe looks like ## 0610009B22Rik_56046 0610010F05Rik_69119 ## M13_PineNeedlesSmolder 123 1378 ## M14_PineNeedlesSmolder 118 1511 ## M15_PineNeedlesSmolder 110 1534 ## M16_PineNeedlesSmolder 167 936 ## M17_PineNeedlesSmolder 147 1301 ## 0610010F05Rik_74637 0610010K14Rik_31619 ## M13_PineNeedlesSmolder 83 462 ## M14_PineNeedlesSmolder 55 580 ## M15_PineNeedlesSmolder 57 625 ## M16_PineNeedlesSmolder 13 560 ## M17_PineNeedlesSmolder 54 604 ## 0610010K14Rik_31873 0610010K14Rik_68949 ## M13_PineNeedlesSmolder 560 207 ## M14_PineNeedlesSmolder 814 182 ## M15_PineNeedlesSmolder 680 104 ## M16_PineNeedlesSmolder 799 202 ## M17_PineNeedlesSmolder 707 166 ## 0610012G03Rik_58446 0610030E20Rik_65200 ## M13_PineNeedlesSmolder 243 152 ## M14_PineNeedlesSmolder 299 105 ## M15_PineNeedlesSmolder 294 92 ## M16_PineNeedlesSmolder 327 87 ## M17_PineNeedlesSmolder 262 150 ## 0610040J01Rik_55628 1110004F10Rik_77756 ## M13_PineNeedlesSmolder 125 324 ## M14_PineNeedlesSmolder 106 377 ## M15_PineNeedlesSmolder 127 356 ## M16_PineNeedlesSmolder 107 233 ## M17_PineNeedlesSmolder 93 276 Next we can run hierarchical clustering in conjunction with the generation of a heatmap. Note that we scale these data for improved visualization. pheatmap(scale(countdata_for_clustering), main=&quot;Hierarchical Clustering&quot;, cluster_rows=TRUE, cluster_cols = FALSE, fontsize_col = 7, treeheight_row = 60, show_colnames = FALSE) Like the PCA findings, heirarchical clustering demonstrated an overall lack of potential sample outliers because there were no obvious sample(s) that grouped separately from the rest along the clustering dendograms. Therefore, neither approach points to outliers that should be removed in this analysis. Answer to Environmental Health Question 2 With this, we can now answer Environmental Health Question #2: When preparing transcriptomics data for statistical analyses, what are three common data filtering steps that are completed during the data QA/QC process? Answer: (1) Background filter to remove genes that are universally lowly expressed; (2) Sample filter to remove samples that may be not have any detectable mRNA; (3) Sample outlier filter to remove samples with underlying data distributions outside of the overall, collective dataset.* Answer to Environmental Health Question 3 With this, we can now also answer Environmental Health Question #3: When identifying potential sample outliers in a typical transcriptomics dataset, what two types of approaches are commonly employed to identify samples with outlying data distributions? Answer: Principal component analysis (PCA) and hierarchical clustering. Controlling for Sources of Sample Heterogeneity Because these transcriptomic data were generated from mouse lung tissues, there is potential for these samples to show heterogeneity based on underlying shifts in cell populations (e.g., neutrophil influx) or other aspects of sample heterogeneity (e.g., batch effects from plating, among other sources of heterogeneity that we may want to control for). For these kinds of complex samples, there are data processing methods that can be leveraged to minimize the influence of these sources of heterogeneity. Example methods include Remove Unwanted Variable (RUV), which is discussed here, as well as others (e.g., Surrogate Variable Analysis (SVA)). Here, we leverage the package called RUVseq to employ RUV on this sequencing dataset. Script was developed based off Bioconductor website, vignette, and original publication. Steps in carrying out RUV using RUVseq on this example dataset: # First we store the treatment IDs and exposure conditions as a separate vector ID &lt;- coldata$ID # And differentiate our treatments and control conditions, first by grabbing the groups associated with each sample groups &lt;- as.factor(coldata$Group) # Let&#39;s view all the groups groups ## [1] PineNeedlesSmolder_4h_Lung PineNeedlesSmolder_4h_Lung ## [3] PineNeedlesSmolder_4h_Lung PineNeedlesSmolder_4h_Lung ## [5] PineNeedlesSmolder_4h_Lung PineNeedlesSmolder_4h_Lung ## [7] PineNeedlesFlame_4h_Lung PineNeedlesFlame_4h_Lung ## [9] PineNeedlesFlame_4h_Lung PineNeedlesFlame_4h_Lung ## [11] PineNeedlesFlame_4h_Lung PineNeedlesFlame_4h_Lung ## [13] Saline_4h_Lung Saline_4h_Lung ## [15] Saline_4h_Lung Saline_4h_Lung ## [17] Saline_4h_Lung Saline_4h_Lung ## [19] LPS_4h_Lung LPS_4h_Lung ## [21] LPS_4h_Lung LPS_4h_Lung ## 4 Levels: LPS_4h_Lung PineNeedlesFlame_4h_Lung ... Saline_4h_Lung # then setting a control label ctrl &lt;- &quot;Saline_4h_Lung&quot; # and extracting a vector of just our treatment groups trt_groups &lt;- setdiff(groups,ctrl) # let&#39;s view this vector trt_groups ## [1] &quot;PineNeedlesSmolder_4h_Lung&quot; &quot;PineNeedlesFlame_4h_Lung&quot; ## [3] &quot;LPS_4h_Lung&quot; RUVseq contains its own set of plotting and normalization functions, though requires input of what’s called an object of S4 class SeqExpressionSet. Let’s go ahead and make this object, using the RUVseq function ‘newSeqExpressionSet’: exprSet &lt;- newSeqExpressionSet(as.matrix(countdata),phenoData = data.frame(groups,row.names=colnames(countdata))) And then use this object to generate some exploratory plots using built-in tools within RUVseq. First starting with some bar charts summarizing overall data distributions per sample: colors &lt;- brewer.pal(4, &quot;Set2&quot;) plotRLE(exprSet, outline=FALSE, ylim=c(-4, 4), col=colors[groups]) We can see from this plot that some of the samples show distributions that may vary from the overall - for instance, one of the flaming pine needles-exposed samples (in orange) is far lower than the rest. Then viewing a PCA plot of these samples: colors &lt;- brewer.pal(4, &quot;Set2&quot;) plotPCA(exprSet, col=colors[groups], cex=1.2) This PCA plot shows pretty good data distributions, with samples mainly showing groupings based upon exposure condition (e.g., LPS), which is to be expected. With this, we can conclude that there may be some sources of unwanted variation, but not a huge amount. Let’s see what the data look like after running RUV. Now to actually run the RUVseq algorithm, to control for potential sources of sample heterogeneity, we need to first construct a matrix specifying the replicates (samples of the same exposure condition): # Construct a matrix specifying the replicates (samples of the same exposure condition) for running RUV differences &lt;- makeGroups(groups) # Viewing this new matrix head(differences) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 19 20 21 22 -1 -1 ## [2,] 7 8 9 10 11 12 ## [3,] 1 2 3 4 5 6 ## [4,] 13 14 15 16 17 18 This matrix groups the samples by exposure condition. Here, each of the four rows represents one of the four exposure conditions, and each of the six columns represents a possible sample. Since the LPS exposure condition only had four samples, instead of six like the rest of the exposure conditions, a value of -1 is automatically used as a place holder to fill out the matrix. The samples in the matrix are identified by the index of the sample in the previously defined ‘groups’ factor that was used to generate the matrix. For example, the PineNeedlesSmolder_4h_Lung samples are the the first six samples contained in the ‘groups’ factor, so in the matrix, samples of this exposure condition are identified as ‘1’,‘2’,‘3’,‘4’,‘5’, and ‘6’. Let’s now implement the RUVseq algorithm and, for this example, capture one factor (k=1) of unwanted variation. Note that the k parameter can be modified to capture additional factors, if necessary. # Now capture 1 factor (k=1) of unwanted variation ruv_set &lt;- RUVs(exprSet, rownames(countdata), k=1, differences) This results in a list of objects within ‘ruv_set’, which include the following important pieces of information: Estimated factors of unwanted variation are provided in the phenoData object, as viewed using the following: # viewing the estimated factors of unwanted variation in the column W_1 pData(ruv_set) ## groups W_1 ## M13_PineNeedlesSmolder PineNeedlesSmolder_4h_Lung 1.3777900 ## M14_PineNeedlesSmolder PineNeedlesSmolder_4h_Lung 1.2392662 ## M15_PineNeedlesSmolder PineNeedlesSmolder_4h_Lung 1.0687702 ## M16_PineNeedlesSmolder PineNeedlesSmolder_4h_Lung 1.2448747 ## M17_PineNeedlesSmolder PineNeedlesSmolder_4h_Lung 1.3156003 ## M18_PineNeedlesSmolder PineNeedlesSmolder_4h_Lung 1.4136600 ## M49_PineNeedlesFlame PineNeedlesFlame_4h_Lung 1.3343115 ## M50_PineNeedlesFlame PineNeedlesFlame_4h_Lung 0.4446084 ## M51_PineNeedlesFlame PineNeedlesFlame_4h_Lung 1.0172990 ## M52_PineNeedlesFlame PineNeedlesFlame_4h_Lung 1.0332373 ## M53_PineNeedlesFlame PineNeedlesFlame_4h_Lung 0.7746752 ## M54_PineNeedlesFlame PineNeedlesFlame_4h_Lung 1.0011086 ## M31_Saline Saline_4h_Lung 0.6582117 ## M32_Saline Saline_4h_Lung 1.2518829 ## M33_Saline Saline_4h_Lung 1.1892456 ## M34_Saline Saline_4h_Lung 1.2607852 ## M35_Saline Saline_4h_Lung 1.3141520 ## M36_Saline Saline_4h_Lung 1.4335200 ## M67_LPS LPS_4h_Lung 1.0341947 ## M68_LPS LPS_4h_Lung 1.2823240 ## M69_LPS LPS_4h_Lung 1.4460347 ## M70_LPS LPS_4h_Lung 1.4376930 Normalized counts obtained by regressing the original counts on the unwanted factors (normalizedCounts object within ‘ruv_set’). Note that the normalized counts should only used for exploratory purposes and not subsequent differential expression analyses. For additional information on this topic, please refer official RUVSeq documentation. The normalized counts can be viewed using the following: # Viewing the head of the normalized count data, accounting for unwanted variation head(normCounts(ruv_set)) ## M13_PineNeedlesSmolder M14_PineNeedlesSmolder ## 0610009B22Rik_56046 4 5 ## 0610010F05Rik_69119 2552 2630 ## 0610010F05Rik_74637 17 13 ## 0610010K14Rik_31619 1602 1774 ## 0610010K14Rik_31873 711 1009 ## 0610010K14Rik_68949 23 25 ## M15_PineNeedlesSmolder M16_PineNeedlesSmolder ## 0610009B22Rik_56046 8 8 ## 0610010F05Rik_69119 2474 1634 ## 0610010F05Rik_74637 17 3 ## 0610010K14Rik_31619 1639 1722 ## 0610010K14Rik_31873 819 992 ## 0610010K14Rik_68949 19 28 ## M17_PineNeedlesSmolder M18_PineNeedlesSmolder ## 0610009B22Rik_56046 6 4 ## 0610010F05Rik_69119 2343 2352 ## 0610010F05Rik_74637 12 7 ## 0610010K14Rik_31619 1979 1726 ## 0610010K14Rik_31873 889 888 ## 0610010K14Rik_68949 20 30 ## M49_PineNeedlesFlame M50_PineNeedlesFlame ## 0610009B22Rik_56046 4 5 ## 0610010F05Rik_69119 1767 1563 ## 0610010F05Rik_74637 12 6 ## 0610010K14Rik_31619 1480 1134 ## 0610010K14Rik_31873 586 599 ## 0610010K14Rik_68949 23 27 ## M51_PineNeedlesFlame M52_PineNeedlesFlame ## 0610009B22Rik_56046 4 4 ## 0610010F05Rik_69119 1918 1711 ## 0610010F05Rik_74637 4 9 ## 0610010K14Rik_31619 1390 1763 ## 0610010K14Rik_31873 683 797 ## 0610010K14Rik_68949 26 27 ## M53_PineNeedlesFlame M54_PineNeedlesFlame M31_Saline ## 0610009B22Rik_56046 5 5 3 ## 0610010F05Rik_69119 1751 1582 1891 ## 0610010F05Rik_74637 6 3 10 ## 0610010K14Rik_31619 1370 1419 1340 ## 0610010K14Rik_31873 593 647 561 ## 0610010K14Rik_68949 20 24 19 ## M32_Saline M33_Saline M34_Saline M35_Saline M36_Saline ## 0610009B22Rik_56046 2 4 3 4 4 ## 0610010F05Rik_69119 2507 2518 2524 1831 1878 ## 0610010F05Rik_74637 7 7 5 7 8 ## 0610010K14Rik_31619 1563 1632 1438 1280 967 ## 0610010K14Rik_31873 685 815 703 646 526 ## 0610010K14Rik_68949 16 21 19 22 20 ## M67_LPS M68_LPS M69_LPS M70_LPS ## 0610009B22Rik_56046 3 2 4 5 ## 0610010F05Rik_69119 1906 1597 1461 1201 ## 0610010F05Rik_74637 6 6 7 5 ## 0610010K14Rik_31619 964 1012 776 638 ## 0610010K14Rik_31873 622 702 522 496 ## 0610010K14Rik_68949 21 21 18 15 Let’s again generate an exploratory plot using this updated dataset, focusing on the bar chart view since that was the most informative pre-RUV. Here are the updated bar charts summarizing overall data distributions per sample: colors &lt;- brewer.pal(4, &quot;Set2&quot;) plotRLE(ruv_set, outline=FALSE, ylim=c(-4, 4), col=colors[groups]) This plot shows overall tighter data that are more similarly distributed across samples. Therefore, it is looking like this RUV addition improved the overall distribution of this dataset. It is important not to over-correct/over-smooth your datasets, so implement these types of pre-processing steps with caution. One strategy that we commonly employ to gage whether data smoothing is needed/applied correctly is to run the statistical models with and without correction of potential sources of heterogeneity, and critically evaluate similarities vs differences produced in the results. Answer to Environmental Health Question 4 With this, we can now answer Environmental Health Question #4: What is an approach that analysts can use when evaluating transcriptomic data from tissues of mixed cellular composition to aid in controlling for sources of sample heterogeneity? Answer: Remove unwanted variation (RUV), among other approaches, including surrogate variable analysis (SVA). Identifying Genes that are Significantly Differentially Expressed by Environmental Exposure Conditions (e.g., Biomass Smoke Exposure) At this point, we have completed several data pre-processing, QA/QC, and additional steps to prepare our example transcriptomics data for statistical analysis. And finally, we are ready to run the overall statistical model to identify genes that are altered in expression in association with different biomass burn conditions. Here we leverage the DESeq2 package to carry out these statistical comparisons. This package is now the most commonly implemented analysis pipeline used for transcriptomic data, including sequencing data as well as transcriptomic data produced via other technologies (e.g., Nanostring, Fluidigm, and other gene expression technologies). This package is extremely well-documented and we encourage trainees to leverage these resources in parallel with the current training module when carrying out their own transcriptomics analyses in R: Bioconductor website Vignette Manual Primary citation: Love MI, Huber W, Anders S. Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. Genome Biol. 2014;15(12):550. doi: 10.1186/s13059-014-0550-8. PMID: 25516281 In brief, the basic calculations employed within the DESeq2 underlying algorithms include the following: 1. Estimate size factors. In the first step, size factors are estimated to help account for potential differences in the sequencing depth across samples. It is similar to a normalization parameter in the model. 2. Normalize count data. DESeq2 employs different normalization algorithms depending on the parameters selected / stage of analysis. The most commonly employed method is called the median of ratios, which takes into account sequencing depth and RNA composition, as described here. Specifically, these normalized values are calculated as counts divided by sample-specific size factors determined by median ratio of gene counts relative to geometric mean per gene. DESeq2 then transforms these data using variance stabilization within the final statistical model. Because of these two steps, we prefer to export both the median of ratios normalized data as well as the variance stabilization transformed data, to save in our records and use when generating plots of expression levels for specific genes we are interested in. These steps are detailed below. 3. Estimate dispersion. The dispersion estimate takes into account the relationship between the variance of an observed count and its mean value. It is similar to a variance parameter. In DESeq2, dispersion is estimated using a maximum likelihood and empirical bayes approach. 4. Fit negative binomial generalized linear model (GLM). Finally, a negative binomial model is fit for each gene using the design formula that will be described within the proceeding code. The Wald test is performed to test if log fold changes in expression (typically calculated as log(average exposed / average unexposed)) significantly differ from zero. Statistical p-values are reported from this test and also adjusted for multiple testing using the Benjamini and Hochberg procedure. Note that these calculations, among others, are embedded within the DESeq2 functions, so we do not need to code for them ourselves. Instead, we just need to make sure that we set-up the DESeq2 functions correctly, such that these calculations are carried out appropriately in our final transcriptomics analyses. Setting up the DESeq2 experiment Here we provide example script that is used to identify which genes are significantly differentially expressed in association with the example biomass smoke exposures, smoldering pine needles and flaming pine needles, as well as a positive inflammation control, LPS. First, we need to set-up the DESeq2 experiment: # Set up our experiment using our RUV adjusted count and phenotype data. # Our design indicates that our count data is dependent on the exposure condition (groups variable) and our factor of unwanted variation, and we have specified that there not be an intercept term through the use of &#39;~0&#39; dds &lt;- DESeqDataSetFromMatrix(countData = counts(ruv_set), # Grabbing count data from the &#39;ruv_set&#39; object colData = pData(ruv_set), # Grabbing the phenotype data and corresponding factor of unwanted variation from the &#39;ruv_set&#39; object design = ~0+groups+W_1) # Setting up the statistical formula (see below) For the formula design, we use a ‘~0’ at the front to not include an intercept term, and then also account for the exposure condition (groups) and the previously calculated factors of unwanted variation (W_1) of the samples. Formula design is an important step and should be carefully considered for each individual analysis. Other resources, including official DESeq2 documentation, are availble for consultation regarding formula design, as the specifics of formula design are beyond the scope of this training module. It is worth noting that, by default, DESeq2 will use the last variable in the design formula (“W_1”) in this case, as the default variable to be output from the “results” function. Additionally, if the variable is categorical, it will display results comparing the reference level to the last level of that variable. To get results for other variables or to see other comparisions within a categorical variable, we can use the “contrast” parameter, which will be demonstrated below. Estimating size factors # Estimate size factors from the dds object that was just created as the experiment above dds &lt;- estimateSizeFactors(dds) sizeFactors(dds) # viewing the size factors ## M13_PineNeedlesSmolder M14_PineNeedlesSmolder M15_PineNeedlesSmolder ## 1.2668895 1.3040762 1.1782071 ## M16_PineNeedlesSmolder M17_PineNeedlesSmolder M18_PineNeedlesSmolder ## 1.1723749 1.2904950 1.3405500 ## M49_PineNeedlesFlame M50_PineNeedlesFlame M51_PineNeedlesFlame ## 1.1313237 0.6313099 0.9328970 ## M52_PineNeedlesFlame M53_PineNeedlesFlame M54_PineNeedlesFlame ## 1.0050414 0.7649632 0.8747854 ## M31_Saline M32_Saline M33_Saline ## 0.8050127 1.0480725 1.0389623 ## M34_Saline M35_Saline M36_Saline ## 1.0282261 0.9519266 1.0409103 ## M67_LPS M68_LPS M69_LPS ## 0.9308588 1.1002076 0.9793051 ## M70_LPS ## 0.8575718 Calculating and exporting normalized counts Here, we extract normalized counts and variance stablized counts. # Extract normalized count data normcounts &lt;- as.data.frame(counts(dds, normalized=TRUE)) # Transforming normalized counts through variance stabilization vsd &lt;- varianceStabilizingTransformation(dds, blind=FALSE) vsd_matrix &lt;- as.matrix(assay(vsd)) We could also export them using code such as: # Export data write.csv(normcounts, &quot;Module6_2_Input/Module6_2_Output_NormalizedCounts.csv&quot;) write.csv(vsd_matrix, &quot;Module6_2_Input/Module6_2_Output_VSDCounts.csv&quot;, row.names=TRUE) Running the final DESeq2 experiment Here, we are finally ready to run the actual statistical comparisons (exposed vs control samples) to calculate fold changes and p-values that describe the degree to which each gene may or may not be altered at the expression level in association with treatment. For this example, we would like to run three different comparisons: (1) Smoldering Pine Needles vs. Control (2) Flaming Pine Needles vs. Control (3) LPS vs. Control which we can easily code for using a loop function, as detailed below. Note that we have commented out the line of code for writing out the CSV because we do not need it for the rest of the module, but this could be used if you need to write out and view results in an external application such as Excel for supplementary materials. # Run experiment dds_run &lt;- DESeq(dds, betaPrior=FALSE) # Loop through and extract and export results for all contrasts (treatments vs. control) for (trt in trt_groups){ # Iterate for each of the treatments listed in &#39;trt_groups&#39; cat(trt) # Print which treatment group we are on in the loop res &lt;- results(dds_run, pAdjustMethod = &quot;BH&quot;, contrast = c(&quot;groups&quot;,trt,ctrl)) # Extract the results of the DESeq2 analysis specifically for the comparison of the treatment group for the current iteration of the loop with the control group summary(res) # Print out a high-level summary of the results ordered &lt;- as.data.frame(res[order(res$padj),]) # Make a dataframe of the results and order them by adjusted p-value from lowest to highest top10 &lt;- head(ordered, n=10) # Make dataframe of the first ten rows of the ordered results cat(&quot;\\nThe 10 most significantly differentially expressed genes by adjusted p-value:\\n\\n&quot;) print(top10) # View the first ten rows of the ordered results pfilt.05 &lt;- nrow(ordered %&gt;% filter(padj&lt;0.05)) # Get the number of genes that are significantly differentially expressed where padj &lt; 0.05 cat(&quot;\\nThe number of genes showing significant differential expression where padj &lt; 0.05 is &quot;, pfilt.05) pfilt.10 &lt;- nrow(ordered %&gt;% filter(padj&lt;0.1)) # Get the number of genes that are significantly differentially expressed where padj &lt; 0.10 cat(&quot;\\nThe number of genes showing significant differential expression where padj &lt; 0.10 is &quot;, pfilt.10,&quot;\\n\\n&quot;) # write.csv(ordered, paste0(&quot;Module6_2_Output_StatisticalResults_&quot;,trt ,&quot;.csv&quot;)) ## Export the full dataframe of ordered results as a csv } ## PineNeedlesSmolder_4h_Lung ## out of 16664 with nonzero total read count ## adjusted p-value &lt; 0.1 ## LFC &gt; 0 (up) : 457, 2.7% ## LFC &lt; 0 (down) : 697, 4.2% ## outliers [1] : 0, 0% ## low counts [2] : 1293, 7.8% ## (mean count &lt; 21) ## [1] see &#39;cooksCutoff&#39; argument of ?results ## [2] see &#39;independentFiltering&#39; argument of ?results ## ## ## The 10 most significantly differentially expressed genes by adjusted p-value: ## ## baseMean log2FoldChange lfcSE stat pvalue ## Hbs1l_72436 367.64772 -1.1465230 0.13614949 -8.421060 3.730933e-17 ## Cetn3_30290 2350.31984 -0.7197604 0.09379276 -7.673944 1.667868e-14 ## Grasp_56900 277.43936 0.9759274 0.14177298 6.883734 5.830390e-12 ## Pcsk7_57804 180.24040 0.8261941 0.13529825 6.106465 1.018617e-09 ## Plscr4_55764 178.95216 1.3193654 0.21751536 6.065620 1.314459e-09 ## Socs3_79479 183.28293 1.2535533 0.20740514 6.043983 1.503550e-09 ## Kat2b_64780 444.97603 -0.7197105 0.12034633 -5.980327 2.226896e-09 ## Ptp4a2_62762 1840.75247 -0.7802857 0.13076704 -5.966990 2.416698e-09 ## Rmi1_56327 377.57936 -0.8548903 0.14288443 -5.983089 2.189449e-09 ## Abcd2_62271 57.46455 -1.2504459 0.21464501 -5.825646 5.689193e-09 ## padj ## Hbs1l_72436 5.734817e-13 ## Cetn3_30290 1.281840e-10 ## Grasp_56900 2.987297e-08 ## Pcsk7_57804 3.851843e-06 ## Plscr4_55764 3.851843e-06 ## Socs3_79479 3.851843e-06 ## Kat2b_64780 4.127451e-06 ## Ptp4a2_62762 4.127451e-06 ## Rmi1_56327 4.127451e-06 ## Abcd2_62271 8.744859e-06 ## ## The number of genes showing significant differential expression where padj &lt; 0.05 is 679 ## The number of genes showing significant differential expression where padj &lt; 0.10 is 1154 ## ## PineNeedlesFlame_4h_Lung ## out of 16664 with nonzero total read count ## adjusted p-value &lt; 0.1 ## LFC &gt; 0 (up) : 411, 2.5% ## LFC &lt; 0 (down) : 575, 3.5% ## outliers [1] : 0, 0% ## low counts [2] : 0, 0% ## (mean count &lt; 10) ## [1] see &#39;cooksCutoff&#39; argument of ?results ## [2] see &#39;independentFiltering&#39; argument of ?results ## ## ## The 10 most significantly differentially expressed genes by adjusted p-value: ## ## baseMean log2FoldChange lfcSE stat pvalue ## Tmem109_70229 351.49267 1.2681226 0.1637785 7.742913 9.716419e-15 ## Rhof_64396 58.57258 1.4767798 0.2210640 6.680328 2.384081e-11 ## Plscr4_55764 178.95216 1.5349460 0.2393976 6.411701 1.439049e-10 ## Pip4k2a_67992 796.64597 0.8262520 0.1299839 6.356571 2.063073e-10 ## Zranb2_68671 739.91379 0.8131628 0.1330277 6.112735 9.793822e-10 ## Cul1_31957 896.52286 -0.7061150 0.1203618 -5.866602 4.448172e-09 ## Ppfibp1_75630 655.07197 1.4148222 0.2481843 5.700692 1.193223e-08 ## Mbnl1_30670 851.18586 -1.0159639 0.1811247 -5.609197 2.032670e-08 ## Atp5k_68198 1096.49479 1.1003470 0.1983469 5.547588 2.896380e-08 ## Gclm_29258 138.54250 1.0993904 0.1985396 5.537387 3.070173e-08 ## padj ## Tmem109_70229 1.619144e-10 ## Rhof_64396 1.986416e-07 ## Plscr4_55764 7.993439e-07 ## Pip4k2a_67992 8.594761e-07 ## Zranb2_68671 3.264085e-06 ## Cul1_31957 1.235406e-05 ## Ppfibp1_75630 2.840553e-05 ## Mbnl1_30670 4.234052e-05 ## Atp5k_68198 5.116136e-05 ## Gclm_29258 5.116136e-05 ## ## The number of genes showing significant differential expression where padj &lt; 0.05 is 515 ## The number of genes showing significant differential expression where padj &lt; 0.10 is 986 ## ## LPS_4h_Lung ## out of 16664 with nonzero total read count ## adjusted p-value &lt; 0.1 ## LFC &gt; 0 (up) : 2960, 18% ## LFC &lt; 0 (down) : 2947, 18% ## outliers [1] : 0, 0% ## low counts [2] : 0, 0% ## (mean count &lt; 10) ## [1] see &#39;cooksCutoff&#39; argument of ?results ## [2] see &#39;independentFiltering&#39; argument of ?results ## ## ## The 10 most significantly differentially expressed genes by adjusted p-value: ## ## baseMean log2FoldChange lfcSE stat pvalue ## Usp18_32270 1319.2259 3.829526 0.1459140 26.24508 8.133515e-152 ## Usp18_67659 414.5959 3.739864 0.1679851 22.26307 8.428171e-110 ## Ifit1_29859 862.3286 5.000365 0.2259553 22.12989 1.629877e-108 ## Oasl2_67419 1437.6182 3.710164 0.1904583 19.48019 1.616902e-84 ## Gbp5_77887 600.7219 5.272287 0.2769982 19.03365 8.977555e-81 ## Oasl2_67043 292.4482 3.898484 0.2096435 18.59578 3.476310e-77 ## Rnf213_57840 1052.0454 3.168052 0.1731212 18.29962 8.332621e-75 ## Cmpk2_70664 429.4232 4.592488 0.2518916 18.23200 2.875888e-74 ## Cmpk2_29189 695.1997 4.264136 0.2386279 17.86939 2.042134e-71 ## Eif2ak2_30238 1585.6071 2.671974 0.1495526 17.86645 2.152596e-71 ## padj ## Usp18_32270 1.355369e-147 ## Usp18_67659 7.022352e-106 ## Ifit1_29859 9.053424e-105 ## Oasl2_67419 6.736013e-81 ## Gbp5_77887 2.992039e-77 ## Oasl2_67043 9.654872e-74 ## Rnf213_57840 1.983640e-71 ## Cmpk2_70664 5.990475e-71 ## Cmpk2_29189 3.587085e-68 ## Eif2ak2_30238 3.587085e-68 ## ## The number of genes showing significant differential expression where padj &lt; 0.05 is 4813 ## The number of genes showing significant differential expression where padj &lt; 0.10 is 5907 Answer to Environmental Health Question 5 With this, we can now answer Environmental Health Question #5: How many genes showed significant differential expression associated with flaming pine needles exposure in the mouse lung, based on a statistical filter of a multiple test corrected p-value (padj) &lt; 0.05? Answer: 515 genes Answer to Environmental Health Question 6 With this, we can also now answer Environmental Health Question #6: How many genes showed significant differential expression associated with smoldering pine needles exposure in the mouse lung, based on a statistical filter of a multiple test corrected p-value (padj) &lt; 0.05? Answer: 679 genes Answer to Environmental Health Question 7 And, we can answer Environmental Health Question #7: How many genes showed significant differential expression associated with lipopolysaccharide (LPS) exposure in the mouse lung, based on a statistical filter of a multiple test corrected p-value (padj) &lt; 0.05? Answer: 4813 genes Together, we find that exposure to both flaming and smoldering of pine needles caused substantial disruptions in gene expression profiles. LPS serves as a positive control for inflammation and produced the greatest transcriptomic response. Visualizing Statistical Results using MA Plots MA plots represent a common method of visualization that illustrates differences between measurements taken in two samples, by transforming the data onto M (log ratio) and A (mean average) scales, then plotting these values. Here, we leverage MA plots to show how log fold changes relate to expression levels. In these plots, the log fold change is plotted on the y-axis and expression values are plotted along the x-axis, and dots are colored according to statistical significance (using padj&lt;0.05 as the statistical filter). Here we will generate an MA plot for Flaming Pine Needles. res &lt;- results(dds_run, pAdjustMethod = &quot;BH&quot;, contrast = c(&quot;groups&quot;,&quot;PineNeedlesFlame_4h_Lung&quot;,ctrl)) # Re-extract the DESeq2 results for the flamming pine needles MA &lt;- data.frame(res) # Make a prelimiary dataframe of the flaming pine needle results MA_ns &lt;- MA[ which(MA$padj&gt;=0.05),] # Non-significant genes to plot MA_up &lt;- MA[ which(MA$padj&lt;0.05 &amp; MA$log2FoldChange &gt; 0),] # Significant up-regulated genes to plot MA_down &lt;- MA[ which(MA$padj&lt;0.05 &amp; MA$log2FoldChange &lt; 0),] #Significant down-regulated genes to plot ggplot(MA_ns, aes(x = baseMean, y = log2FoldChange)) + # Plot data with counts on x-axis and log2 fold change on y-axis geom_point(color=&quot;gray75&quot;, size = .5) + # Set point size and color geom_point(data = MA_up, color=&quot;firebrick&quot;, size=1, show.legend = TRUE) + # Plot the up-regulated significant genes geom_point(data = MA_down, color=&quot;dodgerblue2&quot;, size=1, show.legend = TRUE) + # Plot down-regulated significant genes theme_bw() + # Change theme of plot from gray to black and white # We want to log10 transform x-axis for better visualizations scale_x_continuous(trans = &quot;log10&quot;, breaks=c(1,10,100, 1000, 10000, 100000, 1000000), labels=c(&quot;1&quot;,&quot;10&quot;,&quot;100&quot;, &quot;1000&quot;, &quot;10000&quot;, &quot;100000&quot;, &quot;1000000&quot;)) + # We will bound y axis as well to better fit data while not leaving out too many points scale_y_continuous(limits=c(-2, 2)) + xlab(&quot;Expression (Normalized Count)&quot;) + ylab(&quot;Fold Change (log2)&quot;) + # Add labels for axes labs(title=&quot;MA Plot Flaming Pine Needles 4h Lung&quot;) + # Add title geom_hline(yintercept=0) # Add horizontal line at 0 Visualizing Statistical Results using Volcano Plots Similar to MA plots, volcano plots provide visualizations of fold changes in expression from transcriptomic data. However, instead of plotting these values against expression, log fold change is plotted against (adjusted) p-values in volcano plots. Here, we use functions within the EnhancedVolcano package to generate a volcano plot for Flaming Pine Needles. Running the ‘EnhancedVolcano’ function to generate an example volcano plot: Vol &lt;- data.frame(res) # Dataset to use for plotting EnhancedVolcano(Vol, lab = rownames(res), # Label information from dataset (can be a column name) x = &#39;log2FoldChange&#39;, # Column name in dataset with l2fc information y = &#39;padj&#39;, # Column name in dataset with adjusted p-value information ylab = &quot;-Log(FDR-adjusted p)&quot;, # Y-axis label pCutoff= 0.05, # Set p-value cutoff ylim=c(0,5), # Limit y-axis for better plot visuals xlim=c(-2,2), # Limit x-axis (similar to in MA plot y-axis) title=&quot;Volcano Plot&quot;, # Set title subtitle = &quot;Flaming Pine Needles 4h Lung&quot;, # Set subtitle legendPosition = &#39;bottom&#39;) # Put legend on bottom Interpretting Findings at the Systems Level through Pathway Enrichment Analysis Pathway enrichment analysis is a very helpful tool that can be applied to interpret transcriptomic changes of interest in terms of systems biology. In these types of analyses, gene lists of interest are used to identify biological pathways that include genes present in your dataset more often than expected by chance alone. There are many tools that can be used to carry out pathway enrichment analyses. Here, we are using the R package, PIANO, to carry out the statistical enrichment analysis based on the lists of genes we previously identified with differential expression associated with flaming pine needles exposure. To detail, the following input data are required to run PIANO: (1) Your background gene sets, which represent all genes queried from your experiment (aka your ‘gene universe’) (2) The list of genes you are interested in evaluating pathway enrichment of; here, this represents the genes identified with significant differential expression associated with flaming pine needles (3) A underlying pathway dataset; here, we’re using the KEGG PATHWAY Database (KEGG), summarized through the Molecular Signature Database (MSigDB) into pre-formatted input files (.gmt) ready for PIANO. Let’s organize these three required data inputs. Background gene set: # First grab the rownames of the &#39;res&#39; object, which was redefined as the DESeq2 results for flaming pine needles prior to MA plot generation, and remove the BioSpyder numeric identifier using a sub function, while maintaining the gene symbol and place these IDs into a new list within the &#39;res&#39; object (saved as &#39;id&#39;) res$id &lt;- gsub(&quot;_.*&quot;, &quot;&quot;, rownames(res)); # Because these IDs now contain duplicate gene symbols, we need to remove duplicates # One way to do this is to preferentially retain rows of data with the largest fold change (it doesn&#39;t really matter here, because we&#39;re just identifying unique genes within the background set) res.ordered &lt;- res[order(res$id, -abs(res$log2FoldChange) ), ] # sort by id and reverse of abs(log2foldchange) res.ordered &lt;- res.ordered[ !duplicated(res.ordered$id), ] # removing gene duplicates # Setting this as the background list Background &lt;- toupper(as.character(res.ordered$id)) Background[1:200] # viewing the first 200 genes in this background list ## [1] &quot;0610009B22RIK&quot; &quot;0610010F05RIK&quot; &quot;0610010K14RIK&quot; &quot;0610012G03RIK&quot; ## [5] &quot;0610030E20RIK&quot; &quot;0610040J01RIK&quot; &quot;1110004F10RIK&quot; &quot;1110008P14RIK&quot; ## [9] &quot;1110012L19RIK&quot; &quot;1110017D15RIK&quot; &quot;1110032A03RIK&quot; &quot;1110038F14RIK&quot; ## [13] &quot;1110059E24RIK&quot; &quot;1110059G10RIK&quot; &quot;1110065P20RIK&quot; &quot;1190002N15RIK&quot; ## [17] &quot;1190007I07RIK&quot; &quot;1500011B03RIK&quot; &quot;1500011K16RIK&quot; &quot;1600002K03RIK&quot; ## [21] &quot;1600012H06RIK&quot; &quot;1600014C10RIK&quot; &quot;1700001L19RIK&quot; &quot;1700003E16RIK&quot; ## [25] &quot;1700007K13RIK&quot; &quot;1700012B09RIK&quot; &quot;1700013F07RIK&quot; &quot;1700016K19RIK&quot; ## [29] &quot;1700017B05RIK&quot; &quot;1700020D05RIK&quot; &quot;1700024G13RIK&quot; &quot;1700025G04RIK&quot; ## [33] &quot;1700028P14RIK&quot; &quot;1700029I15RIK&quot; &quot;1700029J07RIK&quot; &quot;1700030K09RIK&quot; ## [37] &quot;1700037C18RIK&quot; &quot;1700037H04RIK&quot; &quot;1700086D15RIK&quot; &quot;1700088E04RIK&quot; ## [41] &quot;1700102P08RIK&quot; &quot;1700109H08RIK&quot; &quot;1700123O20RIK&quot; &quot;1810009A15RIK&quot; ## [45] &quot;1810010H24RIK&quot; &quot;1810013L24RIK&quot; &quot;1810022K09RIK&quot; &quot;1810030O07RIK&quot; ## [49] &quot;1810037I17RIK&quot; &quot;1810043G02RIK&quot; &quot;1810055G02RIK&quot; &quot;2010005H15RIK&quot; ## [53] &quot;2010111I01RIK&quot; &quot;2010300C02RIK&quot; &quot;2010309G21RIK&quot; &quot;2200002D01RIK&quot; ## [57] &quot;2210011C24RIK&quot; &quot;2210016H18RIK&quot; &quot;2210016L21RIK&quot; &quot;2210408I21RIK&quot; ## [61] &quot;2300009A05RIK&quot; &quot;2310007B03RIK&quot; &quot;2310009B15RIK&quot; &quot;2310011J03RIK&quot; ## [65] &quot;2310022A10RIK&quot; &quot;2310022B05RIK&quot; &quot;2310030G06RIK&quot; &quot;2310033P09RIK&quot; ## [69] &quot;2310057M21RIK&quot; &quot;2310061I04RIK&quot; &quot;2410002F23RIK&quot; &quot;2410004B18RIK&quot; ## [73] &quot;2410015M20RIK&quot; &quot;2410131K14RIK&quot; &quot;2510002D24RIK&quot; &quot;2510009E07RIK&quot; ## [77] &quot;2600001M11RIK&quot; &quot;2610001J05RIK&quot; &quot;2610002M06RIK&quot; &quot;2610008E11RIK&quot; ## [81] &quot;2610028H24RIK&quot; &quot;2610301B20RIK&quot; &quot;2610507B11RIK&quot; &quot;2700049A03RIK&quot; ## [85] &quot;2700062C07RIK&quot; &quot;2700081O15RIK&quot; &quot;2700097O09RIK&quot; &quot;2810004N23RIK&quot; ## [89] &quot;2810021J22RIK&quot; &quot;2810408A11RIK&quot; &quot;2900026A02RIK&quot; &quot;3110001I22RIK&quot; ## [93] &quot;3110040N11RIK&quot; &quot;3110082I17RIK&quot; &quot;3830406C13RIK&quot; &quot;4430402I18RIK&quot; ## [97] &quot;4732423E21RIK&quot; &quot;4833414E09RIK&quot; &quot;4833420G17RIK&quot; &quot;4833427G06RIK&quot; ## [101] &quot;4833439L19RIK&quot; &quot;4921524J17RIK&quot; &quot;4930402H24RIK&quot; &quot;4930430F08RIK&quot; ## [105] &quot;4930451G09RIK&quot; &quot;4930453N24RIK&quot; &quot;4930486L24RIK&quot; &quot;4930503L19RIK&quot; ## [109] &quot;4930523C07RIK&quot; &quot;4930550C14RIK&quot; &quot;4930562C15RIK&quot; &quot;4931406C07RIK&quot; ## [113] &quot;4931406P16RIK&quot; &quot;4931414P19RIK&quot; &quot;4932438A13RIK&quot; &quot;4933408B17RIK&quot; ## [117] &quot;4933415F23RIK&quot; &quot;4933427D14RIK&quot; &quot;4933434E20RIK&quot; &quot;5330417C22RIK&quot; ## [121] &quot;5430427O19RIK&quot; &quot;5730480H06RIK&quot; &quot;5830417I10RIK&quot; &quot;6030458C11RIK&quot; ## [125] &quot;6030468B19RIK&quot; &quot;6330403K07RIK&quot; &quot;6330417A16RIK&quot; &quot;6430531B16RIK&quot; ## [129] &quot;6430548M08RIK&quot; &quot;6430550D23RIK&quot; &quot;6720489N17RIK&quot; &quot;8030462N17RIK&quot; ## [133] &quot;9030624G23RIK&quot; &quot;9130008F23RIK&quot; &quot;9130019O22RIK&quot; &quot;9130023H24RIK&quot; ## [137] &quot;9130230L23RIK&quot; &quot;9230104L09RIK&quot; &quot;9330182L06RIK&quot; &quot;9530068E07RIK&quot; ## [141] &quot;9930012K11RIK&quot; &quot;9930021J03RIK&quot; &quot;9930111J21RIK1&quot; &quot;9930111J21RIK2&quot; ## [145] &quot;A130071D04RIK&quot; &quot;A230050P20RIK&quot; &quot;A2ML1&quot; &quot;A430005L14RIK&quot; ## [149] &quot;A430033K04RIK&quot; &quot;A430078G23RIK&quot; &quot;A530032D15RIK&quot; &quot;A630001G21RIK&quot; ## [153] &quot;A730034C02&quot; &quot;A730049H05RIK&quot; &quot;A830018L16RIK&quot; &quot;A930002H24RIK&quot; ## [157] &quot;A930004D18RIK&quot; &quot;AA415038&quot; &quot;AA986860&quot; &quot;AACS&quot; ## [161] &quot;AAED1&quot; &quot;AAGAB&quot; &quot;AAK1&quot; &quot;AAMDC&quot; ## [165] &quot;AAMP&quot; &quot;AAR2&quot; &quot;AARS&quot; &quot;AARS2&quot; ## [169] &quot;AARSD1&quot; &quot;AASDH&quot; &quot;AASS&quot; &quot;AATF&quot; ## [173] &quot;AATK&quot; &quot;AB124611&quot; &quot;ABCA1&quot; &quot;ABCA17&quot; ## [177] &quot;ABCA2&quot; &quot;ABCA3&quot; &quot;ABCA5&quot; &quot;ABCA7&quot; ## [181] &quot;ABCA8A&quot; &quot;ABCA8B&quot; &quot;ABCA9&quot; &quot;ABCB10&quot; ## [185] &quot;ABCB1A&quot; &quot;ABCB1B&quot; &quot;ABCB6&quot; &quot;ABCB7&quot; ## [189] &quot;ABCB8&quot; &quot;ABCC1&quot; &quot;ABCC3&quot; &quot;ABCC4&quot; ## [193] &quot;ABCC5&quot; &quot;ABCD1&quot; &quot;ABCD2&quot; &quot;ABCD3&quot; ## [197] &quot;ABCD4&quot; &quot;ABCE1&quot; &quot;ABCF1&quot; &quot;ABCF3&quot; The list of genes identified with significant differential expression associated with flaming pine needles: # Similar to the above script, but starting with the res$id object # and filtering for genes with padj &lt; 0.05 res.ordered &lt;- res[order(res$id, -abs(res$log2FoldChange) ), ] #sort by id and reverse of abs(log2FC) SigGenes &lt;- toupper(as.character(res.ordered[which(res.ordered$padj&lt;.05),&quot;id&quot;])) # pulling the genes with padj &lt; 0.05 SigGenes &lt;- SigGenes[ !duplicated(SigGenes)] # removing gene duplicates length(SigGenes) # viewing the length of this significant gene list ## [1] 488 Therefore, this gene set includes 488 unique genes significantly associated with the Flaming Pine Needles condition, based on padj&lt;0.05. The underlying KEGG pathway dataset. Note that this file was simply downloaded from MSigDB, ready for upload as a .gmt file. Here, we use the ‘loadGSC’ function enabled through the PIANO package to upload and organize these pathways. KEGG_Pathways &lt;- loadGSC(file=&quot;Module6_2_Input/Module6_2_InputData3_KEGGv7.gmt&quot;, type=&quot;gmt&quot;) length(KEGG_Pathways$gsc) # viewing the number of biological pathways contained in the database ## [1] 186 This KEGG pathway database therefore includes 186 biological pathways available to query With these data inputs ready, we can now run the pathway enrichment analysis. The enrichment statistic that is commonly employed through the PIANO package is based of a hypergeometric test, run through the ‘runGSAhyper’ function. This returns a p-value for each gene set from which you can determine enrichment status. # Running the piano function based on the hypergeometric statistic Results_GSA &lt;- piano::runGSAhyper(genes=SigGenes, universe=Background,gsc=KEGG_Pathways, gsSizeLim=c(1,Inf), adjMethod = &quot;fdr&quot;) # Pulling the pathway enrichment results into a separate dataframe PathwayResults &lt;- as.data.frame(Results_GSA$resTab) # Viewing the top of these pathway enrichment results (which are not ordered at the moment) head(PathwayResults) ## p-value Adjusted p-value ## KEGG_N_GLYCAN_BIOSYNTHESIS 0.77021314 1.0000000 ## KEGG_OTHER_GLYCAN_DEGRADATION 1.00000000 1.0000000 ## KEGG_O_GLYCAN_BIOSYNTHESIS 0.03553158 0.6656139 ## KEGG_GLYCOSAMINOGLYCAN_DEGRADATION 0.42056787 1.0000000 ## KEGG_GLYCOSAMINOGLYCAN_BIOSYNTHESIS_KERATAN_SULFATE 1.00000000 1.0000000 ## KEGG_GLYCEROLIPID_METABOLISM 0.16241736 1.0000000 ## Significant (in gene set) ## KEGG_N_GLYCAN_BIOSYNTHESIS 1 ## KEGG_OTHER_GLYCAN_DEGRADATION 0 ## KEGG_O_GLYCAN_BIOSYNTHESIS 3 ## KEGG_GLYCOSAMINOGLYCAN_DEGRADATION 1 ## KEGG_GLYCOSAMINOGLYCAN_BIOSYNTHESIS_KERATAN_SULFATE 0 ## KEGG_GLYCEROLIPID_METABOLISM 3 ## Non-significant (in gene set) ## KEGG_N_GLYCAN_BIOSYNTHESIS 34 ## KEGG_OTHER_GLYCAN_DEGRADATION 13 ## KEGG_O_GLYCAN_BIOSYNTHESIS 15 ## KEGG_GLYCOSAMINOGLYCAN_DEGRADATION 12 ## KEGG_GLYCOSAMINOGLYCAN_BIOSYNTHESIS_KERATAN_SULFATE 12 ## KEGG_GLYCEROLIPID_METABOLISM 31 ## Significant (not in gene set) ## KEGG_N_GLYCAN_BIOSYNTHESIS 487 ## KEGG_OTHER_GLYCAN_DEGRADATION 488 ## KEGG_O_GLYCAN_BIOSYNTHESIS 485 ## KEGG_GLYCOSAMINOGLYCAN_DEGRADATION 487 ## KEGG_GLYCOSAMINOGLYCAN_BIOSYNTHESIS_KERATAN_SULFATE 488 ## KEGG_GLYCEROLIPID_METABOLISM 485 ## Non-significant (not in gene set) ## KEGG_N_GLYCAN_BIOSYNTHESIS 11355 ## KEGG_OTHER_GLYCAN_DEGRADATION 11376 ## KEGG_O_GLYCAN_BIOSYNTHESIS 11374 ## KEGG_GLYCOSAMINOGLYCAN_DEGRADATION 11377 ## KEGG_GLYCOSAMINOGLYCAN_BIOSYNTHESIS_KERATAN_SULFATE 11377 ## KEGG_GLYCEROLIPID_METABOLISM 11358 This dataframe therefore summarizes the enrichment p-value for each pathway, FDR adjusted p-value, number of significant genes in the gene set that intersect with genes in the pathway, etc With these results, let’s identify which pathways meet a statistical enrichment p-value filter of 0.05: SigPathways &lt;- PathwayResults[which(PathwayResults$`p-value` &lt; 0.05),] rownames(SigPathways) ## [1] &quot;KEGG_O_GLYCAN_BIOSYNTHESIS&quot; ## [2] &quot;KEGG_HYPERTROPHIC_CARDIOMYOPATHY_HCM&quot; ## [3] &quot;KEGG_ARRHYTHMOGENIC_RIGHT_VENTRICULAR_CARDIOMYOPATHY_ARVC&quot; ## [4] &quot;KEGG_PROTEASOME&quot; ## [5] &quot;KEGG_OOCYTE_MEIOSIS&quot; ## [6] &quot;KEGG_VASCULAR_SMOOTH_MUSCLE_CONTRACTION&quot; ## [7] &quot;KEGG_WNT_SIGNALING_PATHWAY&quot; ## [8] &quot;KEGG_HEDGEHOG_SIGNALING_PATHWAY&quot; ## [9] &quot;KEGG_FOCAL_ADHESION&quot; ## [10] &quot;KEGG_ECM_RECEPTOR_INTERACTION&quot; ## [11] &quot;KEGG_COMPLEMENT_AND_COAGULATION_CASCADES&quot; ## [12] &quot;KEGG_GNRH_SIGNALING_PATHWAY&quot; Answer to Environmental Health Question 8 With this, we can now answer Environmental Health Question #8: What biological pathways are disrupted in association with flaming pine needles exposure in the lung, identified through systems level analyses? Answer: Biological pathways involved in cardiopulmonary function (e.g., arrhythmogenic right ventricular cardiomyopathy, hypertrophic cardiomyopathy, vascular smooth muscle contraction), carcinogenesis signaling (e.g., Wnt signaling pathway, hedgehog signaling pathway), and hormone signaling (e.g., Gnrh signaling pathway), among others. Concluding Remarks In this module, users are guided through the uploading, organization, QA/QC, statistical analysis, and systems level analysis of an example -omics dataset based on transcriptomic responses to biomass burn scenarios, representing environmental exposure scenarios of growing concern worldwide. It is worth noting that the methods described herein represent a fraction of the approaches and tools that can be leveraged in the analysis of -omics datasets, and methods should be tailored to the purposes of each individual analysis’ goal. For additional example research projects that have leveraged -omics and systems biology to address environmental health questions, see the following select relevant publications: Genomic publications evaluating gene-environment interactions and relations to disease etiology: Balik-Meisner M, Truong L, Scholl EH, La Du JK, Tanguay RL, Reif DM. Elucidating Gene-by-Environment Interactions Associated with Differential Susceptibility to Chemical Exposure. Environ Health Perspect. 2018 Jun 28;126(6):067010. PMID: 29968567. Ward-Caviness CK, Neas LM, Blach C, Haynes CS, LaRocque-Abramson K, Grass E, Dowdy ZE, Devlin RB, Diaz-Sanchez D, Cascio WE, Miranda ML, Gregory SG, Shah SH, Kraus WE, Hauser ER. A genome-wide trans-ethnic interaction study links the PIGR-FCAMR locus to coronary atherosclerosis via interactions between genetic variants and residential exposure to traffic. PLoS One. 2017 Mar 29;12(3):e0173880. PMID: 28355232. Transcriptomic publications evaluating gene expression responses to environmental exposures and relations to disease etiology: Chang Y, Rager JE, Tilton SC. Linking Coregulated Gene Modules with Polycyclic Aromatic Hydrocarbon-Related Cancer Risk in the 3D Human Bronchial Epithelium. Chem Res Toxicol. 2021 Jun 21;34(6):1445-1455. PMID: 34048650. Chappell GA, Rager JE, Wolf J, Babic M, LeBlanc KJ, Ring CL, Harris MA, Thompson CM. Comparison of Gene Expression Responses in the Small Intestine of Mice Following Exposure to 3 Carcinogens Using the S1500+ Gene Set Informs a Potential Common Adverse Outcome Pathway. Toxicol Pathol. 2019 Oct;47(7):851-864. PMID: 31558096. Manuck TA, Eaves LA, Rager JE, Fry RC. Mid-pregnancy maternal blood nitric oxide-related gene and miRNA expression are associated with preterm birth. Epigenomics. 2021 May;13(9):667-682. PMID: 33890487. Epigenomic publications evaluating microRNA, CpG methylation, and/or histone methylation responses to environmental exposures and relations to disease etiology: Chappell GA, Rager JE. Epigenetics in chemical-induced genotoxic carcinogenesis. Curr Opinion Toxicol. 2017 Oct; 6:10-17. Rager JE, Bailey KA, Smeester L, Miller SK, Parker JS, Laine JE, Drobná Z, Currier J, Douillet C, Olshan AF, Rubio-Andrade M, Stýblo M, García-Vargas G, Fry RC. Prenatal arsenic exposure and the epigenome: altered microRNAs associated with innate and adaptive immune signaling in newborn cord blood. Environ Mol Mutagen. 2014 Apr;55(3):196-208. PMID: 24327377. Rager JE, Bauer RN, Müller LL, Smeester L, Carson JL, Brighton LE, Fry RC, Jaspers I. DNA methylation in nasal epithelial cells from smokers: identification of ULBP3-related effects. Am J Physiol Lung Cell Mol Physiol. 2013 Sep 15;305(6):L432-8. PMID: 23831618. Smeester L, Rager JE, Bailey KA, Guan X, Smith N, García-Vargas G, Del Razo LM, Drobná Z, Kelkar H, Stýblo M, Fry RC. Epigenetic changes in individuals with arsenicosis. Chem Res Toxicol. 2011 Feb 18;24(2):165-7. PMID: 21291286. Metabolomic publications evaluating changes in the metabolome in response to environmental exposures and involved in disease etiology: Lu K, Abo RP, Schlieper KA, Graffam ME, Levine S, Wishnok JS, Swenberg JA, Tannenbaum SR, Fox JG. Arsenic exposure perturbs the gut microbiome and its metabolic profile in mice: an integrated metagenomics and metabolomics analysis. Environ Health Perspect. 2014 Mar;122(3):284-91. PMID: 24413286; PMCID: PMC3948040. Manuck TA, Lai Y, Ru H, Glover AV, Rager JE, Fry RC, Lu K. Metabolites from midtrimester plasma of pregnant patients at high risk for preterm birth. Am J Obstet Gynecol MFM. 2021 Jul;3(4):100393. PMID: 33991707. Microbiome publications evaluating changes in microbiome profiles in relation to the environment and human disease: Chi L, Bian X, Gao B, Ru H, Tu P, Lu K. Sex-Specific Effects of Arsenic Exposure on the Trajectory and Function of the Gut Microbiome. Chem Res Toxicol. 2016 Jun 20;29(6):949-51.PMID: 27268458. Cho I, Blaser MJ. The human microbiome: at the interface of health and disease. Nat Rev Genet. 2012 Mar 13;13(4):260-70. PMID: 22411464. Lu K, Abo RP, Schlieper KA, Graffam ME, Levine S, Wishnok JS, Swenberg JA, Tannenbaum SR, Fox JG. Arsenic exposure perturbs the gut microbiome and its metabolic profile in mice: an integrated metagenomics and metabolomics analysis. Environ Health Perspect. 2014 Mar;122(3):284-91. PMID: 24413286. Exposome publications evaluating changes in chemical signatures in relation to the environment and human disease: Rager JE, Strynar MJ, Liang S, McMahen RL, Richard AM, Grulke CM, Wambaugh JF, Isaacs KK, Judson R, Williams AJ, Sobus JR. Linking high resolution mass spectrometry data with exposure and toxicity forecasts to advance high-throughput environmental monitoring. Environ Int. 2016 Mar;88:269-280. PMID: 26812473. Rappaport SM, Barupal DK, Wishart D, Vineis P, Scalbert A. The blood exposome and its role in discovering causes of disease. Environ Health Perspect. 2014 Aug;122(8):769-74. PMID: 24659601. Viet SM, Falman JC, Merrill LS, Faustman EM, Savitz DA, Mervish N, Barr DB, Peterson LA, Wright R, Balshaw D, O’Brien B. Human Health Exposure Analysis Resource (HHEAR): A model for incorporating the exposome into health studies. Int J Hyg Environ Health. 2021 Jun;235:113768. PMID: 34034040. Test Your Knowledge :::tyk Using “Module6_2_TYKInput1.csv” (gene counts) and “Module6_2_TYKInput2.csv” (sample info) data sets, which have already been run through the QC process described in this module and are ready for analysis: Conduct a differential expression analysis associated with “Season” using DESeq2. (Don’t worry about including any covariates or using RUV). Find the number of significant differentially expressed genes associated with “Season”, at the .05 level. "],["mixtures-analysis-methods-part-1-overview-and-example-with-quantile-g-computation.html", "6.3 Mixtures Analysis Methods Part 1: Overview and Example with Quantile G-Computation Introduction to Training Module Overview of Mixtures Analysis Introduction to Example Data Mixtures Model with Standard qqcomp Concluding Remarks Additional Resources", " 6.3 Mixtures Analysis Methods Part 1: Overview and Example with Quantile G-Computation This training module was developed by Dr. Lauren Eaves, Dr. Kyle Roell, and Dr. Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Historically, toxicology and epidemiology studies have largely focused on analyzing relationships between one chemical and one outcome at a time. This is still important in identifying the degree to which a single chemical exposure is associated with a disease outcome (e.g., UNC Superfund Research Program’s focus on inorganic arsenic exposure and its influence on metabolic disease). However, we are exposed, everyday, to many different stressors in our environment. It is therefore critical to deconvolute what co-occurring stressors (i.e., mixtures) in our environment impact human health! The field of mixtures research continues to grow to address this need, with the goals of developing methods to study environmental exposures using approaches to that better capture the mixture of exposures humans experience in real life. In this module, we will provide an overview of mixtures analysis methods and demonstrate how to use one of these methods, quantile g-computation, to analyzing chemical mixtures in a large geospatial epidemiologic study. Overview of Mixtures Analysis Mixtures Methods Relevance and Challenges Mixtures approaches are recently becoming more routine in environmental health because methodological advancements are just now making mixtures research more feasible. These advancements parallel the following: Advances in the ability to measure many different chemicals (e.g., through suspect screening and non-targeted chemical analysis approaches) and stressors (e.g., through improved collection and storage of survey data and clinical data) in our environment Improvements in data science to organize, store, and analyze big data Developments in statistical methodologies to parse relationships within these data Though statistical methodologies are still evolving, we will be discussing our current knowledge in this module. Some challenges that data analysts may experience when analyzing data from mixtures studies include the following: Size of mixture: As the number of components evaluated increases, your available analysis methods and statistical power may decrease Correlated data structure: Statistical challenge of collinearity: If data include large amounts of collinearity, this may dampen the observed effects from components that are highly correlated (e.g., may commonly co-occur) to other components Methodological challenge of co-occurring contaminant confounding: Co-occurring contaminant confounding may make it difficult to discern what is the true driver of the observed effect. Data analysis method selection: There are many different methods to choose from! A critical rule to address this challenge is to, first and foremost, lay out your study’s question. This question will then help guide your method selection, as discussed below. Overview of Mixtures Methods There are many methods that can be implemented to also elucidate relationships between individual chemicals/chemical groups in complex mixtures and their resulting toxicity/health effects. Some of the more common methods used in mixtures analyses, as identified by our team, are summarized in the below figure according to potential questions that could be asked in a study. Two of the methods, specifically quantile based g-computation (qgcomp) and bayesian kernel machine regression (BKMR) are highlighted as example mixtures scripted activities (qgcomp in this script and BKMR in Mixtures Methods 2). Throughout TAME 2.0 training materials, other methods are included such as Principal Component Analysis (PCA), K-means clustering, hierarchical clustering, and predictive modeling / machine learning (e.g., Random Forest modeling and variable selection). The following figure provides an overview of the types of questions that can be asked regarding mixtures and models that are commonly used to answer these questions: In this module, we will be using quantile based g-computation to analyze our data. This method is used for analysis of a total mixture effect as opposed to individual effects of mixture components. It is similar to previous, popular methods such as weighted quantile sum (WQS) regression, but does not assume directional homogeneity. It also provides access to models for non-additive and non-linear effects of the individual mixture components and overall mixture. Additionally, it runs very quickly and does not require as much computationally as other methods, making it an accessible option for those without access to many computational resources. Introduction to Example Data This script outlines single-contaminant (logistic regression) and multi-contaminant modeling approaches (Quantile G-Computation (qgcomp)). The workflow follows the steps used to generate results published in Eaves et al. 2023. This study examined the relationship between metals in private well water and the risk of preterm birth. The study population was all singleton, non-anomalous births in NC between 2003-2015. Pregnancies were assigned tract-level metal exposure based on maternal residence at delivery. The relationship with single metal exposure was examined with logistic regression and metal mixtures with qgcomp. For more info on qgcomp, see Keil et al. 2020 and the associated vignette. Note that for educational purposes, in this example we are using a randomly sampled dataset of 100,000 births, rather than the full dataset of &gt;1.3million (ie. using less than 10% of the full study population). Therefore the actual results of the analysis outlined below do not match the results published in the paper. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following questions: What is the risk of preterm birth associated with exposure to each of arsenic, lead, cadmium, chromium, manganese, copper and zinc via private well water individually? What is the risk of preterm birth associated with combined exposure to arsenic, lead, cadmium, chromium, manganese, copper and zinc (ie. a mixture) via private well water? Which of these chemicals has the strongest effect on preterm birth risk? Which of these chemicals increases the risk of preterm birth and which decreases the risk of preterm birth? Workspace Preparation Install packages as needed, then load the following packages: #load packages library(tidyverse) library(ggplot2) library(knitr) library(yaml) library(rmarkdown) library(broom) library(ggpubr) library(qgcomp) Optionally, you can also create a current date variable to name output files, and create an output folder. # Create a current date variable to name outputfiles cur_date &lt;- str_replace_all(Sys.Date(),&quot;-&quot;,&quot;&quot;) #Create an output folder Output_Folder &lt;- (&quot;Module6_3_Output/&quot;) Data Import cohort &lt;- read.csv(file=&quot;Module6_3_Input/Module6_3_InputData.csv&quot;) colnames(cohort) ## [1] &quot;ID&quot; &quot;year&quot; ## [3] &quot;sex&quot; &quot;lmpdate&quot; ## [5] &quot;racegp&quot; &quot;mage&quot; ## [7] &quot;mothed&quot; &quot;bwt&quot; ## [9] &quot;calcega&quot; &quot;clinega&quot; ## [11] &quot;plural&quot; &quot;delivery&quot; ## [13] &quot;rescity&quot; &quot;zipcode&quot; ## [15] &quot;state&quot; &quot;ga&quot; ## [17] &quot;preterm&quot; &quot;mage_cat&quot; ## [19] &quot;concep_date&quot; &quot;concep_month&quot; ## [21] &quot;season_concep&quot; &quot;smoke&quot; ## [23] &quot;Arsenic_perc&quot; &quot;Cadmium_perc&quot; ## [25] &quot;Calcium_perc&quot; &quot;Chromium_perc&quot; ## [27] &quot;Copper_perc&quot; &quot;Iron_perc&quot; ## [29] &quot;Lead_perc&quot; &quot;Manganese_perc&quot; ## [31] &quot;Magnesium_perc&quot; &quot;Zinc_perc&quot; ## [33] &quot;Sodium_perc&quot; &quot;Arsenic_limit&quot; ## [35] &quot;Cadmium_limit&quot; &quot;Chromium_limit&quot; ## [37] &quot;Copper_limit&quot; &quot;Iron_limit&quot; ## [39] &quot;Lead_limit&quot; &quot;Manganese_limit&quot; ## [41] &quot;Zinc_limit&quot; &quot;Arsenic.Mean_avg&quot; ## [43] &quot;Cadmium.Mean_avg&quot; &quot;Chromium.Mean_avg&quot; ## [45] &quot;Copper.Mean_avg&quot; &quot;Iron.Mean_avg&quot; ## [47] &quot;Lead.Mean_avg&quot; &quot;Manganese.Mean_avg&quot; ## [49] &quot;Zinc.Mean_avg&quot; &quot;percent_lowincome_2010&quot; ## [51] &quot;percent_lowincome_2015&quot; &quot;percent_lowincome_20102015_avg&quot; ## [53] &quot;Nitr&quot; &quot;mage_sq&quot; ## [55] &quot;Nitr_perc&quot; &quot;pov_perc&quot; head(cohort) ## ID year sex lmpdate racegp mage mothed bwt calcega clinega plural ## 1 9098140 2009 2 &lt;NA&gt; 1 38 2 3175 NA 39 1 ## 2 4105090 2004 1 2004-02-28 5 25 1 3175 37 37 1 ## 3 3015971 2003 1 2002-06-17 3 14 1 3119 37 38 1 ## 4 3103126 2003 1 2003-02-17 3 32 2 3544 39 40 1 ## 5 9030473 2009 2 2008-06-15 2 33 3 3317 40 39 1 ## 6 6024870 2006 2 2005-06-07 2 25 1 3430 40 39 1 ## delivery rescity zipcode state ga preterm mage_cat concep_date ## 1 2 DALLAS 28034 37 39 0 3 2009-01-12 ## 2 1 WAYNESVILLE 28786 37 37 0 2 2004-03-03 ## 3 1 GIBSONVILLE 27249 37 38 0 1 2002-06-14 ## 4 2 GARNER 27529 37 40 0 3 2003-02-10 ## 5 2 GREENVILLE 27858 37 39 0 3 2008-06-25 ## 6 2 EDEN 27288 37 39 0 2 2005-06-16 ## concep_month season_concep smoke Arsenic_perc Cadmium_perc Calcium_perc ## 1 1 1 0 2 2 1 ## 2 3 2 0 0 0 0 ## 3 6 3 0 0 0 1 ## 4 2 1 0 0 2 1 ## 5 6 3 0 0 0 2 ## 6 6 3 1 1 0 1 ## Chromium_perc Copper_perc Iron_perc Lead_perc Manganese_perc Magnesium_perc ## 1 1 0 2 1 1 0 ## 2 1 2 1 1 0 0 ## 3 2 1 1 1 1 1 ## 4 1 1 1 1 1 1 ## 5 0 0 0 0 0 0 ## 6 0 0 1 0 2 1 ## Zinc_perc Sodium_perc Arsenic_limit Cadmium_limit Chromium_limit Copper_limit ## 1 0 0 0 0 0 0 ## 2 1 0 0 0 0 0 ## 3 1 1 0 0 0 0 ## 4 1 1 0 0 0 0 ## 5 0 2 0 0 0 0 ## 6 0 0 0 0 0 0 ## Iron_limit Lead_limit Manganese_limit Zinc_limit Arsenic.Mean_avg ## 1 0 0 0 0 1.50407414 ## 2 1 0 0 0 0.08686698 ## 3 1 0 0 0 0.10619964 ## 4 1 0 0 0 0.07614543 ## 5 1 0 0 0 0.01302664 ## 6 1 0 0 0 0.40594302 ## Cadmium.Mean_avg Chromium.Mean_avg Copper.Mean_avg Iron.Mean_avg ## 1 0.100876988 0.62645522 13.4649631 5199.2885 ## 2 0.006774432 0.62262246 105.3293541 488.2481 ## 3 0.003405709 3.53144521 36.6958692 1472.2966 ## 4 0.120147641 1.00173292 58.6573330 699.0566 ## 5 0.002553201 0.00910553 0.3058865 275.1736 ## 6 0.001376889 0.18293440 15.1655788 985.0538 ## Lead.Mean_avg Manganese.Mean_avg Zinc.Mean_avg percent_lowincome_2010 ## 1 1.5978593 38.06972 54.29592 13.777468 ## 2 2.1463184 13.21209 253.92295 14.787742 ## 3 2.6009340 65.45814 541.73708 17.495030 ## 4 3.1221232 36.06194 511.58471 1.957295 ## 5 0.0305281 30.04164 13.40628 22.706422 ## 6 0.5590962 149.49924 21.95994 20.715677 ## percent_lowincome_2015 percent_lowincome_20102015_avg Nitr mage_sq ## 1 18.332801 16.055134 521.0576 1444 ## 2 18.355821 16.571782 388.9087 625 ## 3 9.235569 13.365300 506.7652 196 ## 4 6.642066 4.299681 NA 1024 ## 5 13.564499 18.135460 388.9087 1089 ## 6 29.359165 25.037421 388.9087 625 ## Nitr_perc pov_perc ## 1 0 2 ## 2 0 2 ## 3 0 1 ## 4 NA 0 ## 5 0 2 ## 6 0 3 Note: there are many steps prior to the modeling steps outlined below. These are being skipped for educational purposes. Additional steps include assessment of normality and transformations as needed, generation of a demographics table and assessing for missing data, imputation of missing data if needed, visualizing trends and distributions in the data, functional form assessments, decisions regarding what confounders to include etc. The following are the metals of interest: arsenic, lead, cadmium, chromium, manganese, copper, zinc. For each metal there are three exposure variables: [metal]_perc: 0: less than or equal to the 50th percentile, 1: above the 50th percentile and less than or equal to the 90th percentile, 3: above the 90th percentile [metal]_limit: 0: &lt;25% f well water tests for a given metal exceeded EPA regulatory standard, 1: 25% or over of well water tests for a given metal exceeded EPA regulatory standard [metal].Mean_avg: the mean concentration of the metal in the tract (ppb). Please see the Eaves et al. 2023 paper linked above for further information on these variables. Other variables of interest (outcome and covariates) in this dataset: preterm: 0= 37 weeks gestational age or greater, 1= less than 37 weeks gestational age mage: maternal age in years, continuous sex: sex of baby at birth: 1=M, 2=F racegp: maternal race ethnicity: 1=white non-Hispanic, 2=Black non-Hispanic, 3=Hispanic, 4=Asian/Pacific Islander, 5=American Indian, 6=other/unknown smoke: maternal smoking in pregnany: 0=non-smoker, 1=smoker season_conep: season of conception: 1=winter (Dec, Jan, Feb), 2=spring (Mar, Apr, May), 3=summer (June, Jul, Aug), 4=fall (Sept, Oct, Nov) mothed: mother’s education: 1=&lt;HS, 2=HS, 3=more than HS pov_perc: ACS estimates for poverty rate in tract: 0=less than or equal to 25th percentile, 1= greater than 25th percentile and less than or equal to 50th percentile, 2= greater than 50th percentile and less than or equal to 75th percentile, 3= greater than 75th percentile Nitr_perc: average of nitrites and nitrates in well water: 0= less than or equal to the 50th percentile, 1= above the 50th percentile and less than or equal to the 90th percentile, 2: above the 90th percentile Check variable formats Ensure that the outcome variable is binomial (factor) and has the correct reference level. Ensure that the exposure variables are categorical (factors). Ensure that covariates are in the correct variable format #outcome variable cohort &lt;- cohort %&gt;% mutate(preterm = as.factor(preterm)) cohort$preterm &lt;- relevel(cohort$preterm, ref = &quot;0&quot;) #exposure variables cohort &lt;- cohort %&gt;% mutate(Arsenic_perc=as.factor(Arsenic_perc)) %&gt;% mutate(Cadmium_perc=as.factor(Cadmium_perc)) %&gt;% mutate(Chromium_perc=as.factor(Chromium_perc)) %&gt;% mutate(Copper_perc=as.factor(Copper_perc)) %&gt;% mutate(Lead_perc=as.factor(Lead_perc)) %&gt;% mutate(Manganese_perc=as.factor(Manganese_perc)) %&gt;% mutate(Zinc_perc=as.factor(Zinc_perc)) %&gt;% mutate(Arsenic_limit=as.factor(Arsenic_limit)) %&gt;% mutate(Cadmium_limit=as.factor(Cadmium_limit)) %&gt;% mutate(Chromium_limit=as.factor(Chromium_limit)) %&gt;% mutate(Copper_limit=as.factor(Copper_limit)) %&gt;% mutate(Lead_limit=as.factor(Lead_limit)) %&gt;% mutate(Manganese_limit=as.factor(Manganese_limit)) %&gt;% mutate(Zinc_limit=as.factor(Zinc_limit)) #ensure covariates are in correct variable type form cohort &lt;- cohort %&gt;% mutate(racegp = as.factor(racegp)) %&gt;% mutate(mage = as.numeric(mage)) %&gt;% mutate(mage_sq = as.numeric(mage_sq)) %&gt;% mutate(smoke = as.numeric(smoke)) %&gt;% mutate(season_concep = as.factor(season_concep)) %&gt;% mutate(mothed = as.numeric(mothed)) %&gt;% mutate(Nitr_perc = as.numeric(Nitr_perc)) %&gt;% mutate(sex = as.factor(sex))%&gt;% mutate(pov_perc = as.factor(pov_perc)) Fit adjusted logistic regression models for each metal, for each categorical variable First, we will fit an adjusted logistic regression model for each metal, for each categorical variable, to demonstrate a variable by variable approach before diving into mixtures methods. Note that there are different regression techniques (linear and logistic are covered in another TAME module) and that here we will start with using percentage variables. metals &lt;- c(&quot;Arsenic&quot;,&quot;Cadmium&quot;,&quot;Chromium&quot;, &quot;Copper&quot;,&quot;Lead&quot;,&quot;Manganese&quot;,&quot;Zinc&quot;) for (i in 1:length(metals)) { metal &lt;- metals[[i]] metal &lt;- as.name(metal) print(metal) print(is.factor(eval(parse(text = paste0(&quot;cohort$&quot;,metal,&quot;_perc&quot;))))) #check that metal var is a factor mod &lt;- glm(preterm ~ eval(parse(text = paste0(metal,&quot;_perc&quot;))) + mage + mage_sq+ racegp + smoke + season_concep + mothed + Nitr_perc + pov_perc, family=binomial, data=cohort) mod_tid &lt;- tidy(mod, conf.int=TRUE, conf.level=0.95) %&gt;% mutate(model_name=paste0(metal,&quot;_adj_perc&quot;)) %&gt;% mutate(OR = exp(estimate)) %&gt;% mutate(OR.conf.high = exp(conf.high)) %&gt;% mutate(OR.conf.low = exp(conf.low)) mod_tid[2,1] &lt;- paste0(metal,&quot;_perc_50to90&quot;) mod_tid[3,1] &lt;- paste0(metal,&quot;_perc_over90&quot;) plot &lt;- mod_tid %&gt;% filter(grepl(&#39;perc_&#39;, term))%&gt;% ggplot(aes(OR, term, xmin = OR.conf.low, xmax = OR.conf.high, height = 0)) + geom_point() + scale_x_continuous(trans=&quot;log10&quot;)+ geom_errorbarh() assign(paste0(metal,&quot;_adj_perc&quot;),mod_tid) assign(paste0(metal,&quot;_adj_perc_plot&quot;),plot) } ## Arsenic ## [1] TRUE ## Cadmium ## [1] TRUE ## Chromium ## [1] TRUE ## Copper ## [1] TRUE ## Lead ## [1] TRUE ## Manganese ## [1] TRUE ## Zinc ## [1] TRUE Plot the results: perc_plots &lt;- ggarrange(Arsenic_adj_perc_plot, Cadmium_adj_perc_plot, Chromium_adj_perc_plot, Copper_adj_perc_plot) plot(perc_plots) perc_plots1 &lt;- ggarrange(Lead_adj_perc_plot, Manganese_adj_perc_plot, Zinc_adj_perc_plot) plot(perc_plots1) Save the plots: tiff(file = (paste0(Output_Folder,&quot;/&quot;, cur_date, &quot;_NCbirths_pretermbirth_singlemetal_adjusted_models_percplots_1.tiff&quot;)), width = 10, height = 8, units = &quot;in&quot;, pointsize = 12, res = 600) plot(perc_plots) dev.off() tiff(file = (paste0(Output_Folder,&quot;/&quot;, cur_date, &quot;_NCbirths_pretermbirth_singlemetal_adjusted_models_percplots_2.tiff&quot;)), width = 10, height = 8, units = &quot;in&quot;, pointsize = 12, res = 600) plot(perc_plots1) dev.off() We can also run the analysis using limit variables: for (i in 1:length(metals)) { metal &lt;- metals[[i]] metal &lt;- as.name(metal) print(metal) print(is.factor(eval(parse(text = paste0(&quot;cohort$&quot;,metal,&quot;_limit&quot;))))) #check that metal var is a factor mod &lt;- glm(preterm ~ eval(parse(text = paste0(metal,&quot;_limit&quot;)))+ mage + mage_sq+ racegp + smoke + season_concep + mothed + Nitr_perc + pov_perc, family=binomial, data=cohort) mod_tid &lt;- tidy(mod, conf.int=TRUE, conf.level=0.95) %&gt;% mutate(model_name=paste0(metal,&quot;_adj_limit&quot;)) %&gt;% mutate(OR = exp(estimate)) %&gt;% mutate(OR.conf.high = exp(conf.high)) %&gt;% mutate(OR.conf.low = exp(conf.low)) mod_tid[2,1] &lt;- paste0(metal,&quot;_limit_over25perc&quot;) plot &lt;- mod_tid %&gt;% filter(grepl(&#39;limit&#39;, term))%&gt;% ggplot(aes(OR, term, xmin = OR.conf.low, xmax = OR.conf.high, height = 0)) + geom_point() + scale_x_continuous(trans=&quot;log10&quot;)+ geom_errorbarh() assign(paste0(metal,&quot;_adj_limit&quot;),mod_tid) assign(paste0(metal,&quot;_adj_limit_plot&quot;),plot) } ## Arsenic ## [1] TRUE ## Cadmium ## [1] TRUE ## Chromium ## [1] TRUE ## Copper ## [1] TRUE ## Lead ## [1] TRUE ## Manganese ## [1] TRUE ## Zinc ## [1] TRUE Note: you will get this warning for some of the models: “Warning: glm.fit: fitted probabilities numerically 0 or 1”. This is because for the variability in the exposure data, ideally the sample size would be larger (as noted above the analysis this draws from was completed on &gt;1.3million observations). Plot the results: limit_plots &lt;- ggarrange(Arsenic_adj_limit_plot, Cadmium_adj_limit_plot, Chromium_adj_limit_plot, Copper_adj_limit_plot) plot(limit_plots) limit_plots1 &lt;- ggarrange(Lead_adj_limit_plot, Manganese_adj_limit_plot, Zinc_adj_limit_plot) plot(limit_plots1) Save the plots: tiff(file = (paste0(Output_Folder,&quot;/&quot;, cur_date, &quot;_NCbirths_pretermbirth_singlemetal_adjusted_models_limitplots1.tiff&quot;)), width = 10, height = 8, units = &quot;in&quot;, pointsize = 12, res = 600) plot(limit_plots) dev.off() tiff(file = (paste0(Output_Folder,&quot;/&quot;, cur_date, &quot;_NCbirths_pretermbirth_singlemetal_adjusted_models_limitplots2.tiff&quot;)), width = 10, height = 8, units = &quot;in&quot;, pointsize = 12, res = 600) plot(limit_plots1) dev.off() Merge all of the logistic regression model results. This is the data frame that you could export for supplementary material or to view the results in Excel. #merge all model output results_df &lt;- rbind(Arsenic_adj_perc, Arsenic_adj_limit, Cadmium_adj_perc, Cadmium_adj_limit, Chromium_adj_perc, Chromium_adj_limit, Copper_adj_perc, Copper_adj_limit, Lead_adj_perc, Lead_adj_limit, Manganese_adj_perc, Manganese_adj_limit, Zinc_adj_perc, Zinc_adj_limit) To select only the coefficients related to the primary exposures: results_df &lt;- results_df %&gt;% filter(str_detect(term, &#39;limit|50to90|over90&#39;)) This file outputs the coefficients and the odds ratios (OR) of the logistic regression models all together. + The ORs in associated with [metal]_perc_50to90 can be interpreted as the OR comparing the odds of preterm birth among individuals in the 50th to 90th percentile of [metal] exposure compared to those below the 50th. + The ORs in associated with [metal]_perc_over90 can be interpreted as the OR comparing the odds of preterm birth among individuals above the 90th percentile of [metal] exposure compared to those below the 50th. + The ORs in associated with [metal]_limit_over25perc can be interpreted as the OR comparing the odds of preterm birth among individuals living in census tracts in with tests exceeding the an EPA standard for [metal] in 25% or more tests versus tracts with less that 25% of tests exceeding the standard Answer to Environmental Health Question 1 With this, we can answer also Environmental Health Question #1: What is the risk of preterm birth associated with exposure to each of arsenic, lead, cadmium, chromium, manganese, copper and zinc via private well water individually? Answer: Using the interpretation guides described in the prior paragraph and the “_NCbirths_pretermbirth_singlemetal_adjusted_models.csv” file, you can answer this question. For example, for cadmium, compared to individuals residing in census tracts with cadmium below the 50th percentile, those residing in tracts with lead between the 50th and 90th percentile had a 7% increase in the adjusted odds of PTB (aOR 1.07 (95% CI: 1.00,1.14)) and those in tracts with cadmium above the 90th percentile had a 8% increased adjusted odds of PTB (aOR 1.08 (95% CI: 0.97,1.20). Compared to individuals in tracts with less than 25% of tests exceeding the standard for lead (note this is the EPA treatment technique action level=15 ppb), individuals residing in census tracts where 25% or more of tests exceeded the MCL had 1.23 (95% CI: 0.81,1.81) times the adjusted odds of preterm birth. IMPORTANT NOTE: as described above, these results differ from the publication (Eaves et al. 2023) because this scripted example is conducted on a smaller subsetted dataset. While the single contaminant models provide useful information, they cannot inform us of the effect of multiple simultaneous exposures or account for co-occurring contaminant confounding. Therefore, we want to utilize quantile g-compuation to assess mixtures. Mixtures Model with Standard qqcomp #list of exposure variables Xnm &lt;- c(&#39;Arsenic.Mean_avg&#39;, &#39;Cadmium.Mean_avg&#39;, &#39;Lead.Mean_avg&#39;, &#39;Manganese.Mean_avg&#39;, &#39;Chromium.Mean_avg&#39;, &#39;Copper.Mean_avg&#39;, &#39;Zinc.Mean_avg&#39;) #list of covariates covars = c(&#39;mage&#39;,&#39;mage_sq&#39;,&#39;racegp&#39;,&#39;smoke&#39;,&#39;season_concep&#39;,&#39;mothed&#39;,&#39;Nitr_perc&#39;,&#39;pov_perc&#39;) #fit adjusted model PTB_adj_ppb &lt;- qgcomp.noboot(preterm~., expnms=Xnm, dat=cohort[,c(Xnm,covars,&#39;preterm&#39;)], family=binomial(), q=4) In English, “preterm~.” is saying fit a model that has preterm (1/0) as the dependent variable and then the independent variables (exposures and covariates) are all other variables in the dataset (“.”). “expnms=Xnm” is saying that the mixture of exposures is given by the vector “Xnm,” defined above. “dat=cohort[,c(Xnm,covars,‘preterm’)]” is saying that the dataset to be used to fit this model includes all columns in the cohort dataset that are listed in the “Xnm” and “covars” vectors and also the “preterm” variable. “family=binomial()” is saying that the outcome is a binary outcome and therefore the model will fit a logistic regression model. “q=4” is saying break the exposures into quartiles, other options woudl be q=3 for teriltes, q=5 for quintiles and so forth. This is a summary of the qgcomp model output PTB_adj_ppb ## Scaled effect size (positive direction, sum of positive coefficients = 0.0969) ## Cadmium.Mean_avg Chromium.Mean_avg Manganese.Mean_avg Zinc.Mean_avg ## 0.4556 0.4027 0.1006 0.0412 ## ## Scaled effect size (negative direction, sum of negative coefficients = -0.0532) ## Arsenic.Mean_avg Copper.Mean_avg Lead.Mean_avg ## 0.452 0.396 0.152 ## ## Mixture log(OR) (Delta method CI): ## ## Estimate Std. Error Lower CI Upper CI Z value Pr(&gt;|z|) ## (Intercept) -2.189951 0.270100 -2.719337 -1.66056 -8.1079 4.441e-16 ## psi1 0.043705 0.025982 -0.007219 0.09463 1.6821 0.09255 This output can be interpreted as: Cadmium, chromium, manganese and zinc had positive effects, as in they increased the risk of preterm birth. Arsenic, coppper and lead had negative effects, as in they reduced the risk of preterm birth. The total effect of all positive acting mixture components is given by the sum of positive coefficients = 0.0969, total effect of all negative acting mixture components is given by the sum of negative coefficients = -0.0532. The numbers underneath each of the individual mixture component are the weights assigned to each component. These sum to 1 in each direction. They represent the relative contribution of each component to the effect in that direction. If only one components was acting in the positive or negative direction, it would have a weight of 1. A component’s weight multiplied by the sum of the coefficient’s in the relevant direction is that individual component’s coefficient and represents the independent effect of that component (e.g. cadmium log(OR) = 0.0969*0.4556=0.0441). The overall mixture effect (i.e. the log(OR) when all exposures are increased by one quartile) is given by psi1. Here it equals 0.0437. Note that this value is equal to combining the sum of coefficients in the positive direction adn the sum in the negative direction (ie. 0.0969-0.0532= 0.0437) IMPORTANT NOTE: as described above, these results differ from the publication (Eaves et al. 2023) because this scripted example is conducted on a smaller subsetted dataset. This is the plot that gives you the weights of the components plot(PTB_adj_ppb) To save the plot: tiff(file = (paste0(Output_Folder,&quot;/&quot;, cur_date, &quot;_NCbirths_pretermbirth_qgcomp_weights.tiff&quot;)), width = 10, height = 8, units = &quot;in&quot;, pointsize = 12, res = 600) plot(PTB_adj_ppb) dev.off() Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: What is the risk of preterm birth associated with combined exposure to arsenic, lead, cadmium, chromium, manganese, copper and zinc (ie. a mixture) via private well water? Answer: When all exposures (arsenic, lead, cadmium, chromium, manganese, copper and zinc) are increased in concentration by one quartile the odds ratio is 1.044 (exp(0.043705)). IMPORTANT NOTE: as described above, these results differ from the publication (Eaves et al. 2023) because this scripted example is conducted on a smaller subsetted dataset. Answer to Environmental Health Question 3 With this, we can answer also Environmental Health Question #3: Which of these chemicals has the strongest effect on preterm birth risk? Answer: The mixture component with the strongest effect is the one that has the largest independent effect given my the component’s coefficient (which can be calculated by (sum of coefficients in relevant direction)(component weight), and, as shown below can also be generated to ouput into results files). In this case, the components with the largest independent effect is cadmium (0.09690.4556=0.0441). IMPORTANT NOTE: as described above, these results differ from the publication (Eaves et al. 2023) because this scripted example is conducted on a smaller subsetted dataset. Answer to Environmental Health Question 4 With this, we can answer also Environmental Health Question #4: Which of these chemicals increases the risk of preterm birth and which decreases the risk of preterm birth? Answer: This is indicated by the direction of effect for each component. Thus, the mixture components that increase the risk of preterm birth are cadmium, chromium, manganese and zinc, while the mixture components that decrease the risk of preterm birth are arsenic, copper and lead. IMPORTANT NOTE: as described above, these results differ from the publication (Eaves et al. 2023) because this scripted example is conducted on a smaller subsetted dataset. We can export the mixtures modeling results using the following code, which stores the data in three different files: + Results_SlopeParams outputs the overall mixture effect results + Results_MetalCoeffs outputs the individual mixture components (metals) coefficients. Note that this will also output coefficient for covariates included in the model. + Results_MetalWeights outputs the individual mixture components (metals) weights allmodels &lt;- c(&quot;PTB_adj_ppb&quot;) #if you run more than one qgcomp model, list them here and the following code can output the results in clean format all together clean_print &lt;- function(x){ output = data.frame( x$coef, sqrt(x$var.coef), x$ci.coef, x$pval ) names(output) = c(&quot;Estimate&quot;, &quot;Std. Error&quot;, &quot;Lower CI&quot;, &quot;Upper CI&quot;, &quot;p value&quot;) return(output) } Results_SlopeParams &lt;- data.frame() #empty vector to append dfs to for (i in allmodels){ print(i) df &lt;- eval(parse(text = paste0(&quot;clean_print(&quot;,i,&quot;)&quot;))) %&gt;% rownames_to_column(&quot;Parameter&quot;) %&gt;% mutate(&quot;Model&quot; = i) Results_SlopeParams &lt;- rbind(Results_SlopeParams,df) } Results_SlopeParams &lt;- Results_SlopeParams %&gt;% mutate(OR=exp(Estimate)) %&gt;% mutate(UpperCI_OR=exp(`Upper CI`)) %&gt;% mutate(LowerCI_OR=exp(`Lower CI`)) Results_MetalCoeffs &lt;- data.frame() for (i in allmodels){ print(i) df &lt;- eval(parse(text = paste0(&quot;as.data.frame(summary(&quot;,i,&quot;$fit)$coefficients[,])&quot;))) %&gt;% mutate(&quot;Model&quot; = i) df &lt;- df %&gt;% rownames_to_column(var=&quot;variable&quot;) Results_MetalCoeffs&lt;- rbind(Results_MetalCoeffs,df) } Results_MetalWeights &lt;- data.frame() for (i in allmodels){ Results_PWeights &lt;- eval(parse(text = paste0(&quot;as.data.frame(&quot;,i,&quot;$pos.weights)&quot;))) %&gt;% rownames_to_column(&quot;Metal&quot;) %&gt;% dplyr::rename(&quot;Weight&quot; = 2) %&gt;% mutate(&quot;Weight Direction&quot; = &quot;Positive&quot;) Results_NWeights &lt;- eval(parse(text = paste0(&quot;as.data.frame(&quot;,i,&quot;$neg.weights)&quot;))) %&gt;% rownames_to_column(&quot;Metal&quot;) %&gt;% dplyr::rename(&quot;Weight&quot; = 2) %&gt;% mutate(&quot;Weight Direction&quot; = &quot;Negative&quot;) Results_Weights &lt;- rbind(Results_PWeights, Results_NWeights) %&gt;% mutate(&quot;Model&quot; = i) %&gt;% as.data.frame() Results_MetalWeights &lt;- rbind(Results_MetalWeights, Results_Weights) } write.csv(Results_SlopeParams, paste0(Output_Folder,&quot;/&quot;, cur_date, &quot;_qgcomp_Results_SlopeParams.csv&quot;), row.names=TRUE) write.csv(Results_MetalCoeffs, paste0(Output_Folder,&quot;/&quot;, cur_date, &quot;_qgcomp_Results_MetalCoeffs.csv&quot;), row.names=TRUE) write.csv(Results_MetalWeights, paste0(Output_Folder,&quot;/&quot;, cur_date, &quot;_qgcomp_Results_MetalWeights.csv&quot;), row.names=TRUE) Concluding Remarks In conclusion, this module reviews a suite of methodologies researches can use to answer different questions relevant to environmental mixtures and their relationships to health outcomes. In this specific scripted example we utilized a large epidemiological dataset (for educational purposes, subsetted to a reduced sample size), to demonstrate using logistic regression to assess single contaminant associations with a health outcome (preterm birth) and quantile g computation to assess mixture effects with a health outcome. Additional Resources The field of mixtures is vast, with many different approaches and example studies to learn from as analysts lead in their own analyses. Some resources that can be helpful include the following reviews: Our recent review on mixtures methodologies, particularly in the field of sufficient similarity, titled Wrangling whole mixtures risk assessment: Recent advances in determining sufficient similarity Two more general, epidemiology-focused reviews on mixtures questions and methodologies, titled Complex Mixtures, Complex Analyses: an Emphasis on Interpretable Results and Environmental exposure mixtures: questions and methods to address them A helpful online toolkit for mixtures analyses generated by Andrea Bellavia, PhD Some helpful mixtures case studies include the following: Our recent study that implemented quantile g-computation statistics to identify chemicals present in wildfire smoke emissions that impact toxicity, published as the following: Rager JE, Clark J, Eaves LA, Avula V, Niehoff NM, Kim YH, Jaspers I, Gilmour MI. Mixtures modeling identifies chemical inducers versus repressors of toxicity associated with wildfire smoke. Sci Total Environ. 2021 Jun 25;775:145759. PMID: 33611182. Another study from our group that implemented quantile g-computation identify placental gene networks that had altered expression in response to cord tissue mixtures of metals, published as the following: Eaves LA, Bulka CM, Rager JE, Galusha AL, Parsons PJ, O’Shea TM and Fry RC. Metals mixtures modeling identifies birth weight-associated gene networks in the placentas of children born extremely preterm. Chemosphere. 2022;137469.PMID:36493891 Many other groups also leverage quantile g-computation, with the following as exemplar case studies: Prenatal exposure to consumer product chemical mixtures and size for gestational age at delivery Use of personal care product mixtures and incident hormone-sensitive cancers in the Sister Study: A U.S.-wide prospective cohort Test Your Knowledge Using the metals dataset within the qgcomp package (see the package vignette for how to access), answer the following three mixtures-related environmental health questions using quantile g-computation, focusing on a mixture of arsenic, copper, zinc and lead: What is the risk of disease associated with combined exposure to each of the chemicals? Which of these chemicals has the strongest effect on disease? Which of these chemicals increases the risk of disease and which decreases the risk of disease? Note that disease is given by the variable “disease_state” (1 = case, 0 = non-case). "],["mixtures-analysis-methods-part-2-bayesian-kernel-machine-regression.html", "6.4 Mixtures Analysis Methods Part 2: Bayesian Kernel Machine Regression Introduction to Training Module Introduction to Example Data Introduction to BKMR Run BKMR Concluding Remarks Additional Resources", " 6.4 Mixtures Analysis Methods Part 2: Bayesian Kernel Machine Regression This training module was developed by Dr. Lauren Eaves, Dr. Kyle Roell, and Dr. Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module In this training module, we will continue to explore mixtures analysis method, this time with a scripted example of Bayesian Kernel Machine Regression (BKMR). Please refer to TAME 2.0 Module 6.3, Mixtures Analysis Methods Part 1: Overview and Example with Quantile G-Computation for an overview of mixtures methodologies and a scripted example using Quantile g-Computation. Introduction to Example Data In this scripted example, we will use a dataset from the Extremely Low Gestational Age Newborn (ELGAN) cohort. Specifically, we will analyze metal mixtures assessed in cord tissue collected at delivery with neonatal inflammation measured over the first two weeks of life. For more information on the cord tissue metals data, please see the following two publications: Eaves LA, Bulka CM, Rager JE, Galusha AL, Parsons PJ, O’Shea TM and Fry RC. Metals mixtures modeling identifies birth weight-associated gene networks in the placentas of children born extremely preterm. Chemosphere. 2022;137469. doi:10.1016/j.chemosphere.2022.137469. Bulka CM, Eaves LA, Gardner AJ, Parsons PJ, Kyle RR, Smeester L, O”Shea TM, Fry RC. Prenatal exposure to multiple metallic and metalloid trace elements and the risk of bacterial sepsis in extremely low gestational age newborns: A prospective cohort study. Front Epidemiol. 2022;2. doi:10.3389/fepid.2022.958389 For more information on the neonatal inflammation data, please see the following publication: Eaves LA, Enggasser AE, Camerota M, Gogcu S, Gower WA, Hartwell H, Jackson WM, Jensen E, Joseph RM, Marsit CJ, Roell K, Santos HP Jr, Shenberger JS, Smeester L, Yanni D, Kuban KCK, O’Shea TM, Fry RC. CpG methylation patterns in placenta and neonatal blood are differentially associated with neonatal inflammation. Pediatr Res. June 2022. doi:10.1038/s41390-022-02150-4 Here, we have a dataset of n=254 participants for which we have complete data on neonatal inflammation, cord tissue metals and key demographic variables that will be included as confounders in the analysis. Extensive research in the ELGAN study has demonstrated that neonatal inflammation is predictive of cerebral palsy, ASD, ADHD, obesity, cognitive impairment, attention problems,cerebral white matter damage, and decreased total brain volume, among other adverse outcomes. Therefore identifying exposures that lead to neonatal inflammation and could be intervened upon to reduce the risk of neonatal inflammation is critical to improve neonatal health. Environmental exposures during pregnancy such as metals may contribute to neonatal inflammation. As is often the case in environmental health, these chemical exposures are likely co-occurring and therefore mixtures methods are needed. Introduction to BKMR BKMR offers a flexible, non-parametric method to estimate: The single exposure effect: odds ratio of inflammation when a single exposure is at its 75th percentile compared to its 25th percentile, with other exposures at their 50th percentile and covariates held constant The overall mixture effect: odds ratio of inflammation when all exposures are fixed at their 75th percentile compared to when all of the factors are fixed to their 25th percentile; The interactive effect: the difference in the single-exposure effect when all of the other exposures are fixed at their 75th percentile, as compared to when all of the other factors are fixed at their 25th percentile; There are numerous excellent summaries of BKMR, including the publications in which it was first introduced: Bobb et al. Bayesian kernel machine regression for estimating the health effects of multi-pollutant mixtures Bobb et al. Statistical software for analyzing the health effects of multiple concurrent exposures via Bayesian kernel machine regression And other vignettes and toolkits including: Jennifer Bobb’s Introduction to Bayesian kernel machine regression and the bkmr R package Andrea Bellavia’s Bayesian kernel machine regression While BKMR can do many things other methods cannot, it can require a lot of computational resources and take a long time to run. Before working with your final dataset and analysis, if very large or complex, it is often recommended to start with a smaller sample to make sure everything is working correctly before starting an analysis that make takes days to complete. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following questions, which mirror the questions in TAME 2.0 Module 6.3, but are just in a different order: Which of these chemicals has the strongest effect on neonatal inflammation risk? Which of these chemicals increases the risk of neonatal inflammation and which decreases the risk of neonatal inflammation? What is the risk of neonatal inflammation associated with exposure to each of manganese, copper, zinc, arsenic, selenium, cadmium, mercury, lead individually? What is the risk of neonatal inflammation associated with combined exposure to manganese, copper, zinc, arsenic, selenium, cadmium, mercury, lead (ie. a mixture)? and in addition to the questions addressed in Mixtures Methods 1, we additionally can answer: Are there interactions among manganese, copper, zinc, arsenic, selenium, cadmium, mercury, lead in relation to neonatal inflammation? Run BKMR Workspace Preparation Install packages as needed, then load the following packages: #load packages library(tidyverse) library(ggplot2) library(knitr) library(yaml) library(rmarkdown) library(broom) library(ggpubr) library(bkmr) Optionally, you can also create a current date variable to name output files, and create an output folder. #Create a current date variable to name outputfiles cur_date &lt;- str_replace_all(Sys.Date(),&quot;-&quot;,&quot;&quot;) #Create an output folder Output_Folder &lt;- (&quot;Module6_4_Output/&quot;) Data Import cohort &lt;- read.csv(file=&quot;Module6_4_Input/Module6_4_InputData.csv&quot;) colnames(cohort) ## [1] &quot;X&quot; &quot;id&quot; &quot;inflam_intense&quot; &quot;race1&quot; ## [5] &quot;sex&quot; &quot;gadays&quot; &quot;magecat&quot; &quot;medu&quot; ## [9] &quot;smoke&quot; &quot;Mn_ugg&quot; &quot;Cu_ugg&quot; &quot;Zn_ugg&quot; ## [13] &quot;As_ngg&quot; &quot;Se_ugg&quot; &quot;Cd_ngg&quot; &quot;Hg_ngg&quot; ## [17] &quot;Pb_ngg&quot; head(cohort) ## X id inflam_intense race1 sex gadays magecat medu smoke Mn_ugg Cu_ugg ## 1 1 1100751 0 3 1 195 2 1 0 0.272 3.229 ## 2 3 1100841 0 2 0 171 3 3 0 0.505 2.868 ## 3 4 1100852 0 1 0 171 3 3 0 0.387 5.504 ## 4 5 1100853 0 1 0 171 3 3 0 0.441 4.295 ## 5 6 1100872 0 1 1 167 2 4 0 0.457 14.310 ## 6 7 1100881 0 1 1 194 2 2 0 0.388 3.816 ## Zn_ugg As_ngg Se_ugg Cd_ngg Hg_ngg Pb_ngg ## 1 77.6 6.29 0.86 0.78 17.23 26.2 ## 2 58.1 8.04 0.93 3.79 20.35 305.3 ## 3 60.9 3.48 1.02 1.19 20.30 50.3 ## 4 56.8 4.63 1.08 1.37 21.06 26.5 ## 5 133.4 7.48 1.98 3.74 12.33 72.9 ## 6 81.4 3.89 1.04 1.17 10.54 35.3 The variables in this dataset include sample and demographic information and cort tissue metal exposure in \\(mu\\)g/g or ng/g. Sample and Demographic Variables “id”: unique study ID outcome: “inflam_intense”: 1= high inflammation, 0=low inflammation covariates: “race1”: maternal race, 1=White, 2=Black, 0=Other “sex”: neonatal sex, 0=female, 1=male “gadays”: gestational age at delivery in days “magecat”: maternal age, 1= &lt;21, 2=21-35, 3= &gt; 35 “medu”:maternal education: 1= &lt;12, 2=12, 3=13-15, 4=16, 5= &gt;16 “smoke”: maternal smoking while pregnant, 0=no, 1=yes Exposure Variables “Mn_ugg” “Cu_ugg” “Zn_ugg” “As_ngg” “Se_ugg” “Cd_ngg” “Hg_ngg” “Pb_ngg” There are many steps prior to the modeling steps outlined below. These are being skipped for educational purposes. Additional steps include assessment of normality and transformations as needed, generation of a demographics table and assessing for missing data, imputation of missing data if needed, visualizing trends and distributions in the data, assessing correlations between exposures, functional form assessments, and decisions regarding what confounders to include. In addition, it is highly recommended to conduct single-contaminant modeling initially to understand individual chemical relationships with the outcomes of focus before conducting mixtures assessment. For an example of this, see TAME 2.0 Module 6.3, Mixtures Analysis Methods Part 1: Overview and Example with Quantile G-Computation. BKMR, as a flexible non-parametric modeling approach, does not allow for classical null-hypothesis testing, and 95% CI are interpreted as credible intervals, not confidence intervals. One approach therefore could be to explore non-linearities and interactions within BKMR to then validate generated hypotheses using quantile g-computation. Fit the BKMR Model First, define a matrix/vector of the exposure mixture, outcome, and confounders/covariates. BKMR performs better when the exposures are on a similar scale and when there are not outliers. Thus, we center and scale the exposure variables first. As noted above, in a complete analysis, thorough examination of exposure variable distributions, including outliers and normality, would be conducted before any exposure-outcome modeling. For more information on normality testing, see TAME 2.0 Module 3.3, Normality Tests and Data Transformations. First, we’ll assign the matrix variables to their own data frame and scale the data. #exposure mixture variables mixture &lt;- as.matrix(cohort[,10:17]) mixture &lt;- log(mixture) mixture &lt;-scale(mixture, center=TRUE) summary(mixture) ## Mn_ugg Cu_ugg Zn_ugg As_ngg ## Min. :-3.3963 Min. :-2.6253 Min. :-2.0090 Min. :-1.9903 ## 1st Qu.:-0.5536 1st Qu.:-0.6558 1st Qu.:-0.5211 1st Qu.:-0.6150 ## Median :-0.1527 Median :-0.1452 Median :-0.1559 Median :-0.1327 ## Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 ## 3rd Qu.: 0.4048 3rd Qu.: 0.6038 3rd Qu.: 0.3180 3rd Qu.: 0.5385 ## Max. : 7.3059 Max. : 4.3512 Max. : 5.4446 Max. : 3.8928 ## Se_ugg Cd_ngg Hg_ngg Pb_ngg ## Min. :-3.468991 Min. :-1.5007 Min. :-2.34070 Min. :-2.0013 ## 1st Qu.:-0.567767 1st Qu.:-0.6649 1st Qu.:-0.70797 1st Qu.:-0.6711 ## Median : 0.005317 Median :-0.2632 Median :-0.01572 Median :-0.1036 ## Mean : 0.000000 Mean : 0.0000 Mean : 0.00000 Mean : 0.0000 ## 3rd Qu.: 0.629320 3rd Qu.: 0.3312 3rd Qu.: 0.67767 3rd Qu.: 0.5312 ## Max. : 4.328620 Max. : 5.3945 Max. : 2.74547 Max. : 3.8156 Then, we’ll define the outcome variable and ensure it is the proper class and leveling. #outcome variable cohort$inflam_intense &lt;-as.factor(cohort$inflam_intense) cohort$inflam_intense &lt;- relevel(cohort$inflam_intense, ref = &quot;0&quot;) y&lt;-as.numeric(as.character(cohort$inflam_intense)) Next, we’ll assign the covariates to a matrix. #covariates covariates&lt;-as.matrix(cohort[,7:9]) Then, we can fit the BKMR model. Note that this script will take a few minutes to run. set.seed(111) fitkm &lt;- kmbayes(y = y, Z = mixture, X = covariates, iter = 5000, verbose = FALSE, varsel = TRUE, family=&quot;binomial&quot;, est.h = TRUE) ## Fitting probit regression model ## Iteration: 500 (10% completed; 18.49358 secs elapsed) ## Iteration: 1000 (20% completed; 42.91745 secs elapsed) ## Iteration: 1500 (30% completed; 1.21062 mins elapsed) ## Iteration: 2000 (40% completed; 1.68292 mins elapsed) ## Iteration: 2500 (50% completed; 2.1736 mins elapsed) ## Iteration: 3000 (60% completed; 2.66898 mins elapsed) ## Iteration: 3500 (70% completed; 3.15693 mins elapsed) ## Iteration: 4000 (80% completed; 3.61519 mins elapsed) ## Iteration: 4500 (90% completed; 4.09723 mins elapsed) ## Iteration: 5000 (100% completed; 4.56782 mins elapsed) For full information regarding options for the kmbayes function, refer to the BKMR reference manual: https://cran.r-project.org/web/packages/bkmr/bkmr.pdf Assess Variable Importance BKMR conducts a variable selection procedure and generates posterior inclusion probabilities (PIP). The larger the PIP, the more a variable is contributing to the overall exposure-outcome effect. These are relative to each other,so there is no threshold as to when a variable becomes an “important” contributor (similar to the weights in quantile g-computation). ExtractPIPs(fitkm) ## variable PIP ## 1 Mn_ugg 0.1468 ## 2 Cu_ugg 0.1568 ## 3 Zn_ugg 0.4760 ## 4 As_ngg 0.0820 ## 5 Se_ugg 0.1668 ## 6 Cd_ngg 0.3204 ## 7 Hg_ngg 0.1848 ## 8 Pb_ngg 0.1676 Relative to each other, the contributions to the effect of the mixture on neonatal inflammation are shown above for each component of the mixture. Note that if a variable PIP=0, BKMR will drop it from the model and the overall mixture effect will not include this exposure. Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question #1: Which of these chemicals has the strongest effect on neonatal inflammation risk? Answer: Based on the PIPs: Cadmium. Assess Model Convergence We can use trace plots to evaluate how the parameters in the model converge over the many iterations. We hope to see that the line moves randomly but centers around a straight line sel&lt;-seq(0,5000,by=1) TracePlot(fit = fitkm, par = &quot;beta&quot;, sel=sel) Based on this plot, it looks like the burn in period is roughly 1000 iterations. We will remove these from the results. sel&lt;-seq(1000,5000,by=1) TracePlot(fit = fitkm, par = &quot;beta&quot;, sel=sel) Presenting Model Results Single exposure effects As described above, one way to examine single effects is to calculate the odds ratio of inflammation when a single exposure is at its 75th percentile compared to its 25th percentile, with other exposures are at their 50th percentile and covariates are held constant. Here, we use the PredictorResponseUnivar() function to generate a dataset that details, at varying levels of each exposure (“z”), the relationship between that exposure and the outcome, holding other exposures at their 50th percentile and covariates constant. This relationship is given by a beta value, which because we have a binomial outcome and fit a probit model represents the log(odds) (“est”). The standard error for the beta value is also calculated (“se”). pred.resp.univar &lt;- PredictorResponseUnivar(fit=fitkm, sel=sel, method=&quot;approx&quot;, q.fixed = 0.5) head(pred.resp.univar) ## # A tibble: 6 × 4 ## variable z est se ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mn_ugg -3.40 -0.150 0.246 ## 2 Mn_ugg -3.18 -0.145 0.231 ## 3 Mn_ugg -2.96 -0.140 0.216 ## 4 Mn_ugg -2.74 -0.135 0.201 ## 5 Mn_ugg -2.52 -0.130 0.187 ## 6 Mn_ugg -2.30 -0.125 0.173 We can then plot these data for each exposure to visualize the exposure-response function for each exposure. ggplot(pred.resp.univar, aes(z, est, ymin = est - 1.96*se, ymax = est + 1.96*se)) + geom_smooth(stat = &quot;identity&quot;) + ylab(&quot;h(z)&quot;) + facet_wrap(~ variable) Then, we can generate a dataset that contains for each exposure (“variable”), the log(OR) (“est”) (and its standard deviation (“sd”)) corresponding to the odds of neonatal inflammation when an exposure is at its 75th compared to the odds when at the 25th percentile. The log(OR) is estimated at three levels of the other exposures (25th, 50th and 75th percentiles). We can use this dataset to identify odds ratios for neonatal inflammation (comparing the 75th to 25th percentile odds) for each exposure at differing levels of the other exposures. These odds ratios approximate risk, whereby an odds ratio &gt;1 means there is increased risk of neonatal inflammation when that exposure is at its 75th percentile compared to its 25th percentile. We can then plot these data to see the logOR for each metal in relation to neonatal inflammation at varying levels of the rest of the exposures. risks.singvar &lt;- SingVarRiskSummaries(fit=fitkm, qs.diff = c(0.25, 0.75), q.fixed = c(0.25, 0.50, 0.75), method = &quot;approx&quot;) ggplot(risks.singvar, aes(variable, est, ymin = est - 1.96*sd, ymax = est + 1.96*sd, col = q.fixed)) + geom_hline(aes(yintercept=0), linetype=&quot;dashed&quot;, color=&quot;gray&quot;) + geom_pointrange(position = position_dodge(width = 0.75)) + coord_flip() + theme(legend.position=&quot;none&quot;)+scale_x_discrete(name=&quot;&quot;) + scale_y_continuous(name=&quot;estimate&quot;) Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: Which of these chemicals increases the risk of neonatal inflammation and which decreases the risk of neonatal inflammation? Answer: At all levels of the other exposures, lead, cadmium, selenium, arsenic and zinc reduce the odds of neonatal inflammation, while manganese and mercury appear to increase the odds of neonatal inflammation. Copper appears has a null effect. Notice that the credibility intervals however for all metals span the null meaning we are not confident in the independent effect of any of the metals. Answer to Environmental Health Question 3 With this, we can also answer also Environmental Health Question #3: What is the risk of neonatal inflammation associated with exposure to each of manganese, copper, zinc, arsenic, selenium, cadmium, mercury, lead individually? Answer: As an example, take manganese: when all other exposures are at their 50th percentile, the log(OR) for Mn comparing being at the 75th to the 25th percentile is 0.024, which equals an odds ratio of 1.02. From this, you should be able to calculate the odds ratios for the other metals yourself. Calculating the overall mixture effect Next, we can generate a dataset that details the effect (ie. log(OR) (“est”) and corresponding standard deviation (“sd”)) on neonatal inflammation of all exposures when at a particular quantile (“quantile”) compared to all exposures being at the 50th percentile. We can use this dataset to identify odds ratios for neonatal inflammation upon simultaneous exposure to the entire mixture for different quantile threshold comparisons. These odds ratios approximate risk, whereby an odds ratio &gt;1 means there is increased risk of neonatal inflammation when the entire mixture is set at the index quantile, compared to the 50th percentile. We can also plot these results to visualize the overall mixture effect dose-response relationship. risks.overall &lt;- OverallRiskSummaries(fit=fitkm, qs=seq(0.25, 0.75, by=0.05), q.fixed = 0.5, method = &quot;approx&quot;, sel=sel) ggplot(risks.overall, aes(quantile, est, ymin = est - 1.96*sd, ymax = est + 1.96*sd)) + geom_hline(yintercept=00, linetype=&quot;dashed&quot;, color=&quot;gray&quot;) + geom_pointrange() + scale_y_continuous(name=&quot;estimate&quot;) Answer to Environmental Health Question 4 With this, we can answer Environmental Health Question #4: What is the risk of neonatal inflammation associated with combined exposure to manganese, copper, zinc, arsenic, selenium, cadmium, mercury, lead (ie. a mixture)? Answer: When every exposure is at its 25th percentile concentration compared to their 50th percentile concentration, the odds ratio for neonatal inflammation is 1.11 (exp(0.10073680)). When every exposure is at its 75th percentile concentration compared to their 50th percentile concentration, the odds ratio for neonatal inflammation is 0.87 (exp(-0.12000889)). Evaluating interactive effects To understand bivariate interactions, we can generate a dataset that for each pairing of exposures details at varying levels of both exposures, the log(odds) (“est”, and associated standard deviation (“sd”)) of neonatal inflammation when all the other exposures are held constant. These plots can be tricky to interpret, so another way of looking at these results is to take “cross sections” at specific quantiles of the second exposure (see next step). pred.resp.bivar &lt;- PredictorResponseBivar(fit=fitkm, min.plot.dist = 1, sel=sel, method=&quot;approx&quot;) ggplot(pred.resp.bivar, aes(z1, z2, fill = est)) + geom_raster() + facet_grid(variable2 ~ variable1) + scale_fill_gradientn(colours=c(&quot;#0000FFFF&quot;,&quot;#FFFFFFFF&quot;,&quot;#FF0000FF&quot;)) + xlab(&quot;expos1&quot;) + ylab(&quot;expos2&quot;) + ggtitle(&quot;h(expos1, expos2)&quot;) Next, we generate a dataset that includes for each pairing of exposures, the log(odds) (“est” and associated standard deviation “sd”) of neonatal inflammation at varying concentrations (“z1”) of the first exposure (“variable 1”) when the second exposure (“variable 2” is at its 25th, 50th and 75th percentile (“quantile”). pred.resp.bivar.levels &lt;- PredictorResponseBivarLevels(pred.resp.df= pred.resp.bivar, Z = mixture, both_pairs=TRUE, qs = c(0.25, 0.5, 0.75)) ggplot(pred.resp.bivar.levels, aes(z1, est)) + geom_smooth(aes(col = quantile), stat = &quot;identity&quot;) + facet_grid(variable2 ~ variable1) + ggtitle(&quot;h(expos1 | quantiles of expos2)&quot;) + xlab(&quot;expos1&quot;) There is evidence of an interactive effect between two exposures when the exposure-response function for exposure 1 varies in form between the different quantiles of exposure 2. You can also zoom in on one plot, for example: HgCd &lt;- pred.resp.bivar.levels %&gt;% filter(variable1==&quot;Hg_ngg&quot;) %&gt;% filter(variable2==&quot;Cd_ngg&quot;) ggplot(HgCd, aes(z1, est)) + geom_smooth(aes(col = quantile), stat = &quot;identity&quot;) + ggtitle(&quot;h(expos1 | quantiles of expos2)&quot;) + xlab(&quot;expos1&quot;) CdHg &lt;- pred.resp.bivar.levels %&gt;% filter(variable1==&quot;Cd_ngg&quot;) %&gt;% filter(variable2==&quot;Hg_ngg&quot;) ggplot(CdHg, aes(z1, est)) + geom_smooth(aes(col = quantile), stat = &quot;identity&quot;) + ggtitle(&quot;h(expos1 | quantiles of expos2)&quot;) + xlab(&quot;expos1&quot;) To visualize interactions between one exposure and the rest of the exposure components, we generate a dataset that details the difference in each exposure’s (“variable”) log(OR) comparing 75th to 25th percentile (“est”, and associated standard deviation “sd”) when the other exposure components are at their 75th versus 25th percentile. Perhaps more intuitively, these estimates represent the blue - red points plotted in the second figure under the single exposure effects section. risks.int &lt;- SingVarIntSummaries(fit=fitkm, qs.diff = c(0.25, 0.75), qs.fixed = c(0.25, 0.75)) ggplot(risks.int, aes(variable, est, ymin = est - 1.96*sd, ymax = est + 1.96*sd)) + geom_pointrange(position = position_dodge(width = 0.75)) + geom_hline(yintercept = 0, lty = 2, col = &quot;brown&quot;) + coord_flip() Answer to Environmental Health Question 5 With this, we can answer Environmental Health Question #5: Are there interactions among manganese, copper, zinc, arsenic, selenium, cadmium, mercury, lead in relation to neonatal inflammation? Answer: There do not appear to be any single exposure and rest of mixture interactions (previous plot); however, there is suggestive evidence of a bivariate interaction between cadmium and mercury. Concluding Remarks In conclusion, this module extends upon TAME 2.0 Module 6.3, Mixtures Analysis Methods Part 1: Overview and Example with Quantile G-Computation. In this scripted example, we used a dataset from a human population study (n=246) of cord tissue metals and examined the outcome of neonatal inflammation. We found that increasing the entire mixture of metals reduced the risk of neonatal inflammation; however, certain metals increased the risk and others decreased the risk. There was also a suggestive interactive effect found between cadmium and mercury. Additional Resources The field of mixtures is vast, with many different approaches and example studies to learn from as analysts lead in their own analyses. Some resources that can be helpful include the following reviews: Our recent review on mixtures methodologies, particularly in the field of sufficient similarity, titled Wrangling whole mixtures risk assessment: Recent advances in determining sufficient similarity Two more general, epidemiology-focused reviews on mixtures questions and methodologies, titled Complex Mixtures, Complex Analyses: an Emphasis on Interpretable Results and Environmental exposure mixtures: questions and methods to address them A helpful online toolkit for mixtures analyses generated by Andrea Bellavia, PhD Some helpful mixtures case studies using BKMR include the following: Prenatal metal concentrations and childhood cardio-metabolic risk using Bayesian Kernel Machine Regression to assess mixture and interaction effects Associations between Phthalate Metabolite Concentrations in Follicular Fluid and Reproductive Outcomes among Women Undergoing in Vitro Fertilization/Intracytoplasmic Sperm Injection Treatment Associations of Prenatal Per- and Polyfluoroalkyl Substance (PFAS) Exposures with Offspring Adiposity and Body Composition at 16–20 Years of Age: Project Viva Test Your Knowledge Using the simulated dataset within the bkmr package (see below code for how to call and store this dataset), answer the key environmental health questions using BKMR. Which of these chemicals has the strongest effect on the outcome? Which of these chemicals increases the outome and which decreases the outcome? What is the effect on the outcome with exposure to each of the chemicals individually? What is the effect on the outcome associated with combined exposure to all chemicals? Are there interactions among the chemicals relation to the outcome? Note that the outcome (y) variable is a continuous variable here, rather than binary as in the scripted example. # Set seed for reproducibility set.seed(111) # Create a dataset with 100 participants and 4 mixtures components data &lt;- SimData(n = 100, M = 4) # Save outcome variable (y) y &lt;- data$y # Save mixtures variables (Z and X) Z &lt;- data$Z X &lt;- data$X "],["mixtures-analysis-methods-part-3-sufficient-similarity.html", "6.5 Mixtures Analysis Methods Part 3: Sufficient Similarity Introduction to Training Module Introduction to Toxicant and Dataset Workspace Preparation and Data Import Chemistry-Based Sufficient Similarity Analysis Toxicity-Based Sufficient Similarity Analysis Comparing Chemistry vs. Toxicity Sufficient Similarity Analyses Concluding Remarks", " 6.5 Mixtures Analysis Methods Part 3: Sufficient Similarity This training module was developed by Cynthia Rider, with contributions from Lauren E. Koval and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module Humans are rarely, if ever, exposed to single chemicals at a time. Instead, humans are often exposed to multiple stressors in their everyday environments in the form of mixtures. These stressors can include environmental chemicals and pharmaceuticals, and they can also include other types of stressors such as socioeconomic factors and other attributes that can place individuals at increased risk of acquiring disease. Because it is not possible to test every possible combination of exposure that an individual might experience in their lifetime, approaches that take into account variable and complex exposure conditions through mixtures modeling are needed. There are different computational approaches that can be implemented to address this research topic. In this training module, we will demonstrate how to use sufficient similarity to determine which groups of exposure conditions are chemically/biologically similar enough to be regulated for safety together, based on the same set of regulatory criteria. Here, our example mixtures analysis will focus on characterizing the nutritional supplement Ginkgo biloba. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Based on the chemical analysis, which Ginkgo biloba extract looks the most different? When viewing the variability between chemical profiles, how many groupings of potentially ‘sufficiently similar’ Ginkgo biloba samples do you see? Based on the chemical analysis, which chemicals do you think are important in differentiating between the different Ginkgo biloba samples? After removing two samples that have the most different chemical profiles (and are thus, potential outliers), do we obtain similar chemical groupings? When viewing the variability between toxicity profiles, how many groupings of potentially ‘sufficiently similar’ Ginkgo biloba samples do you see? Based on the toxicity analysis, which genes do you think are important in differentiating between the different Ginkgo biloba samples? Were similar chemical groups identified when looking at just the chemistry vs. just the toxicity? How could this impact regulatory decisions, if we only had one of these datasets? Introduction to Toxicant and Dataset Ginkgo biloba represents a popular type of botanical supplement currently on the market. People take Ginkgo biloba to improve brain function, but there is conflicting data on its efficacy. Like other botanicals, Ginkgo biloba is a complex mixture with 100s-1000s of constituents. Here, the variability in chemical and toxicological profiles across samples of Ginkgo biloba purchased from different commercial sources is evaluated. We can use data from a well-characterized sample (reference sample) to evaluate the safety of other samples that are ‘sufficiently similar’ to the reference sample. Samples that are different (i.e., do not meet the standards of sufficient similarity) from the reference sample would require additional safety data. A total of 29 Ginkgo biloba extract samples were analyzed. These samples are abbreviated as “GbE_” followed by a unique sample identifier (GbE = Ginkgo biloba Extract). These data have been previously published: Catlin NR, Collins BJ, Auerbach SS, Ferguson SS, Harnly JM, Gennings C, Waidyanatha S, Rice GE, Smith-Roe SL, Witt KL, Rider CV. How similar is similar enough? A sufficient similarity case study with Ginkgo biloba extract. Food Chem Toxicol. 2018 Aug;118:328-339. PMID: 29752982. Collins BJ, Kerns SP, Aillon K, Mueller G, Rider CV, DeRose EF, London RE, Harnly JM, Waidyanatha S. Comparison of phytochemical composition of Ginkgo biloba extracts using a combination of non-targeted and targeted analytical approaches. Anal Bioanal Chem. 2020 Oct;412(25):6789-6809. PMID: 32865633. Ginkgo biloba Chemistry Dataset Overview The chemical profiles of these sample extracts were first analyzed using targeted mass spectrometry-based approaches. The concentrations of 12 Ginkgo biloba marker compounds were measured in units of mean weight as a ratio [g chemical / g sample]. Note that in this dataset, non-detects have been replaced with values of zero for simplicity; though there are more advanced methods to impute values for non-detects. Script is provided to evaluate how Ginkgo biloba extracts group together, based on chemical profiles. Ginkgo biloba Toxicity Dataset Overview The toxicological profiles of these samples were also analyzed using in vitro test methods. These data represent area under the curve (AUC) values indicating changes in gene expression across various concentrations of the Ginkgo biloba extract samples. Positive AUC values indicate a gene that was collectively increased in expression as concentration increased, and a negative AUC value indicates a gene that was collectively decreased in expression as exposure concentration increased. Script is provided to evaluate how Ginkgo biloba extracts group together, based on toxicity profiles. Workspace Preparation and Data Import Install required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); if (!requireNamespace(&quot;readxl&quot;)) install.packages(&quot;readxl&quot;); if (!requireNamespace(&quot;factoextra&quot;)) install.packages(&quot;factoextra&quot;); if (!requireNamespace(&quot;pheatmap&quot;)) install.packages(&quot;pheatmap&quot;); if (!requireNamespace(&quot;gridExtra&quot;)) install.packages(&quot;gridExtra&quot;); if (!requireNamespace(&quot;ggplotify&quot;)) install.packages(&quot;ggplotify&quot;) Loading required packages library(readxl) #used to read in and work with excel files library(factoextra) #used to run and visualize multivariate analyses, here PCA library(pheatmap) #used to make heatmaps. This can be done in ggplot2 but pheatmap is easier and nicer library(gridExtra) #used to arrange and visualize multiple figures at once library(ggplotify) #used to make non ggplot figures (like a pheatmap) gg compatible library(tidyverse) #all tidyverse packages, including dplyr and ggplot2 Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Import example Ginkgo biloba dataset We need to first read in the chemistry and toxicity data from the provided excel file. Here, data were originally organized such that the actual observations start on row 2 (dataset descriptions were in the first row). So let’s implement skip=1, which skips reading in the first row. chem &lt;- read_xlsx(&quot;Module6_5_Input/Module6_5_InputData.xlsx&quot; , sheet = &quot;chemistry data&quot;, skip=1) # loads the chemistry data tab tox &lt;- read_xlsx(&quot;Module6_5_Input/Module6_5_InputData.xlsx&quot; , sheet = &quot;in vitro data&quot;, skip=1) # loads the toxicity data tab View example dataset Let’s first see how many rows and columns of data are present in both datasets: dim(chem) ## [1] 29 13 The chemistry dataset contains information on 29 samples (rows); and 1 sample identifier + 12 chemicals (total of 13 columns). dim(tox) ## [1] 29 6 The tox dataset contains information on 29 samples (rows); and 1 sample identifier + 5 genes (total of 6 columns). Let’s also see what kind of data are organized within the datasets: colnames(chem) ## [1] &quot;Sample&quot; &quot;Bilobalide&quot; &quot;Ginkgolide_A&quot; ## [4] &quot;Ginkgolide_B&quot; &quot;Ginkgolide_C&quot; &quot;Ginkgolide_J&quot; ## [7] &quot;Rutin&quot; &quot;Quercetin&quot; &quot;Kaempferol&quot; ## [10] &quot;Isorhamnetin&quot; &quot;Ginkgolic_Acid_C15&quot; &quot;Ginkgolic_Acid_C17&quot; ## [13] &quot;Ginkgotoxin&quot; head(chem) ## # A tibble: 6 × 13 ## Sample Bilobalide Ginkgolide_A Ginkgolide_B Ginkgolide_C Ginkgolide_J Rutin ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GbE_A 1.28 0 0 0 1.77 0 ## 2 GbE_B 0 0 0 0 0 0.05 ## 3 GbE_C 0 0 0 0 0 0.01 ## 4 GbE_D 1.28 2.6 1.6 2.79 1.18 1.11 ## 5 GbE_E 1.5 2.13 1.46 2.6 1.21 1.21 ## 6 GbE_F 0 0 0 0 0 0.04 ## # ℹ 6 more variables: Quercetin &lt;dbl&gt;, Kaempferol &lt;dbl&gt;, Isorhamnetin &lt;dbl&gt;, ## # Ginkgolic_Acid_C15 &lt;dbl&gt;, Ginkgolic_Acid_C17 &lt;dbl&gt;, Ginkgotoxin &lt;dbl&gt; colnames(tox) ## [1] &quot;Sample&quot; &quot;ABCB11&quot; &quot;CYP1A2&quot; &quot;CYP2B6&quot; &quot;CYP3A4&quot; &quot;HMGCS2&quot; head(tox) ## # A tibble: 6 × 6 ## Sample ABCB11 CYP1A2 CYP2B6 CYP3A4 HMGCS2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GbE_A -0.450 0.778 0.124 -1.39 -0.0208 ## 2 GbE_B -0.210 0.902 0.456 -1.22 -0.149 ## 3 GbE_C -1.10 1.32 1.58 -1.62 0.195 ## 4 GbE_D -0.818 1.61 2.46 0.935 -1.62 ## 5 GbE_E -0.963 2.27 2.44 1.33 -1.54 ## 6 GbE_F -0.0828 1.2 0.587 -1.29 -0.147 Chemistry-Based Sufficient Similarity Analysis The first method employed in this Sufficient Similarity analysis is Principal Component Analysis (PCA). PCA is a very common dimensionality reduction technique, as detailed in TAME 2.0 Module 5.4 Unsupervised Machine Learning. In summary, PCA finds dimensions (eigenvectors) in the higher dimensional original data that capture as much of the variation as possible, which you can then plot. This allows you to project higher dimensional data, in this case 12 dimensions (representing 12 measured chemicals), in fewer dimensions (we’ll use 2). These dimensions, or components, capture the “essence” of the original dataset. Before we can run PCA on this chemistry dataset, we first need to scale the data across samples. We do this here for the chemistry dataset, because we specifically want to evaluate and potentially highlight/emphasize chemicals that may be at relatively low abundance. These low-abundance chemicals may actually be contaminants that drive toxicological effects. Let’s first re-save the original chemistry dataset to compare off of: chem_original &lt;- chem Then, we’ll make a scaled version to carry forward in this analysis. To do this, we move the sample column the row names and then scale and center the data. chem &lt;- chem %&gt;% column_to_rownames(&quot;Sample&quot;) chem &lt;- as.data.frame(scale(as.matrix(chem))) Let’s now compare one of the rows of data (here, sample GbE_E) to see what scaling did: chem_original[5,] ## # A tibble: 1 × 13 ## Sample Bilobalide Ginkgolide_A Ginkgolide_B Ginkgolide_C Ginkgolide_J Rutin ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GbE_E 1.5 2.13 1.46 2.6 1.21 1.21 ## # ℹ 6 more variables: Quercetin &lt;dbl&gt;, Kaempferol &lt;dbl&gt;, Isorhamnetin &lt;dbl&gt;, ## # Ginkgolic_Acid_C15 &lt;dbl&gt;, Ginkgolic_Acid_C17 &lt;dbl&gt;, Ginkgotoxin &lt;dbl&gt; chem[5,] ## Bilobalide Ginkgolide_A Ginkgolide_B Ginkgolide_C Ginkgolide_J Rutin ## GbE_E -0.4996768 0.02749315 0.2559249 1.36305 0.3842836 0.5758008 ## Quercetin Kaempferol Isorhamnetin Ginkgolic_Acid_C15 Ginkgolic_Acid_C17 ## GbE_E 0.2059607 -0.2420023 -0.61695 -0.2737699 -0.2317415 ## Ginkgotoxin ## GbE_E -0.8114437 You can see that scaling made the concentrations distributed across each chemical center around 0. Now, we can run PCA on the scaled data: chem_pca &lt;- princomp(chem) Looking at the scree plot, we see the first two principal components capture most of the variance in the data (~64%): fviz_eig(chem_pca) Here are the resulting PCA scores for each sample, for each principal component (shown here as components 1-12): head(chem_pca$scores) ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 ## GbE_A -2.6256689 -0.53470486 0.38196672 1.5147505 1.34638144 -0.24020735 ## GbE_B -3.7343187 -1.07490206 0.07766108 0.2529618 -0.01731847 -0.02446797 ## GbE_C -3.8019563 -1.06232680 0.08335445 0.2588084 -0.01912736 -0.03423438 ## GbE_D 0.7175782 -0.09566345 0.90082998 -0.7762090 -0.40507924 -0.78361086 ## GbE_E 0.4157644 -0.14794948 1.16402759 -0.4856176 -0.15497152 -0.64206760 ## GbE_F -3.7621819 -1.04784203 0.08687721 0.2503188 -0.02195005 -0.04030373 ## Comp.7 Comp.8 Comp.9 Comp.10 Comp.11 Comp.12 ## GbE_A -0.8890488 0.8145597 0.15021776 -0.54318277 -0.32353295 0.049538024 ## GbE_B 0.3988596 -0.3227102 -0.10344907 -0.12911495 0.11127631 0.008306532 ## GbE_C 0.3817061 -0.2788579 -0.13057528 -0.02613584 0.08148776 0.011485316 ## GbE_D -1.1916851 -0.4306198 0.08460588 0.26115540 0.01065657 -0.053819603 ## GbE_E -1.1412900 -0.5632547 0.12309347 -0.02872126 0.24882196 0.047691048 ## GbE_F 0.3948245 -0.3105324 -0.10539998 -0.11015645 0.10607314 0.012066512 And the resulting loading factors of each chemical’s contribution towards each principal component. Results are arranged by a chemical’s contribution to PC1, the component accounting for most of the variation in the data. head(chem_pca$loadings) ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Bilobalide 0.3260729 0.085117226 0.24541927 0.36968834 0.03575030 ## Ginkgolide_A 0.3732546 0.184101347 0.08586157 0.02446723 -0.03299212 ## Ginkgolide_B 0.3611463 0.201280307 0.05996588 -0.19928568 -0.05965128 ## Ginkgolide_C 0.3616497 0.001365838 0.12157943 -0.24092774 -0.10470516 ## Ginkgolide_J 0.2995761 0.178689209 0.09144978 0.35348565 0.47372240 ## Rutin 0.1763429 0.012553984 0.59496115 -0.47712128 0.08063637 ## Comp.6 Comp.7 Comp.8 Comp.9 Comp.10 ## Bilobalide 0.004975969 0.55079870 0.35062913 0.23584371 0.25226337 ## Ginkgolide_A -0.247529927 0.30284388 -0.30206946 -0.01601813 0.20620248 ## Ginkgolide_B -0.068401213 -0.00687696 0.09497565 -0.83752355 0.02062813 ## Ginkgolide_C -0.185456111 -0.45474675 -0.34421625 0.32067335 0.45797955 ## Ginkgolide_J -0.062170758 -0.55785820 0.36066655 0.03820900 -0.14858754 ## Rutin 0.517141873 0.05060803 0.03111005 0.15083257 -0.27469825 ## Comp.11 Comp.12 ## Bilobalide 0.29295985 0.23837797 ## Ginkgolide_A -0.60099943 -0.41172878 ## Ginkgolide_B 0.24851189 0.06938891 ## Ginkgolide_C 0.28752202 0.17463609 ## Ginkgolide_J -0.22581449 -0.03024110 ## Rutin -0.09884752 -0.04092322 We can save the chemical-specific loadings into a separate matrix and view them from highest to lowest values for PC1. loadings &lt;- as.data.frame.matrix(chem_pca$loadings) loadings %&gt;% arrange(desc(Comp.1)) ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Quercetin 0.3801459 -0.001945021 -0.19193647 0.04697879 0.007656212 ## Ginkgolide_A 0.3732546 0.184101347 0.08586157 0.02446723 -0.032992122 ## Ginkgolide_C 0.3616497 0.001365838 0.12157943 -0.24092774 -0.104705164 ## Ginkgolide_B 0.3611463 0.201280307 0.05996588 -0.19928568 -0.059651275 ## Bilobalide 0.3260729 0.085117226 0.24541927 0.36968834 0.035750299 ## Kaempferol 0.3001354 -0.085004317 -0.29667523 -0.15611039 -0.655825688 ## Ginkgolide_J 0.2995761 0.178689209 0.09144978 0.35348565 0.473722400 ## Isorhamnetin 0.2740348 -0.075119327 -0.29665890 0.39008587 -0.060880190 ## Rutin 0.1763429 0.012553984 0.59496115 -0.47712128 0.080636368 ## Ginkgotoxin 0.1675373 -0.034318656 -0.56232119 -0.47935782 0.551341021 ## Ginkgolic_Acid_C15 -0.1201265 0.667543042 -0.11607308 -0.05802100 -0.085384063 ## Ginkgolic_Acid_C17 -0.1418140 0.653142232 -0.09559022 -0.02189315 -0.084006824 ## Comp.6 Comp.7 Comp.8 Comp.9 Comp.10 ## Quercetin -0.249799236 0.13263831 -0.30855302 0.18612332 -0.72929122 ## Ginkgolide_A -0.247529927 0.30284388 -0.30206946 -0.01601813 0.20620248 ## Ginkgolide_C -0.185456111 -0.45474675 -0.34421625 0.32067335 0.45797955 ## Ginkgolide_B -0.068401213 -0.00687696 0.09497565 -0.83752355 0.02062813 ## Bilobalide 0.004975969 0.55079870 0.35062913 0.23584371 0.25226337 ## Kaempferol 0.050018874 -0.12464461 0.51491286 0.16055155 -0.07828551 ## Ginkgolide_J -0.062170758 -0.55785820 0.36066655 0.03820900 -0.14858754 ## Isorhamnetin 0.730543567 -0.06658953 -0.34052044 -0.10456587 0.10158173 ## Rutin 0.517141873 0.05060803 0.03111005 0.15083257 -0.27469825 ## Ginkgotoxin 0.092918281 0.20744490 0.16087302 0.11145659 0.17463719 ## Ginkgolic_Acid_C15 0.057775937 0.03440329 -0.13417826 0.06575084 -0.07780733 ## Ginkgolic_Acid_C17 0.133278823 -0.05017155 0.08415192 0.16593739 0.03564092 ## Comp.11 Comp.12 ## Quercetin 0.273123642 0.008854815 ## Ginkgolide_A -0.600999427 -0.411728782 ## Ginkgolide_C 0.287522018 0.174636086 ## Ginkgolide_B 0.248511890 0.069388910 ## Bilobalide 0.292959851 0.238377968 ## Kaempferol -0.211380567 -0.020939233 ## Ginkgolide_J -0.225814490 -0.030241100 ## Isorhamnetin 0.002690835 -0.006305513 ## Rutin -0.098847524 -0.040923217 ## Ginkgotoxin -0.005807642 0.016904160 ## Ginkgolic_Acid_C15 -0.285797465 0.633437667 ## Ginkgolic_Acid_C17 0.383124914 -0.577639931 These resulting loading factors allow us to identify which constituents (of the 12 total) contribute to the principal components explaining data variabilities. For instance, we can see here that Quercetin is listed at the top, with the largest loading value for principal component 1. Thus, Quercetin represents the constituents that contributes to the overall variability in the dataset to the greatest extent. The next three chemicals are all Ginkgolide constituents, followed by Bilobalide and Kaempferol, and so forth. If we look at principal component 2 (PC2), we can now see a different set of chemicals contributing to the variability captured in this component: loadings %&gt;% arrange(desc(Comp.2)) ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Ginkgolic_Acid_C15 -0.1201265 0.667543042 -0.11607308 -0.05802100 -0.085384063 ## Ginkgolic_Acid_C17 -0.1418140 0.653142232 -0.09559022 -0.02189315 -0.084006824 ## Ginkgolide_B 0.3611463 0.201280307 0.05996588 -0.19928568 -0.059651275 ## Ginkgolide_A 0.3732546 0.184101347 0.08586157 0.02446723 -0.032992122 ## Ginkgolide_J 0.2995761 0.178689209 0.09144978 0.35348565 0.473722400 ## Bilobalide 0.3260729 0.085117226 0.24541927 0.36968834 0.035750299 ## Rutin 0.1763429 0.012553984 0.59496115 -0.47712128 0.080636368 ## Ginkgolide_C 0.3616497 0.001365838 0.12157943 -0.24092774 -0.104705164 ## Quercetin 0.3801459 -0.001945021 -0.19193647 0.04697879 0.007656212 ## Ginkgotoxin 0.1675373 -0.034318656 -0.56232119 -0.47935782 0.551341021 ## Isorhamnetin 0.2740348 -0.075119327 -0.29665890 0.39008587 -0.060880190 ## Kaempferol 0.3001354 -0.085004317 -0.29667523 -0.15611039 -0.655825688 ## Comp.6 Comp.7 Comp.8 Comp.9 Comp.10 ## Ginkgolic_Acid_C15 0.057775937 0.03440329 -0.13417826 0.06575084 -0.07780733 ## Ginkgolic_Acid_C17 0.133278823 -0.05017155 0.08415192 0.16593739 0.03564092 ## Ginkgolide_B -0.068401213 -0.00687696 0.09497565 -0.83752355 0.02062813 ## Ginkgolide_A -0.247529927 0.30284388 -0.30206946 -0.01601813 0.20620248 ## Ginkgolide_J -0.062170758 -0.55785820 0.36066655 0.03820900 -0.14858754 ## Bilobalide 0.004975969 0.55079870 0.35062913 0.23584371 0.25226337 ## Rutin 0.517141873 0.05060803 0.03111005 0.15083257 -0.27469825 ## Ginkgolide_C -0.185456111 -0.45474675 -0.34421625 0.32067335 0.45797955 ## Quercetin -0.249799236 0.13263831 -0.30855302 0.18612332 -0.72929122 ## Ginkgotoxin 0.092918281 0.20744490 0.16087302 0.11145659 0.17463719 ## Isorhamnetin 0.730543567 -0.06658953 -0.34052044 -0.10456587 0.10158173 ## Kaempferol 0.050018874 -0.12464461 0.51491286 0.16055155 -0.07828551 ## Comp.11 Comp.12 ## Ginkgolic_Acid_C15 -0.285797465 0.633437667 ## Ginkgolic_Acid_C17 0.383124914 -0.577639931 ## Ginkgolide_B 0.248511890 0.069388910 ## Ginkgolide_A -0.600999427 -0.411728782 ## Ginkgolide_J -0.225814490 -0.030241100 ## Bilobalide 0.292959851 0.238377968 ## Rutin -0.098847524 -0.040923217 ## Ginkgolide_C 0.287522018 0.174636086 ## Quercetin 0.273123642 0.008854815 ## Ginkgotoxin -0.005807642 0.016904160 ## Isorhamnetin 0.002690835 -0.006305513 ## Kaempferol -0.211380567 -0.020939233 Here, Ginkgolic Acids are listed first. We can also visualize sample groupings based on these principal components 1 &amp; 2: # First pull the percent variation captured by each component pca_percent &lt;- round(100*chem_pca$sdev^2/sum(chem_pca$sdev^2),1) # Then make a dataframe for the PCA plot generation script using first three components pca_df &lt;- data.frame(PC1 = chem_pca$scores[,1], PC2 = chem_pca$scores[,2]) # Plot this dataframe chem_pca_plt &lt;- ggplot(pca_df, aes(PC1,PC2))+ geom_hline(yintercept = 0, size=0.3)+ geom_vline(xintercept = 0, size=0.3)+ geom_point(size=3, color=&quot;deepskyblue3&quot;) + geom_text(aes(label=rownames(pca_df)), fontface=&quot;bold&quot;, position=position_jitter(width=0.4,height=0.4))+ labs(x=paste0(&quot;PC1 (&quot;,pca_percent[1],&quot;%)&quot;), y=paste0(&quot;PC2 (&quot;,pca_percent[2],&quot;%)&quot;))+ ggtitle(&quot;GbE Sample PCA by Chemistry Profiles&quot;) # Changing the colors of the titles and axis text chem_pca_plt &lt;- chem_pca_plt + theme(plot.title=element_text(color=&quot;deepskyblue3&quot;, face=&quot;bold&quot;), axis.title.x=element_text(color=&quot;deepskyblue3&quot;, face=&quot;bold&quot;), axis.title.y=element_text(color=&quot;deepskyblue3&quot;, face=&quot;bold&quot;)) # Viewing this resulting plot chem_pca_plt This plot tells us a lot about sample groupings based on chemical profiles! Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question 1: Based on the chemical analysis, which Ginkgo biloba extract looks the most different? Answer: GbE_G Answer to Environmental Health Question 2 We can also answer Environmental Health Question 2: When viewing the variability between chemical profiles, how many groupings of potentially ‘sufficiently similar’ Ginkgo biloba samples do you see? Answer: Approximately 4 (though could argue +1/-1): bottom left group; bottom right group; and two completely separate samples of GbE_G and GbE_N As an alternative way of viewing the chemical profile data, we can make a heat map of the scaled chemistry data. We concurrently run hierarchical clustering that shows us how closely samples are related to each other, based on different algorithms than data reduction-based PCA. Samples that fall on nearby branches are more similar. Samples that don’t share branches with many/any others are often considered outliers. By default, pheatmap uses a Euclidean distance to cluster the observations, which is a very common clustering algorithm. For more details, see the following description of Euclidean distance. chem_hm &lt;- pheatmap(chem, main=&quot;GbE Sample Heatmap by Chemistry Profiles&quot;, cluster_rows=TRUE, cluster_cols = FALSE, angle_col = 45, fontsize_col = 7, treeheight_row = 60) This plot tells us a lot about the individual chemicals that differentiate the sample groupings. Answer to Environmental Health Question 3 With this, we can answer Environmental Health Question 3: Based on the chemical analysis, which chemicals do you think are important in differentiating between the different Ginkgo biloba samples? Answer: All of the chemicals technically contribute to these sample patterns, but here are some that stand out: (i) Ginkgolic_Acid_C15 and Ginkgolic_Acid_C17 appear to drive the clustering of one particular GbE sample, GbE_G, as well as potentially GbE_N; (ii) Isorhamnetin influences the clustering of GbE_T; (iii) Bilobalide, Ginkgolides A &amp; B, and Quercetin are also important because they show a general cluster of abundance at decreased levels at the bottom and increased levels at the top. Let’s now revisit the PCA plot: chem_pca_plt GbE_G and GbE_N look so different from the rest of the samples that they could be outliers and potentially influencing overall data trends. Let’s make sure that, if we remove these two samples, our sample groupings still look the same. First, we remove those two samples from the dataframe: chem_filt &lt;- chem %&gt;% rownames_to_column(&quot;Sample&quot;) %&gt;% filter(!Sample %in% c(&quot;GbE_G&quot;,&quot;GbE_N&quot;)) %&gt;% column_to_rownames(&quot;Sample&quot;) Then, we can re-run PCA and generate a heatmap of the chemical data with these outlier samples removed: chem_filt_pca &lt;- princomp(chem_filt) # Get the percent variation captured by each component pca_percent_filt &lt;- round(100*chem_filt_pca$sdev^2/sum(chem_filt_pca$sdev^2),1) # Make dataframe for PCA plot generation using first three components pca_df_filt &lt;- data.frame(PC1 = chem_filt_pca$scores[,1], PC2 = chem_filt_pca$scores[,2]) # Plot this dataframe chem_filt_pca_plt &lt;- ggplot(pca_df_filt, aes(PC1,PC2))+ geom_hline(yintercept = 0, size=0.3)+ geom_vline(xintercept = 0, size=0.3)+ geom_point(size=3, color=&quot;aquamarine2&quot;) + geom_text(aes(label=rownames(pca_df_filt)), fontface=&quot;bold&quot;, position=position_jitter(width=0.5,height=0.5))+ labs(x=paste0(&quot;PC1 (&quot;,pca_percent[1],&quot;%)&quot;), y=paste0(&quot;PC2 (&quot;,pca_percent[2],&quot;%)&quot;))+ ggtitle(&quot;GbE Sample PCA by Chemistry Profiles excluding Potential Outliers&quot;) # Changing the colors of the titles and axis text chem_filt_pca_plt &lt;- chem_filt_pca_plt + theme(plot.title=element_text(color=&quot;aquamarine2&quot;, face=&quot;bold&quot;), axis.title.x=element_text(color=&quot;aquamarine2&quot;, face=&quot;bold&quot;), axis.title.y=element_text(color=&quot;aquamarine2&quot;, face=&quot;bold&quot;)) # Viewing this resulting plot chem_filt_pca_plt To view the PCA plots of all samples vs filtered samples: grid.arrange(chem_pca_plt, chem_filt_pca_plt) Answer to Environmental Health Question 4 With this, we can answer Environmental Health Question 4: After removing two samples that have the most different chemical profiles (and are thus, potential outliers), do we obtain similar chemical groupings? Answer: Yes! Removal of the potential outliers basically spreads the rest of the remaining data points out, since there is less variance in the overall dataset, and thus, more room to show variance amongst the remaining samples. The general locations of the samples on the PCA plot, however, remain consistent. We now feel confident that our similarity analysis is producing consistent grouping results. Toxicity-Based Sufficient Similarity Analysis Now, we will perform sufficient similarity analysis using the toxicity data.Unlike the chemistry dataset, we can use the toxicity dataset as is without scaling because we want to focus on genes that are showing a large response. Similarly, we want to de-emphasize genes that are showing a strong response to the exposure condition. If we scale these data, we will reduce this needed variability. Here, we first move the sample column to row names: tox &lt;- tox %&gt;% column_to_rownames(&quot;Sample&quot;) Then, we can run PCA on this tox dataframe: tox_pca &lt;- princomp(tox) Looking at the scree plot, we see the first two principal components capture most of the variation (~93%): fviz_eig(tox_pca) We can then create a plot of the samples by principal components: # Get the percent variation captured by each component pca_percent &lt;- round(100*tox_pca$sdev^2/sum(tox_pca$sdev^2),1) # Make dataframe for PCA plot generation using first three components tox_pca_df &lt;- data.frame(PC1 = tox_pca$scores[,1], PC2 = tox_pca$scores[,2]) # Plot the first two components tox_pca_plt &lt;- ggplot(tox_pca_df, aes(PC1,PC2))+ geom_hline(yintercept = 0, size=0.3)+ geom_vline(xintercept = 0, size=0.3)+ geom_point(size=3, color=&quot;deeppink3&quot;) + geom_text(aes(label=rownames(pca_df)), fontface=&quot;bold&quot;, position=position_jitter(width=0.25,height=0.25))+ labs(x=paste0(&quot;PC1 (&quot;,pca_percent[1],&quot;%)&quot;), y=paste0(&quot;PC2 (&quot;,pca_percent[2],&quot;%)&quot;))+ ggtitle(&quot;GbE Sample PCA by Toxicity Profiles&quot;) # Changing the colors of the titles and axis text tox_pca_plt &lt;- tox_pca_plt + theme(plot.title=element_text(color=&quot;deeppink3&quot;, face=&quot;bold&quot;), axis.title.x=element_text(color=&quot;deeppink3&quot;, face=&quot;bold&quot;), axis.title.y=element_text(color=&quot;deeppink3&quot;, face=&quot;bold&quot;)) tox_pca_plt This plot tells us a lot about sample groupings based on toxicity profiles! Answer to Environmental Health Question 5 With this, we can answer Environmental Health Question 5: When viewing the variability between toxicity profiles, how many groupings of potentially ‘sufficiently similar’ Ginkgo biloba samples do you see? Answer: Approximately 3 (though could argue +1/-1): top left group; top right group; GbE_M and GbE_W. Similar to the chemistry data, as an alternative way of viewing the toxicity profile data, we can make a heatmap of the toxicity data: tox_hm &lt;- pheatmap(tox, main=&quot;GbE Sample Heatmap by Toxicity Profiles&quot;, cluster_rows=TRUE, cluster_cols = FALSE, angle_col = 45, fontsize_col = 7, treeheight_row = 60) This plot tells us a lot about the individual genes that differentiate the sample groupings! Answer to Environmental Health Question 6 With this, we can answer Environmental Health Question 6: Based on the toxicity analysis, which genes do you think are important in differentiating between the different Ginkgo biloba samples? Answer: It looks like the CYP enzyme genes, particularly CYP2B6, are highly up-regulated in response to several of these sample exposures, and thus dictate a lot of these groupings. Comparing Chemistry vs. Toxicity Sufficient Similarity Analyses Let’s view the PCA plots for both datasets together, side-by-side: pca_compare &lt;- grid.arrange(chem_pca_plt,tox_pca_plt, nrow=1) Let’s also view the PCA plots for both datasets together, top-to-bottom, to visualize the trends along both axes better between these two views: pca_compare &lt;- grid.arrange(chem_pca_plt,tox_pca_plt) Here is an edited version of the above figures, highlighting with colored circles some chemical groups of interest identified through chemistry vs toxicity-based sufficient similarity analyses: Answer to Environmental Health Question 7 With this, we can answer Environmental Health Question 7: Were similar chemical groups identified when looking at just the chemistry vs. just the toxicity? How could this impact regulatory action, if we only had one of these datasets? Answer: There are some similarities between groupings, though there are also notable differences. For example, samples GbE_A, GbE_B, GbE_C, GbE_F, and GbE_H group together from the chemistry and toxicity similarity analyses. Though samples GbE_G, GbE_W, GbE_N, and others clearly demonstrate differences in grouping assignments. These differences could impact the accuracy of how regulatory decisions are made, where if regulation was dictated solely on the chemistry (without toxicity data) and/or vice versa, we may miss important information that could aid in accurate health risk evaluations. Additional Methods Although we focused on sufficient similarity for this module, a number of other approaches exist to evaluate mixutres. For example, relative potency factors is another component-based approach that can be used to evalaute mixtures. Component-based approaches use data from individual chemicals (components of the mixture) and additivity models to estimate the effects of the mixture. For other methods, also see TAME 2.0 Module 6.3 Mixtures I: Overview and Quantile G-Computation Application and TAME 2.0 Module 6.4 Mixtures II: BKMR Application. Concluding Remarks In this module, we evaluated the similarity between variable lots of Ginkgo biloba and identified sample groupings that could be used for chemical risk assessment purposes. Together, this example highlights the utility of sufficient similarity analyses to address environmental health research questions. Additional Resources Some helpful resources that provide further background on the topic of mixtures toxicology and mixtures modeling include the following: Carlin DJ, Rider CV, Woychik R, Birnbaum LS. Unraveling the health effects of environmental mixtures: an NIEHS priority. Environ Health Perspect. 2013 Jan;121(1):A6-8. PMID: 23409283. Drakvik E, Altenburger R, Aoki Y, Backhaus T, Bahadori T, Barouki R, Brack W, Cronin MTD, Demeneix B, Hougaard Bennekou S, van Klaveren J, Kneuer C, Kolossa-Gehring M, Lebret E, Posthuma L, Reiber L, Rider C, Rüegg J, Testa G, van der Burg B, van der Voet H, Warhurst AM, van de Water B, Yamazaki K, Öberg M, Bergman Å. Statement on advancing the assessment of chemical mixtures and their risks for human health and the environment. Environ Int. 2020 Jan;134:105267. PMID: 31704565. Rider CV, McHale CM, Webster TF, Lowe L, Goodson WH 3rd, La Merrill MA, Rice G, Zeise L, Zhang L, Smith MT. Using the Key Characteristics of Carcinogens to Develop Research on Chemical Mixtures and Cancer. Environ Health Perspect. 2021 Mar;129(3):35003. PMID: 33784186. Taylor KW, Joubert BR, Braun JM, Dilworth C, Gennings C, Hauser R, Heindel JJ, Rider CV, Webster TF, Carlin DJ. Statistical Approaches for Assessing Health Effects of Environmental Chemical Mixtures in Epidemiology: Lessons from an Innovative Workshop. Environ Health Perspect. 2016 Dec 1;124(12):A227-A229. PMID: 27905274. For more information and additional examples in environmental health research, see the following relevant publications implementing sufficient similarity methods to address complex mixtures: Catlin NR, Collins BJ, Auerbach SS, Ferguson SS, Harnly JM, Gennings C, Waidyanatha S, Rice GE, Smith-Roe SL, Witt KL, Rider CV. How similar is similar enough? A sufficient similarity case study with Ginkgo biloba extract. Food Chem Toxicol. 2018 Aug;118:328-339. PMID: 29752982. Collins BJ, Kerns SP, Aillon K, Mueller G, Rider CV, DeRose EF, London RE, Harnly JM, Waidyanatha S. Comparison of phytochemical composition of Ginkgo biloba extracts using a combination of non-targeted and targeted analytical approaches. Anal Bioanal Chem. 2020 Oct;412(25):6789-6809. PMID: 32865633. Ryan KR, Huang MC, Ferguson SS, Waidyanatha S, Ramaiahgari S, Rice JR, Dunlap PE, Auerbach SS, Mutlu E, Cristy T, Peirfelice J, DeVito MJ, Smith-Roe SL, Rider CV. Evaluating Sufficient Similarity of Botanical Dietary Supplements: Combining Chemical and In Vitro Biological Data. Toxicol Sci. 2019 Dec 1;172(2):316-329. PMID: 31504990. Rice GE, Teuschler LK, Bull RJ, Simmons JE, Feder PI. Evaluating the similarity of complex drinking-water disinfection by-product mixtures: overview of the issues. J Toxicol Environ Health A. 2009;72(7):429-36. PMID: 19267305. Test Your Knowledge We recently published a study evaluating similarities across wildfire chemistry profiles using a more advanced analysis approach than described in this module (PMID: 36399130). For this test your knowledge box, let’s implement the more simple, PCA-based sufficient similarity analysis to identify groups of biomass smoke exposure signatures using chemical profiles. The relevant dataset is included in the file Module6_5_TYKInput.csv. Specifically: Perform a PCA on the chemistry data and visualize the proximity of each chemical signature to other signatures according to the first two principal components. Identify major groupings of biomass smoke exposure signatures. "],["toxicokinetic-modeling.html", "6.6 Toxicokinetic Modeling Introduction to Training Module Introduction to Toxicokinetic Modeling Data and Models used in Toxicokinetic Modeling (TK) Reverse Toxicokinetics Capturing Population Variability in Toxicokinetics, and Uncertainty in Chemical-Specific Parameters Reverse TK: Calculating Administered Equivalent Doses for ToxCast Bioactive Concentrations Comparing Equivalent Doses Estimated to Elicit Toxicity (Hazard) to External Exposure Estimates (Exposure), for Chemical Prioritization by Bioactivity-Exposure Ratios (BERs) Concluding Remarks", " 6.6 Toxicokinetic Modeling This training module was developed by Caroline Ring and Lauren Koval. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Disclaimer: The views expressed in this document are those of the author and do not necessarily reflect the views or policies of the U.S. EPA. Introduction to Training Module This module serves as an example to guide trainees through the basics of toxicokinetic (TK) modeling and how this type of modeling can be used in the high-throughput setting for environmental health research applications. In this activity, the capabilities of a high-throughput toxicokinetic modeling package titled ‘httk’ are demonstrated on a suite of environmentally relevant chemicals. The httk R package implements high-throughput toxicokinetic modeling (hence, ‘httk’), including a generic physiologically based toxicokinetic (PBTK) model as well as tables of chemical-specific parameters needed to solve the model for hundreds of chemicals. In this activity, the capabilities of ‘httk’ are demonstrated and explored. Example modeling estimates are produced for the high interest environmental chemical, bisphenol-A. Then, an example script is provided to derive the plasma concentration at steady state for an example environmental chemical, bisphenol-A. The concept of reverse toxicokinetics is explained and demonstrated, again using bisphenol-A as an example chemical. This module then demonstrates the derivation of the bioactivity-exposure ratio (BER) across many chemicals leveraging the capabilities of httk, while incorporating exposure measures. BERs are particularly useful in the evaluation of chemical risk, as they take into account both toxicity (i.e., in vitro potency) and exposure rates, the two essential components used in risk calculations for chemical safety and prioritization evaluations. Therefore, the estimates of both potency and exposure and needed to calculate BERs, which are described in this training module. For potency estimates, the ToxCast high-throughput screening library is introduced as an example high-throughput dataset to carry out in vitro to in vivo extrapolation (IVIVE) modeling through httk. ToxCast activity concentrations that elicit 50% maximal bioactivity (AC50) are uploaded and organized as inputs, and then the tenth percentile ToxCast AC50 is calculated for each chemical (in other words, across all ToxCast screening assays, the tenth percentile of AC50 values were carried forward). These concentration estimates then serve as concentration estimates for potency. For exposure estimates, previously generated exposure estimates that have been inferred from CDC NHANES urinary biomonitoring data are used. The bioactivity-exposure ratio (BER) is then calculated across chemicals with both potency and exposure estimate information. This ratio is simply calculated as the ratio of the lower-end equivalent dose (for the most-sensitive 5% of the population) divided by the upper-end estimated exposure (here, the upper bound on the inferred population median exposure). Chemicals are then ranked based on resulting BERs and visualized through plots. The importance of these chemical prioritization are then discussed in relation to environmental health research and corresponding regulatory decisions. Introduction to Toxicokinetic Modeling To understand what toxicokinetic modeling is, consider the following scenario: Simply put, toxicokinetics answers these questions by describing “what the body does to the chemical” after an exposure scenario. More technically, toxicokinetic modeling refers to the evaluation of the uptake and disposition of a chemical in the body. Notes on terminology Pharmacokinetics (PK) is a synonym for toxicokinetics (TK). They are often used interchangeably. PK connotes pharmaceuticals; TK connotes environmental chemicals – but those connotations are weak. A common abbreviation that you will also see in this research field is ADME, which stands for: Absorption: How does the chemical get absorbed into the body tissues? Distribution: Where does the chemical go inside the body? Metabolism: How do enzymes in the body break apart the chemical molecules? Excretion: How does the chemical leave the body? To place this term into the context of TK, TK models describe ADME mathematically by representing the body as compartments and flows. Types of TK models TK models describe the body mathematically as one or more “compartments” connected by “flows.” The compartments represent organs or tissues. Using mass balance equations, the amount or concentration of chemical in each compartment is described as a function of time. Types of models discussed throughout this training module are described here. 1 Compartment Model The simplest TK model is a 1-compartment model, where the body is assumed to be one big well-mixed compartment. 3 Compartment Model A 3-compartment model mathematically incorporates three distinct body compartments, that can exhibit different parameters contributing to their individual mass balance. Commonly used compartments in 3-compartment modeling can include tissues like blood plasma, liver, gut, kidney, and/or ‘rest of body’ terms; though the specific compartments included depend on the chemical under evaluation, exposure scenario, and modeling assumptions. PBTK Model A physiologically-based TK (PBTK) model incorporates compartments and flows that represent real physiological quantities (as opposed to the aforementioned empirical 1- and 3-compartment models). PBTK models have more parameters overall, including parameters representing physiological quantities that are known a priori based on studies of anatomy. The only PBTK model parameters that need to be estimated for each new chemical are parameters representing chemical-body interactions, which can include the following: Rate of hepatic metabolism of chemical: How fast does liver break down chemical? Plasma protein binding: How tightly does the chemical bind to proteins in blood plasma? Liver may not be able to break down chemical that is bound to plasma protein. Blood:tissue partition coefficients: Assuming chemical diffuses between blood and other tissues very fast compared to the rate of blood flow, the ratio of concentration in blood to concentration in each tissue is approximately constant = partition coefficient. Rate of active transport into/out of a tissue: If chemical moves between blood and tissues not just by passive diffusion, but by cells actively transporting it in or out of the tissue Binding to other tissues: Some chemical may be bound inside a tissue and not available for diffusion or transport in/out Types of TK modeling can also fall into the following major categories: 1. Forward TK Modeling: Where external exposure doses are converted into internal doses (or concentrations of chemicals/drugs in one or more body tissues of interest) 2. Reverse TK Modeling: The reverse of the above, where internal doses are converted into external exposure doses. Other TK modeling resources For further information on TK modeling background, math, and example models, there are additional resources online including a helpful course website on Basic Pharmacokinetics by Dr. Bourne. Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if(!nzchar(system.file(package = &quot;ggplot2&quot;))){ install.packages(&quot;ggplot2&quot;)} if(!nzchar(system.file(package = &quot;reshape2&quot;))){ install.packages(&quot;reshape2&quot;)} if(!nzchar(system.file(package = &quot;stringr&quot;))){ install.packages(&quot;stringr&quot;)} if(!nzchar(system.file(package = &quot;httk&quot;))){ install.packages(&quot;httk&quot;)} if(!nzchar(system.file(package = &quot;eulerr&quot;))){ install.packages(&quot;eulerr&quot;)} Loading R packages required for this session library(ggplot2) # ggplot2 will be used to generate associated graphics library(reshape2) # reshape2 will be used to organize and transform datasets library(stringr) # stringr will be used to aid in various data manipulation steps through this module library(httk) # httk package will be used to carry out all toxicokinetic modeling steps library(eulerr) #eulerr package will be used to generate Venn/Euler diagram graphics For more information on the ggplot2 package, see its associated CRAN webpage and RDocumentation webpage. For more information on the reshape2 package, see its associated CRAN webpage and RDocumentation webpage. For more information on the stringr package, see its associated CRAN webpage and RDocumentation webpage. For more information on the httk package, see its associated CRAN webpage and parent publication by Pearce et al. (2017). More information on the httk package You can see an overview of the httk package by typing ?httk at the R command line. You can see a browsable index of all functions in the httk package by typing help(package=\"httk\") at the R command line. You can see a browsable list of vignettes by typing browseVignettes(\"httk\") at the R command line. (Please note that some of these vignettes were written using older versions of the package and may no longer work as written – specifically the Ring (2017) vignette, which I wrote back in 2016. The httk team is actively working on updating these.) You can get information about any function in httk, or indeed any function in any R package, by typing help() and placing the function name in quotation marks inside the parentheses. For example, to get information about the httk function solve_model(), type this: help(&quot;solve_model&quot;) Note that this module was run with httk version 2.4.0. Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: After solving the TK model that evaluates bisphenol-A, what is the maximum concentration of bisphenol-A estimated to occur in human plasma, after 1 exposure dose of 1 mg/kg/day? After solving the TK model that evaluates bisphenol-A, what is the steady-state concentration of bisphenol-A estimated to occur in human plasma, for a long-term oral infusion dose of 1 mg/kg/day? What is the predicted range of bisphenol-A concentrations in plasma that can occur in a human population, assuming a long-term exposure rate of 1 mg/kg/day and steady-state conditions? Provide estimates at the 5th, 50th, and 95th percentile? Considering the chemicals evaluated in the above TK modeling example, do the \\(C_{ss}\\)-dose slope distributions become wider as the median \\(C_{ss}\\)-dose slope increases? How many chemicals have available AC50 values to evaluate in the current ToxCast/Tox21 high-throughput screening database? What are the chemicals with the three lowest predicted equivalent doses (for tenth-percentile ToxCast AC50s), for the most-sensitive 5% of the population? Based on httk modeling estimates, are chemicals with higher bioactivity-exposure ratios always less potent than chemicals with lower bioactivity-exposure ratios? Based on httk modeling estimates, do chemicals with higher bioactivity-exposure ratios always have lower estimated exposures than chemicals with lower bioactivity-exposure ratios? How are chemical prioritization results different when using only hazard information vs. only exposure information vs. bioactivity-exposure ratios? Of the three data sets used in this training module – bioactivity from ToxCast, TK data from httk, and exposure inferred from NHANES urinary biomonitoring – which one most limits the number of chemicals that can be prioritized using BERs? Data and Models used in Toxicokinetic Modeling (TK) Common Models used in TK Modeling, that are Provided as Built-in Models in ‘httk’ There are five TK models currently built into httk. They are: pbtk: A physiologically-based TK model with oral absorption. Contains the following compartments: gutlumen, gut, liver, kidneys, veins, arteries, lungs, and the rest of the body. Chemical is metabolized by the liver and excreted by the kidneys via glomerular filtration. gas_pbtk: A PBTK model with absorption via inhalation. Contains the same compartments as pbtk. 1compartment: A simple one-compartment TK model with oral absorption. 3compartment: A three-compartment TK model with oral absorption. Compartments are gut, liver, and rest of body. 3compartmentss: The steady-state solution to the 3-compartment model under an assumption of constant infusion dosing, without considering tissue partitioning. This was the first httk model (see Wambaugh et al. 2015, Wetmore et al. 2012, Rotroff et al. 2010). Chemical-Specific TK Data Built Into ‘httk’ Each of these TK models has chemical-specific parameters. The chemical-specific TK information needed to parameterize these models is built into httk, in the form of a built-in lookup table in a data.frame called chem.physical_and_invitro.data. This lookup table means that in order to run a TK model for a particular chemical, you only need to specify the chemical. Look at the first few rows of this data.frame to see everything that’s in there (it is a lot of information). head(chem.physical_and_invitro.data) ## Compound CAS ## 2971-36-0 2,2-bis(4-hydroxyphenyl)-1,1,1-trichloroethane (hpte) 2971-36-0 ## 94-75-7 2,4-d 94-75-7 ## 94-82-6 2,4-db 94-82-6 ## 90-43-7 2-phenylphenol 90-43-7 ## 1007-28-9 6-desisopropylatrazine 1007-28-9 ## 71751-41-2 Abamectin 71751-41-2 ## CAS.Checksum DTXSID Formula ## 2971-36-0 TRUE DTXSID8022325 C14H11Cl3O2 ## 94-75-7 TRUE DTXSID0020442 C8H6Cl2O3 ## 94-82-6 TRUE DTXSID7024035 C10H10Cl2O3 ## 90-43-7 TRUE DTXSID2021151 C12H10O ## 1007-28-9 TRUE DTXSID0037495 C5H8ClN5 ## 71751-41-2 TRUE DTXSID8023892 &lt;NA&gt; ## All.Compound.Names ## 2971-36-0 2,2-bis(4-hydroxyphenyl)-1,1,1-trichloroethane (hpte)|2,2-bis(4-hydroxyphenyl)-1,1,1-trichloroethane|Dtxsid8022325|2971-36-0 ## 94-75-7 2,4-d|Dichlorophenoxy|2,4-dichlorophenoxyacetic acid|94-75-7|Dtxsid0020442 ## 94-82-6 2,4-db|2,4-dichlorophenoxybutyric acid|Dtxsid7024035|94-82-6 ## 90-43-7 2-phenylphenol|Dtxsid2021151|90-43-7 ## 1007-28-9 6-desisopropylatrazine|Deisopropylatrazine|Dtxsid0037495|1007-28-9 ## 71751-41-2 Abamectin|71751-41-2 ## logHenry logHenry.Reference logMA logMA.Reference logP ## 2971-36-0 -7.179 EPA-CCD-OPERA NA &lt;NA&gt; 4.622 ## 94-75-7 -8.529 EPA-CCD-OPERA NA &lt;NA&gt; 2.809 ## 94-82-6 -8.833 EPA-CCD-OPERA NA &lt;NA&gt; 3.528 ## 90-43-7 -7.143 EPA-CCD-OPERA 3.46 Endo 2011 3.091 ## 1007-28-9 -8.003 EPA-CCD-OPERA NA &lt;NA&gt; 1.150 ## 71751-41-2 NA &lt;NA&gt; NA &lt;NA&gt; 4.480 ## logP.Reference logPwa logPwa.Reference logWSol logWSol.Reference ## 2971-36-0 EPA-CCD-OPERA 4.528 EPA-CCD-OPERA -3.707 EPA-CCD-OPERA ## 94-75-7 EPA-CCD-OPERA 5.840 EPA-CCD-OPERA -2.165 EPA-CCD-OPERA ## 94-82-6 EPA-CCD-OPERA 4.998 EPA-CCD-OPERA -3.202 EPA-CCD-OPERA ## 90-43-7 EPA-CCD-OPERA 6.108 EPA-CCD-OPERA -1.812 EPA-CCD-OPERA ## 1007-28-9 EPA-CCD-OPERA 6.989 EPA-CCD-OPERA -2.413 EPA-CCD-OPERA ## 71751-41-2 Tonnelier 2012 NA &lt;NA&gt; NA &lt;NA&gt; ## MP MP.Reference MW MW.Reference pKa_Accept ## 2971-36-0 171.40 EPA-CCD-OPERA 317.6 EPA-CCD-OPERA &lt;NA&gt; ## 94-75-7 140.60 EPA-CCD-OPERA 221.0 EPA-CCD-OPERA &lt;NA&gt; ## 94-82-6 118.10 EPA-CCD-OPERA 249.1 EPA-CCD-OPERA &lt;NA&gt; ## 90-43-7 59.03 EPA-CCD-OPERA 170.2 EPA-CCD-OPERA &lt;NA&gt; ## 1007-28-9 155.00 EPA-CCD-OPERA 173.6 EPA-CCD-OPERA 3.43 ## 71751-41-2 NA &lt;NA&gt; 819.0 Tonnelier 2012 &lt;NA&gt; ## pKa_Accept.Reference pKa_Donor pKa_Donor.Reference ## 2971-36-0 &lt;NA&gt; 8.33 OPERAv2.9 ## 94-75-7 &lt;NA&gt; 2.42 OPERAv2.9 ## 94-82-6 &lt;NA&gt; 3.11 OPERAv2.9 ## 90-43-7 &lt;NA&gt; 9.35 OPERAv2.9 ## 1007-28-9 OPERAv2.9 &lt;NA&gt; &lt;NA&gt; ## 71751-41-2 &lt;NA&gt; 12.47,13.17,13.80 Strope 2018 ## All.Species Dog.Foral Dog.Foral.Reference DTXSID.Reference ## 2971-36-0 Human NA &lt;NA&gt; EPA-CCD-OPERA ## 94-75-7 Human|Rat NA &lt;NA&gt; EPA-CCD-OPERA ## 94-82-6 Human NA &lt;NA&gt; EPA-CCD-OPERA ## 90-43-7 Human NA &lt;NA&gt; EPA-CCD-OPERA ## 1007-28-9 Human NA &lt;NA&gt; EPA-CCD-OPERA ## 71751-41-2 Human NA &lt;NA&gt; EPA-CCD-OPERA ## Formula.Reference Human.Caco2.Pab Human.Caco2.Pab.Reference ## 2971-36-0 EPA-CCD-OPERA &lt;NA&gt; &lt;NA&gt; ## 94-75-7 EPA-CCD-OPERA 13.4,7.44,24.1 HondaUnpublished ## 94-82-6 EPA-CCD-OPERA &lt;NA&gt; &lt;NA&gt; ## 90-43-7 EPA-CCD-OPERA &lt;NA&gt; &lt;NA&gt; ## 1007-28-9 EPA-CCD-OPERA 52.4,29.2,94.3 HondaUnpublished ## 71751-41-2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## Human.Clint Human.Clint.pValue Human.Clint.pValue.Reference ## 2971-36-0 136.5 0.0000357 Wetmore 2012 ## 94-75-7 0 0.1488000 Wetmore 2012 ## 94-82-6 0 0.1038000 Wetmore 2012 ## 90-43-7 2.077 0.1635000 Wetmore 2012 ## 1007-28-9 0 0.5387000 Wetmore 2012 ## 71751-41-2 5.24 0.0009170 Wetmore 2012 ## Human.Clint.Reference Human.Fabs Human.Fabs.Reference Human.Fgut ## 2971-36-0 Wetmore 2012 NA &lt;NA&gt; NA ## 94-75-7 Wetmore 2012 NA &lt;NA&gt; NA ## 94-82-6 Wetmore 2012 NA &lt;NA&gt; NA ## 90-43-7 Wetmore 2012 NA &lt;NA&gt; NA ## 1007-28-9 Wetmore 2012 NA &lt;NA&gt; NA ## 71751-41-2 Wetmore 2012 NA &lt;NA&gt; NA ## Human.Fgut.Reference Human.Fhep Human.Fhep.Reference Human.Foral ## 2971-36-0 &lt;NA&gt; NA &lt;NA&gt; NA ## 94-75-7 &lt;NA&gt; NA &lt;NA&gt; NA ## 94-82-6 &lt;NA&gt; NA &lt;NA&gt; NA ## 90-43-7 &lt;NA&gt; NA &lt;NA&gt; NA ## 1007-28-9 &lt;NA&gt; NA &lt;NA&gt; NA ## 71751-41-2 &lt;NA&gt; NA &lt;NA&gt; NA ## Human.Foral.Reference Human.Funbound.plasma ## 2971-36-0 &lt;NA&gt; 0 ## 94-75-7 &lt;NA&gt; 0.04001 ## 94-82-6 &lt;NA&gt; 0.006623 ## 90-43-7 &lt;NA&gt; 0.04105 ## 1007-28-9 &lt;NA&gt; 0.4588 ## 71751-41-2 &lt;NA&gt; 0.06687 ## Human.Funbound.plasma.Reference Human.Rblood2plasma ## 2971-36-0 Wetmore 2012 NA ## 94-75-7 Wetmore 2012 2.11 ## 94-82-6 Wetmore 2012 NA ## 90-43-7 Wetmore 2012 NA ## 1007-28-9 Wetmore 2012 NA ## 71751-41-2 Wetmore 2012 NA ## Human.Rblood2plasma.Reference Monkey.Foral Monkey.Foral.Reference ## 2971-36-0 &lt;NA&gt; NA &lt;NA&gt; ## 94-75-7 TNO NA &lt;NA&gt; ## 94-82-6 &lt;NA&gt; NA &lt;NA&gt; ## 90-43-7 &lt;NA&gt; NA &lt;NA&gt; ## 1007-28-9 &lt;NA&gt; NA &lt;NA&gt; ## 71751-41-2 &lt;NA&gt; NA &lt;NA&gt; ## Mouse.Foral Mouse.Foral.Reference Mouse.Funbound.plasma ## 2971-36-0 NA &lt;NA&gt; &lt;NA&gt; ## 94-75-7 NA &lt;NA&gt; &lt;NA&gt; ## 94-82-6 NA &lt;NA&gt; &lt;NA&gt; ## 90-43-7 NA &lt;NA&gt; &lt;NA&gt; ## 1007-28-9 NA &lt;NA&gt; &lt;NA&gt; ## 71751-41-2 NA &lt;NA&gt; &lt;NA&gt; ## Mouse.Funbound.plasma.Reference Rabbit.Funbound.plasma ## 2971-36-0 &lt;NA&gt; &lt;NA&gt; ## 94-75-7 &lt;NA&gt; &lt;NA&gt; ## 94-82-6 &lt;NA&gt; &lt;NA&gt; ## 90-43-7 &lt;NA&gt; &lt;NA&gt; ## 1007-28-9 &lt;NA&gt; &lt;NA&gt; ## 71751-41-2 &lt;NA&gt; &lt;NA&gt; ## Rabbit.Funbound.plasma.Reference Rat.Clint Rat.Clint.pValue ## 2971-36-0 &lt;NA&gt; &lt;NA&gt; NA ## 94-75-7 &lt;NA&gt; 0 0.1365 ## 94-82-6 &lt;NA&gt; &lt;NA&gt; NA ## 90-43-7 &lt;NA&gt; &lt;NA&gt; NA ## 1007-28-9 &lt;NA&gt; &lt;NA&gt; NA ## 71751-41-2 &lt;NA&gt; &lt;NA&gt; NA ## Rat.Clint.pValue.Reference Rat.Clint.Reference Rat.Foral ## 2971-36-0 &lt;NA&gt; &lt;NA&gt; NA ## 94-75-7 Wetmore 2013 Wetmore 2013 1 ## 94-82-6 &lt;NA&gt; &lt;NA&gt; NA ## 90-43-7 &lt;NA&gt; &lt;NA&gt; NA ## 1007-28-9 &lt;NA&gt; &lt;NA&gt; NA ## 71751-41-2 &lt;NA&gt; &lt;NA&gt; NA ## Rat.Foral.Reference Rat.Funbound.plasma ## 2971-36-0 &lt;NA&gt; &lt;NA&gt; ## 94-75-7 Wambaugh 2018 0.02976 ## 94-82-6 &lt;NA&gt; &lt;NA&gt; ## 90-43-7 &lt;NA&gt; &lt;NA&gt; ## 1007-28-9 &lt;NA&gt; &lt;NA&gt; ## 71751-41-2 &lt;NA&gt; &lt;NA&gt; ## Rat.Funbound.plasma.Reference Rat.Rblood2plasma ## 2971-36-0 &lt;NA&gt; NA ## 94-75-7 Wetmore 2013 NA ## 94-82-6 &lt;NA&gt; NA ## 90-43-7 &lt;NA&gt; NA ## 1007-28-9 &lt;NA&gt; NA ## 71751-41-2 &lt;NA&gt; NA ## Rat.Rblood2plasma.Reference Chemical.Class ## 2971-36-0 &lt;NA&gt; ## 94-75-7 &lt;NA&gt; ## 94-82-6 &lt;NA&gt; ## 90-43-7 &lt;NA&gt; ## 1007-28-9 &lt;NA&gt; ## 71751-41-2 &lt;NA&gt; The table contains chemical identifiers: name, CASRN (Chemical Abstract Service Registry Number), and DTXSID (DSSTox ID, a chemical identifier from the EPA Distributed Structure-Searchable Toxicity Database, DSSTox for short – more information can be found at https://www.epa.gov/chemical-research/distributed-structure-searchable-toxicity-dsstox-database). The table also contains physical-chemical properties for each chemical. These are used in predicting tissue partitioning. The table contains in vitro measured chemical-specific TK parameters, if available. These chemical-specific parameters include intrinsic hepatic clearance (Clint) and fraction unbound to plasma protein (Funbound.plasma) for each chemical. It also contains measured values for oral absorption fraction Fgutabs, and for the partition coefficient between blood and plasma Rblood2plasma, if these values have been measured for a given chemical. If available, there may be chemical-specific TK values for multiple species. Listing chemicals for which a TK model can be parameterized You can easily get a list of all the chemicals for which a specific TK model can be parameterized (for a given species, if needed) using the function get_cheminfo(). For example, here is how you get a list of all the chemicals for which the PBTK model can be parameterized for humans. chems_pbtk &lt;- get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = &quot;pbtk&quot;, species = &quot;Human&quot;) ## Warning in get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = &quot;pbtk&quot;, : Excluding compounds that have one or more needed parameters missing in chem.physical_and_invitro.table. ## ## For model pbtk each chemical must have non-NA values for:Human.Clint, Human.Funbound.plasma, logP, MW ## Warning in get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = &quot;pbtk&quot;, ## : Excluding compounds without a &#39;fup&#39; value (i.e. fup value = NA) or a &#39;fup&#39; ## value of 0. ## Warning in get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = &quot;pbtk&quot;, ## : Excluding compounds with uncertain &#39;fup&#39; confidence/credible intervals. ## Warning in get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = &quot;pbtk&quot;, ## : Excluding compounds that do not have a clint value or distribution of clint ## values. ## Warning in get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = &quot;pbtk&quot;, ## : Excluding volatile compounds defined as log.Henry &gt;= -4.5. ## Warning in get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = &quot;pbtk&quot;, ## : Excluding compounds that are categorized in one or more of the following ## chemical classes: PFAS. head(chems_pbtk) #first few rows ## Compound CAS DTXSID ## 1 2,4-d 94-75-7 DTXSID0020442 ## 2 2,4-db 94-82-6 DTXSID7024035 ## 3 2-phenylphenol 90-43-7 DTXSID2021151 ## 4 6-desisopropylatrazine 1007-28-9 DTXSID0037495 ## 5 Abamectin 71751-41-2 DTXSID8023892 ## 6 Acephate 30560-19-1 DTXSID8023846 How many such chemicals have parameter data to run a PBTK model in this package? nrow(chems_pbtk) ## [1] 957 Here is how you get all the chemicals for which the 3-compartment steady-state model can be parameterized for humans. chems_3compss &lt;- get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = &quot;3compartmentss&quot;, species = &quot;Human&quot;) ## Warning in get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = &quot;3compartmentss&quot;, : Excluding compounds that have one or more needed parameters missing in chem.physical_and_invitro.table. ## ## For model 3compartmentss each chemical must have non-NA values for:Human.Clint, Human.Funbound.plasma, logP, MW ## Warning in get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = ## &quot;3compartmentss&quot;, : Excluding compounds without a &#39;fup&#39; value (i.e. fup value = ## NA). ## Warning in get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = ## &quot;3compartmentss&quot;, : Excluding compounds with uncertain &#39;fup&#39; ## confidence/credible intervals. ## Warning in get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = ## &quot;3compartmentss&quot;, : Excluding compounds that do not have a clint value or ## distribution of clint values. ## Warning in get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = ## &quot;3compartmentss&quot;, : Excluding volatile compounds defined as log.Henry &gt;= -4.5. ## Warning in get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = ## &quot;3compartmentss&quot;, : Excluding compounds that are categorized in one or more of ## the following chemical classes: PFAS. How many such chemicals have parameter data to run a 3-compartment steady-state model in this package? nrow(chems_3compss) ## [1] 1021 The 3-compartment steady-state model can be parameterized for a few more chemicals than the PBTK model, because it is a simpler model and requires less data to parameterize. Specifically, the 3-compartment steady-state model does not require estimating tissue partition coefficients, unlike the PBTK model. Solving Toxicokinetic Models to Obtain Internal Chemical Concentration vs. Time Predictions You can solve any of the models for a specified chemical and specified dosing protocol, and get concentration vs. time predictions, using the function solve_model(). For example: sol_pbtk &lt;- solve_model(chem.name = &quot;Bisphenol-A&quot;, #chemical to simulate model = &quot;pbtk&quot;, #TK model to use dosing = list(initial.dose = NULL, #for repeated dosing, if first dose is different from the rest, specify first dose here doses.per.day = 1, #number of doses per day daily.dose = 1, #total daily dose in mg/kg units dosing.matrix = NULL), #used to specify more complicated dosing protocols days = 1) #number of days to simulate ## None of the monitored components undergo unit conversions (i.e. conversion factor of 1). ## ## AUC is area under the plasma concentration curve in uM*days units with Rblood2plasma = 0.795. ## The model outputs are provided in the following units: ## umol: Agutlumen, Atubules, Ametabolized ## uM: Cgut, Cliver, Cven, Clung, Cart, Crest, Ckidney, Cplasma ## uM*days: AUC There are some cryptic-sounding warnings that can safely be ignored. (They are providing information about certain assumptions that were made while solving the model). Then there is a final message providing the units of the output. The output, assigned to sol_pbtk, is a matrix with concentration vs. time data for each of the compartments in the pbtk model. Time is in units of days. Additionally, the output traces the amount excreted via passive renal filtration (Atubules), the amount metabolized in the liver (Ametabolized), and the cumulative area under the curve for plasma concentration vs. time (AUC). Here are the first few rows of sol_pbtk so you can see the format. head(sol_pbtk) ## time Agutlumen Cgut Cliver Cven Clung Cart Crest ## [1,] 0.0000 0.0 0.0000 0.000000 0.000000 0.0000 0.00000 0.000000 ## [2,] 0.0001 197.3 0.1582 0.000479 0.000001 0.0000 0.00000 0.000000 ## [3,] 0.0104 180.0 10.1300 3.063000 0.036390 0.2967 0.03130 0.007976 ## [4,] 0.0208 164.1 13.6200 7.596000 0.102500 0.8846 0.09679 0.056220 ## [5,] 0.0312 149.6 14.7600 11.110000 0.162000 1.4230 0.15740 0.149600 ## [6,] 0.0416 136.3 14.9800 13.380000 0.206900 1.8320 0.20360 0.275500 ## Ckidney Cplasma Atubules Ametabolized AUC ## [1,] 0.0000 0.000000 0.000000 0.000000 0.000000 ## [2,] 0.0000 0.000001 0.000000 0.000000 0.000000 ## [3,] 0.3831 0.045780 0.000583 0.003302 0.000170 ## [4,] 1.7530 0.128900 0.004221 0.018490 0.001074 ## [5,] 3.2900 0.203700 0.011570 0.045260 0.002818 ## [6,] 4.5510 0.260300 0.021980 0.080160 0.005247 You can plot the results, for example plasma concentration vs. time. sol_pbtk &lt;- as.data.frame(sol_pbtk) #because ggplot2 requires data.frame input, not matrix ggplot(sol_pbtk) + geom_line(aes(x = time, y = Cplasma)) + theme_bw() + xlab(&quot;Time, days&quot;) + ylab(&quot;Cplasma, uM&quot;) + ggtitle(&quot;Plasma concentration vs. time for single dose 1 mg/kg Bisphenol-A&quot;) Calculating summary metrics of internal dose produced from TK models We can calculate summary metrics of internal dose – peak concentration, average concentration, and AUC – using the function calc_tkstats(). We have to specify the dosing protocol and length of simulation. Here, we use the same dosing protocol and simulation length as in the plot above. tkstats &lt;- calc_tkstats(chem.name = &quot;Bisphenol-A&quot;, #chemical to simulate stats = c(&quot;AUC&quot;, &quot;peak&quot;, &quot;mean&quot;), #which metrics to return (these are the only three choices) model = &quot;pbtk&quot;, #model to use tissue = &quot;plasma&quot;, #tissue for which to return internal dose metrics days = 1, #length of simulation daily.dose = 1, #total daily dose in mg/kg/day doses.per.day = 1) #number of doses per day ## Human plasma concentrations returned in uM units. ## AUC is area under plasma concentration curve in uM * days units with Rblood2plasma = 0.7949 . print(tkstats) ## $AUC ## [1] 0.3163 ## ## $peak ## [1] 0.3779 ## ## $mean ## [1] 0.3163 Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question #1: After solving the TK model that evaluates bisphenol-A, what is the maximum concentration of bisphenol-A estimated to occur in human plasma, after 1 exposure dose of 1 mg/kg/day? Answer: The peak plasma concentration estimate for bisphenol-A, under the conditions tested, is 0.3779 uM. Calculating steady-state concentration Another summary metric is the steady-state concentration: If the same dose is given repeatedly over many days, the body concentration will (usually) reach a steady state after some time. The value of this steady-state concentration, and the time needed to achieve steady state, are different for different chemicals. Steady-state concentrations are useful when considering long-term, low-level exposures, which is frequently the situation in environmental health. For example, here is a plot of plasma concentration vs. time for 1 mg/kg/day Bisphenol-A, administered for 12 days. You can see how the average plasma concentration reaches a steady state around 1.5 uM. Each peak represents one day’s dose. foo &lt;- as.data.frame(solve_pbtk( chem.name=&#39;Bisphenol-A&#39;, daily.dose=1, days=12, doses.per.day=1, tsteps=2)) ## None of the monitored components undergo unit conversions (i.e. conversion factor of 1). ## ## AUC is area under the plasma concentration curve in uM*days units with Rblood2plasma = 0.795. ## The model outputs are provided in the following units: ## umol: Agutlumen, Atubules, Ametabolized ## uM: Cgut, Cliver, Cven, Clung, Cart, Crest, Ckidney, Cplasma ## uM*days: AUC ggplot(foo) + geom_line(aes(x = time, y= Cplasma)) + scale_x_continuous(breaks = seq(0,12)) + xlab(&quot;Time, days&quot;) + ylab(&quot;Cplasma, uM&quot;) httk includes a function calc_analytic_css() to calculate the steady-state plasma concentration (\\(C_{ss}\\) for short) analytically for each model, for a specified chemical and daily oral dose. This function assumes that the daily oral dose is administered as an oral infusion, rather than a single oral bolus dose – in effect, that the daily dose is divided into many small doses over the day. Therefore, the result of calc_analytic_css() may be slightly different than our previous estimate based on the concentration vs. time plot from a single oral bolus dose every day. Here is the result of calc_analytic_css() for a 1 mg/kg/day dose of bisphenol-A. calc_analytic_css(chem.name = &quot;Bisphenol-A&quot;, daily.dose = 1, output.units = &quot;uM&quot;, model = &quot;pbtk&quot;, concentration = &quot;plasma&quot;) ## Plasma concentration returned in uM units. ## [1] 0.9417 Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: After solving the TK model that evaluates bisphenol-A, what is the steady-state concentration of bisphenol-A estimated to occur in human plasma, for a long-term oral infusion dose of 1 mg/kg/day? Answer: The steady-state plasma concentration estimate for bisphenol-A, under the conditions tested, is 0.9417 uM. Steady-state concentration is linear with dose for httk models For the TK models included in the httk package, steady-state concentration is linear with dose for a given chemical. The slope of the line is simply the steady-state concentration for a dose of 1 mg/kg/day. This can be shown by solving calc_analytic_css() for several doses, and plotting the dose-\\(C_{ss}\\) points along a line whose slope is equal to \\(C_{ss}\\) for 1 mg/kg/day. #choose five doses at which to find the Css doses &lt;- c(0.1, #all mg/kg/day 0.5, 1.0, 1.5, 2.0) suppressWarnings(bpa_css &lt;- sapply(doses, function(dose) calc_analytic_css(chem.name = &quot;Bisphenol-A&quot;, daily.dose = dose, output.units = &quot;uM&quot;, model = &quot;pbtk&quot;, concentration = &quot;plasma&quot;, suppress.messages = TRUE))) DF &lt;- data.frame(dose = doses, Css = bpa_css) #Plot the results Cssdosefig &lt;- ggplot(DF) + geom_point(aes(x = dose, y = Css), size = 3) + geom_abline( #plot a straight line intercept = 0, #intercept 0 slope = DF[DF$dose==1, #slope = Css for 1 mg/kg/day &quot;Css&quot;], linetype = 2 ) + xlab(&quot;Daily dose, mg/kg/day&quot;) + ylab(&quot;Css, uM&quot;) print(Cssdosefig) Reverse Toxicokinetics In the previous TK examples, we started with a specified dosing protocol, then solved the TK models to find the resulting concentration in the body (e.g., in plasma). This allows us to convert from external exposure metrics to internal exposure metrics. However, many environmental health questions require the reverse: converting from internal exposure metrics to external exposure metrics. For example, when health effects of environmental chemicals are studied in epidemiological cohorts, adverse health effects are often related to internal exposure metrics, such as blood or plasma concentration of a chemical. Similarly, in vitro studies of chemical bioactivity (for example, the ToxCast program) relate bioactivity to in vitro concentration, which can be consdered analogous to internal exposure or body concentration. So we may know the internal exposure level associated with some adverse health effect of a chemical. However, risk assessors and risk managers typically control external exposure to reduce the risk of adverse health effects. They need some way to start from an internal exposure associated with adverse health effects, and convert to the corresponding external exposure. The solution is reverse toxicokinetics (reverse TK). Starting with a specified internal exposure metric (body concentration), solve the TK model in reverse to find the corresponding external exposure that produced that concentration. When exposures are long-term and low-level (as environmental exposures often are), then the relevant internal exposure metric is the steady-state concentration. In this case, it is useful to remember the linear relationship between \\(C_{ss}\\) and dose for the httk TK models. It gives you a quick and easy way to perform reverse TK for the steady-state case. The procedure is illustrated graphically below. Begin with a “target” concentration on the y-axis (labeled \\(C_{\\textrm{target}}\\)). For example, \\(C_{\\textrm{target}}\\) may be the in vitro concentration associated with bioactivity in a ToxCast assay, or the plasma concentration associated with an adverse health effect in an epidemiological study. Draw a horizontal line over to the \\(C_{ss}\\)-dose line. Drop down vertically to the x-axis and read off the corresponding dose. This is the administered equivalent dose (AED): the the external dose or exposure rate, in mg/kg/day, that would produce an internal steady-state plasma concentration equal to the target concentration. Mathematically, the relation is very simple: \\[ AED = \\frac{C_{\\textrm{target}}}{C_{ss}\\textrm{-dose slope}} \\] Since the \\(C_{ss}\\)-dose slope is simply \\(C_{ss}\\) for a daily dose of 1 mg/kg/day, this equation can be rewritten as \\[ AED = \\frac{C_{\\textrm{target}}}{C_{ss}\\textrm{ for 1 mg/kg/day}} \\] Capturing Population Variability in Toxicokinetics, and Uncertainty in Chemical-Specific Parameters For a given dose, \\(C_{ss}\\) is determined by the values of the parameters of the TK model. These parameters describe absorption, distribution, metabolism, and excretion (ADME) of each chemical. They include both chemical-specific parameters, describing hepatic clearance and protein binding, and chemical-independent parameters, describing physiology. A table of these parameters is presented below. Parameter Details Estimated Type Intrinsic hepatic clearance rate Rate at which liver removes chemical from blood Measured in vitro Chemical-specific Fraction unbound to plasma protein Free fraction of chemical in plasma Measured in vitro Chemical-specific Tissue:plasma partition coefficients Ratio of concentration in body tissues to concentration in plasma Estimated from chemical and tissue properties Chemical-specific Tissue masses Mass of each body tissue (including total body weight) From anatomical literature Chemical-independent Tissue blood flows Blood flow rate to each body tissue From anatomical literature Chemical-independent Glomerular filtration rate Rate at which kidneys remove chemical from blood From anatomical literature Chemical-independent Hepatocellularity Number of cells per mg liver From anatomical literature Chemical-independent Because these parameters represent physiology and chemical-body interactions, their exact values will vary across individuals in a population, reflecting population physiological variability. Additionally, parameters are subject to measurement uncertainty. Since the \\(C_{ss}\\)-dose relation is determined by these parameters, variability and uncertainty in the TK parameters translates directly into variability and uncertainty in \\(C_{ss}\\) for a given dose. In other words, there is a distribution of \\(C_{ss}\\) values for each daily dose level of a chemical. The \\(C_{ss}\\)-dose relationship is still linear when variability and uncertainty are taken into account. However, rather than a single \\(C_{ss}\\)-dose slope, there is a distribution of \\(C_{ss}\\)-dose slopes. Because the \\(C_{ss}\\)-dose slope is simply the \\(C_{ss}\\) value for an exposure rate of 1 mg/kg/day, the distribution of the \\(C_{ss}\\)-dose slope is the same as the \\(C_{ss}\\) distribution for an exposure rate of 1 mg/kg/day. A distribution of \\(C_{ss}\\)-dose slopes is illustrated in the figure below, along with boxplots illustrating the distributions for \\(C_{ss}\\) itself at five different dose levels: 0.05, 0.25, 0.5, 0.75, and 0.95 mg/kg/day. ## Warning: A numeric `legend.position` argument in `theme()` was deprecated in ## ggplot2 3.5.0. ## ℹ Please use the `legend.position.inside` argument of `theme()` ## instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this ## warning was generated. Figure 2: Boxplots: Distributions of Css for five daily dose levels of Bisphenol-A. Boxes extend from 25th to 75th percentile. Lower whisker = 5th percentile; upper whisker = 95th percentile. Lines: Css-dose relations for each quantile. Variability and Uncertainty in Reverse Toxicokinetics Earlier, we found that with a linear \\(C_{ss}\\)-dose relation, reverse toxicokinetics became a matter of a simple linear equation. For a given target concentration – for example, a plasma concentration associated with adverse health effects in vivo, or a concentration associated with bioactivity in vitro – we could predict an AED (administered equivalent dose), the external exposure rate in mg/kg/day that would produce the target concentration at steady state. \\[ AED = \\frac{C_{\\textrm{target}}}{C_{ss}\\textrm{-dose slope}} \\] Since AED depends on the \\(C_{ss}\\)-dose slope, variability and uncertainty in that slope will induce variability and uncertainty in the AED. A distribution of slopes will lead to a distribution of AEDs for the same target concentration. For example, a graphical representation of finding the AED distribution for a target concentration of 1 uM looks like this, for the same arbitrary example chemical used to illustrate the distribution of \\(C_{ss}\\)-dose slopes above. (The lines shown in this plot are the same as the previous plot, but the plot has been “zoomed in” on the y-axis.) The steps are the same as before: Begin with a “target” concentration on the y-axis, here 1 uM. Draw a horizontal line over to intersect each \\(C_{ss}\\)-dose line. Where the horizontal line intersects each \\(C_{ss}\\)-dose line, drop down vertically to the x-axis and read off each corresponding AED (marked with colored circles matching the color of each \\(C_{ss}\\)-dose line). Notice that the line with the steepest, 95th-percentile slope (the purple line) yields the lowest AED (the purple dot, approximately 0.07 mg/kg/day for this example chemical), and the line with the shallowest, 5th-percentile slope (the turquoise blue line) yields the highest AED (the turquoise dot, approximately 2 mg/kg/day for this example chemical). In general, the 95th-percentile \\(C_{ss}\\)-dose slope represents the most-sensitive 5% of the population – individuals who will reach the target concentration in their body with the smallest daily doses. Therefore, using the AED for the 95th-percentile \\(C_{ss}\\)-dose slope is a conservative choice, health-protective for 95% of the estimated population. Monte Carlo approach to simulating variability and uncertainty The httk package implements a Monte Carlo approach for simulating variability and uncertainty in TK. httk first defines distributions for the TK model parameters, representing population variabilty. These distributions are defined based on real data about U.S. population demographics and physiology collected as part of the Centers for Disease Control’s National Health and Nutrition Examination Survey (NHANES) (Ring et al., 2017). TK parameters with known measurement uncertainty (intrinsic hepatic clearance rate and fraction of chemical unbound in plasma) additionally have distributions defined to represent their uncertainty (Wambaugh et al., 2019). Then, httk samples sets of TK parameter values from these distributions (including appropriate correlations: for example, liver mass is correlated with body weight). Each sampled set of TK parameter values represents one “simulated individual.” Next, httk calculates the \\(C_{ss}\\)-dose slope for each “simulated individual.” The resulting sample of \\(C_{ss}\\)-dose slopes can be used to characterize the distribution of \\(C_{ss}\\)-dose slopes – for example, by calculating percentiles. httk makes this whole Monte Carlo process simple and transparent for the user, You just need to call one function, calc_mc_css(), specifying the chemical whose \\(C_{ss}\\)-dose slope distribution you want to calculate. Behind the scenes, httk will perform all the Monte Carlo calculations. It will return percentiles of the \\(C_{ss}\\)-dose slope (by default), or it can return all individual samples of \\(C_{ss}\\)-dose slope (if you want to do some calculations of your own). Chemical-Specific Example Capturing Population Variability for Bisphenol-A Plasma Concentration Estimates The following code estimates the 5th percentile, 50th percentile, and 95th percentile of the \\(C_{ss}\\)-dose slope for the chemical bisphenol-A. For the sake of simplicity, we will use the 3-compartment steady-state model (rather than the PBTK model used in the previous examples). css_examp &lt;- calc_mc_css(chem.name = &quot;Bisphenol-A&quot;, which.quantile = c(0.05, #specify which quantiles to return 0.5, 0.95), model = &quot;3compartmentss&quot;, #which model to use to calculate Css output.units = &quot;uM&quot;) #could also choose mg/Lo ## Human plasma concentration returned in uM units for 0.05 0.5 0.95 quantile. print(css_examp) ## 5% 50% 95% ## 0.2974 1.3460 8.5100 Recall that the \\(C_{ss}\\)-dose slope is the same as \\(C_{ss}\\) for a daily dose of 1 mg/kg/day. The function calc_mc_css() therefore assumes a dose of 1 mg/kg/day and calculates the resulting \\(C_{ss}\\) distribution. If you need to calculate the \\(C_{ss}\\) distribution for a different dose, e.g. 2 mg/kg/day, you can simply multiply the \\(C_{ss}\\) percentiles from calc_mc_css() by your desired dose. The steady-state plasma concentration for 1 mg/kg/day dose is returned in units of uM. The three requested quantiles are returned as a named numeric vector (whose names in this case are 5%, 50%, and 95%). Answer to Environmental Health Question 3 With this, we can answer Environmental Health Question #3: What is the predicted range of bisphenol-A concentrations in plasma that can occur in a human population, assuming a long-term exposure rate of 1 mg/kg/day and steady-state conditions? Provide estimates at the 5th, 50th, and 95th percentile? Answer: For a human population exposed to 1 mg/kg/day bisphenol-A, plasma concentrations are estimated to be 0.2974 uM at the 5th percentile, 1.346 uM at the 50th percentile, and 8.51 uM at the 95th percentile. High-Throughput Example Capturing Population Variability for ~1000 Chemicals We can easily and (fairly) quickly do this for all 998 chemicals for which the 3-compartment steady-state model can be parameterized, using sapply() to loop over the chemicals. This will take a few minutes to run (for example, it takes about 10-15 minutes on a Dell Latitude with an Intel i7 processor). In order to make the Monte Carlo sampling reproducible, set a seed for the random number generator. It doesn’t matter what seed you choose – it can be any integer. Here, the seed is set to 42, because it’s the answer to the ultimate question of life, the universe, and everything (Adams, 1979). set.seed(42) system.time( suppressWarnings( css_3compss &lt;- sapply(chems_3compss$CAS, calc_mc_css, #additional arguments to calc_mc_css() model = &quot;3compartmentss&quot;, which.quantile = c(0.05, 0.5, 0.95), output.units = &quot;uM&quot;, suppress.messages = TRUE) ) ) ## user system elapsed ## 829.89 22.47 906.52 Organizing the results: #css_3compss comes out as a 3 x 998 array, #where rows are quantiles and columns are chemicals #transpose it so that rows are chemicals and columns are quantiles css_3compss &lt;- t(css_3compss) #convert to data.frame css_3compss &lt;- as.data.frame(css_3compss) #make a column for CAS, rather than just leaving it as the row names css_3compss$CAS &lt;- row.names(css_3compss) head(css_3compss) #View first few rows ## 5% 50% 95% CAS ## 2971-36-0 0.1449 0.678 6.102 2971-36-0 ## 94-75-7 18.1500 68.390 431.300 94-75-7 ## 94-82-6 56.9200 232.100 1783.000 94-82-6 ## 90-43-7 23.5200 76.800 344.100 90-43-7 ## 1007-28-9 2.6670 7.203 29.320 1007-28-9 ## 71751-41-2 0.7825 3.198 18.720 71751-41-2 Plotting the \\(C_{ss}\\)-dose slope distribution quantiles across these ~1000 chemicals Here, we will plot the resulting concentration distribution quantiles for each chemical, while sorting the chemicals from lowest to highest median value. By default, ggplot2 will plot the chemical CASRNs in alphabetically-sorted order. To force it to plot them in another order, we have to explicitly specify the desired order. The easiest way to do this is to add a column in the data.frame that contains the chemical names as a factor (categorical) variable, whose levels (categories) are explicitly set to be the CASRNs in our desired plotting order. Then we can tell ggplot2 to plot that factor variable on the x-axis, rather than the original CASRN variable. Set the ordering of the chemical CASRNs from lowest to highest median value chemical_order &lt;- order(css_3compss$`50%`) Create a factor (categorical) CAS column where the factor levels are given by the CASRNs with this ordering. css_3compss$CAS_factor &lt;- factor(css_3compss$CAS, levels = css_3compss$CAS[chemical_order]) For plotting ease, reshape the data.frame into “long” format – rather than having one column for each quantile of the \\(C_{ss}\\) distribution, have a row for each chemical/quantile combination. We use the melt function from the reshape2 package. css_3compss_melt &lt;- melt(css_3compss, id.vars = &quot;CAS_factor&quot;, measure.vars = c(&quot;5%&quot;, &quot;50%&quot;, &quot;95%&quot;), variable.name = &quot;Percentile&quot;, value.name = &quot;Css_slope&quot;) ## Warning in melt.default(css_3compss, id.vars = &quot;CAS_factor&quot;, measure.vars = ## c(&quot;5%&quot;, : The melt generic in data.table has been passed a data.frame and will ## attempt to redirect to the relevant reshape2 method; please note that reshape2 ## is superseded and is no longer actively developed, and this redirection is now ## deprecated. To continue using melt methods from reshape2 while both libraries ## are attached, e.g. melt.list, you can prepend the namespace, i.e. ## reshape2::melt(css_3compss). In the next version, this warning will become an ## error. head(css_3compss_melt) ## CAS_factor Percentile Css_slope ## 1 2971-36-0 5% 0.1449 ## 2 94-75-7 5% 18.1500 ## 3 94-82-6 5% 56.9200 ## 4 90-43-7 5% 23.5200 ## 5 1007-28-9 5% 2.6670 ## 6 71751-41-2 5% 0.7825 Plot the slope percentiles. Use a log scale for the y-axis because the slopes span orders of magnitude. Suppress the x-axis labels (the CASRNs) because they are not readable anyway. ggplot(css_3compss_melt) + geom_point(aes(x=CAS_factor, y = Css_slope, color = Percentile)) + scale_color_brewer(palette = &quot;Set2&quot;) + #use better color scheme than default scale_y_log10() + #use log scale for y axis xlab(&quot;Chemical&quot;) + ylab(&quot;Css-dose slope (uM per mg/kg/day)&quot;) + annotation_logticks(sides = &quot;l&quot;) + #add log ticks to y axis theme_bw() + #plot with white plot background instead of gray theme(axis.text.x = element_blank(), #suppress x-axis labels panel.grid.major.x = element_blank(), #suppress vertical grid lines legend.position = c(0.1,0.8) #place legend in lower right corner ) Chemicals along the x-axis are in order from lowest to highest median (50th percentile) predicted \\(C_{ss}\\)-dose slope. The orange points represent that 50th percentile \\(C_{ss}\\)-dose slope for each chemical. The green points represent the 5th percentile \\(C_{ss}\\)-dose slopes, and the purple points represent the 95th percentile \\(C_{ss}\\)-dose slope for each chemical. Each chemical has one orange point (50th percentile), one green point (5th percentile), and one purple point (95th percentile), characterizing the distribution of \\(C_{ss}\\)-dose slopes across the U.S. population for that chemical. The width of the distribution for each chemical is roughly represented by the vertical distance between the green and purple points for that chemical. Answer to Environmental Health Question 4 With this, we can answer Environmental Health Question #4: Considering the chemicals evaluated in the above TK modeling example, do the \\(C_{ss}\\)-dose slope distributions become wider as the median \\(C_{ss}\\)-dose slope increases? Answer: No – the \\(C_{ss}\\)-dose slope distributions generally become narrower as the median \\(C_{ss}\\)-dose slope increases. This can be seen by looking at the right end of the plot, where the highest-median chemicals are located – the distance between the green points and purple points, representing the 5th and 95th percentiles, are much smaller for these higher-median chemicals. Reverse TK: Calculating Administered Equivalent Doses for ToxCast Bioactive Concentrations As described in an earlier section of this document, the slope defining the linear relation between \\(C_{ss}\\) and dose is useful for reverse toxicokinetics: converting an internal dose metric to an external dose metric. The internal dose metric may, for example, be a concentration associated with an in vivo health effect, or in vitro bioactivity. Here, we will consider in vitro bioactivity – specifically, from the ToxCast program. ToxCast tests chemicals in multiple concentration-response format across a battery of in vitro assays that measure activity in a wide variety of biological endpoints. If a chemical showed any activity in an assay at any of its tested concentrations, then one metric of concentration associated with bioactivity is AC50 – the concentration at which the assay response is halfway between its minimum and its maximum. The module won’t address the details of how ToxCast determines assay activity and AC50s from raw concentration-response data. There is an entire R package for the ToxCast data processing workflow, called tcpl. If you want to learn more about those details, start here. Lots of information is available if you install the tcpl R package and look at the package vignette; it essentially walks you through the full ToxCast data processing workflow. In this module, we will begin with pre-computed ToxCast AC50 values for various chemicals and assays. We will use httk to convert ToxCast AC50 values into administered equivalent doses (AEDs). Loading ToxCast AC50s The latest public release of ToxCast high-throughput screening assay data can be downloaded here. Previous public releases of ToxCast data included a matrix of AC50s by chemical and assay. The data format of the latest public release does not contain this kind of matrix. So this dataset was pre-processed to prepare a simple data.frame of AC50s for each chemical/assay combination for the purposes of this training module. Read in the pre-processed data set and view the first few rows. toxcast &lt;- read.csv(&quot;Module6_6_Input/Module6_6_InputData1.csv&quot;) head(toxcast) ## Compound CAS DTXSID aenm ## 1 Acetohexamide 968-81-0 DTXSID7020007 ACEA_ER_80hr ## 2 2-Methoxyaniline hydrochloride 134-29-2 DTXSID8020092 ACEA_ER_80hr ## 3 Sodium L-ascorbate 134-03-2 DTXSID0020105 ACEA_ER_80hr ## 4 Sodium azide 26628-22-8 DTXSID8020121 ACEA_ER_80hr ## 5 Benzotrichloride 98-07-7 DTXSID1020148 ACEA_ER_80hr ## 6 Benzyl acetate 140-11-4 DTXSID0020151 ACEA_ER_80hr ## log10_ac50 ## 1 0.6524155 ## 2 -1.3141432 ## 3 0.8248535 ## 4 1.9839338 ## 5 1.8370790 ## 6 -0.3299611 The columns of this data frame are: Compound: The compound name. CAS: The compound’s CASRN. DTXSID: The compound’s DSSTox Substance ID. aenm: Assay identifier. “aenm” stands for “Assay Endpoint Name.” More information about the ToxCast assays is available on the ToxCast data download page. log10_ac50: The AC50 for the chemical/assay combination on each row, in log10 uM units. How many ToxCast chemicals are in this data set? length(unique(toxcast$DTXSID)) ## [1] 7863 Answer to Environmental Health Question 5 With this, we can answer Environmental Health Question #5: How many chemicals have available AC50 values to evaluate in the current ToxCast/Tox21 high-throughput screening database? Answer: 7863 chemicals. Subsetting the ToxCast Chemicals to include those that are also in httk Not all of the ToxCast chemicals have TK data built into httk such that we can perform reverse TK using the HTTK models. Let’s subset the ToxCast data to include only the chemicals for which we can run the 3-compartment steady-state models. Previously, we used get_cheminfo() to get a list of chemicals for which we could run the 3-compartment steady state model, including the names, CASRNs, and DSSTox IDs of those chemicals. That list is stored in variable chems_3compss, a data.frame with compound name, CASRN, and DTXSID. Now, we can use that chemical list to subset the ToxCast data. toxcast_httk &lt;- subset(toxcast, subset = toxcast$DTXSID %in% chems_3compss$DTXSID) How many chemicals are in this subset? length(unique(toxcast_httk$DTXSID)) ## [1] 869 There were 869 httk chemicals for which we could run the 3-compartment steady-state model; only 911 of them had ToxCast data. Conversely, most of the 7863 ToxCast chemicals do not have TK data in httk such that we can run the 3-compartment steady state model. Identifying the Lower-Bound In Vitro AC50 Value per Chemical ToxCast/Tox21 screens chemicals across multiple assays, such that each chemical has multiple resulting AC50 values, spanning a range of values. For example, here are boxplots of the AC50s for the first 20 chemicals listed in chems_3compss. Note that the chemical identifiers, DTXSID, are used here in these visualizations to represent unique chemicals. ggplot(toxcast_httk[toxcast_httk$DTXSID %in% chems_3compss[1:20, &quot;DTXSID&quot;], ] ) + geom_boxplot(aes(x=DTXSID, y = log10_ac50)) + ylab(&quot;log10 AC50&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Sometimes we have an interest in getting the equivalent dose for an AC50 for one specific assay. For example, if we happen to be interested in estrogen-receptor activity, we might look specifically at one of the assays that measures estrogen receptor activity. However, sometimes we just want a general idea of what concentrations showed bioactivity in any of the ToxCast assays, regardless of the specific biological endpoint of each assay. In this case, typically, we are interested in a “reasonable lower bound” of bioactive concentrations across assays for each chemical. Intuitively, we suspect that the very lowest AC50s for each chemical might represent false activity. Therefore, we often select the tenth percentile of ToxCast AC50s for each chemical as that “reasonable lower bound” on bioactive concentrations. Let’s calculate the tenth percentile ToxCast AC50 for each chemical. Here, we use the base-R function aggregate, which groups a vector (specified in the x argument) by a list of factors (specified in the by argument), and applies a function to each group (specified in the FUN argument). You can add any extra arguments to the FUN function as named arguments to aggregate. toxcast_httk_P10 &lt;- aggregate(x = toxcast_httk$log10_ac50, #aggregate the AC50s by = list(DTXSID = toxcast_httk$DTXSID), #group AC50s by DTXSID FUN = quantile, #the function to apply to each group prob = 0.1) #an argument to the quantile() function #by default the names of the output data.frame will be &#39;DTXSID&#39; and &#39;x&#39; #let&#39;s change &#39;x&#39; to be a more informative name names(toxcast_httk_P10) &lt;- c(&quot;DTXSID&quot;, &quot;log10_ac50_P10&quot;) Let’s transform the tenth-percentile AC50 values back to the natural scale (they are currently on the log10 scale) and put them in a new column AC50. These AC50s will be in uM. toxcast_httk_P10$AC50 &lt;- 10^(toxcast_httk_P10$log10_ac50_P10) View the first few rows: head(toxcast_httk_P10) ## DTXSID log10_ac50_P10 AC50 ## 1 DTXSID0020022 0.8932512 7.82079968 ## 2 DTXSID0020232 0.2903537 1.95143342 ## 3 DTXSID0020286 -1.3763735 0.04203649 ## 4 DTXSID0020311 1.1513461 14.16922669 ## 5 DTXSID0020319 -0.1934652 0.64052306 ## 6 DTXSID0020365 0.4308058 2.69653361 Calculating Equivalent Doses for 10th Percentile ToxCast AC50s We can calculate equivalent doses in one line of R code – again including all of the Monte Carlo for TK uncertainty and variability – just by using the httk function calc_mc_oral_equiv(). Note that in calc_mc_oral_equiv(), the which.quantile argument refers to the quantile of the \\(C_{ss}\\)-dose slope, not the quantile of the equivalent dose itself. So specifying which.quantile = 0.95 will yield a lower equivalent dose than which.quantile = 0.05. Under the hood, calc_mc_oral_equiv() first calls calc_mc_css() to get percentiles of the \\(C_{ss}\\)-dose slope for a chemical. It then divides a user-specified target concentration (specified in argument conc) by each quantile of \\(C_{ss}\\)-dose slope to get the equivalent dose corresponding to that target concentration for each slope quantile. Here, we’re using the mapply() function in base R to call calc_mc_oral_equiv() in a loop over chemicals. This is because calc_mc_oral_equiv() requires two chemical-specific arguments – the chemical identifier and the concentration for which to compute the equivalent dose. mapply() lets us provide vectors of values for each argument (in the named arguments dtxsid and conc), and will automatically loop over those vectors. We also use the argument MoreArgs, a named list of additional arguments to the function in FUN that will be the same for every iteration of the loop. Note that this line of code takes a few minutes to run. set.seed(42) system.time( suppressWarnings( toxcast_equiv_dose &lt;- mapply(FUN = calc_mc_oral_equiv, conc = toxcast_httk_P10$AC50, dtxsid = toxcast_httk_P10$DTXSID, MoreArgs = list(model = &quot;3compartmentss&quot;, #model to use which.quantile = c(0.05, 0.5, 0.95), #quantiles of Css-dose slope suppress.messages = TRUE) ) ) ) #by default, the results are a 3 x 869 matrix, where rows are quantiles and columns are chemicals toxcast_equiv_dose &lt;- t(toxcast_equiv_dose) #transpose so that rows are chemicals toxcast_equiv_dose &lt;- as.data.frame(toxcast_equiv_dose) #convert to data.frame head(toxcast_equiv_dose) #look at first few rows Let’s add the DTXSIDs back into this data.frame. toxcast_equiv_dose$DTXSID &lt;- toxcast_httk_P10$DTXSID We can get the names of these chemicals by using the list of chemicals for which the 3-compartment steady-state model can be parameterized, which was stored in the variable `chems_3compss. In that dataframe, we have the compound name and CASRN corresponding to each DTXSID. head(chems_3compss) ## Compound CAS ## 1 2,2-bis(4-hydroxyphenyl)-1,1,1-trichloroethane (hpte) 2971-36-0 ## 2 2,4-d 94-75-7 ## 3 2,4-db 94-82-6 ## 4 2-phenylphenol 90-43-7 ## 5 6-desisopropylatrazine 1007-28-9 ## 6 Abamectin 71751-41-2 ## DTXSID ## 1 DTXSID8022325 ## 2 DTXSID0020442 ## 3 DTXSID7024035 ## 4 DTXSID2021151 ## 5 DTXSID0037495 ## 6 DTXSID8023892 Merge chems_3compss with toxcast_equiv_dose. toxcast_equiv_dose &lt;- merge(chems_3compss, toxcast_equiv_dose, by = &quot;DTXSID&quot;, all.x = FALSE, all.y = TRUE) head(toxcast_equiv_dose) ## DTXSID Compound CAS 5% 50% 95% ## 1 DTXSID0020022 Acifluorfen 50594-66-6 0.3224 0.10940 0.03568 ## 2 DTXSID0020232 Caffeine 58-08-2 2.1720 0.89270 0.32510 ## 3 DTXSID0020286 3-chloro-4-methylaniline 95-74-9 1.7450 0.30310 0.04205 ## 4 DTXSID0020311 Monuron 150-68-5 46.5300 12.37000 2.69300 ## 5 DTXSID0020319 Chlorothalonil 1897-45-6 8.4260 0.05517 0.01892 ## 6 DTXSID0020365 Cyclosporin a 59865-13-3 3.1970 0.57990 0.05113 To find the chemicals with the lowest equivalent doses at the 95th percentile level (corresponding to the most-sensitive 5% of the population), sort this data.frame in ascending order on the 95% column. toxcast_equiv_dose &lt;- toxcast_equiv_dose[order(toxcast_equiv_dose$`95%`), ] head(toxcast_equiv_dose, 10) #first ten rows of sorted table ## DTXSID Compound CAS 5% ## 8 DTXSID0020442 2,4-d 94-75-7 3.260e-06 ## 771 DTXSID8037594 Secbumeton 26259-45-0 6.757e-05 ## 349 DTXSID4020533 1,4-dioxane 123-91-1 7.834e-05 ## 129 DTXSID1026035 Sodium 2-mercaptobenzothiolate 2492-26-4 1.336e-04 ## 727 DTXSID8023214 Levothyroxine 51-48-9 1.374e-04 ## 120 DTXSID1024049 Diflubenzuron 35367-38-5 4.975e-05 ## 726 DTXSID8023187 Ketamine 6740-88-1 4.916e-04 ## 604 DTXSID6046478 Gestodene 60282-87-3 3.727e-04 ## 588 DTXSID6032356 Cycloate 1134-23-2 3.842e-04 ## 696 DTXSID7047306 Cp-634384 290352-28-2 2.441e-04 ## 50% 95% ## 8 8.940e-07 1.790e-07 ## 771 1.245e-05 2.045e-06 ## 349 1.553e-05 2.281e-06 ## 129 3.016e-05 3.533e-06 ## 727 1.508e-05 5.051e-06 ## 120 1.744e-05 5.702e-06 ## 726 7.533e-05 5.728e-06 ## 604 6.351e-05 7.311e-06 ## 588 8.099e-05 8.092e-06 ## 696 4.766e-05 1.073e-05 Answer to Environmental Health Question 6 With this, we can answer Environmental Health Question #6: What are the chemicals with the three lowest predicted equivalent doses (for tenth-percentile ToxCast AC50s), for the most-sensitive 5% of the population? Answer: 2,4-d; secbumeton, and 1,4-dioxane Comparing Equivalent Doses Estimated to Elicit Toxicity (Hazard) to External Exposure Estimates (Exposure), for Chemical Prioritization by Bioactivity-Exposure Ratios (BERs) To estimate potential risk, hazard – in the form of the equivalent dose for the 10th percentile Toxcast AC50 – now needs to be compared to exposure. A quantitative metric for this comparison is the ratio of the lowest 5% of equivalent doses to the highest 5% of potential exposures. This metric is termed the Bioactivity-Exposure Ratio, or BER. Lower BER corresponds to higher potential risk. With BERs calculated for each chemical, we can ultimately rank all of the chemicals from lowest to highest BER, to achieve a chemical prioritization based on potential risk. Human Exposure Estimates Here, we will use exposure estimates that have been inferred from CDC NHANES urinary biomonitoring data (Ring et al., 2019). These estimates consist of an estimated median, and estimated upper and lower 95% credible interval bounds representing uncertainty in that estimated median. These estimates are provided here in the following csv file: exposure &lt;- read.csv(&quot;Module6_6_Input/Module6_6_InputData2.csv&quot;) head(exposure) #view first few rows ## Compound ## 1 1,2,3,4,5,6-Hexachlorocyclohexane (mixed isomers) ## 2 1,2,4-Trichlorobenzene ## 3 1,3,5-Trichlorobenzene ## 4 1,3-Dichlorobenzene ## 5 1,4-Dichlorobenzene ## 6 2,3-Dihydro-2,2-dimethyl-7-benzofuryl 2,4-dimethyl-6-oxa-5-oxo-3-thia-2,4-diazadecanoate ## DTXSID CAS Median low95 up95 ## 1 DTXSID7020687 608-73-1 1.237622e-07 1.144743e-10 8.464811e-06 ## 2 DTXSID0021965 120-82-1 1.157387e-08 5.005691e-11 2.950528e-07 ## 3 DTXSID8026195 108-70-3 8.970557e-08 1.292361e-10 2.563596e-06 ## 4 DTXSID6022056 541-73-1 9.802174e-08 9.421797e-11 8.343616e-06 ## 5 DTXSID1020431 106-46-7 9.050628e-05 8.456633e-05 9.731353e-05 ## 6 DTXSID3052725 65907-30-4 4.245608e-08 1.070856e-10 1.236776e-06 Merging Exposure Estimates with Equivalent Dose Estimates of Toxicity (Hazard) To calculate a BER for a chemical, it needs to have both an equivalent dose and an exposure estimate. Not all of the chemicals for which equivalent doses could be computed (i.e., chemicals with both ToxCast AC50s and httk data) also have exposure estimates inferred from NHANES. Find out how many do. length(intersect(toxcast_equiv_dose$DTXSID, exposure$DTXSID)) ## [1] 58 This means that, using the ToxCast AC50 data for bioactive concentrations, the NHANES urinary inference data for exposures, and the httk package to convert bioactive concentrations to equivalent doses, we can compute BERs for 58 chemicals. Merge together the ToxCast equivalent doses and the exposure data into a single data frame. Keep only the chemicals that have data in both ToxCast equivalent doses and exposure data frames. hazard_exposure &lt;- merge(toxcast_equiv_dose, exposure, by = &quot;DTXSID&quot;, all = FALSE) head(hazard_exposure) #view first few rows of result ## DTXSID Compound.x CAS.x 5% 50% 95% ## 1 DTXSID0020442 2,4-d 94-75-7 3.260e-06 8.940e-07 1.790e-07 ## 2 DTXSID0021389 Trichlorfon 52-68-6 3.252e+01 5.706e+00 7.463e-01 ## 3 DTXSID0024266 Pirimiphos-methyl 29232-93-7 1.128e+01 2.755e+00 4.818e-01 ## 4 DTXSID1020855 Methyl parathion 298-00-0 6.342e+00 9.903e-01 1.149e-01 ## 5 DTXSID1021956 Di-n-octyl phthalate 117-84-0 6.131e-04 2.551e-04 1.058e-04 ## 6 DTXSID1022265 Alachlor 15972-60-8 4.350e+01 1.017e+01 1.563e+00 ## Compound.y CAS.y Median low95 ## 1 2,4-Dichlorophenoxyacetic acid 94-75-7 6.349713e-06 3.100467e-06 ## 2 Trichlorfon 52-68-6 5.021397e-08 8.309014e-11 ## 3 Pirimiphos-methyl 29232-93-7 2.569640e-07 1.765961e-10 ## 4 Methyl parathion 298-00-0 7.396964e-08 1.559956e-10 ## 5 Dioctyl phthalate 117-84-0 8.039695e-05 7.674705e-05 ## 6 Alachlor 15972-60-8 2.249506e-07 1.325551e-07 ## up95 ## 1 1.815981e-05 ## 2 3.302746e-06 ## 3 6.640505e-05 ## 4 3.575740e-06 ## 5 8.422537e-05 ## 6 3.111993e-07 Plotting Hazard and Exposure Together We can visually compare the equivalent doses and the inferred exposure estimates by plotting them together. ggplot(hazard_exposure) + geom_crossbar(aes(x = Compound.x, #Boxes for equivalent doses y = `50%`, ymax = `5%`, ymin = `95%`, color = &quot;Equiv. dose&quot;)) + geom_crossbar(aes( x= Compound.x, #Boxes for exposures y = Median, ymax = up95, ymin = low95, color = &quot;Exposure&quot;)) + scale_color_manual(values = c(&quot;Equiv. dose&quot; = &quot;black&quot;, &quot;Exposure&quot; = &quot;Orange&quot;), name = NULL) + scale_x_discrete(label = function(x) str_trunc(x, 20) ) + #truncate chemical names to 20 chars scale_y_log10() + annotation_logticks(sides = &quot;l&quot;) + ylab(&quot;Equiv. dose or Exposure, mg/kg/day&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 6), axis.title.x = element_blank(), legend.position = &quot;top&quot;) Calculating Bioactivity-Exposure Ratios (BERs) The bioactivity-exposure ratio (BER) is simply the ratio of the lower-end equivalent dose (for the most-sensitive 5% of the population) divided by the upper-end estimated exposure (here, the upper bound on the inferred population median exposure). In the data frame hazard_exposure containing the hazard and exposure data, the lower-end equivalent dose is in column 95% (corresponding to the 95th-percentile \\(C_{ss}\\)-dose slope) and the upper-end exposure is in column up95. Calculate the BER, and assign the result to a new column in the hazard_exposure data frame called BER. hazard_exposure[[&quot;BER&quot;]] &lt;- hazard_exposure[[&quot;95%&quot;]]/hazard_exposure[[&quot;up95&quot;]] Prioritizing Chemicals by BER To prioritize chemicals according to potential risk, they can be sorted from lowest to highest BER. The lower the BER, the higher the priority. Sort the rows of the data.frame from lowest to highest BER. hazard_exposure &lt;- hazard_exposure[order(hazard_exposure$BER), ] head(hazard_exposure) ## DTXSID Compound.x CAS.x 5% 50% ## 1 DTXSID0020442 2,4-d 94-75-7 3.260e-06 8.940e-07 ## 29 DTXSID5020607 Diethylhexyl phthalate (dehp) 117-81-7 1.674e-03 6.497e-04 ## 5 DTXSID1021956 Di-n-octyl phthalate 117-84-0 6.131e-04 2.551e-04 ## 19 DTXSID3022455 Dimethyl phthalate 131-11-3 1.410e-03 4.971e-04 ## 25 DTXSID4022529 Methylparaben 99-76-3 2.682e+00 4.710e-01 ## 27 DTXSID4032613 Fenitrothion 122-14-5 3.992e-01 6.875e-02 ## 95% Compound.y CAS.y Median low95 ## 1 1.790e-07 2,4-Dichlorophenoxyacetic acid 94-75-7 6.349713e-06 3.100467e-06 ## 29 2.774e-04 Di(2-ethylhexyl) phthalate 117-81-7 9.343466e-04 9.133541e-04 ## 5 1.058e-04 Dioctyl phthalate 117-84-0 8.039695e-05 7.674705e-05 ## 19 1.109e-04 Dimethyl phthalate 131-11-3 1.413887e-05 1.314067e-05 ## 25 6.599e-02 Methylparaben 99-76-3 9.525392e-04 8.949158e-04 ## 27 7.191e-03 Fenitrothion 122-14-5 2.055267e-07 1.277038e-10 ## up95 BER ## 1 1.815981e-05 9.856933e-03 ## 29 9.546859e-04 2.905668e-01 ## 5 8.422537e-05 1.256154e+00 ## 19 1.520612e-05 7.293116e+00 ## 25 1.013307e-03 6.512342e+01 ## 27 5.930099e-05 1.212627e+02 The hazard-exposure plot above showed chemicals in alphabetical order. It can be revised to show chemicals in order of priority, from lowest to highest BER. First, create a categorical (factor) variable for the compound names, whose levels are in order of increasing BER. (Since we already sorted the data.frame in order of increasing BER, we can just take the compound names in the order that they appear.) hazard_exposure$Compound_factor &lt;- factor(hazard_exposure$Compound.x, levels = hazard_exposure$Compound.x) Now, make the same plot as before, but use Compound_factor as the x-axis variable instead of Compound. ggplot(hazard_exposure) + geom_crossbar(aes(x = Compound_factor, #Boxes for equivalent dose y = `50%`, ymax = `5%`, ymin = `95%`, color = &quot;Equiv. dose&quot;)) + geom_crossbar(aes( x= Compound_factor, #Boxes for exposure y = Median, ymax = up95, ymin = low95, color = &quot;Exposure&quot;)) + scale_color_manual(values = c(&quot;Equiv. dose&quot; = &quot;black&quot;, &quot;Exposure&quot; = &quot;Orange&quot;), name = NULL) + scale_x_discrete(label = function(x) str_trunc(x, 20) ) + #truncate chemical names scale_y_log10() + ylab(&quot;Equiv. dose or Exposure, mg/kg/day&quot;) + annotation_logticks(sides = &quot;l&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 6), axis.title.x = element_blank(), legend.position = &quot;top&quot;) Now, the chemicals are displayed in order of increasing BER. From left to right, you can visually see the distance increase between the lower bound of equivalent doses (the bottom of the black boxes) and the upper bound of exposure estimates (the top of the orange boxes). Since the y-axis is put on a log10 scale, the distance between the boxes corresponds to the BER. We can gather a lot of information from this plot! Answer to Environmental Health Question 7 With this, we can answer Environmental Health Question #7: Based on httk modeling estimates, are chemicals with higher bioactivity-exposure ratios always less potent than chemicals with lower bioactivity-exposure ratios? Answer: Answer: No – some chemicals with high potency (low equivalent doses) demonstrate high BERs because they have relatively low human exposure estimates; and vice versa. Answer to Environmental Health Question 8 With this, we can also answer Environmental Health Question #8: Based on httk modeling estimates, do chemicals with higher bioactivity-exposure ratios always have lower estimated exposures than chemicals with lower bioactivity-exposure ratios? Answer: No – some chemicals with high estimated exposures have equivalent doses that are higher still, resulting in a high BER despite the higher estimated exposure. Likewise, some chemicals with low estimated exposures also have lower equivalent doses, resulting in a low BER despite the low estimated exposure. Answer to Environmental Health Question 9 With this, we can also answer Environmental Health Question #9: How are chemical prioritization results different when using only hazard information vs. only exposure information vs. bioactivity-exposure ratios? Answer: When chemicals are prioritized solely on the basis of hazard, more-potent chemicals will be highly prioritized. However, if humans are never exposed to these chemicals, or exposure is extremely low compared to potency, then despite the high potency, the potential risk may be low. Conversely, if chemicals are prioritized solely on the basis of exposure, then ubiquitous chemicals will be highly prioritized. However, if these chemicals are inert and do not produce adverse effects, then despite the high exposure, the potential risk may be low. For these reasons, risk-based chemical prioritization efforts consider both hazard (toxicity) and exposure, for instance through bioactivity-exposure ratios. Filling Hazard and Exposure Data Gaps to Prioritize More Chemicals To calculate a BER for a chemical, both bioactivity and exposure data are required, as well as sufficient TK data to perform reverse TK. In this training module, bioactivity data came from ToxCast AC50s; exposure data consisted of exposure inferences made from NHANES urinary biomonitoring data; and TK data consisted of parameter values measured in vitro and built into the httk R package. The intersections are illustrated in an Euler diagram below. BERs can only be calculated for chemicals in the triple intersection. fit &lt;- eulerr::euler(list(&#39;ToxCast AC50s&#39; = unique(toxcast$DTXSID), &#39;HTTK&#39; = unique(chems_3compss$DTXSID), &#39;NHANES inferred exposure&#39; = unique(exposure$DTXSID) ), shape = &quot;ellipse&quot;) plot(fit, legend = TRUE, quantities = TRUE ) Clearly, it would be useful to gather more data to allow calculation of BERs for more chemicals. Answer to Environmental Health Question 10 With this, we can also answer Environmental Health Question #10: Of the three data sets used in this training module – bioactivity from ToxCast, TK data from httk, and exposure inferred from NHANES urinary biomonitoring – which one most limits the number of chemicals that can be prioritized using BERs? Answer: The exposure data set includes the fewest chemicals and is therefore the most limiting. The exposure data set used in this training module is limited to chemicals for which NHANES did urinary biomonitoring for markers of exposure, which is a fairly small set of chemicals that were of interest to NHANES due to existing concerns about health effects of exposure, and/or other reasons. This data set was chosen because it is a convenient set of exposure estimates to use for demonstration purposes, but it could be expanded by including other sources of exposure data and exposure model predictions. Further discussion is beyond the scope of this training module, but as an example of this kind of high-throughput exposure modeling, see Ring et al., 2019. It would additionally be useful to gather TK data for additional chemicals. In vitro measurement efforts are ongoing. Additonally, in silico modeling can produce useful predictions of TK properties to facilitate chemical prioritization. Efforts are ongoing to develop computational models to predict TK parameters from chemical structure and properties. Concluding Remarks This training module provides an overview of toxicokinetic modeling using the httk R package, and its application to in vitro-in vivo extrapolation in the form of placing in vitro data in the context of exposure by calculating equivalent doses for in vitro bioactive concentrations. We would like to acknowledge the developers of the httk package, as detailed below via the CRAN website: This module also summarizes the use of the Bioactivity-Exposure Ratio (BER) for chemical prioritization, and provides examples of calculating the BER and ranking chemicals accordingly. Together, these approaches can be used to more efficiently identify chemicals present in the environment that pose a potential risk to human health. For additional case studies that leverage TK and/or httk modeling techniques, see the following publications that also address environmental health questions: Breen M, Ring CL, Kreutz A, Goldsmith MR, Wambaugh JF. High-throughput PBTK models for in vitro to in vivo extrapolation. Expert Opin Drug Metab Toxicol. 2021 Aug;17(8):903-921. PMID: 34056988. Klaren WD, Ring C, Harris MA, Thompson CM, Borghoff S, Sipes NS, Hsieh JH, Auerbach SS, Rager JE. Identifying Attributes That Influence In Vitro-to-In Vivo Concordance by Comparing In Vitro Tox21 Bioactivity Versus In Vivo DrugMatrix Transcriptomic Responses Across 130 Chemicals. Toxicol Sci. 2019 Jan 1;167(1):157-171. PMID: 30202884. Pearce RG, Setzer RW, Strope CL, Wambaugh JF, Sipes NS. httk: R Package for High-Throughput Toxicokinetics. J Stat Softw. 2017;79(4):1-26. PMID 30220889. Ring CL, Pearce RG, Setzer RW, Wetmore BA, Wambaugh JF. Identifying populations sensitive to environmental chemicals by simulating toxicokinetic variability. Environ Int. 2017 Sep;106:105-118. PMID: 28628784. Ring C, Sipes NS, Hsieh JH, Carberry C, Koval LE, Klaren WD, Harris MA, Auerbach SS, Rager JE. Predictive modeling of biological responses in the rat liver using in vitro Tox21 bioactivity: Benefits from high-throughput toxicokinetics. Comput Toxicol. 2021 May;18:100166. PMID: 34013136. Rotroff DM, Wetmore BA, Dix DJ, Ferguson SS, Clewell HJ, Houck KA, Lecluyse EL, Andersen ME, Judson RS, Smith CM, Sochaski MA, Kavlock RJ, Boellmann F, Martin MT, Reif DM, Wambaugh JF, Thomas RS. Incorporating human dosimetry and exposure into high-throughput in vitro toxicity screening. Toxicol Sci. 2010 Oct;117(2):348-58. PMID: 20639261. Wetmore BA, Wambaugh JF, Ferguson SS, Sochaski MA, Rotroff DM, Freeman K, Clewell HJ 3rd, Dix DJ, Andersen ME, Houck KA, Allen B, Judson RS, Singh R, Kavlock RJ, Richard AM, Thomas RS. Integration of dosimetry, exposure, and high-throughput screening data in chemical toxicity assessment. Toxicol Sci. 2012 Jan;125(1):157-74. PMID: 21948869. Wambaugh JF, Wetmore BA, Pearce R, Strope C, Goldsmith R, Sluka JP, Sedykh A, Tropsha A, Bosgra S, Shah I, Judson R, Thomas RS, Setzer RW. Toxicokinetic Triage for Environmental Chemicals. Toxicol Sci. 2015 Sep;147(1):55-67. PMID: 26085347. Wambaugh JF, Wetmore BA, Ring CL, Nicolas CI, Pearce RG, Honda GS, Dinallo R, Angus D, Gilbert J, Sierra T, Badrinarayanan A, Snodgrass B, Brockman A, Strock C, Setzer RW, Thomas RS. Assessing Toxicokinetic Uncertainty and Variability in Risk Prioritization. Toxicol Sci. 2019 Dec 1;172(2):235-251. doi: 10.1093/toxsci/kfz205. PMID: 31532498. Test Your Knowledge After exposure to a single daily dose of 1 mg/kg/day methylparaben, what is the maximum concentration of methylparaben estimated to occur in human liver, estimated by the 3-comprtment model implemented in httk? What is the predicted range of methylparaben concentrations in plasma that can occur in a human population, assuming a long-term exposure rate of 1 mg/kg/day and 3-compartment steady-state conditions? Provide estimates at the 5th, 50th, and 95th percentile. "],["chemical-read-across-for-toxicity-predictions.html", "Chemical Read-Across for Toxicity Predictions Introduction to Training Module Introduction to Activity Read-Across Example Analysis Calculating Chemical Similarities Chemical Read-Across to Predict Acute Toxicity Concluding Remarks", " Chemical Read-Across for Toxicity Predictions This training module was developed by Grace Patlewicz, Lauren E. Koval, Alexis Payton, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Disclaimer: The views expressed in this document are those of the author and do not necessarily reflect the views or policies of the U.S. EPA. Introduction to Training Module The method of read-across represents one type of computational approach that is commonly used to predict a chemical’s toxicological effects using its properties. Other types of approaches that you will hear commonly used in this field include SAR and QSAR analyses. A high-level overview of each of these definitions and simple illustrative examples of these three computational modeling approaches is provided in the following schematic: Focusing more on read-across, this computational approach represents the method of filling a data gap whereby a chemical with existing data values is used to make a prediction for a ‘similar’ chemical, typically one which is structurally similar. Thus, information from chemicals with data is read across to chemical(s) without data. In a typical read-across workflow, the first step is to determine the problem definition - what question are we trying to address. The second step starts the process of identifying chemical analogues that have information that can be used to inform this question, imparting information towards a chemical of interest that is lacking data. A specific type of read-across that is commonly employed is termed ‘Generalized Read-Across’ or GenRA, which is based upon similarity-weighted activity predictions. This type of read-across approach will be used here when conducting the example chemical read-across training module. This approach has been previously described and published: Shah I, Liu J, Judson RS, Thomas RS, Patlewicz G. Systematically evaluating read-across prediction and performance using a local validity approach characterized by chemical structure and bioactivity information. Regul Toxicol Pharmacol. 2016 79:12-24. PMID: 27174420 Introduction to Activity In this activity we are going to consider a chemical of interest (which we call the target chemical) that is lacking acute oral toxicity information. Specifically, we would like to obtain estimates of the dose that causes lethality after acute (meaning, short-term) exposure conditions. These dose values are typically presented as LD50 values, and are usually collected through animal testing. There is huge interest surrounding the reduced reliance upon animal testing, and we would like to avoid further animal testing as much as possible. With this goal in mind, this activity aims to estimate an LD50 value for the target chemical using completely computational approaches, leveraging existing data as best we can. To achieve this aim, we explore ways in which we can search for structurally similar chemicals that have acute toxicity data already available. Data on these structurally similar chemicals, termed ‘source analogues’, are then used to predict acute toxicity for the target chemical of interest using the GenRA approach. The dataset used for this training module were previously compiled and published in the following manuscript: Helman G, Shah I, Patlewicz G. Transitioning the Generalised Read-Across approach (GenRA) to quantitative predictions: A case study using acute oral toxicity data. Comput Toxicol. 2019 Nov 1;12(November 2019):10.1016/j.comtox.2019.100097. doi: 10.1016/j.comtox.2019.100097. PMID: 33623834 With associated data available at: https://github.com/USEPA/CompTox-GenRA-acutetox-comptoxicol/tree/master/input This exercise will specifically predict LD50 values for the chemical, 1-chloro-4-nitrobenzene (DTXSID5020281). This chemical is an organic compound with the formula ClC˜6˜H˜4˜NO˜2˜, and is a common intermediate in the production of a number of industrial compounds, including common antioxidants found in rubber. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: How many chemicals with acute toxicity data are structurally similar to 1-chloro-4-nitrobenzene? What is the predicted LD50 for 1-chloro-4-nitrobenzene using the GenRA approach? How different is the predicted vs. experimentally observed LD50 for 1-chloro-4-nitrobenzene? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you: if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); if (!requireNamespace(&quot;fingerprint&quot;)) install.packages(&quot;fingerprint&quot;); if (!requireNamespace(&quot;rcdk&quot;)) install.packages(&quot;rcdk&quot;); Loading R packages required for this session library(tidyverse) #all tidyverse packages, including dplyr and ggplot2 library(fingerprint) # a package that supports operations on molecular fingerprint data library(rcdk) # a package that interfaces with the &#39;CDK&#39;, a Java framework for chemoinformatics libraries packaged for R Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Read-Across Example Analysis Loading Example Datasets Let’s start by loading the datasets needed for this training module. We are going to use a dataset of substances that have chemical identification information ready in the form of SMILES, as well as acute toxicity data, in the form of LD50 values. The first file to upload is named Module6_6_InputData1.csv and contains the list of substances and their structural information, in the form of SMILES nomenclature. SMILES stands for Simplified molecular-input line-entry system, a form of line notation to describe the structure of a chemical. The second file to upload is named Module6_6_InputData2.csv and contains the substances and their acute toxicity information. substances &lt;- read.csv(&quot;Module6_7_Input/Module6_7_InputData1.csv&quot;) acute_data &lt;- read.csv(&quot;Module6_7_Input/Module6_7_InputData2.csv&quot;) Let’s first view the substances dataset: dim(substances) ## [1] 6955 4 colnames(substances) ## [1] &quot;DTXSID&quot; &quot;PREFERRED_NAME&quot; &quot;SMILES&quot; ## [4] &quot;QSAR_READY_SMILES&quot; head(substances) ## DTXSID PREFERRED_NAME ## 1 DTXSID00142939 (Acetyloxy)acetonitrile ## 2 DTXSID00143108 Acrylic acid, 2-(hydroxymethyl)-, ethyl ester ## 3 DTXSID00143880 4-Heptyl-2,6-dimethylphenol ## 4 DTXSID00144796 Pyrimidine, 2-amino-4-(2-dimethylaminoethoxy)- ## 5 DTXSID00144933 O,O,O-Tris(2-chloroethyl) phosphorothioate ## 6 DTXSID00146356 Citenamide ## SMILES QSAR_READY_SMILES ## 1 CC(=O)OCC#N CC(=O)OCC#N ## 2 CCOC(=O)C(=C)CO CCOC(=O)C(=C)CO ## 3 CCCCCCCC1=CC(C)=C(O)C(C)=C1 CCCCCCCC1=CC(C)=C(O)C(C)=C1 ## 4 CN(C)CCOC1=NC(N)=NC=C1 CN(C)CCOC1=NC(N)=NC=C1 ## 5 ClCCOP(=S)(OCCCl)OCCCl ClCCOP(=S)(OCCCl)OCCCl ## 6 NC(=O)C1C2=CC=CC=C2C=CC2=CC=CC=C12 NC(=O)C1C2=CC=CC=C2C=CC2=CC=CC=C12 We can see that this dataset contains information on 6955 chemicals (rows). The columns are further described below: DTXSIDs: a substance identifier provided through the U.S. EPA’s Computational Toxicology Dashboard SMILES and QSAR_READY_SMILES: Chemical identifiers. The QSAR_READY_SMILES values are what we will specifically need in a later step, to construct chemical fingerprints from. QSAR_READY_SMILES: SMILES that have been standardized related to salts, tautomers, inorganics, aromaticity, and stereochemistry (among other factors) prior to any QSAR modeling or prediction. Let’s make sure that these values are recognized as character format and placed in its own vector, to ensure proper execution of functions throughout this script: all_smiles &lt;- as.character(substances$QSAR_READY_SMILES) Now let’s view the acute toxicity dataset: dim(acute_data) ## [1] 6955 9 colnames(acute_data) ## [1] &quot;DTXSID&quot; &quot;very_toxic&quot; &quot;nontoxic&quot; &quot;LD50_mgkg&quot; &quot;EPA_category&quot; ## [6] &quot;GHS_category&quot; &quot;casrn&quot; &quot;mol_weight&quot; &quot;LD50_LM&quot; head(acute_data) ## DTXSID very_toxic nontoxic LD50_mgkg EPA_category GHS_category ## 1 DTXSID00142939 TRUE FALSE 32 1 2 ## 2 DTXSID00143108 FALSE FALSE 620 3 4 ## 3 DTXSID00143880 FALSE FALSE 1600 3 4 ## 4 DTXSID00144796 FALSE FALSE 1500 3 4 ## 5 DTXSID00144933 FALSE FALSE 820 3 4 ## 6 DTXSID00146356 FALSE FALSE 1800 3 4 ## casrn mol_weight LD50_LM ## 1 1001-55-4 99.089 0.4908755 ## 2 10029-04-6 130.143 -0.6779709 ## 3 10138-19-9 220.356 -0.8609951 ## 4 102207-77-2 182.227 -0.9154785 ## 5 10235-09-3 301.540 -0.4344689 ## 6 10423-37-7 235.286 -0.8836764 We can see that this dataset contains information on 6955 chemicals (rows). Some notable columns are explained below: + DTXSIDs: a substance identifier provided through the U.S. EPA’s Computational Toxicology Dashboard + casrn: CASRN number + mol_weight: molecular weight + LD50_LM: the -log10 of the millimolar LD50. LD stands for ‘Lethal Dose’. The LD50 value is the dose of substance given all at once which causes the death of 50% of a group of test animals. The lower the LD50 in mg/kg, the more toxic that substance is. Important Notes on Units In modeling studies, the convention is to convert toxicity values expressed as mg per unit into their molar or millimolar values and then to convert these to the base 10 logarithm. To increase clarity when plotting, such that higher toxicities would be expressed by higher values, the negative logarithm is then taken. For example, substance DTXSID00142939 has a molecular weight of 99.089 (grams per mole) and a LD50 of 32 mg/kg. This would be converted to a toxicity value of (\\(\\frac{32}{99.089} = 0.322942~mmol/kg\\)). The logarithm of that would be -0.4908755. By convention, the negative logarithm of the millimolar concentration would then be used i.e. -log[mmol/kg]. This conversion has been used to create the LD50_LM values in the acute toxicity dataset. Let’s check to see whether the same chemicals are present in both datasets: # First need to make sure that both dataframes are sorted by the identifier, DTXSID substances &lt;- substances[order(substances$DTXSID),] acute_data &lt;- acute_data[order(acute_data$DTXSID),] # Then test to see whether data in these columns are equal unique(substances$DTXSID == acute_data$DTXSID) ## [1] TRUE All accounts are true, meaning they are all equal (the same chemical). Data Visualizations of Acute Toxicity Values Let’s create a plot to show the distribution of the LD50 values in the dataset. ggplot(data = acute_data, aes(LD50_mgkg)) + stat_ecdf(geom = &quot;point&quot;) ggplot(data = acute_data, aes(LD50_LM)) + stat_ecdf(geom = &quot;point&quot;) Can you see a difference between these two plots? Yes, if the LD50 mg/kg values are converted into -log[mmol/kg] scale (LD50_LM), then the distribution resembles a normal cumulative distribution curve. Selecting the ‘Target’ Chemical of Interest for Read-Across Analysis For this exercise, we will select a ‘target’ substance of interest from our dataset, and assume that we have no acute toxicity data for it, and we will perform read-across for this target chemical. Note that this module’s example dataset actually has full data coverage (meaning all chemicals have acute toxicity data), but this exercise is beneficial, because we can make toxicity predictions, and then check to see how close we are by viewing the experimentally observed values. Our target substance for this exercise is going to be DTXSID5020281, which is 1-chloro-4-nitrobenzene. This chemical is an organic compound with the formula ClC6H4NO2, and is a common intermediate in the production of a number of industrially useful compounds, including common antioxidants found in rubber. Here is an image of the chemical structure (https://comptox.epa.gov/dashboard/dsstoxdb/results?search=DTXSID5020281): Filtering the dataframes for only data on this target substance: target_substance &lt;-filter(substances, DTXSID == &#39;DTXSID5020281&#39;) target_acute_data &lt;- filter(acute_data, DTXSID == &#39;DTXSID5020281&#39;) Calculating Structural Similarities between Substances To eventually identify chemical analogues with information that can be ‘read-across’ to our target chemical (1-chloro-4-nitrobenzene), we first need to evaluate how similar each chemical is to one another. In this example, we will base our search for similar substances upon similarities between chemical structure fingerprint representations. Once these chemical structure fingerprints are derived, they will be used to calculate the degree to which each possible pair of chemicals is similar, leveraging the Tanimoto metric. These findings will yield a similarity matrix of all possible pairwise similarity scores. Converting Chemical Identifiers into Molecular Objects (MOL) To derive structure fingerprints across all evaluated substances, we need to first convert the chemical identifiers originally provided as QSAR_READY_SMILES into molecular objects. The standard exchange format for molecular information is a MOL file. This is a chemical file format that contains plain text information and stores information about atoms, bonds and their connections. We can carry out these identifier conversions using the ‘parse.smiles’ function within the rcdk package. Here we do this for the target chemical of interest, as well as all substances in the dataset. target_mol &lt;- parse.smiles(as.character(target_substance$QSAR_READY_SMILES)) all_mols &lt;-parse.smiles(all_smiles) Computing chemical fingerprints With these mol data, we can now compute the fingerprints for our target substance, as well as all the substances in the dataset. We can compute fingerprints leveraging the get.fingerprint() function. Let’s first run it on the target chemical: target.fp &lt;- get.fingerprint(target_mol[[1]], type = &#39;standard&#39;) target.fp # View fingerprint ## Fingerprint object ## name = ## length = 1024 ## folded = FALSE ## source = CDK ## bits on = 13 18 96 162 165 174 183 203 214 224 235 254 305 313 400 513 575 602 619 638 662 723 742 743 744 770 771 787 839 844 845 884 932 958 978 989 We can run the same function over the entire all_mols dataset, leveraging the lapply() function: all.fp &lt;- lapply(all_mols, get.fingerprint, type = &#39;standard&#39;) Calculating Chemical Similarities Using these molecular fingerprint data, we can now calculate the degree to which each chemical is similar to another chemical, based on structural similarity. The method employed in this example is the Tanimoto method. The Tanimoto similarity metric is a unitless number between zero and one that measures how similar two sets (in this case 2 chemicals) are from one another. A Tanimoto index of 1 means the 2 chemicals are identical whereas a index of 0 means that the chemicals share nothing in common. In the context of the fingerprints, a Tanimoto index of 0.5 means that half of the fingerprint matches between two chemicals whilst the other half does not match. Once these Tanimoto similarity indices are calculated between every possible chemical pair, the similarity results can be viewed in the form of a similarity matrix. In this matrix, all substances are listed across the rows and columns, and the degree to which every possible chemical pair is similar is summarized through values contained within the matrix. Further information about chemical similarity can be found here: https://en.wikipedia.org/wiki/Chemical_similarity Steps to generate this similarity matrix are detailed here: all.fp.sim &lt;- fingerprint::fp.sim.matrix(all.fp, method = &#39;tanimoto&#39;) all.fp.sim &lt;- as.data.frame(all.fp.sim) # Convert the outputted matrix to a dataframe colnames(all.fp.sim) = substances$DTXSID # Placing chemical identifiers back as column headers row.names(all.fp.sim) = substances$DTXSID # Placing chemical identifiers back as row names Since we are querying a large number of chemicals, it is difficult to view the entire resulting similarity matrix. Let’s, instead view portions of these results: all.fp.sim[1:5,1:5] # Viewing the first five rows and columns of data ## DTXSID00142939 DTXSID00143108 DTXSID00143880 DTXSID00144796 ## DTXSID00142939 1.00000000 0.38235294 0.09090909 0.10389610 ## DTXSID00143108 0.38235294 1.00000000 0.09523810 0.09302326 ## DTXSID00143880 0.09090909 0.09523810 1.00000000 0.09183673 ## DTXSID00144796 0.10389610 0.09302326 0.09183673 1.00000000 ## DTXSID00144933 0.17948718 0.14583333 0.09677419 0.06896552 ## DTXSID00144933 ## DTXSID00142939 0.17948718 ## DTXSID00143108 0.14583333 ## DTXSID00143880 0.09677419 ## DTXSID00144796 0.06896552 ## DTXSID00144933 1.00000000 all.fp.sim[6:10,6:10] # Viewing the next five rows and columns of data ## DTXSID00146356 DTXSID00147863 DTXSID00148532 DTXSID00148976 ## DTXSID00146356 1.00000000 0.05128205 0.12574850 0.10077519 ## DTXSID00147863 0.05128205 1.00000000 0.04166667 0.05050505 ## DTXSID00148532 0.12574850 0.04166667 1.00000000 0.08808290 ## DTXSID00148976 0.10077519 0.05050505 0.08808290 1.00000000 ## DTXSID00149721 0.35294118 0.07526882 0.15083799 0.08843537 ## DTXSID00149721 ## DTXSID00146356 0.35294118 ## DTXSID00147863 0.07526882 ## DTXSID00148532 0.15083799 ## DTXSID00148976 0.08843537 ## DTXSID00149721 1.00000000 You can see that there is an identity line within this similarity matrix, where instances when a chemical’s structure is being compared to itself, the similarity values are 1.00000. All other possible chemical pairings show variable similarity scores, ranging from: min(all.fp.sim) ## [1] 0 a minimum of zero, indicating no similarities between chemical structures. max(all.fp.sim) ## [1] 1 a maximum of 1, indicating the identical chemical structure (which occurs when comparing a chemical to itself). Identifying Chemical Analogues This step will find substances that are structurally similar to the target chemical, 1-chloro-4-nitrobenzene (with DTXSID5020281). Structurally similar chemicals are referred to as ‘source analogues’, with information that will be carried forward in this read-across analysis. The first step to identifying chemical analogues is to subset the full similarity matrix to focus just on our target chemical. target.sim &lt;- all.fp.sim %&gt;% filter(row.names(all.fp.sim) == &#39;DTXSID5020281&#39;) Then we’ll extract the substances that exceed a similarity threshold of 0.75 by selecting to keep columns which are &gt; 0.75. target.sim &lt;- target.sim %&gt;% select_if(function(x) any(x &gt; 0.75)) dim(target.sim) # Show dimensions of subsetted matrix ## [1] 1 12 This gives us our analogues list! Specifically, we selected 12 columns of data, representing our target chemical plus 11 structurally similar chemicals. Let’s create a dataframe of these substance identifiers to carry forward in the read-across analysis: source_analogues &lt;- t(target.sim) # Transposing the filtered similarity matrix results DTXSID &lt;-rownames(source_analogues) # Temporarily grabbing the dtxsid identifiers from this matrix source_analogues &lt;- cbind(DTXSID, source_analogues) # Adding these identifiers as a column rownames(source_analogues) &lt;- NULL # Removing the rownames from this dataframe, to land on a cleaned dataframe colnames(source_analogues) &lt;- c(&#39;DTXSID&#39;, &#39;Target_TanimotoSim&#39;) # Renaming column headers source_analogues[1:12,1:2] # Viewing the cleaned dataframe of analogues ## DTXSID Target_TanimotoSim ## [1,] &quot;DTXSID0020280&quot; &quot;0.846153846153846&quot; ## [2,] &quot;DTXSID2021105&quot; &quot;0.8&quot; ## [3,] &quot;DTXSID3024998&quot; &quot;0.878048780487805&quot; ## [4,] &quot;DTXSID4021971&quot; &quot;0.825&quot; ## [5,] &quot;DTXSID4030384&quot; &quot;0.80952380952381&quot; ## [6,] &quot;DTXSID5020281&quot; &quot;1&quot; ## [7,] &quot;DTXSID6020278&quot; &quot;0.782608695652174&quot; ## [8,] &quot;DTXSID6038827&quot; &quot;0.878048780487805&quot; ## [9,] &quot;DTXSID7052319&quot; &quot;0.790697674418605&quot; ## [10,] &quot;DTXSID7060596&quot; &quot;0.765957446808511&quot; ## [11,] &quot;DTXSID8024997&quot; &quot;0.767441860465116&quot; ## [12,] &quot;DTXSID8024999&quot; &quot;0.9&quot; Answer to Environmental Health Question 1 With these, we can answer Environmental Health Question #1: How many chemicals with acute toxicity data are structurally similar to 1-chloro-4-nitrobenzene? Answer: In this dataset, 11 chemicals are structurally similar to the target chemical, based on a Tanimoto similiary score of &gt; 0.75. Chemical Read-Across to Predict Acute Toxicity Acute toxicity data from these chemical analogues can now be extracted and read across to the target chemical (1-chloro-4-nitrobenzene) to make predictions about its toxicity. Let’s first merge the acute data for these analogues into our working dataframe: source_analogues &lt;- merge(source_analogues, acute_data, by.x = &#39;DTXSID&#39;, by.y = &#39;DTXSID&#39;) Then, let’s remove the target chemical of interest and create a new dataframe of just the source analogues: source_analogues_only &lt;- source_analogues %&gt;% filter(Target_TanimotoSim!=1) # Removing the row of data with the target chemical, identified as the chemical with a similarity of 1 to itself source_analogues_only[1:11,1:10] # Viewing the combined dataset of source analogues ## DTXSID Target_TanimotoSim very_toxic nontoxic LD50_mgkg EPA_category ## 1 DTXSID0020280 0.846153846153846 FALSE FALSE 251 2 ## 2 DTXSID2021105 0.8 FALSE FALSE 1100 3 ## 3 DTXSID3024998 0.878048780487805 FALSE FALSE 379 2 ## 4 DTXSID4021971 0.825 FALSE FALSE 300 2 ## 5 DTXSID4030384 0.80952380952381 FALSE FALSE 400 2 ## 6 DTXSID6020278 0.782608695652174 FALSE FALSE 640 3 ## 7 DTXSID6038827 0.878048780487805 FALSE TRUE 2015 3 ## 8 DTXSID7052319 0.790697674418605 FALSE FALSE 400 2 ## 9 DTXSID7060596 0.765957446808511 FALSE FALSE 1000 3 ## 10 DTXSID8024997 0.767441860465116 FALSE FALSE 381 2 ## 11 DTXSID8024999 0.9 FALSE FALSE 625 3 ## GHS_category casrn mol_weight LD50_LM ## 1 3 88-73-3 157.55 -0.2022553 ## 2 4 82-68-8 295.32 -0.5710998 ## 3 4 611-06-3 192.00 -0.2953380 ## 4 4 121-73-3 157.55 -0.2797028 ## 5 4 89-63-4 172.57 -0.3650947 ## 6 4 97-00-7 202.55 -0.4996477 ## 7 5 6283-25-6 172.57 -1.0673097 ## 8 4 635-22-3 172.57 -0.3650947 ## 9 4 611-07-4 173.55 -0.7605754 ## 10 4 3209-22-1 192.00 -0.2976237 ## 11 4 99-54-7 192.00 -0.5125788 Read-across Calculations using GenRA The final generalized read-across (GenRA) prediction is based on a similarity-weighted activity score. This score is specifically calculated as the following weighted average: (pairwise similarity between the target and source analogue) * (the toxicity of the source analogue), summed across each individual analogue; and then this value is divided by the sum of all pairwise similarities. For further details surrounding this algorithm and its spelled out formulation, see Shah et al.. Here are the underlying calculations needed to derive the similarity weighted activity score for this current exercise: source_analogues_only$wt_tox_calc &lt;- as.numeric(source_analogues_only$Target_TanimotoSim) * source_analogues_only$LD50_LM # Calculating (pairwise similarity between the target and source analogue) * (the toxicity of the source analogue) # for each analogy, and saving it as a new column titled &#39;wt_tox_calc&#39; source_analogues_only[1:3,1:11] # Viewing a portion of the updated dataframe with the &#39;wt_tox_cal&#39; column ## DTXSID Target_TanimotoSim very_toxic nontoxic LD50_mgkg EPA_category ## 1 DTXSID0020280 0.846153846153846 FALSE FALSE 251 2 ## 2 DTXSID2021105 0.8 FALSE FALSE 1100 3 ## 3 DTXSID3024998 0.878048780487805 FALSE FALSE 379 2 ## GHS_category casrn mol_weight LD50_LM wt_tox_calc ## 1 3 88-73-3 157.55 -0.2022553 -0.1711391 ## 2 4 82-68-8 295.32 -0.5710998 -0.4568799 ## 3 4 611-06-3 192.00 -0.2953380 -0.2593212 sum_tox &lt;- sum(source_analogues_only$wt_tox_calc) #Summing this wt_tox_calc value across all analogues sum_sims &lt;- sum(as.numeric(source_analogues_only$Target_TanimotoSim)) # Summing all of the pairwise Tanimoto similarity scores ReadAcross_Pred &lt;- sum_tox/sum_sims # Final calculation for the weighted activity score (i.e., read-across prediction) Converting LD50 Units Right now, these results are in units of -log10 millimolar. So we still need to convert them into mg/kg equivalent, by converting out of -log10 and multiplying by the molecular weight of 1-chloro-4-nitrobenzene (g/mol): ReadAcross_Pred &lt;- (10^(-ReadAcross_Pred))*157.55 ReadAcross_Pred ## [1] 471.2042 Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question #2: What is the predicted LD50 for 1-chloro-4-nitrobenzene, using the GenRA approach? Answer: 1-chloro-4-nitrobenzene has a predicted LD50 (mg/kg) of 471 mg/kg. Visual Representation of this Read-Across Approach Here is a schematic summarizing the steps we employed in this analysis: Comparing Read-Across Predictions to Experimental Observations Let’s now compare how close this computationally-based prediction is to the experimentally observed LD50 value target_acute_data$LD50_mgkg ## [1] 460 We can see that the experimentally observed LD50 values for this chemical is 460 mg/kg. Answer to Environmental Health Question 3 With this, we can answer Environmental Health Question #3: How different is the predicted vs. experimentally observed LD50 for 1-chloro-4-nitrobenzene? Answer: The predicted LD50 is 471 mg/kg, and the experimentally observed LD50 is 460 mg/kg, which is reasonably close! Concluding Remarks In conclusion, this training module leverages a dataset of substances with structural representations and toxicity data to create chemical fingerprint representations. We have selected a chemical of interest (target) and used the most similar analogues based on a similarity threshold to predict the acute toxicity of that target using the generalized read-across formula of weighted activity by similarity. We have seen that the prediction is in close agreement with that already reported for the target chemical in the dataset. Similar methods can be used to predict other toxicity endpoints, based on other datasets of chemicals. Additionally, further efforts are aimed at expanding read-across approaches to integrate in vitro data. More information on the GenRA approach as implemented in the EPA CompTox Chemicals Dashboard, as well as the extension of read-across to include bioactivity information, are described in the following manuscripts: Shah I, Liu J, Judson RS, Thomas RS, Patlewicz G. Systematically evaluating read-across prediction and performance using a local validity approach characterized by chemical structure and bioactivity information. Regul Toxicol Pharmacol. 2016 79:12-24. PMID: 27174420 Helman G, Shah I, Williams AJ, Edwards J, Dunne J, Patlewicz G. Generalized Read-Across (GenRA): A workflow implemented into the EPA CompTox Chemicals Dashboard. ALTEX. 2019;36(3):462-465. PMID: 30741315. GenRA has also been implemented as a standalone python package. Test Your Knowledge Use the same input data we used in this module to answer the following questions. How many source analogues are structurally similar to methylparaben (DTXSID4022529) when considering a similarity threshold of 0.75? What is the predicted LD50 for methylparaben in mg/kg, and how does this compare to the measured LD50 for methylparaben? "],["comparative-toxicogenomics-database.html", "7.1 Comparative Toxicogenomics Database Introduction to Training Module CTD Data in R Identifying Genes Under Epigenetic Control Concluding Remarks", " 7.1 Comparative Toxicogenomics Database This training module was developed by Lauren E. Koval, Kyle R. Roell, and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module The Comparative Toxicogenomics Database (CTD) is a publicly available, online database that provides manually curated information about chemical-gene/protein interactions, chemical-disease and gene-disease relationships. CTD also recently incorporated curation of exposure data and chemical-phenotype relationships. CTD is located at: http://ctdbase.org/. Here is a screenshot of the CTD homepage (as of August 5, 2021): In this module, we will be using CTD to access and download data to perform data organization and analysis as an applications-based example towards environmental health research. This activity represents a demonstration of basic data manipulation, filtering, and organization steps in R, while highlighting the utility of CTD to identify novel genomic/epigenomic relationships to environmental exposures. Example visualizations are also included in this training module’s script, providing visualizations of gene list comparison results. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Which genes show altered expression in response to arsenic exposure? Of the genes showing altered expression, which may be under epigenetic control? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you. if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;) if (!requireNamespace(&quot;VennDiagram&quot;)) install.packages(&quot;VennDiagram&quot;) if (!requireNamespace(&quot;grid&quot;)) install.packages(&quot;grid&quot;) Loading R packages required for this session library(tidyverse) library(VennDiagram) library(grid) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) CTD Data in R Organizing Example Dataset from CTD CTD requires manual querying of its database, outside of the R scripting environment. Because of this, let’s first manually pull the data we need for this example analysis. We can answer both of the example questions by pulling all chemical-gene relationship data for arsenic, which we can do by following the below steps: Navigate to the main CTD website: http://ctdbase.org/. Select at the top, ‘Search’ -&gt; ‘Chemical-Gene Interactions’. Select to query all chemical-gene interaction data for arsenic. Note that there are lots of results, represented by many many rows of data! Scroll to the bottom of the webpage and select to download as ‘CSV’. This is the file that we can now use to import into the R environment and analyze! Note that the data pulled here represent data available on August 1, 2021 Loading the Example CTD Dataset into R Read in the csv file of the results from CTD query: ctd = read_csv(&quot;Module7_1_Input/Module7_1_InputData1.csv&quot;) Let’s first see how many rows and columns of data this file contains: dim(ctd) ## [1] 6280 9 This dataset includes 6280 observations (represented by rows) linking arsenic exposure to gene-level alterations With information spanning across 9 columns Let’s also see what kind of data are organized within the columns: colnames(ctd) ## [1] &quot;Chemical Name&quot; &quot;Chemical ID&quot; &quot;CAS RN&quot; ## [4] &quot;Gene Symbol&quot; &quot;Gene ID&quot; &quot;Interaction&quot; ## [7] &quot;Interaction Actions&quot; &quot;Reference Count&quot; &quot;Organism Count&quot; # Viewing the first five rows of data, across all 9 columns ctd[1:9,1:5] ## # A tibble: 9 × 5 ## `Chemical Name` `Chemical ID` `CAS RN` `Gene Symbol` `Gene ID` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Arsenic D001151 7440-38-2 AACSP1 729522 ## 2 Arsenic D001151 7440-38-2 AADACL2 344752 ## 3 Arsenic D001151 7440-38-2 AAGAB 79719 ## 4 Arsenic D001151 7440-38-2 AAK1 22848 ## 5 Arsenic D001151 7440-38-2 AAMDC 28971 ## 6 Arsenic D001151 7440-38-2 AAR2 25980 ## 7 Arsenic D001151 7440-38-2 AASS 10157 ## 8 Arsenic D001151 7440-38-2 ABCA1 19 ## 9 Arsenic D001151 7440-38-2 ABCA12 26154 Filtering data for genes with altered expression To identify genes with altered expression in association with arsenic, we can leverage the results of our CTD query and filter this dataset to include only the rows that contain the term “expression” in the “Interaction Actions” column. exp_filt = ctd %&gt;% filter(grepl(&quot;expression&quot;, `Interaction Actions`)) We now have 2586 observations, representing instances of arsenic exposure causing a changes in a target gene’s expression levels. dim(exp_filt) ## [1] 2586 9 Let’s see how many unique genes this represents: length(unique(exp_filt$`Gene Symbol`)) ## [1] 1878 This reflects 1878 unique genes that show altered expression in association with arsenic. Let’s make a separate dataframe that includes only the unique genes, based on the “Gene Symbol” column. exp_genes = exp_filt %&gt;% distinct(`Gene Symbol`, .keep_all=TRUE) # Removing columns besides gene identifier exp_genes = exp_genes[,4] # Viewing the first 10 genes listed exp_genes[1:10,] ## # A tibble: 10 × 1 ## `Gene Symbol` ## &lt;chr&gt; ## 1 AADACL2 ## 2 AAK1 ## 3 AASS ## 4 ABCA12 ## 5 ABCC1 ## 6 ABCC2 ## 7 ABCC3 ## 8 ABCC4 ## 9 ABCG4 ## 10 ABHD12B This now provides us a list of 1878 genes showing altered expression in association with arsenic. Technical notes on running the distinct function within tidyverse: By default, the distinct function keeps the first instance of a duplicated value. This does have implications if the rest of the values in the rows differ. You will only retain the data associated with the first instance of the duplicated value (which is why we just retained the gene column here). It may be useful to first find the rows with the duplicate value and verify that results are as you would expect before removing observations. For example, in this dataset, expression levels can increase or decrease. If you were looking for just increases in expression, and there were genes that showed increased and decreased expression across different samples, using the distinct function just on “Gene Symbol” would not give you the results you wanted. If the first instance of the gene symbol noted decreased expression, that gene would not be returned in the results even though it might be one you would want. For this example case, we only care about expression change, regardless of direction, so this is not an issue. The distinct function can also take multiple columns to consider jointly as the value to check for duplicates if you are concerned about this. Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question 1: Which genes show altered expression in response to arsenic exposure? Answer: This list of 1878 genes have published evidence supporting their altered expression levels associated with arsenic exposure. Identifying Genes Under Epigenetic Control For this dataset, let’s focus on gene-level methylation as a marker of epigenetic regulation. Let’s return to our main dataframe, representing the results of the CTD query, and filter these results for only the rows that contain the term “methylation” in the “Interaction Actions” column. met_filt = ctd %&gt;% filter(grepl(&quot;methylation&quot;,`Interaction Actions`)) We now have 3211 observations, representing instances of arsenic exposure causing a changes in a target gene’s methylation levels. dim(met_filt) ## [1] 3211 9 Let’s see how many unique genes this represents. length(unique(met_filt$`Gene Symbol`)) ## [1] 3142 This reflects 3142 unique genes that show altered methylation in association with arsenic Let’s make a separate dataframe that includes only the unique genes, based on the “Gene Symbol” column. met_genes = met_filt %&gt;% distinct(`Gene Symbol`, .keep_all=TRUE) # Removing columns besides gene identifier met_genes = met_genes[,4] This now provides us a list of 3142 genes showing altered methylation in association with arsenic. With this list of genes with altered methylation, we can now compare it to previous list of genes with altered expression to yeild our final list of genes of interest. To achieve this last step, we present two different methods to carry out list comparisons below. Method 1 for list comparisons: Merging Merge the expression results with the methylation resuts on the Gene Symbol column found in both datasets. merge_df = merge(exp_genes, met_genes, by = &quot;Gene Symbol&quot;) We end up with 315 rows reflecting the 315 genes that show altered expression and altered methylation Let’s view these genes: merge_df[1:315,] ## [1] &quot;ABCC4&quot; &quot;ABHD17A&quot; &quot;ABLIM2&quot; &quot;ACAD9&quot; &quot;ACKR2&quot; &quot;ACP3&quot; ## [7] &quot;ADAMTS1&quot; &quot;AFF1&quot; &quot;AGO2&quot; &quot;ALDH3B2&quot; &quot;ANPEP&quot; &quot;AOPEP&quot; ## [13] &quot;AP3D1&quot; &quot;APBB3&quot; &quot;APP&quot; &quot;AQP1&quot; &quot;ARF1&quot; &quot;ARID5B&quot; ## [19] &quot;AS3MT&quot; &quot;ASAP1&quot; &quot;ATF2&quot; &quot;ATG7&quot; &quot;ATP6V1C2&quot; &quot;ATXN1&quot; ## [25] &quot;ATXN7&quot; &quot;BACH1&quot; &quot;BCAR1&quot; &quot;BCL2&quot; &quot;BCL6&quot; &quot;BDNF&quot; ## [31] &quot;BECN1&quot; &quot;BMI1&quot; &quot;BMPR1A&quot; &quot;C1GALT1C1&quot; &quot;C1S&quot; &quot;C2CD3&quot; ## [37] &quot;CAMP&quot; &quot;CARD18&quot; &quot;CASP8&quot; &quot;CASTOR1&quot; &quot;CBR4&quot; &quot;CBS&quot; ## [43] &quot;CCDC68&quot; &quot;CCL14&quot; &quot;CCL20&quot; &quot;CCL24&quot; &quot;CCR2&quot; &quot;CD2&quot; ## [49] &quot;CD27&quot; &quot;CD40&quot; &quot;CDC42&quot; &quot;CDH1&quot; &quot;CDK2&quot; &quot;CDK4&quot; ## [55] &quot;CDK5&quot; &quot;CDK6&quot; &quot;CDKN1B&quot; &quot;CDKN2A&quot; &quot;CELF1&quot; &quot;CENPM&quot; ## [61] &quot;CEP72&quot; &quot;CERK&quot; &quot;CES4A&quot; &quot;CFAP300&quot; &quot;CHORDC1&quot; &quot;CLEC4D&quot; ## [67] &quot;CLIC5&quot; &quot;CMBL&quot; &quot;CNTNAP2&quot; &quot;CRCP&quot; &quot;CREBBP&quot; &quot;CUX2&quot; ## [73] &quot;CYP1B1&quot; &quot;CYP26B1&quot; &quot;CYP2U1&quot; &quot;DAPK1&quot; &quot;DAXX&quot; &quot;DCAF7&quot; ## [79] &quot;DDB2&quot; &quot;DHX32&quot; &quot;DLK1&quot; &quot;DNMT1&quot; &quot;DSG1&quot; &quot;DYNC2I2&quot; ## [85] &quot;ECHS1&quot; &quot;EDAR&quot; &quot;EFCAB2&quot; &quot;EHMT2&quot; &quot;EML2&quot; &quot;EPHA1&quot; ## [91] &quot;EPHA2&quot; &quot;EPM2AIP1&quot; &quot;ERBB4&quot; &quot;ERCC2&quot; &quot;ERN2&quot; &quot;ESR1&quot; ## [97] &quot;ETFB&quot; &quot;ETFDH&quot; &quot;F3&quot; &quot;FAM25A&quot; &quot;FAM43A&quot; &quot;FAM50B&quot; ## [103] &quot;FAM53C&quot; &quot;FAS&quot; &quot;FBLN2&quot; &quot;FBXO32&quot; &quot;FGF2&quot; &quot;FGFR3&quot; ## [109] &quot;FGGY&quot; &quot;FOSB&quot; &quot;FPR2&quot; &quot;FTH1P3&quot; &quot;FTL&quot; &quot;GAK&quot; ## [115] &quot;GAS1&quot; &quot;GFRA1&quot; &quot;GGACT&quot; &quot;GLI2&quot; &quot;GLI3&quot; &quot;GNPDA1&quot; ## [121] &quot;GOLGA4&quot; &quot;GSTM3&quot; &quot;GTSE1&quot; &quot;H2AC6&quot; &quot;H6PD&quot; &quot;HAPLN2&quot; ## [127] &quot;HCRT&quot; &quot;HDAC4&quot; &quot;HGF&quot; &quot;HLA-DQA1&quot; &quot;HOTAIR&quot; &quot;HSD17B2&quot; ## [133] &quot;HSPA1B&quot; &quot;HSPA1L&quot; &quot;HYAL1&quot; &quot;IER3&quot; &quot;IFNAR2&quot; &quot;IFNG&quot; ## [139] &quot;IGF1&quot; &quot;IKBKB&quot; &quot;IL10&quot; &quot;IL16&quot; &quot;IL1R1&quot; &quot;IL1RAP&quot; ## [145] &quot;IL20RA&quot; &quot;INPP4B&quot; &quot;IRF1&quot; &quot;ITGA8&quot; &quot;ITGAM&quot; &quot;ITGB1&quot; ## [151] &quot;JMJD6&quot; &quot;JUP&quot; &quot;KCNQ1&quot; &quot;KEAP1&quot; &quot;KLC1&quot; &quot;KLHL21&quot; ## [157] &quot;KRT1&quot; &quot;KRT18&quot; &quot;KRT27&quot; &quot;LAMB1&quot; &quot;LCE2B&quot; &quot;LEPR&quot; ## [163] &quot;LGALS7&quot; &quot;LMF1&quot; &quot;LMNA&quot; &quot;LRP8&quot; &quot;LRRC20&quot; &quot;MALAT1&quot; ## [169] &quot;MAOA&quot; &quot;MAP2&quot; &quot;MAP2K6&quot; &quot;MAP3K8&quot; &quot;MAPT&quot; &quot;MARVELD3&quot; ## [175] &quot;MBNL2&quot; &quot;MEF2C&quot; &quot;MEG3&quot; &quot;MGMT&quot; &quot;MICB&quot; &quot;MLC1&quot; ## [181] &quot;MLH1&quot; &quot;MMP19&quot; &quot;MOSMO&quot; &quot;MPG&quot; &quot;MRAP&quot; &quot;MSH2&quot; ## [187] &quot;MSI2&quot; &quot;MT1M&quot; &quot;MTOR&quot; &quot;MUC1&quot; &quot;MYH14&quot; &quot;MYL9&quot; ## [193] &quot;MYRIP&quot; &quot;NCL&quot; &quot;NEBL&quot; &quot;NEDD4&quot; &quot;NES&quot; &quot;NEU1&quot; ## [199] &quot;NFE2L2&quot; &quot;NLRP3&quot; &quot;NOS2&quot; &quot;NPM1&quot; &quot;NRF1&quot; &quot;NRG1&quot; ## [205] &quot;NRP2&quot; &quot;NTM&quot; &quot;NUAK2&quot; &quot;NUP62CL&quot; &quot;OASL&quot; &quot;OCLN&quot; ## [211] &quot;OSBPL5&quot; &quot;PALS1&quot; &quot;PCSK6&quot; &quot;PDZD2&quot; &quot;PECAM1&quot; &quot;PFKFB3&quot; ## [217] &quot;PGAP2&quot; &quot;PGK1&quot; &quot;PIAS1&quot; &quot;PLA2G4D&quot; &quot;PLCD1&quot; &quot;PLEC&quot; ## [223] &quot;PLEKHA6&quot; &quot;PLEKHG3&quot; &quot;PPFIA4&quot; &quot;PPFIBP2&quot; &quot;PPTC7&quot; &quot;PRDX1&quot; ## [229] &quot;PRKCQ&quot; &quot;PRMT6&quot; &quot;PRR5L&quot; &quot;PRSS3&quot; &quot;PTGS2&quot; &quot;PTPRE&quot; ## [235] &quot;PVT1&quot; &quot;PYROXD2&quot; &quot;RAB11FIP3&quot; &quot;RAMP1&quot; &quot;RAP1GAP2&quot; &quot;RAPGEF1&quot; ## [241] &quot;RASAL2&quot; &quot;RELCH&quot; &quot;RGMA&quot; &quot;RHEBL1&quot; &quot;RHOH&quot; &quot;RIPOR1&quot; ## [247] &quot;RNF213&quot; &quot;RNF216&quot; &quot;ROBO1&quot; &quot;S100P&quot; &quot;S1PR1&quot; &quot;SBF1&quot; ## [253] &quot;SBNO2&quot; &quot;SCGB3A1&quot; &quot;SCHIP1&quot; &quot;SELENOW&quot; &quot;SEMA5B&quot; &quot;SGMS1&quot; ## [259] &quot;SH2B2&quot; &quot;SKP2&quot; &quot;SLC22A5&quot; &quot;SLC44A2&quot; &quot;SLC6A6&quot; &quot;SNCA&quot; ## [265] &quot;SNHG32&quot; &quot;SNX1&quot; &quot;SORL1&quot; &quot;SPHK1&quot; &quot;SPINK1&quot; &quot;SPSB1&quot; ## [271] &quot;SPTBN1&quot; &quot;SQSTM1&quot; &quot;SRGAP1&quot; &quot;SSU72&quot; &quot;STAT3&quot; &quot;STK17B&quot; ## [277] &quot;STX1A&quot; &quot;STX3&quot; &quot;SULT2B1&quot; &quot;TCEA3&quot; &quot;TERT&quot; &quot;TGFB1&quot; ## [283] &quot;TGFB3&quot; &quot;TGFBR2&quot; &quot;THNSL2&quot; &quot;TIMP2&quot; &quot;TLR10&quot; &quot;TMEM86A&quot; ## [289] &quot;TNFRSF10B&quot; &quot;TNFRSF10D&quot; &quot;TNFRSF1B&quot; &quot;TNFSF10&quot; &quot;TNNC2&quot; &quot;TP53&quot; ## [295] &quot;TRIB1&quot; &quot;TRNP1&quot; &quot;TSC22D3&quot; &quot;TSLP&quot; &quot;TXNRD1&quot; &quot;UAP1&quot; ## [301] &quot;UBE2J2&quot; &quot;ULK1&quot; &quot;USP36&quot; &quot;VAV3&quot; &quot;VWF&quot; &quot;WDR26&quot; ## [307] &quot;WDR55&quot; &quot;WNK1&quot; &quot;WWTR1&quot; &quot;XDH&quot; &quot;ZBTB25&quot; &quot;ZEB1&quot; ## [313] &quot;ZNF200&quot; &quot;ZNF267&quot; &quot;ZNF696&quot; Answer to Environmental Health Question 2 With this, we can answer Environmental Health Question 2: Of the genes showing altered expression, which may be under epigenetic control? Answer: We identified 315 genes with altered expression resulting from arsenic exposure, that also demonstrate epigenetic modifications from arsenic. These genes include many high interest molecules involved in regulating cell health, including several cyclin dependent kinases (e.g., CDK2, CDK4, CDK5, CDK6), molecules involved in oxidative stress (e.g., FOSB, NOS2), and cytokines involved in inflammatory response pathways (e.g., IFNG, IL10, IL16, IL1R1, IR1RAP, TGFB1, TGFB3). Method 2 for list comparisons: Intersection For further training, shown here is another method for pulling this list of interest, through the use of the ‘intersection’ function. Obtain a list of the overlapping genes in the overall expression results and the methylation results. inxn = intersect(exp_filt$`Gene Symbol`,met_filt$`Gene Symbol`) Again, we end up with a list of 315 unique genes that show altered expression and altered methylation. This list can be viewed on its own or converted to a dataframe (df). inxn_df = data.frame(genes=inxn) This list can also be conveniently used to filter the original query results. inxn_df_all_data = ctd %&gt;% filter(`Gene Symbol` %in% inxn) Note that in this last case, the same 315 genes are present, but this time the results contain all records from the original query results, hence the 875 rows (875 records observations reflecting the 315 genes). summary(unique(sort(inxn_df_all_data$`Gene Symbol`))==sort(merge_df$`Gene Symbol`)) ## Mode TRUE ## logical 315 dim(inxn_df_all_data) ## [1] 875 9 Visually we can represent this as a Venn diagram. Here, we use the “VennDiagram” R package. # Use the data we previously used for intersection in the venn diagram function venn.plt = venn.diagram( x = list(exp_filt$`Gene Symbol`, met_filt$`Gene Symbol`), category.names = c(&quot;Altered Expression&quot; , &quot;Altered Methylation&quot;), filename = NULL, # Change font size, type, and position cat.cex = 1.15, cat.fontface = &quot;bold&quot;, cat.default.pos = &quot;outer&quot;, cat.pos = c(-27, 27), cat.dist = c(0.055, 0.055), # Change color of ovals col=c(&quot;#440154ff&quot;, &#39;#21908dff&#39;), fill = c(alpha(&quot;#440154ff&quot;,0.3), alpha(&#39;#21908dff&#39;,0.3)), ) Concluding Remarks In conclusion, we identified 315 genes that show altered expression in response to arsenic exposure that may be under epigenetic control. These genes represent critical mediators of oxidative stress and inflammation, among other important cellular processes. Results yielded an important list of genes representing potential targets for further evaluation, to better understand mechanism of environmental exposure-induced disease. Together, this example highlights the utility of CTD to address environmental health research questions. For more information, see the recently updated primary CTD publication: Davis AP, Grondin CJ, Johnson RJ, Sciaky D, Wiegers J, Wiegers TC, Mattingly CJ. Comparative Toxicogenomics Database (CTD): update 2021. Nucleic Acids Res. 2021 Jan 8;49(D1):D1138-D1143. PMID: 33068428. Additional case studies relevant to environmental health research include the following: An example publication leveraging CTD findings to identify mechanisms of metals-induced birth defects: Ahir BK, Sanders AP, Rager JE, Fry RC. Systems biology and birth defects prevention: blockade of the glucocorticoid receptor prevents arsenic-induced birth defects. Environ Health Perspect. 2013 Mar;121(3):332-8. PMID: 23458687. An example publication leveraging CTD to help fill data gaps on data poor chemicals, in combination with ToxCast/Tox21 data streams, to elucidate environmental influences on disease pathways: Kosnik MB, Planchart A, Marvel SW, Reif DM, Mattingly CJ. Integration of curated and high-throughput screening data to elucidate environmental influences on disease pathways. Comput Toxicol. 2019 Nov;12:100094. PMID: 31453412. An example publication leveraging CTD to extract chemical-disease relationships used to derive new chemical risk values, with the goal of prioritizing connections between environmental factors, genetic variants, and human diseases: Kosnik MB, Reif DM. Determination of chemical-disease risk values to prioritize connections between environmental factors, genetic variants, and human diseases. Toxicol Appl Pharmacol. 2019 Sep 15;379:114674. PMID: 31323264. Test Your Knowledge Using the same dataset from this module (available at the GitHub site and as Module7_1_TYKInput.csv): Filter the data using the grepl function to look at only those observations that specifically decrease the target gene’s “expression” level. How many observations are there? Similarly, filter the data to identify how many observations there are where the target gene’s “expression” level is simply “affected”. Create a venn diagram to help visualize any overlap between these two filtered datasets. "],["gene-expression-omnibus.html", "7.2 Gene Expression Omnibus Introduction to Training Module Introduction to GEO Introduction to Example Data GEO Data in R Visualizing Data Statistical Analyses Concluding Remarks", " 7.2 Gene Expression Omnibus This training module was developed by Kyle R. Roell and Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Introduction to Training Module GEO is a publicly available database repository of high-throughput gene expression data and hybridization arrays, chips, and microarrays that span genome-wide endpoints of genomics, transcriptomics, and epigenomics. This training module specifically guides trainees through the loading of required packages and data, including the manual upload of GEO data as well as the upload/organization of data leveraging the GEOquery package. Data are then further organized and combined with gene annotation information through the merging of platform annotation files. Example visualizations are then produced, including boxplots to evaluate the overall distribution of expression data across samples, as well as heat map visualizations that compare unscaled versus scaled gene expression values. Statistical analyses are then included to identify which genes are significantly altered in expression upon exposure to formaldehyde. Together, this training module serves as a simple example showing methods to access and download GEO data and to perform data organization, analysis, and visualization tasks through applications-based questions. Introduction to GEO The GEO repository is organized and managed by the The National Center for Biotechnology Information (NCBI), which seeks to advance science and health by providing access to biomedical and genomic information. The three overall goals of GEO are to: (1) Provide a robust, versatile database in which to efficiently store high-throughput functional genomic data, (2) Offer simple submission procedures and formats that support complete and well-annotated data deposits from the research community, and (3) Provide user-friendly mechanisms that allow users to query, locate, review and download studies and gene expression profiles of interest. Of high relevance to environmental health, data organized within GEO can be pulled and analyzed to address new environmental health questions, leveraging previously generated data. For example, we have pulled gene expression data from acute myeloid leukemia patients and re-analyzed these data to elucidate new mechanisms of epigenetically-regulated networks involved in cancer, that in turn, may be modified by environmental insults, as previously published in Rager et al. 2012. We have also pulled and analyzed gene expression data from published studies evaluating toxicity resulting from hexavalent chromium exposure, to further substantiate the role of epigenetic mediators in hexavelent chromium-induced carcinogenesis (see Rager et al. 2019). This training exercise leverages an additional dataset that we published and deposited through GEO to evaluate the effects of formaldehyde inhalation exposure, as detailed below. Introduction to Example Data In this training module, data will be pulled from the published GEO dataset recorded through the online series GSE42394. This series represents Affymetrix rat genome-wide microarray data generated from our previous study, aimed at evaluating the transcriptomic effects of formaldehyde across three tissues: the nose, blood, and bone marrow. For the purposes of this training module, we will focus on evaluating gene expression profiles from nasal samples after 7 days of exposure, collected from rats exposed to 2 ppm formaldehyde via inhalation. These findings, in addition to other epigenomic endpoint measures, have been previously published (see Rager et al. 2014). Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: What kind of molecular identifiers are commonly used in microarray-based -omics technologies? How can we convert platform-specific molecular identifiers used in -omics study designs to gene-level information? Why do we often scale gene expression signatures prior to heat map visualizations? What genes are altered in expression by formaldehyde inhalation exposure? What are the potential biological consequences of these gene-level perturbations? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;) if (!requireNamespace(&quot;reshape2&quot;)) install.packages(&quot;reshape2&quot;) # GEOquery, this will install BiocManager if you don&#39;t have it installed if (!requireNamespace(&quot;BiocManager&quot;)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;GEOquery&quot;) ## Warning: package(s) not installed when version(s) same as or greater than current; use ## `force = TRUE` to re-install: &#39;GEOquery&#39; Loading R packages required for this session library(tidyverse) library(reshape2) library(GEOquery) For more information on the tidyverse package, see its associated CRAN webpage, primary webpage, and peer-reviewed article released in 2018. For more information on the reshape2 package, see its associated CRAN webpage, R Documentation, and helpful website providing an introduction to the reshape2 package. For more information on the GEOquery package, see its associated Bioconductor website and R Documentation file. Set your working directory setwd(&quot;/filepath to where your input files are&quot;) GEO Data in R Let’s start by loading the GEO dataset needed for this training module. As explained in the introduction, this module walks through two methods of uploading GEO data: manual option vs automatic option using the GEOquery package. These two methods are detailed below. 1. Manually Downloading and Uploading GEO Files In this first method, we will navigate to the dataset within the GEO website, manually download its associated text data file, save it in our working directory, and then upload it into our global environment in R. For the purposes of this training exercise, we manually downloaded the GEO series matrix file from the GEO series webpage, located at: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE42394. The specific file that was downloaded was noted as “GSE42394_series_matrix.txt”, pulled by clicking on the link indicated by the red arrow from the GEO series webpage: For simplicity, we also have already pre-filtered this file for the samples we are interested in, focusing on the rat nasal gene expression data after 7 days of exposure to gaseous formaldehyde. This filtered file was saved as “GSE42394_series_matrix_filtered.txt”, then renamed “Module7_2_InputData1.txt” for use in this module. At this point, we can simply read in this pre-filtered text file for the purposes of this training module geodata_manual = read.table(file=&quot;Module7_2_Input/Module7_2_InputData1.txt&quot;, header=T) Because this is a manual approach, we have to also manually define the treated and untreated samples (based on manually opening the surrounding metadata from the GEO webpage) Manually defining treated and untreated for these samples of interest: exposed_manual = c(&quot;GSM1150940&quot;, &quot;GSM1150941&quot;, &quot;GSM1150942&quot;) unexposed_manual = c(&quot;GSM1150937&quot;, &quot;GSM1150938&quot;, &quot;GSM1150939&quot;) 2. Loading and Organizing GEO Files through the GEOquery Package In this second method, we will leverage the GEOquery package, which allows for easier downloading and reading in of data from GEO without having to manually download raw text files, and manually assign sample attributes (e.g., exposed vs unexposed). This package is set-up to automatically merge sample information from GEO metadata files with raw genome-wide datasets. Let’s first use the getGEO function (from the GEOquery package) to load data from our series matrix (“GSE42394_series_matrix.txt”, renamed “Module7_2_InputData2.txt” for use in this module). Note that this line of code may take a couple of minutes to run. geo.getGEO.data = getGEO(filename=&#39;Module7_2_Input/Module7_2_InputData2.txt&#39;) One of the reasons the getGEO package is so helpful is that we can automatically link a dataset with nicely organized sample information using the pData() function. sampleInfo = pData(geo.getGEO.data) Let’s view this sample information / metadata file, first by viewing what the column headers are. colnames(sampleInfo) ## [1] &quot;title&quot; &quot;geo_accession&quot; ## [3] &quot;status&quot; &quot;submission_date&quot; ## [5] &quot;last_update_date&quot; &quot;type&quot; ## [7] &quot;channel_count&quot; &quot;source_name_ch1&quot; ## [9] &quot;organism_ch1&quot; &quot;characteristics_ch1&quot; ## [11] &quot;characteristics_ch1.1&quot; &quot;characteristics_ch1.2&quot; ## [13] &quot;characteristics_ch1.3&quot; &quot;characteristics_ch1.4&quot; ## [15] &quot;characteristics_ch1.5&quot; &quot;treatment_protocol_ch1&quot; ## [17] &quot;growth_protocol_ch1&quot; &quot;molecule_ch1&quot; ## [19] &quot;extract_protocol_ch1&quot; &quot;label_ch1&quot; ## [21] &quot;label_protocol_ch1&quot; &quot;taxid_ch1&quot; ## [23] &quot;hyb_protocol&quot; &quot;scan_protocol&quot; ## [25] &quot;description&quot; &quot;data_processing&quot; ## [27] &quot;platform_id&quot; &quot;contact_name&quot; ## [29] &quot;contact_email&quot; &quot;contact_department&quot; ## [31] &quot;contact_institute&quot; &quot;contact_address&quot; ## [33] &quot;contact_city&quot; &quot;contact_zip/postal_code&quot; ## [35] &quot;contact_country&quot; &quot;supplementary_file&quot; ## [37] &quot;data_row_count&quot; &quot;age:ch1&quot; ## [39] &quot;cell type:ch1&quot; &quot;gender:ch1&quot; ## [41] &quot;strain:ch1&quot; &quot;time:ch1&quot; ## [43] &quot;treatment:ch1&quot; Then viewing the first five columns. sampleInfo[1:10,1:5] ## title geo_accession ## GSM1150937 Nose_7DayControl_Rep1 [Affymetrix] GSM1150937 ## GSM1150938 Nose_7DayControl_Rep2 [Affymetrix] GSM1150938 ## GSM1150939 Nose_7DayControl_Rep3 [Affymetrix] GSM1150939 ## GSM1150940 Nose_7DayExposed_Rep1 [Affymetrix] GSM1150940 ## GSM1150941 Nose_7DayExposed_Rep2 [Affymetrix] GSM1150941 ## GSM1150942 Nose_7DayExposed_Rep3 [Affymetrix] GSM1150942 ## GSM1150943 WhiteBloodCells_7DayControl_Rep1 [Affymetrix] GSM1150943 ## GSM1150944 WhiteBloodCells_7DayControl_Rep2 [Affymetrix] GSM1150944 ## GSM1150945 WhiteBloodCells_7DayControl_Rep3 [Affymetrix] GSM1150945 ## GSM1150946 WhiteBloodCells_7DayExposed_Rep1 [Affymetrix] GSM1150946 ## status submission_date last_update_date ## GSM1150937 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150938 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150939 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150940 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150941 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150942 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150943 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150944 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150945 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150946 Public on Jan 07 2014 May 29 2013 Jan 07 2014 This shows that each sample is provided with a unique number starting with “GSM”, and these are described by information summarized in the “title” column. We can also see that these data were made public on Jan 7, 2014. Let’s view the next five columns. sampleInfo[1:10,6:10] ## type channel_count source_name_ch1 ## GSM1150937 RNA 1 Nasal epithelial cells, 7 day, unexposed ## GSM1150938 RNA 1 Nasal epithelial cells, 7 day, unexposed ## GSM1150939 RNA 1 Nasal epithelial cells, 7 day, unexposed ## GSM1150940 RNA 1 Nasal epithelial cells, 7 day, exposed ## GSM1150941 RNA 1 Nasal epithelial cells, 7 day, exposed ## GSM1150942 RNA 1 Nasal epithelial cells, 7 day, exposed ## GSM1150943 RNA 1 Circulating white blood cells, 7 day, unexposed ## GSM1150944 RNA 1 Circulating white blood cells, 7 day, unexposed ## GSM1150945 RNA 1 Circulating white blood cells, 7 day, unexposed ## GSM1150946 RNA 1 Circulating white blood cells, 7 day, exposed ## organism_ch1 characteristics_ch1 ## GSM1150937 Rattus norvegicus gender: male ## GSM1150938 Rattus norvegicus gender: male ## GSM1150939 Rattus norvegicus gender: male ## GSM1150940 Rattus norvegicus gender: male ## GSM1150941 Rattus norvegicus gender: male ## GSM1150942 Rattus norvegicus gender: male ## GSM1150943 Rattus norvegicus gender: male ## GSM1150944 Rattus norvegicus gender: male ## GSM1150945 Rattus norvegicus gender: male ## GSM1150946 Rattus norvegicus gender: male We can see that information is provided here surrounding the type of sample that was analyzed (i.e., RNA), more information on the collected samples within the column source_name_ch1, and the organism (rat) is provided in the organism_ch1 column. More detailed metadata information is provided throughout this file, as seen when viewing the column headers above. Defining samples Now, we can use this information to define the samples we want to analyze. Note that this is the same step we did manually above. In this training exercise, we are focusing on responses in the nose, so we can easily filter for cell type = Nasal epithelial cells (specifically in the cell type:ch1 variable). We are also focusing on responses collected after 7 days of exposure, which we can filter for using time = 7 day (specifically in the time:ch1 variable). We will also define exposed and unexposed samples using the variable treatment:ch1. First, let’s subset the sampleInfo dataframe to just keep the samples we’re interested in # Define a vector variable (here we call it &#39;keep&#39;) that will store rows we want to keep keep = rownames(sampleInfo[which(sampleInfo$`cell type:ch1`==&quot;Nasal epithelial cells&quot; &amp; sampleInfo$`time:ch1`==&quot;7 day&quot;),]) # Then subset the sample info for just those samples we defined in keep variable sampleInfo = sampleInfo[keep,] Next, we can pull the exposed and unexposed animal IDs. Let’s first see how these are labeled within the treatment:ch1 variable. unique(sampleInfo$`treatment:ch1`) ## [1] &quot;unexposed&quot; &quot;2 ppm formaldehyde&quot; And then search for the rows of data, pulling the sample animal IDs (which are in the variable geo_accession). exposedIDs = sampleInfo[which(sampleInfo$`treatment:ch1`==&quot;2 ppm formaldehyde&quot;), &quot;geo_accession&quot;] unexposedIDs = sampleInfo[which(sampleInfo$`treatment:ch1`==&quot;unexposed&quot;), &quot;geo_accession&quot;] The next step is to pull the expression data we want to use in our analyses. The GEOquery function, exprs(), allows us to easily pull these data. Here, we can pull the data we’re interested in using the exprs() function, while defining the data we want to pull based off our previously generated ‘keep’ vector. # As a reminder, this is what the &#39;keep&#39; vector includes # (i.e., animal IDs that we&#39;re interested in) keep ## [1] &quot;GSM1150937&quot; &quot;GSM1150938&quot; &quot;GSM1150939&quot; &quot;GSM1150940&quot; &quot;GSM1150941&quot; ## [6] &quot;GSM1150942&quot; # Using the exprs() function geodata = exprs(geo.getGEO.data[,keep]) Let’s view the full dataset as is now: head(geodata) ## GSM1150937 GSM1150938 GSM1150939 GSM1150940 GSM1150941 GSM1150942 ## 10700001 5786.60 5830.08 5637.34 5313.33 5557.04 5469.90 ## 10700002 192.92 206.86 220.83 183.12 177.16 198.64 ## 10700003 1820.98 1795.79 1735.70 1578.02 1681.58 1632.20 ## 10700004 66.95 65.61 64.41 60.19 60.41 60.67 ## 10700005 770.07 753.41 731.20 684.53 657.25 667.66 ## 10700006 5.80 5.35 5.48 5.58 5.39 5.35 This now represents a matrix of data, with animal IDs as column headers and expression levels within the matrix. Simplifying column names These column names are not the easiest to interpret, so let’s rename these columns to indicate which animals were from the exposed vs. unexposed groups. We need to first convert our expression dataset to a dataframe so we can edit columns names, and continue with downstream data manipulations that require dataframe formats. geodata = data.frame(geodata) Let’s remind ourselves what the column names are: colnames(geodata) ## [1] &quot;GSM1150937&quot; &quot;GSM1150938&quot; &quot;GSM1150939&quot; &quot;GSM1150940&quot; &quot;GSM1150941&quot; ## [6] &quot;GSM1150942&quot; Which ones of these are exposed vs unexposed animals can be determined by viewing our previously defined vectors. exposedIDs ## [1] &quot;GSM1150940&quot; &quot;GSM1150941&quot; &quot;GSM1150942&quot; unexposedIDs ## [1] &quot;GSM1150937&quot; &quot;GSM1150938&quot; &quot;GSM1150939&quot; With this we can tell that the first three listed IDs are from unexposed animals, and the last three IDs are from exposed animals. Let’s simplify the names of these columns to indicate exposure status and replicate number. colnames(geodata) = c(&quot;Control_1&quot;, &quot;Control_2&quot;, &quot;Control_3&quot;, &quot;Exposed_1&quot;, &quot;Exposed_2&quot;, &quot;Exposed_3&quot;) And we’ll now need to re-define our ‘exposed’ vs ‘unexposed’ vectors for downstream script. exposedIDs = c(&quot;Exposed_1&quot;, &quot;Exposed_2&quot;, &quot;Exposed_3&quot;) unexposedIDs = c(&quot;Control_1&quot;, &quot;Control_2&quot;, &quot;Control_3&quot;) Viewing the data again: head(geodata) ## Control_1 Control_2 Control_3 Exposed_1 Exposed_2 Exposed_3 ## 10700001 5786.60 5830.08 5637.34 5313.33 5557.04 5469.90 ## 10700002 192.92 206.86 220.83 183.12 177.16 198.64 ## 10700003 1820.98 1795.79 1735.70 1578.02 1681.58 1632.20 ## 10700004 66.95 65.61 64.41 60.19 60.41 60.67 ## 10700005 770.07 753.41 731.20 684.53 657.25 667.66 ## 10700006 5.80 5.35 5.48 5.58 5.39 5.35 These data are now looking easier to interpret/analyze. Still, the row identifiers include 8 digit numbers starting with “107…”. We know that this dataset is a gene expression dataset, but these identifiers, in themselves, don’t tell us much about what genes these are referring to. These numeric IDs specifically represent microarray probesetIDs, that were produced by the Affymetrix platform used in the original study. But how can we tell which genes are represented by these data?! Adding gene symbol information Each -omics dataset contained within GEO points to a specific platform that was used to obtain measurements. In instances where we want more information surrounding the molecular identifiers, we can merge the platform-specific annotation file with the molecular IDs given in the full dataset. For example, let’s pull the platform-specific annotation file for this experiment. Let’s revisit the website that contained the original dataset on GEO. Scroll down to where it lists “Platforms”, and there is a hyperlinked platform number “GPL6247” (see arrow below). Click on this, and you will be navigated to a different GEO website describing the Affymetrix rat array platform that was used in this analysis. Note that this website also includes information on when this array became available, links to other experiments that have used this platform within GEO, and much more. Here, we’re interested in pulling the corresponding gene symbol information for the probeset IDs. To do so, scroll to the bottom, and click “Annotation SOFT table…” and download the corresponding .gz file within your working directory. Unzip this, and you will find the master annotation file: “GPL6247.annot”. In this exercise, we’ve already done these steps and unzipped the file in our working directory. So at this point, we can simply read in this annotation dataset, renamed “Module7_2_InputData2.annot”, still using the GEOquery() function to help automate. geo.annot = GEOquery::getGEO(filename=&quot;Module7_2_Input/Module7_2_InputData3.annot&quot;) Now we can use the Table() function from GEOquery to pull data from the annotation dataset. id.gene.table = GEOquery::Table(geo.annot)[,c(&quot;ID&quot;, &quot;Gene symbol&quot;)] id.gene.table[1:10,1:2] ## ID Gene symbol ## 1 10701620 Vom2r67///Vom2r5///Vom2r6///Vom2r4 ## 2 10701630 ## 3 10701632 ## 4 10701636 ## 5 10701643 ## 6 10701648 Vom2r5 ## 7 10701654 Vom2r6 ## 8 10701663 ## 9 10701666 ## 10 10701668 Vom2r65///Vom2r1 With these two columns of data, we now have the needed IDs and gene symbols to match with our dataset. Within the full dataset, we need to add a new column for the probeset ID, taken from the rownames, in preparation for the merging step. geodata$ID = rownames(geodata) We can now merge the gene symbol information by ID with our expression data. geodata_genes = merge(geodata, id.gene.table, by=&quot;ID&quot;) head(geodata_genes) ## ID Control_1 Control_2 Control_3 Exposed_1 Exposed_2 Exposed_3 ## 1 10700001 5786.60 5830.08 5637.34 5313.33 5557.04 5469.90 ## 2 10700002 192.92 206.86 220.83 183.12 177.16 198.64 ## 3 10700003 1820.98 1795.79 1735.70 1578.02 1681.58 1632.20 ## 4 10700004 66.95 65.61 64.41 60.19 60.41 60.67 ## 5 10700005 770.07 753.41 731.20 684.53 657.25 667.66 ## 6 10700006 5.80 5.35 5.48 5.58 5.39 5.35 ## Gene symbol ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 Note that many of the probeset IDs do not map to full gene symbols, which is shown here by viewing the top few rows - this is expected in genome-wide analyses based on microarray platforms. Let’s look at the first 25 unique genes in these data: UniqueGenes = unique(geodata_genes$`Gene symbol`) UniqueGenes[1:25] ## [1] &quot;&quot; ## [2] &quot;Vom2r67///Vom2r5///Vom2r6///Vom2r4&quot; ## [3] &quot;Vom2r5&quot; ## [4] &quot;Vom2r6&quot; ## [5] &quot;Vom2r65///Vom2r1&quot; ## [6] &quot;Vom2r65///Vom2r5///Vom2r1///Vom2r6///Vom2r4&quot; ## [7] &quot;Vom2r65///Vom2r5///Vom2r6///Vom2r4&quot; ## [8] &quot;Raet1e&quot; ## [9] &quot;Lrp11&quot; ## [10] &quot;Katna1&quot; ## [11] &quot;Ppil4&quot; ## [12] &quot;Zc3h12d&quot; ## [13] &quot;Shprh&quot; ## [14] &quot;Fbxo30&quot; ## [15] &quot;Epm2a&quot; ## [16] &quot;Sf3b5&quot; ## [17] &quot;Plagl1&quot; ## [18] &quot;Fuca2&quot; ## [19] &quot;Adat2&quot; ## [20] &quot;Hivep2&quot; ## [21] &quot;Nmbr&quot; ## [22] &quot;Cited2&quot; ## [23] &quot;Txlnb&quot; ## [24] &quot;Reps1&quot; ## [25] &quot;Perp&quot; Again, you can see that the first value listed is blank, representing probesetIDs that do not match to fully annotated gene symbols. Though the rest pertain for gene symbols annotated to the rat genome. You can also see that some gene symbols have multiple entries, separated by “///” To simplify identifiers, we can pull just the first gene symbol, and remove the rest by using gsub(). geodata_genes$`Gene symbol` = gsub(&quot;///.*&quot;, &quot;&quot;, geodata_genes$`Gene symbol`) Let’s alphabetize by main expression dataframe by gene symbol. geodata_genes = geodata_genes[order(geodata_genes$`Gene symbol`),] And then re-view these data: geodata_genes[1:5,] ## ID Control_1 Control_2 Control_3 Exposed_1 Exposed_2 Exposed_3 ## 1 10700001 5786.60 5830.08 5637.34 5313.33 5557.04 5469.90 ## 2 10700002 192.92 206.86 220.83 183.12 177.16 198.64 ## 3 10700003 1820.98 1795.79 1735.70 1578.02 1681.58 1632.20 ## 4 10700004 66.95 65.61 64.41 60.19 60.41 60.67 ## 5 10700005 770.07 753.41 731.20 684.53 657.25 667.66 ## Gene symbol ## 1 ## 2 ## 3 ## 4 ## 5 In preparation for the visualization steps below, let’s reset the probeset IDs to rownames. rownames(geodata_genes) = geodata_genes$ID # Can then remove this column within the dataframe geodata_genes$ID = NULL Finally let’s rearrange this dataset to include gene symbols as the first column, right after rownames (probeset IDs). geodata_genes = geodata_genes[,c(ncol(geodata_genes),1:(ncol(geodata_genes)-1))] geodata_genes[1:5,] ## Gene symbol Control_1 Control_2 Control_3 Exposed_1 Exposed_2 ## 10700001 5786.60 5830.08 5637.34 5313.33 5557.04 ## 10700002 192.92 206.86 220.83 183.12 177.16 ## 10700003 1820.98 1795.79 1735.70 1578.02 1681.58 ## 10700004 66.95 65.61 64.41 60.19 60.41 ## 10700005 770.07 753.41 731.20 684.53 657.25 ## Exposed_3 ## 10700001 5469.90 ## 10700002 198.64 ## 10700003 1632.20 ## 10700004 60.67 ## 10700005 667.66 dim(geodata_genes) ## [1] 29214 7 Note that this dataset includes expression measures across 29,214 probes, representing 14,019 unique genes. For simplicity in the final exercises, let’s just filter for rows representing mapped genes. geodata_genes = geodata_genes[!(geodata_genes$`Gene symbol` == &quot;&quot;), ] dim(geodata_genes) ## [1] 16024 7 Note that this dataset now includes 16,024 rows with mapped gene symbol identifiers. Answer to Environmental Health Question 1 With this, we can now answer Environmental Health Question 1: What kind of molecular identifiers are commonly used in microarray-based -omics technologies? Answer: Platform-specific probeset IDs. Answer to Environmental Health Question 2 We can also answer Environmental Health Question 2: How can we convert platform-specific molecular identifiers used in -omics study designs to gene-level information? Answer: We can merge platform-specific IDs with gene-level information using annotation files. Visualizing Data Visualizing Gene Expression Data using Boxplots and Heat Maps To visualize the -omics data, we can generate boxplots, heat maps, any many other types of visualizations. Here, we provide an example to plot a boxplot, which can be used to visualize the variability amongst samples. We also provide an example to plot a heat map, comparing unscaled vs scaled gene expression profiles. These visualizations can be useful to both simply visualize the data as well as identify patterns across samples or genes Boxplot visualizations For this example, let’s simply use R’s built in boxplot() function. We only want to use columns with our expression data (2 to 7), so let’s pull those columns when running the boxplot function. boxplot(geodata_genes[,2:7]) There seem to be a lot of variability within each sample’s range of expression levels, with many outliers. This makes sense given that we are analyzing the expression levels across the rat’s entire genome, where some genes won’t be expressed at all while others will be highly expressed due to biological and/or potential technical variability. To show plots without outliers, we can simply use outline=F. boxplot(geodata_genes[,2:7], outline=F) Heat Map visualizations Heat maps are also useful when evaluating large datasets. There are many different packages you can use to generate heat maps. Here, we use the superheat package. It also takes awhile to plot all genes across the genome, so to save time for this training module, let’s randomly select 100 rows to plot. # To ensure that the same subset of genes are selected each time set.seed = 101 # Random selection of 100 rows row.sample = sample(1:nrow(geodata_genes),100) # Heat map code superheat::superheat(geodata_genes[row.sample,2:7], # Only want to plot non-id/gene symbol columns (2 to 7) pretty.order.rows = TRUE, pretty.order.cols = TRUE, col.dendrogram = T, row.dendrogram = T) This produces a heat map with sample IDs along the x-axis and probeset IDs along the y-axis. Here, the values being displayed represent normalized expression values. One way to improve our ability to distinguish differences between samples is to scale expression values across probes. Scaling data Z-score is a very common method of scaling that transforms data points to reflect the number of standard deviations they are from the overall mean. Z-score scaling data results in the overall transformation of a dataset to have an overall mean = 0 and standard deviation = 1. Let’s see what happens when we scale this gene expression dataset by z-score across each probe. This can be easily done using the scale() function. This specific scale() function works by centering and scaling across columns, but since we want to use it across probesets (organized as rows), we need to first transpose our dataset, then run the scale function. geodata_genes_scaled = scale(t(geodata_genes[,2:7]), center=T, scale=T) Now we can transpose it back to the original format (i.e., before it was transposed). geodata_genes_scaled = t(geodata_genes_scaled) And then view what the normalized and now scaled expression data look like for now a random subset of 100 probesets (representing genes). With these data now scaled, we can more easily visualize patterns between samples. Answer to Environmental Health Question 3 We can also answer Environmental Health Question 3: Why do we often scale gene expression signatures prior to heat map visualizations? Answer: To better visualize patterns in expression signatures between samples. Now, with these data nicely organized, we can next explore how statistics can help us find which genes show trends in expression associated with formaldehyde exposure. Statistical Analyses Statistical Analyses to Identify Genes altered by Formaldehyde A simple way to identify differences between formaldehyde-exposed and unexposed samples is to use a t-test. Because there are so many tests being performed, one for each gene, it is also important to carry out multiple test corrections through a p-value adjustment method. We need to run a t-test for each row of our dataset. This exercise demonstrates two different methods to run a t-test: Method 1: using a ‘for loop’ Method 2: using the apply function (more computationally efficient) Method 1 (m1): ‘For Loop’ Let’s first re-save the molecular probe IDs to a column within the dataframe, since we need those values in the loop function. geodata_genes$ID = rownames(geodata_genes) We also need to initially create an empty dataframe to eventually store p-values. pValue_m1 = matrix(0, nrow=nrow(geodata_genes), ncol=3) colnames(pValue_m1) = c(&quot;ID&quot;, &quot;pval&quot;, &quot;padj&quot;) head(pValue_m1) ## ID pval padj ## [1,] 0 0 0 ## [2,] 0 0 0 ## [3,] 0 0 0 ## [4,] 0 0 0 ## [5,] 0 0 0 ## [6,] 0 0 0 You can see the empty dataframe that was generated through this code. Then we can loop through the entire dataset to acquire p-values from t-test statistics, comparing n=3 exposed vs n=3 unexposed samples. for (i in 1:nrow(geodata_genes)) { #Get the ID ID.i = geodata_genes[i, &quot;ID&quot;]; #Run the t-test and get the p-value pval.i = t.test(geodata_genes[i,exposedIDs], geodata_genes[i,unexposedIDs])$p.value; #Store the data in the empty dataframe pValue_m1[i,&quot;ID&quot;] = ID.i; pValue_m1[i,&quot;pval&quot;] = pval.i } View the results: # Note that we&#39;re not pulling the last column (padj) since we haven&#39;t calculated these yet pValue_m1[1:5,1:2] ## ID pval ## [1,] &quot;10903987&quot; &quot;0.0812802229304083&quot; ## [2,] &quot;10714794&quot; &quot;0.757311314118124&quot; ## [3,] &quot;10858408&quot; &quot;0.390952310869689&quot; ## [4,] &quot;10872252&quot; &quot;0.0548937136005506&quot; ## [5,] &quot;10905819&quot; &quot;0.173539535577791&quot; Method 2 (m2): Apply Function For the second method, we can use the apply() function to calculate resulting t-test p-values more efficiently labeled. pValue_m2 = apply(geodata_genes[,2:7], 1, function(x) t.test(x[unexposedIDs], x[exposedIDs])$p.value) names(pValue_m2) = geodata_genes[,&quot;ID&quot;] We can convert the results into a dataframe to make it similar to m1 matrix we created above. pValue_m2 = data.frame(pValue_m2) # Now create an ID column pValue_m2$ID = rownames(pValue_m2) Then we can view at the two datasets to see they result in the same pvalues. head(pValue_m1) ## ID pval padj ## [1,] &quot;10903987&quot; &quot;0.0812802229304083&quot; &quot;0&quot; ## [2,] &quot;10714794&quot; &quot;0.757311314118124&quot; &quot;0&quot; ## [3,] &quot;10858408&quot; &quot;0.390952310869689&quot; &quot;0&quot; ## [4,] &quot;10872252&quot; &quot;0.0548937136005506&quot; &quot;0&quot; ## [5,] &quot;10905819&quot; &quot;0.173539535577791&quot; &quot;0&quot; ## [6,] &quot;10907585&quot; &quot;0.215200167867295&quot; &quot;0&quot; head(pValue_m2) ## pValue_m2 ID ## 10903987 0.08128022 10903987 ## 10714794 0.75731131 10714794 ## 10858408 0.39095231 10858408 ## 10872252 0.05489371 10872252 ## 10905819 0.17353954 10905819 ## 10907585 0.21520017 10907585 We can see from these results that both methods (m1 and m2) generate the same statistical p-values. Interpreting Results Let’s again merge these data with the gene symbols to tell which genes are significant. First, let’s convert to a dataframe and then merge as before, for one of the above methods as an example (m1). pValue_m1 = data.frame(pValue_m1) pValue_m1 = merge(pValue_m1, id.gene.table, by=&quot;ID&quot;) We can also add a multiple test correction by applying a false discovery rate-adjusted p-value; here, using the Benjamini Hochberg (BH) method. # Here fdr is an alias for B-H method pValue_m1[,&quot;padj&quot;] = p.adjust(pValue_m1[,&quot;pval&quot;], method=c(&quot;fdr&quot;)) Now, we can sort these statistical results by adjusted p-values. pValue_m1.sorted = pValue_m1[order(pValue_m1[,&#39;padj&#39;]),] head(pValue_m1.sorted) ## ID pval padj Gene symbol ## 9143 10837582 4.57288413593085e-07 0.00732759 Olr633 ## 5640 10783648 1.93688668590855e-06 0.01551834 Slc7a8 ## 8 10701699 0.0166773380386967 0.13089115 Lrp11 ## 17 10701817 0.0131845685452954 0.13089115 Fuca2 ## 19 10701830 0.00586885826460337 0.13089115 Hivep2 ## 23 10701880 0.00749149990409956 0.13089115 Reps1 Pulling just the significant genes using an adjusted p-value threshold of 0.05. adj.pval.sig = pValue_m1[which(pValue_m1[,&#39;padj&#39;] &lt; .05),] # Viewing these genes adj.pval.sig ## ID pval padj Gene symbol ## 5640 10783648 1.93688668590855e-06 0.01551834 Slc7a8 ## 9143 10837582 4.57288413593085e-07 0.00732759 Olr633 Answer to Environmental Health Question 4 With this, we can answer Environmental Health Question 4: What genes are altered in expression by formaldehyde inhalation exposure? Answer: Olr633 and Slc7a8. Finally, let’s plot these using a mini heat map. Note that we can use probesetIDs, then gene symbols, in rownames to have them show in heat map labels. Note that this statistical filter is pretty strict when comparing only n=3 vs n=3 biological replicates. If we loosen the statistical criteria to p-value &lt; 0.05, this is what we can find: pval.sig = pValue_m1[which(pValue_m1[,&#39;pval&#39;] &lt; .05),] nrow(pval.sig) ## [1] 5327 5327 genes with significantly altered expression! Note that other filters are commonly applied to further focus these lists (e.g., background and fold change filters) prior to statistical evaluation, which can impact the final results. See Rager et al. 2013 for further statistical approaches and visualizations. Answer to Environmental Health Question 5 With this, we can answer Environmental Health Question 5: What are the potential biological consequences of these gene-level perturbations? Answer: Olr633 stands for ‘olfactory receptor 633’. Olr633 is up-regulated in expression, meaning that formaldehyde inhalation exposure has a smell that resulted in ‘activated’ olfactory receptors in the nose of these exposed rats. Slc7a8 stands for ‘solute carrier family 7 member 8’. Slc7a8 is down-regulated in expression, and it plays a role in many biological processes, that when altered, can lead to changes in cellular homeostasis and disease. Concluding Remarks In conclusion, this training module provides an overview of pulling, organizing, visualizing, and analyzing -omics data from the online repository, Gene Expression Omnibus (GEO). Trainees are guided through the overall organization of an example high dimensional dataset, focusing on transcriptomic responses in the nasal epithelium of rats exposed to formaldehyde. Data are visualized and then analyzed using standard two-group comparisons. Findings are interpreted for biological relevance, yielding insight into the effects resulting from formaldehyde exposure. For additional case studies that leverage GEO, see the following publications that also address environmental health questions from our research group: Rager JE, Fry RC. The aryl hydrocarbon receptor pathway: a key component of the microRNA-mediated AML signalisome. Int J Environ Res Public Health. 2012 May;9(5):1939-53. doi: 10.3390/ijerph9051939. Epub 2012 May 18. PMID: 22754483; PMCID: PMC3386597. Rager JE, Suh M, Chappell GA, Thompson CM, Proctor DM. Review of transcriptomic responses to hexavalent chromium exposure in lung cells supports a role of epigenetic mediators in carcinogenesis. Toxicol Lett. 2019 May 1;305:40-50. PMID: 30690063. Test Your Knowledge Using the same dataset that was used in this module, available from the UNC-SRP TAME2 GitHub website: 1. Load the downloaded GEO dataset into R using the packages and functions mentioned in this tutorial. 2. Filter the data to just those with “cell type” of “Circulating white blood cells”. 3. Report the means of the first 5 rows of the gene expression data (10700001, 10700002, 10700003, 10700004, 10700005), across all samples. "],["comptox-dashboard-data-through-apis.html", "7.3 CompTox Dashboard Data through APIs Introduction to Training Module Training Module’s Environmental Health Questions Script Preparations Introduction to CompTox Chemicals Dashboard CCTE’s CTX Application Programming Interfaces (APIs) for Automated Batch Search of the CCD Comparing Physico-chemical Properties between Two Important Environmental Contaminant Lists Hazard Data: Genotoxicity Hazard Resource Concluding Remarks", " 7.3 CompTox Dashboard Data through APIs This training module was developed by Paul Kruse and Caroline Ring, with contributions from Julia E. Rager. All input files (script, data, and figures) can be downloaded from the UNC-SRP TAME2 GitHub website. Disclaimer: The views expressed in this document are those of the authors and do not necessarily reflect the views or policies of the U.S. EPA. Introduction to Training Module Environmental health research related to chemical exposures often requires accessing and wrangling chemical-specific data. The CompTox Chemicals Dashboard (CCD), developed by the United States Environmental Protection Agency, is a publicly-accessible database that integrates chemical data from multiple domains. Chemical data available on the CCD include physicochemical, environmental fate and transport, exposure, toxicokinetics, functional use, in vivo toxicity, in vitro bioassay, and mass spectra data. The CCD was first described in Williams et al. (2017), and has been continuously expanded since. The CCD is heavily used by researchers who do cheminformatics work of various kinds – computational toxicology, computational exposure science, analytical chemistry, chemical safety assessment, etc. The CCD is used by cheminformaticians not only at EPA, but across governmental agencies both within the U.S. and worldwide; in private industry; in non-governmental organizations; in academia; and others. It has become an indispensable tool for many researchers. This training module provides an overview of the physico-chemical, hazard, and bioactivity data available through the CCD; different ways to access these data; and some examples of how these data may be used. We will first introduce the CCD and how to access it. Then we will focus on an automated, programmatic method for retrieving data from the CCD using the ctxR R package. Through some basic data visualization and analysis using the R programming language, we will explore some data retrieved from the CCD, and gain insights both in how to wrangle the data and combine different methods of accessing the data to build automated pipelines for use in more complex settings. Note, as the ctxR package accesses data that is periodically updated, some code chunks will produce numbers that may change slightly with data updates. Keep this in mind when running these code chunks in the future. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following questions: After automatically pulling the fourth Drinking Water Contaminant Candidate List from the CompTox Chemicals Dashboard, list the properties and property types present in the data. What are the mean values for a specific property when grouped by property type and when ungrouped? The physico-chemical property data are reported with both experimental and predicted values present for many chemicals. Are there differences between the mean predicted and experimental results for a variety of physico-chemical properties? After pulling the genotoxicity data for the different environmental contaminant data sets, list the assays associated with the chemicals in each data set. How many unique assays are there in each data set? What are the different assay categories and how many unique assays for each assay category are there? The genotoxicity data contains information on which assays have been conducted for different chemicals and the results of those assays. How many chemicals in each data set have a ‘positive’, ‘negative’, and ‘equivocal’ value for the assay result? Based on the genotoxicity data reported for the chemical with DTXSID identifier DTXSID0020153, how many assays resulted in a positive/equivocal/negative value? Which of the assays were positive and how many of each were there for the most reported assays? After pulling the hazard data for the different data sets, list the different exposure routes for which there is data. What are the unique risk assessment classes for hazard values for the oral route and for the inhalation exposure route? For each such exposure route, which risk assessment class is most represented by the data sets? There are several types of toxicity values for each exposure route. List the unique toxicity values for the oral and inhalation routes. What are the unique types of toxicity values for the oral route and for the inhalation route? How many of these are common to both the oral and inhalation routes for each data set? When examining different toxicity values, the data may be reported in multiple units. To assess the relative hazard from this data, it is important to take into account the different units and adjust accordingly. List the units reported for the cancer slope factor, reference dose, and reference concentration values associated with the oral and inhalation exposure routes for human hazard. Which chemicals in each data set have the highest cancer slope factor, lowest reference dose, and lowest reference concentration values? Script Preparations Cleaning the Global Environment rm(list=ls()) Installing Required R Packages if (!requireNamespace(&#39;ctxR&#39;)) install.packages(&#39;ctxR&#39;) if (!requireNamespace(&#39;ggplot2&#39;)) install.packages(&#39;ggplot2&#39;) Loading R Packages # Used to interface with CompTox Chemicals Dashboard library(ctxR) # Used to visualize data in a variety of plot designs library(ggplot2) Introduction to CompTox Chemicals Dashboard Accessing chemical data and wrangling it is a vital step in many types of workflows related to chemical, biological, and environmental modeling. While there are many resources available from which one can pull data, the CompTox Chemicals Dashboard built and maintained by the United States Environmental Protection Agency is particularly well-designed and suitable for these purposes. Originally introduced in The CompTox Chemistry Dashboard: a community data resource for environmental chemistry, the CCD contains information on over 1.2 million chemicals as of December 2023. The CCD includes chemical information from many different domains, including physicochemical, environmental fate and transport, exposure, usage, in vivo toxicity, and in vitro bioassay data (Williams et al., 2017). The CCD can be searched either one chemical at a time, or using a batch search. Searching One Chemical at a Time (Single-substance Search) In single-substance search, the user types a full or partial chemical identifier (name, CASRN, InChiKey, or DSSTox ID) into a search box on the CCD homepage. Autocomplete provides a list of possible matches; the user selects one by clicking on it, and is then taken to the CCD page for that substance. Here is an example of the CCD page for the chemical Bisphenol A: The different domains of data available for this chemical are shown by the tabs on the left side of the page: for example, “Physchem Prop.” (physico-chemical properties), “Env. Fate/Transport” (environmental fate and transport data), and “Hazard Data” (in vivo hazard and toxicity data), among others. Batch Search In batch search, the user enters a list of search inputs, separated by newlines, into a batch-search box on https://comptox.epa.gov/dashboard/batch-search . The user selects the type(s) of inputs by selecting one or more checkboxes – these may include chemical identifiers, monoisotopic masses, or molecular formulas. Then, the user selects “Display All Chemicals” to display the list of substances matching the batch-search inputs, or “Choose Export Options” to choose options for exporting the batch-search results as a spreadsheet. The exported spreadsheet may include data from most of the domains available on an individual substance’s CCD page. The user can download the selected information in various formats, such as Excel (.xlsx), comma-separated values (.csv), or different types of chemical table files (.e.g, MOL). The web interface for batch search only allows input of 10,000 identifiers at a time. If a user needs to retrieve information for more than 10,000 chemicals, they will need to separate their identifiers into multiple batches and search each one separately. Challenges of Web-based Dashboard Search Practicing researchers typically end up with a Dashboard workflow that looks something like this: Start with a dataset that includes your chemical identifiers of interest. These may include chemical names, Chemical Abstract Service Registry Numbers (CASRNs), Distributed Searchable Structure-Toxicity Database (DSSTox) identifiers, or InChIKeys. Export the chemical identifiers to a spreadsheet. Often, this is done by importing the data into an environment such as R or Python, in order to do some data wrangling (e.g., to select only the unique substance identfiers; to clean up improperly-formatted CASRNs; etc.). Then, the identifiers are saved in a spreadsheet (an Excel, .csv, or .txt file), one chemical identifier per row. Copy and paste the chemical identifiers from the spreadsheet into the CCD Batch Search box. If there are more than 10,000 total chemical identifiers, divide them into batches of 10,000 or less, and search each batch separately. Choose your desired export options on the CCD Batch Search page. Download the exported spreadsheet of CCD data. By default, the downloaded spreadsheet will be given a file name that includes the timestamp of the download. Repeat steps 3-5 for each batch of 10,000 identifiers produced in step 2. Import the downloaded spreadsheet(s) of CCD data into the analysis tool you are using (e.g. R or Python). Merge the table(s) of downloaded CCD data with your original dataset of interest. Proceed with research-related data analysis using the chemical data downloaded from the CCD (e.g., statistical modeling, visualization, etc.) Because each of these workflow steps requires manual interaction with the search and download process, the risk of human error inevitably creeps in. Here are a few real-world possibilities (the authors can neither confirm nor deny that they have personally committed any of these errors): Researchers could copy/paste the wrong identifiers into the CCD batch search, especially if they have more than 10,000 identifiers and have to divide them into batches. Chemical identifiers could be corrupted during the process of exporting to a spreadsheet. For example, if a researcher opens and resaves a CSV file using Microsoft Excel, any information that appears to be in date-like format will be automatically converted to a date (unless the researcher has the most recently-updated version of Excel and has selected the option in Settings that will stop Excel from auto-detecting dates). This behavior has long been identified as a problem in genomics, where gene names can appear date-like to Excel (Abeysooriya et al. 2021). It also affects cheminformatics, where chemical identifiers can appear date-like to Excel. For example, the valid CASRN “1990-07-4” would automatically be converted to “07/04/1990” (if Excel is set to use MM/DD/YYYY date formats). CCD batch search cannot recognize “07/04/1990” as a valid chemical identifier and will be unable to return any chemical data. Researchers could accidentally rename a downloaded CCD data file to overwrite a previous download (for example, when searching multiple batches of identifiers). Researchers could mistakenly import the wrong CCD download file back into their analysis environment (for example, when searching multiple batches of identifiers). Moreover, the manual stages of this kind of workflow are also non-transparent and not easily reproducible. CCTE’s CTX Application Programming Interfaces (APIs) for Automated Batch Search of the CCD Recently, the Center for Computational Toxicology and Exposure (CCTE) developed a set of Application Programming Interfaces (APIs) that allows programmatic access to the CCD, bypassing the manual steps of the web-based batch search workflow. The Computational Toxicology and Exposure (CTX) APIs effectively automate the process of accessing and downloading data from the web pages that make up the CCD. The CTX APIs are publicly available at no cost to the user. However, in order to use the CTX APIs, you must have an API key. The API key uniquely identifies you to the CTX servers and verifies that you have permission to access the database. Getting an API key is free, but requires contacting the API support team at ccte_api@epa.gov. For more information on the data accessible through the CTX APIs and related tools, please visit the US EPA page on Computational Toxicology and Exposure Online Resources. The CTX APIs are one of many resources developed within this research realm and make available many of the data resources beyond the CCD. The APIs are organized into four sets of “endpoints” (chemical data domains): Chemical, Hazard, Bioactivity, and Exposure. Pictured below is what the Chemical section looks like and can be found at CTX API Chemical Endpoints. The APIs can be explored through the pictured web interface at https://api-ccte.epa.gov/docs/chemical.html . CTX API Authentication Authentication is the first tab on the left. Authentication is required to use the APIs. To authenticate yourself in the API web interface, input your unique API key. CTX API Endpoints On the left of the API web interface, there are several different tabs, one for each endpoint in the Chemical domain. The endpoints are organized by the type of information provided. For instance, the Chemical Details Resource endpoint provides basic chemical information; the Chemical Property Resource endpoint provides more comprehensive physico-chemical property information; the Chemical Fate Resource endpoint provides chemical fate and transport information; and so on. Constructing CTX API Requests As mentioned above, APIs effectively automate the process of accessing and downloading data from the web pages that make up the CCD. APIs do this by automatically constructing requests using the Hypertext Transfer Protocol (HTTP) that enables communication between clients (e.g. your computer) and servers (e.g. the CCD). In the CTX API web interface, the colored boxes next to each endpoint indicate the type of the associated HTTP method: either a GET request (“GET”, blue) or a a POST request (“POS”, green). GET is used to request data from a specific web resource (e.g. a specific URL); POST is used to send data to a server to create or update a web resource. For the CTX APIs, POST requests are used to perform multiple (batch) searches in a single API call; GET requests are used for non-batch searches. You do not need to understand the details of POST and GET requests in order to use the API. Click on the second item under Chemical Details Resource, the tab labeled Get data by dtxsid. The following page will appear. This page has two subheadings: “Path Parameters” and “Query-String Parameters”. “Path Parameters” contains user-specified parameters that are required in order to tell the API what URL (web address) to access. In this case, the required parameter is a string for the DTXSID identifying the chemical to be searched. “Query-String Parameters” contain user-specific parameters (usually optional) that tell the API what specific type(s) of information to download from the specified URL. In this case, the optional parameter is a projection parameter, a string that can take one of five values (chemicaldetailall, chemicaldetailstandard, chemicalidentifier, chemicalstructure, ntatoolkit). Depending on the value of this string, the API can return different sets of information about the chemical. If the projection parameter is left blank, then a default set of chemical information is returned. The default return format is displayed below and includes a variety of fields with data types represented. We show what returned data from searching Bisphenol A looks like using this endpoint with the chemicaldetailstandard value for projection selected. Formatting an http request is not necessarily intuitive nor worth the time for someone not already familiar with the process, so these endpoints may provide a resource that for many would require a significant investment in time and energy to learn how to use. However, there is a solution to this in the form of the R package ctxR. ctxR was developed to streamline the process of accessing the information available through the CTX APIs without requiring prior knowledge of how to use APIs. The ctxR package is available in stable form on CRAN and a development version may be found at the USEPA ctxR GitHub repository. As an example, we demonstrate the ease with which one may retrieve the information given by this endpoint for Bisphenol A using the ctxR approach and contrast it with the approach using the CCD website or CTX Chemical API Endpoint website. Setting, using, and storing the API key We store the API key required to access the APIs. To do this for the current session, run the first command. If you want to store your key across multiple sessions, run the second command. # This stores the key in the current session register_ctx_api_key(key = &#39;&lt;YOUR API KEY&gt;&#39;) # This stores the key across multiple sessions and only needs to be run once. # If the key changes, rerun this with the new key. register_ctx_api_key(key = &#39;&lt;YOUR API KEY&gt;&#39;, write = TRUE) To check that your key has successfully been stored for the session, run the following command. ctx_key() Retrieving chemical details Now, we demonstrate how to retrieve the information for BPA given by the Chemical Detail Resource endpoint under the chemicaldetailstandard value for projection. Note, this projection value is the default value for the function get_chemical_details(). BPA_chemical_detail &lt;- get_chemical_details(DTXSID = &#39;DTXSID7020182&#39;) dim(BPA_chemical_detail) #&gt; [1] 1 37 class(BPA_chemical_detail) #&gt; [1] &quot;data.table&quot; &quot;data.frame&quot; names(BPA_chemical_detail) #&gt; [1] &quot;id&quot; &quot;cpdataCount&quot; &quot;inchikey&quot; #&gt; [4] &quot;wikipediaArticle&quot; &quot;monoisotopicMass&quot; &quot;percentAssays&quot; #&gt; [7] &quot;pubchemCount&quot; &quot;pubmedCount&quot; &quot;sourcesCount&quot; #&gt; [10] &quot;qcLevel&quot; &quot;qcLevelDesc&quot; &quot;isotope&quot; #&gt; [13] &quot;multicomponent&quot; &quot;totalAssays&quot; &quot;pubchemCid&quot; #&gt; [16] &quot;relatedSubstanceCount&quot; &quot;relatedStructureCount&quot; &quot;casrn&quot; #&gt; [19] &quot;compoundId&quot; &quot;genericSubstanceId&quot; &quot;preferredName&quot; #&gt; [22] &quot;activeAssays&quot; &quot;molFormula&quot; &quot;hasStructureImage&quot; #&gt; [25] &quot;iupacName&quot; &quot;smiles&quot; &quot;inchiString&quot; #&gt; [28] &quot;qcNotes&quot; &quot;qsarReadySmiles&quot; &quot;msReadySmiles&quot; #&gt; [31] &quot;irisLink&quot; &quot;pprtvLink&quot; &quot;descriptorStringTsv&quot; #&gt; [34] &quot;isMarkush&quot; &quot;dtxsid&quot; &quot;dtxcid&quot; #&gt; [37] &quot;toxcastSelect&quot; Comparing Physico-chemical Properties between Two Important Environmental Contaminant Lists We study two different data sets contained in the CCD and observe how they relate and how they differ. The two data sets that we will explore are a water contaminant priority list and an air toxics list. The fourth Drinking Water Contaminant Candidate List (CCL4) is a set of chemicals that “…are not subject to any proposed or promulgated national primary drinking water regulations, but are known or anticipated to occur in public water systems….” Moreover, this list “…was announced on November 17, 2016. The CCL 4 includes 97 chemicals or chemical groups and 12 microbial contaminants….” The National-Scale Air Toxics Assessments (NATA) is “… EPA’s ongoing comprehensive evaluation of air toxics in the United States… a state-of-the-science screening tool for State/Local/Tribal agencies to prioritize pollutants, emission sources and locations of interest for further study in order to gain a better understanding of risks… use general information about sources to develop estimates of risks which are more likely to overestimate impacts than underestimate them….” These lists can be found in the CCD at CCL4 with additional information at CCL4 information and NATADB with additional information at NATA information. The quotes from the previous paragraph were excerpted from list detail descriptions found using the CCD links. We explore details about these two lists of chemicals before diving into analyzing the data contained in each list. options(width = 100) ccl4_information &lt;- get_public_chemical_list_by_name(&#39;CCL4&#39;) print(ccl4_information, trunc.cols = TRUE) #&gt; id type label visibility #&gt; 1 443 federal WATER|EPA: Chemical Contaminants - CCL 4 PUBLIC #&gt; longDescription #&gt; 1 The Contaminant Candidate List (CCL) is a list of contaminants that, at the time of publication, are not subject to any proposed or promulgated national primary drinking water regulations, but are known or anticipated to occur in public water systems. Contaminants listed on the CCL may require future regulation under the Safe Drinking Water Act (SDWA). EPA announced the &lt;a href=&#39;https://www.epa.gov/ccl/contaminant-candidate-list-4-ccl-4-0&#39; target=&#39;_blank&#39;&gt;fourth Drinking Water Contaminant Candidate List (CCL 4)&lt;/a&gt; on November 17, 2016. The CCL 4 includes 97 chemicals or chemical groups and 12 microbial contaminants. The group of cyanotoxins on CCL 4 includes, but is not limited to: anatoxin-a, cylindrospermopsin, microcystins, and saxitoxin. The CCL Chemical Candidate Lists are versioned iteratively and this description navigates between the various versions of the lists. The list of substances displayed below represents only the chemical CCL 4 contaminants. For the versioned lists, please use the hyperlinked lists below.&lt;br/&gt;&lt;br/&gt; \\r\\n\\r\\n&lt;a href=&#39;https://comptox.epa.gov/dashboard/chemical_lists/CCL5&#39; target=&#39;_blank&#39;&gt;CCL5 - November 2022&lt;/a&gt; &lt;br/&gt;&lt;br/&gt;\\r\\n&lt;a href=&#39;https://comptox.epa.gov/dashboard/chemical_lists/CCL4&#39; target=&#39;_blank&#39;&gt;CCL4 - November 2016&lt;/a&gt; \\r\\n This list&lt;br/&gt;&lt;br/&gt;\\r\\n&lt;a href=&#39;https://comptox.epa.gov/dashboard/chemical_lists/CCL3&#39; target=&#39;_blank&#39;&gt;CCL3 - October 2009&lt;/a&gt; &lt;br/&gt;&lt;br/&gt;\\r\\n&lt;a href=&#39;https://comptox.epa.gov/dashboard/chemical_lists/CCL2&#39; target=&#39;_blank&#39;&gt;CCL2 - February 2005&lt;/a&gt;&lt;br/&gt;&lt;br/&gt;\\r\\n&lt;a href=&#39;https://comptox.epa.gov/dashboard/chemical_lists/CCL1&#39; target=&#39;_blank&#39;&gt;CCL1 - March 1998&lt;/a&gt;&lt;br/&gt;&lt;br/&gt; #&gt; chemicalCount createdAt updatedAt listName #&gt; 1 100 2017-12-28T17:58:36Z 2022-10-26T21:14:27Z CCL4 #&gt; shortDescription #&gt; 1 The Contaminant Candidate List (CCL) is a list of contaminants that are known or anticipated to occur in public water systems. Version 4 is known as CCL 4. natadb_information &lt;- get_public_chemical_list_by_name(&#39;NATADB&#39;) print(natadb_information, trunc.cols = TRUE) #&gt; id type label visibility #&gt; 1 454 federal EPA: National-Scale Air Toxics Assessment (NATA) PUBLIC #&gt; longDescription #&gt; 1 The National-Scale Air Toxics Assessment (NATA) is EPA&#39;s ongoing comprehensive evaluation of air toxics in the United States. EPA developed the NATA as a state-of-the-science screening tool for State/Local/Tribal Agencies to prioritize pollutants, emission sources and locations of interest for further study in order to gain a better understanding of risks. NATA assessments do not incorporate refined information about emission sources but, rather, use general information about sources to develop estimates of risks which are more likely to overestimate impacts than underestimate them.\\r\\n\\r\\nNATA provides estimates of the risk of cancer and other serious health effects from breathing (inhaling) air toxics in order to inform both national and more localized efforts to identify and prioritize air toxics, emission source types and locations which are of greatest potential concern in terms of contributing to population risk. This in turn helps air pollution experts focus limited analytical resources on areas and or populations where the potential for health risks are highest. Assessments include estimates of cancer and non-cancer health effects based on chronic exposure from outdoor sources, including assessments of non-cancer health effects for Diesel Particulate Matter (PM). Assessments provide a snapshot of the outdoor air quality and the risks to human health that would result if air toxic emissions levels remained unchanged. #&gt; listName chemicalCount createdAt updatedAt #&gt; 1 NATADB 163 2018-02-21T12:04:16Z 2018-11-16T21:42:01Z #&gt; shortDescription #&gt; 1 The National-Scale Air Toxics Assessment (NATA) is EPA&#39;s ongoing comprehensive evaluation of air toxics in the United States. Now we pull the actual chemicals contained in the lists using the APIs. ccl4 &lt;- get_chemicals_in_list(&#39;ccl4&#39;) ccl4 &lt;- data.table::as.data.table(ccl4) natadb &lt;- get_chemicals_in_list(&#39;NATADB&#39;) natadb &lt;- data.table::as.data.table(natadb) We examine the dimensions of the data, the column names, and display a single row for illustrative purposes. dim(ccl4) #&gt; [1] 100 37 dim(natadb) #&gt; [1] 163 37 colnames(ccl4) #&gt; [1] &quot;id&quot; &quot;cpdataCount&quot; &quot;inchikey&quot; &quot;wikipediaArticle&quot; #&gt; [5] &quot;monoisotopicMass&quot; &quot;percentAssays&quot; &quot;pubchemCount&quot; &quot;pubmedCount&quot; #&gt; [9] &quot;sourcesCount&quot; &quot;qcLevel&quot; &quot;qcLevelDesc&quot; &quot;isotope&quot; #&gt; [13] &quot;multicomponent&quot; &quot;totalAssays&quot; &quot;pubchemCid&quot; &quot;relatedSubstanceCount&quot; #&gt; [17] &quot;relatedStructureCount&quot; &quot;casrn&quot; &quot;compoundId&quot; &quot;genericSubstanceId&quot; #&gt; [21] &quot;preferredName&quot; &quot;activeAssays&quot; &quot;molFormula&quot; &quot;hasStructureImage&quot; #&gt; [25] &quot;iupacName&quot; &quot;smiles&quot; &quot;inchiString&quot; &quot;qcNotes&quot; #&gt; [29] &quot;qsarReadySmiles&quot; &quot;msReadySmiles&quot; &quot;irisLink&quot; &quot;pprtvLink&quot; #&gt; [33] &quot;descriptorStringTsv&quot; &quot;isMarkush&quot; &quot;dtxsid&quot; &quot;dtxcid&quot; #&gt; [37] &quot;toxcastSelect&quot; head(ccl4, 1) #&gt; id cpdataCount inchikey wikipediaArticle monoisotopicMass percentAssays pubchemCount #&gt; &lt;char&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;int&gt; #&gt; 1: 627129 NA &lt;NA&gt; &lt;NA&gt; NA NA NA #&gt; 30 variable(s) not shown: [pubmedCount &lt;num&gt;, sourcesCount &lt;int&gt;, qcLevel &lt;int&gt;, qcLevelDesc &lt;char&gt;, isotope &lt;int&gt;, multicomponent &lt;int&gt;, totalAssays &lt;int&gt;, pubchemCid &lt;int&gt;, relatedSubstanceCount &lt;int&gt;, relatedStructureCount &lt;int&gt;, ...] Accessing the Physico-chemical Property Data Once we have the chemicals in each list, we access their physico-chemical properties. We will use the batch search forms of the function get_chem_info(), to which we supply a list of DTXSIDs. ccl4$dtxsid #&gt; [1] &quot;DTXSID001024118&quot; &quot;DTXSID0020153&quot; &quot;DTXSID0020446&quot; &quot;DTXSID0020573&quot; &quot;DTXSID0020600&quot; #&gt; [6] &quot;DTXSID0020814&quot; &quot;DTXSID0021464&quot; &quot;DTXSID0021541&quot; &quot;DTXSID0021917&quot; &quot;DTXSID0024052&quot; #&gt; [11] &quot;DTXSID0024341&quot; &quot;DTXSID0032578&quot; &quot;DTXSID1020437&quot; &quot;DTXSID1021407&quot; &quot;DTXSID1021409&quot; #&gt; [16] &quot;DTXSID1021740&quot; &quot;DTXSID1021798&quot; &quot;DTXSID1024174&quot; &quot;DTXSID1024207&quot; &quot;DTXSID1024338&quot; #&gt; [21] &quot;DTXSID1026164&quot; &quot;DTXSID1031040&quot; &quot;DTXSID1037484&quot; &quot;DTXSID1037486&quot; &quot;DTXSID1037567&quot; #&gt; [26] &quot;DTXSID2020684&quot; &quot;DTXSID2021028&quot; &quot;DTXSID2021317&quot; &quot;DTXSID2021731&quot; &quot;DTXSID2022333&quot; #&gt; [31] &quot;DTXSID2024169&quot; &quot;DTXSID2031083&quot; &quot;DTXSID2037506&quot; &quot;DTXSID2040282&quot; &quot;DTXSID2052156&quot; #&gt; [36] &quot;DTXSID3020203&quot; &quot;DTXSID3020702&quot; &quot;DTXSID3020833&quot; &quot;DTXSID3020964&quot; &quot;DTXSID3021857&quot; #&gt; [41] &quot;DTXSID3024366&quot; &quot;DTXSID3024869&quot; &quot;DTXSID3031864&quot; &quot;DTXSID3032464&quot; &quot;DTXSID3034458&quot; #&gt; [46] &quot;DTXSID3042219&quot; &quot;DTXSID3073137&quot; &quot;DTXSID3074313&quot; &quot;DTXSID4020533&quot; &quot;DTXSID4021503&quot; #&gt; [51] &quot;DTXSID4022361&quot; &quot;DTXSID4022367&quot; &quot;DTXSID4022448&quot; &quot;DTXSID4022991&quot; &quot;DTXSID4032611&quot; #&gt; [56] &quot;DTXSID4034948&quot; &quot;DTXSID5020023&quot; &quot;DTXSID5020576&quot; &quot;DTXSID5020601&quot; &quot;DTXSID5021207&quot; #&gt; [61] &quot;DTXSID5024182&quot; &quot;DTXSID5039224&quot; &quot;DTXSID50867064&quot; &quot;DTXSID6020301&quot; &quot;DTXSID6020856&quot; #&gt; [66] &quot;DTXSID6021030&quot; &quot;DTXSID6021032&quot; &quot;DTXSID6022422&quot; &quot;DTXSID6024177&quot; &quot;DTXSID6037483&quot; #&gt; [71] &quot;DTXSID6037485&quot; &quot;DTXSID6037568&quot; &quot;DTXSID7020005&quot; &quot;DTXSID7020215&quot; &quot;DTXSID7020637&quot; #&gt; [76] &quot;DTXSID7021029&quot; &quot;DTXSID7024241&quot; &quot;DTXSID7047433&quot; &quot;DTXSID8020044&quot; &quot;DTXSID8020090&quot; #&gt; [81] &quot;DTXSID8020597&quot; &quot;DTXSID8020832&quot; &quot;DTXSID8021062&quot; &quot;DTXSID8022292&quot; &quot;DTXSID8022377&quot; #&gt; [86] &quot;DTXSID8023846&quot; &quot;DTXSID8023848&quot; &quot;DTXSID8025541&quot; &quot;DTXSID8031865&quot; &quot;DTXSID8052483&quot; #&gt; [91] &quot;DTXSID9020243&quot; &quot;DTXSID9021390&quot; &quot;DTXSID9021427&quot; &quot;DTXSID9022366&quot; &quot;DTXSID9023380&quot; #&gt; [96] &quot;DTXSID9023914&quot; &quot;DTXSID9024142&quot; &quot;DTXSID9032113&quot; &quot;DTXSID9032119&quot; &quot;DTXSID9032329&quot; natadb$dtxsid #&gt; [1] &quot;DTXSID0020153&quot; &quot;DTXSID0020448&quot; &quot;DTXSID0020523&quot; &quot;DTXSID0020529&quot; &quot;DTXSID0020600&quot; #&gt; [6] &quot;DTXSID0020868&quot; &quot;DTXSID0021381&quot; &quot;DTXSID0021383&quot; &quot;DTXSID0021541&quot; &quot;DTXSID0021834&quot; #&gt; [11] &quot;DTXSID0021917&quot; &quot;DTXSID0021965&quot; &quot;DTXSID0024187&quot; &quot;DTXSID0024260&quot; &quot;DTXSID0039227&quot; #&gt; [16] &quot;DTXSID0039229&quot; &quot;DTXSID00872421&quot; &quot;DTXSID1020148&quot; &quot;DTXSID1020273&quot; &quot;DTXSID1020302&quot; #&gt; [21] &quot;DTXSID1020306&quot; &quot;DTXSID1020431&quot; &quot;DTXSID1020437&quot; &quot;DTXSID1020512&quot; &quot;DTXSID1020516&quot; #&gt; [26] &quot;DTXSID1020566&quot; &quot;DTXSID1021374&quot; &quot;DTXSID1021798&quot; &quot;DTXSID1021827&quot; &quot;DTXSID1022057&quot; #&gt; [31] &quot;DTXSID1023786&quot; &quot;DTXSID1024045&quot; &quot;DTXSID1024382&quot; &quot;DTXSID1026164&quot; &quot;DTXSID1049641&quot; #&gt; [36] &quot;DTXSID10872417&quot; &quot;DTXSID2020137&quot; &quot;DTXSID2020262&quot; &quot;DTXSID2020507&quot; &quot;DTXSID2020682&quot; #&gt; [41] &quot;DTXSID2020688&quot; &quot;DTXSID2020711&quot; &quot;DTXSID2020844&quot; &quot;DTXSID2021105&quot; &quot;DTXSID2021157&quot; #&gt; [46] &quot;DTXSID2021159&quot; &quot;DTXSID2021284&quot; &quot;DTXSID2021286&quot; &quot;DTXSID2021319&quot; &quot;DTXSID2021446&quot; #&gt; [51] &quot;DTXSID2021658&quot; &quot;DTXSID2021731&quot; &quot;DTXSID2021781&quot; &quot;DTXSID2021993&quot; &quot;DTXSID3020203&quot; #&gt; [56] &quot;DTXSID3020257&quot; &quot;DTXSID3020413&quot; &quot;DTXSID3020415&quot; &quot;DTXSID3020596&quot; &quot;DTXSID3020679&quot; #&gt; [61] &quot;DTXSID3020702&quot; &quot;DTXSID3020833&quot; &quot;DTXSID3020964&quot; &quot;DTXSID3021431&quot; &quot;DTXSID3021932&quot; #&gt; [66] &quot;DTXSID3022455&quot; &quot;DTXSID3024366&quot; &quot;DTXSID3025091&quot; &quot;DTXSID3039242&quot; &quot;DTXSID30872414&quot; #&gt; [71] &quot;DTXSID30872419&quot; &quot;DTXSID4020161&quot; &quot;DTXSID4020298&quot; &quot;DTXSID4020402&quot; &quot;DTXSID4020533&quot; #&gt; [76] &quot;DTXSID4020583&quot; &quot;DTXSID4020874&quot; &quot;DTXSID4020901&quot; &quot;DTXSID4021006&quot; &quot;DTXSID4021056&quot; #&gt; [81] &quot;DTXSID4021395&quot; &quot;DTXSID4024143&quot; &quot;DTXSID4024359&quot; &quot;DTXSID4039231&quot; &quot;DTXSID40872425&quot; #&gt; [86] &quot;DTXSID5020023&quot; &quot;DTXSID5020027&quot; &quot;DTXSID5020029&quot; &quot;DTXSID5020071&quot; &quot;DTXSID5020316&quot; #&gt; [91] &quot;DTXSID5020449&quot; &quot;DTXSID5020491&quot; &quot;DTXSID5020601&quot; &quot;DTXSID5020607&quot; &quot;DTXSID5020865&quot; #&gt; [96] &quot;DTXSID5021124&quot; &quot;DTXSID5021207&quot; &quot;DTXSID5021380&quot; &quot;DTXSID5021386&quot; &quot;DTXSID5021889&quot; #&gt; [101] &quot;DTXSID5024055&quot; &quot;DTXSID5024059&quot; &quot;DTXSID5024267&quot; &quot;DTXSID5039224&quot; &quot;DTXSID6020145&quot; #&gt; [106] &quot;DTXSID6020307&quot; &quot;DTXSID6020353&quot; &quot;DTXSID6020432&quot; &quot;DTXSID6020438&quot; &quot;DTXSID6020515&quot; #&gt; [111] &quot;DTXSID6020569&quot; &quot;DTXSID6020981&quot; &quot;DTXSID6021828&quot; &quot;DTXSID6022422&quot; &quot;DTXSID6023947&quot; #&gt; [116] &quot;DTXSID6023949&quot; &quot;DTXSID7020005&quot; &quot;DTXSID7020009&quot; &quot;DTXSID7020267&quot; &quot;DTXSID7020637&quot; #&gt; [121] &quot;DTXSID7020683&quot; &quot;DTXSID7020687&quot; &quot;DTXSID7020689&quot; &quot;DTXSID7020710&quot; &quot;DTXSID7020716&quot; #&gt; [126] &quot;DTXSID7021029&quot; &quot;DTXSID7021100&quot; &quot;DTXSID7021106&quot; &quot;DTXSID7021318&quot; &quot;DTXSID7021360&quot; #&gt; [131] &quot;DTXSID7021368&quot; &quot;DTXSID7021948&quot; &quot;DTXSID7023984&quot; &quot;DTXSID7024166&quot; &quot;DTXSID7024370&quot; #&gt; [136] &quot;DTXSID7024532&quot; &quot;DTXSID7025180&quot; &quot;DTXSID7026156&quot; &quot;DTXSID8020090&quot; &quot;DTXSID8020173&quot; #&gt; [141] &quot;DTXSID8020250&quot; &quot;DTXSID8020597&quot; &quot;DTXSID8020599&quot; &quot;DTXSID8020759&quot; &quot;DTXSID8020832&quot; #&gt; [146] &quot;DTXSID8020913&quot; &quot;DTXSID8021195&quot; &quot;DTXSID8021197&quot; &quot;DTXSID8021432&quot; &quot;DTXSID8021434&quot; #&gt; [151] &quot;DTXSID8021438&quot; &quot;DTXSID8024286&quot; &quot;DTXSID8042476&quot; &quot;DTXSID9020168&quot; &quot;DTXSID9020243&quot; #&gt; [156] &quot;DTXSID9020247&quot; &quot;DTXSID9020293&quot; &quot;DTXSID9020299&quot; &quot;DTXSID9020827&quot; &quot;DTXSID9021138&quot; #&gt; [161] &quot;DTXSID9021261&quot; &quot;DTXSID9041522&quot; &quot;DTXSID90872415&quot; ccl4_phys_chem &lt;- get_chem_info_batch(ccl4$dtxsid) #&gt; Warning in get_chem_info_batch(ccl4$dtxsid): Setting type to &#39;&#39;! natadb_phys_chem &lt;- get_chem_info_batch(natadb$dtxsid) #&gt; Warning in get_chem_info_batch(natadb$dtxsid): Setting type to &#39;&#39;! Observe that this returns a single data.table for each query, and the data.table contains the physico-chemical properties available from the CompTox Chemicals Dashboard for each chemical in the query. Note, a warning message was triggered, Warning: Setting type to ''!, which indicates the the parameter type was not given a value. A default value is set within the function and more information can be found in the associated documentation. We examine the set of physico-chemical properties for the first chemical in CCL4. Before any deeper analysis, let’s take a look at the dimensions of the data and the column names. dim(ccl4_phys_chem) #&gt; [1] 3641 10 colnames(ccl4_phys_chem) #&gt; [1] &quot;name&quot; &quot;value&quot; &quot;id&quot; &quot;source&quot; &quot;description&quot; &quot;propType&quot; #&gt; [7] &quot;unit&quot; &quot;propertyId&quot; &quot;dtxsid&quot; &quot;dtxcid&quot; Next, we display the unique values for the columns propertyID and propType. ccl4_phys_chem[, unique(propertyId)] #&gt; [1] &quot;boiling-point&quot; &quot;logkow-octanol-water&quot; &quot;melting-point&quot; &quot;vapor-pressure&quot; #&gt; [5] &quot;water-solubility&quot; &quot;density&quot; &quot;flash-point&quot; &quot;henrys-law&quot; #&gt; [9] &quot;index-of-refraction&quot; &quot;logkoa-octanol-air&quot; &quot;molar-refractivity&quot; &quot;molar-volume&quot; #&gt; [13] &quot;polarizability&quot; &quot;surface-tension&quot; &quot;thermal-conductivity&quot; &quot;viscosity&quot; #&gt; [17] &quot;pka-acidic-apparent&quot; &quot;pka-basic-apparent&quot; ccl4_phys_chem[, unique(propType)] #&gt; [1] &quot;experimental&quot; &quot;predicted&quot; Let’s explore this further by examining the mean of the “boiling-point” and “melting-point” data. ccl4_phys_chem[propertyId == &#39;boiling-point&#39;, .(Mean = mean(value))] #&gt; Mean #&gt; &lt;num&gt; #&gt; 1: 253.7974 ccl4_phys_chem[propertyId == &#39;boiling-point&#39;, .(Mean = mean(value)), by = .(propType)] #&gt; propType Mean #&gt; &lt;char&gt; &lt;num&gt; #&gt; 1: experimental 250.5943 #&gt; 2: predicted 255.5472 ccl4_phys_chem[propertyId == &#39;melting-point&#39;, .(Mean = mean(value))] #&gt; Mean #&gt; &lt;num&gt; #&gt; 1: 36.14033 ccl4_phys_chem[propertyId == &#39;melting-point&#39;, .(Mean = mean(value)), by = .(propType)] #&gt; propType Mean #&gt; &lt;char&gt; &lt;num&gt; #&gt; 1: experimental 23.64972 #&gt; 2: predicted 51.93584 These results tell us about some of the reported physico-chemical properties of the data sets. Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question 1: After automatically pulling the fourth Drinking Water Contaminant Candidate List from the CompTox Chemicals Dashboard, list the properties and property types present in the data. What are the mean values for a specific property when grouped by property type and when ungrouped? Answer: The mean “boiling-point” is 253.7974 degrees Celsius for CCL4, with mean values of 250.5943 and 255.5472 for experimental and predicted, respectively. The mean “melting-point” is 36.14033 degrees Celsius for CCL4, with mean values of 23.64972 and 51.93584 for experimental and predicted, respectively. To explore all the values of the physico-chemical properties and calculate their means, we can do the following procedure. First we look at all the physico-chemical properties individually, then group them by each property (“boiling-point”, “melting-point”, etc…), and then additionally group those by property type (“experimental” vs “predicted”). In the grouping, we look at the columns value, unit, propertyID and propType. We also demonstrate how take the mean of the values for each grouping. We examine the chemical with DTXSID “DTXSID0020153” from CCL4. head(ccl4_phys_chem[dtxsid == &#39;DTXSID0020153&#39;, ]) #&gt; name value id source description #&gt; &lt;char&gt; &lt;num&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; #&gt; 1: Boiling Point 179.000 21943219 SynQuest Labs (Chemical c SynQuest has a focus on f #&gt; 2: Boiling Point 178.889 23836860 NIOSH The NIOSH Pocket Guide to #&gt; 3: Boiling Point 179.000 11959465 PhysPropNCCT The PHYSPROP data sets ar #&gt; 4: Boiling Point 179.000 15585758 Biosynth (Chemical compan Biosynth produces reagen #&gt; 5: Boiling Point 179.000 8401215 Alfa Aesar (Chemical comp Alfa Aesar is a leading i #&gt; 6: LogKow: Octanol-Water 2.300 17007695 PhysPropNCCT The PHYSPROP data sets ar #&gt; 5 variable(s) not shown: [propType &lt;char&gt;, unit &lt;char&gt;, propertyId &lt;char&gt;, dtxsid &lt;char&gt;, dtxcid &lt;char&gt;] ccl4_phys_chem[dtxsid == &#39;DTXSID0020153&#39;, .(propType, value, unit), by = .(propertyId)] #&gt; propertyId propType value unit #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: boiling-point experimental 179.00000000 °C #&gt; 2: boiling-point experimental 178.88900000 °C #&gt; 3: boiling-point experimental 179.00000000 °C #&gt; 4: boiling-point experimental 179.00000000 °C #&gt; 5: boiling-point experimental 179.00000000 °C #&gt; 6: boiling-point predicted 178.47300000 °C #&gt; 7: boiling-point predicted 179.39900000 °C #&gt; 8: boiling-point predicted 178.70000000 °C #&gt; 9: boiling-point predicted 184.18000000 °C #&gt; 10: logkow-octanol-water experimental 2.30000000 &lt;NA&gt; #&gt; 11: logkow-octanol-water predicted 2.63800000 &lt;NA&gt; #&gt; 12: logkow-octanol-water predicted 2.30213000 &lt;NA&gt; #&gt; 13: logkow-octanol-water predicted 2.48700000 &lt;NA&gt; #&gt; 14: logkow-octanol-water predicted 2.79000000 &lt;NA&gt; #&gt; 15: melting-point experimental -39.20000000 °C #&gt; 16: melting-point experimental -43.00000000 °C #&gt; 17: melting-point experimental -45.00000000 °C #&gt; 18: melting-point experimental -38.88890000 °C #&gt; 19: melting-point experimental -45.00000000 °C #&gt; 20: melting-point experimental -43.00000000 °C #&gt; 21: melting-point experimental -43.00000000 °C #&gt; 22: melting-point predicted -26.57100000 °C #&gt; 23: melting-point predicted -44.01140000 °C #&gt; 24: melting-point predicted -27.15000000 °C #&gt; 25: vapor-pressure experimental 1.22999000 mmHg #&gt; 26: vapor-pressure predicted 1.23752000 mmHg #&gt; 27: vapor-pressure predicted 1.99986000 mmHg #&gt; 28: vapor-pressure predicted 1.27700000 mmHg #&gt; 29: water-solubility experimental 0.00407380 mol/L #&gt; 30: water-solubility experimental 0.00955000 mol/L #&gt; 31: water-solubility experimental 0.00417000 mol/L #&gt; 32: water-solubility experimental 0.00407380 mol/L #&gt; 33: water-solubility experimental 0.00414763 mol/L #&gt; 34: water-solubility predicted 0.00300608 mol/L #&gt; 35: water-solubility predicted 0.00813715 mol/L #&gt; 36: water-solubility predicted 0.00484108 mol/L #&gt; 37: water-solubility predicted 0.00100000 mol/L #&gt; 38: density predicted 1.08100000 g/cm^3 #&gt; 39: density predicted 1.10000000 g/cm^3 #&gt; 40: flash-point predicted 73.88900000 °C #&gt; 41: flash-point predicted 73.39500000 °C #&gt; 42: henrys-law predicted 0.00291872 atm-m3/mole #&gt; 43: index-of-refraction predicted 1.52700000 &lt;NA&gt; #&gt; 44: logkoa-octanol-air predicted 4.15646000 &lt;NA&gt; #&gt; 45: molar-refractivity predicted 36.01800000 cm^3 #&gt; 46: molar-volume predicted 117.12900000 cm^3 #&gt; 47: polarizability predicted 14.27900000 Å^3 #&gt; 48: surface-tension predicted 34.66800000 dyn/cm #&gt; 49: surface-tension predicted 33.85300000 dyn/cm #&gt; 50: thermal-conductivity predicted 131.28800000 mW/(m*K) #&gt; 51: viscosity predicted 1.40281000 cP #&gt; propertyId propType value unit ccl4_phys_chem[dtxsid == &#39;DTXSID0020153&#39;, .(value, unit), by = .(propertyId, propType)] #&gt; propertyId propType value unit #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: boiling-point experimental 179.00000000 °C #&gt; 2: boiling-point experimental 178.88900000 °C #&gt; 3: boiling-point experimental 179.00000000 °C #&gt; 4: boiling-point experimental 179.00000000 °C #&gt; 5: boiling-point experimental 179.00000000 °C #&gt; 6: logkow-octanol-water experimental 2.30000000 &lt;NA&gt; #&gt; 7: melting-point experimental -39.20000000 °C #&gt; 8: melting-point experimental -43.00000000 °C #&gt; 9: melting-point experimental -45.00000000 °C #&gt; 10: melting-point experimental -38.88890000 °C #&gt; 11: melting-point experimental -45.00000000 °C #&gt; 12: melting-point experimental -43.00000000 °C #&gt; 13: melting-point experimental -43.00000000 °C #&gt; 14: vapor-pressure experimental 1.22999000 mmHg #&gt; 15: water-solubility experimental 0.00407380 mol/L #&gt; 16: water-solubility experimental 0.00955000 mol/L #&gt; 17: water-solubility experimental 0.00417000 mol/L #&gt; 18: water-solubility experimental 0.00407380 mol/L #&gt; 19: water-solubility experimental 0.00414763 mol/L #&gt; 20: boiling-point predicted 178.47300000 °C #&gt; 21: boiling-point predicted 179.39900000 °C #&gt; 22: boiling-point predicted 178.70000000 °C #&gt; 23: boiling-point predicted 184.18000000 °C #&gt; 24: density predicted 1.08100000 g/cm^3 #&gt; 25: density predicted 1.10000000 g/cm^3 #&gt; 26: flash-point predicted 73.88900000 °C #&gt; 27: flash-point predicted 73.39500000 °C #&gt; 28: henrys-law predicted 0.00291872 atm-m3/mole #&gt; 29: index-of-refraction predicted 1.52700000 &lt;NA&gt; #&gt; 30: logkoa-octanol-air predicted 4.15646000 &lt;NA&gt; #&gt; 31: logkow-octanol-water predicted 2.63800000 &lt;NA&gt; #&gt; 32: logkow-octanol-water predicted 2.30213000 &lt;NA&gt; #&gt; 33: logkow-octanol-water predicted 2.48700000 &lt;NA&gt; #&gt; 34: logkow-octanol-water predicted 2.79000000 &lt;NA&gt; #&gt; 35: melting-point predicted -26.57100000 °C #&gt; 36: melting-point predicted -44.01140000 °C #&gt; 37: melting-point predicted -27.15000000 °C #&gt; 38: molar-refractivity predicted 36.01800000 cm^3 #&gt; 39: molar-volume predicted 117.12900000 cm^3 #&gt; 40: polarizability predicted 14.27900000 Å^3 #&gt; 41: surface-tension predicted 34.66800000 dyn/cm #&gt; 42: surface-tension predicted 33.85300000 dyn/cm #&gt; 43: thermal-conductivity predicted 131.28800000 mW/(m*K) #&gt; 44: vapor-pressure predicted 1.23752000 mmHg #&gt; 45: vapor-pressure predicted 1.99986000 mmHg #&gt; 46: vapor-pressure predicted 1.27700000 mmHg #&gt; 47: viscosity predicted 1.40281000 cP #&gt; 48: water-solubility predicted 0.00300608 mol/L #&gt; 49: water-solubility predicted 0.00813715 mol/L #&gt; 50: water-solubility predicted 0.00484108 mol/L #&gt; 51: water-solubility predicted 0.00100000 mol/L #&gt; propertyId propType value unit ccl4_phys_chem[dtxsid == &#39;DTXSID0020153&#39;, .(Mean_value = sapply(.SD, mean)), by = .(propertyId, unit), .SDcols = c(&quot;value&quot;)] #&gt; propertyId unit Mean_value #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; #&gt; 1: boiling-point °C 179.515666667 #&gt; 2: logkow-octanol-water &lt;NA&gt; 2.503426000 #&gt; 3: melting-point °C -39.482130000 #&gt; 4: vapor-pressure mmHg 1.436092500 #&gt; 5: water-solubility mol/L 0.004777727 #&gt; 6: density g/cm^3 1.090500000 #&gt; 7: flash-point °C 73.642000000 #&gt; 8: henrys-law atm-m3/mole 0.002918720 #&gt; 9: index-of-refraction &lt;NA&gt; 1.527000000 #&gt; 10: logkoa-octanol-air &lt;NA&gt; 4.156460000 #&gt; 11: molar-refractivity cm^3 36.018000000 #&gt; 12: molar-volume cm^3 117.129000000 #&gt; 13: polarizability Å^3 14.279000000 #&gt; 14: surface-tension dyn/cm 34.260500000 #&gt; 15: thermal-conductivity mW/(m*K) 131.288000000 #&gt; 16: viscosity cP 1.402810000 ccl4_phys_chem[dtxsid == &#39;DTXSID0020153&#39;, .(Mean_value = sapply(.SD, mean)), by = .(propertyId, unit, propType), .SDcols = c(&quot;value&quot;)][order(propertyId)] #&gt; propertyId unit propType Mean_value #&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; #&gt; 1: boiling-point °C experimental 178.977800000 #&gt; 2: boiling-point °C predicted 180.188000000 #&gt; 3: density g/cm^3 predicted 1.090500000 #&gt; 4: flash-point °C predicted 73.642000000 #&gt; 5: henrys-law atm-m3/mole predicted 0.002918720 #&gt; 6: index-of-refraction &lt;NA&gt; predicted 1.527000000 #&gt; 7: logkoa-octanol-air &lt;NA&gt; predicted 4.156460000 #&gt; 8: logkow-octanol-water &lt;NA&gt; experimental 2.300000000 #&gt; 9: logkow-octanol-water &lt;NA&gt; predicted 2.554282500 #&gt; 10: melting-point °C experimental -42.441271429 #&gt; 11: melting-point °C predicted -32.577466667 #&gt; 12: molar-refractivity cm^3 predicted 36.018000000 #&gt; 13: molar-volume cm^3 predicted 117.129000000 #&gt; 14: polarizability Å^3 predicted 14.279000000 #&gt; 15: surface-tension dyn/cm predicted 34.260500000 #&gt; 16: thermal-conductivity mW/(m*K) predicted 131.288000000 #&gt; 17: vapor-pressure mmHg experimental 1.229990000 #&gt; 18: vapor-pressure mmHg predicted 1.504793333 #&gt; 19: viscosity cP predicted 1.402810000 #&gt; 20: water-solubility mol/L experimental 0.005203046 #&gt; 21: water-solubility mol/L predicted 0.004246077 #&gt; propertyId unit propType Mean_value Analyzing and Visualizing Physico-chemical Properties from Two Environmental Contaminant Lists We consider exploring the differences in mean predicted and experimental values for a variety of physico-chemical properties in an effort to understand better the CCL4 and NATADB lists. In particular, we examine “vapor-pressure”, “henrys-law”, and “boiling-point” and plot the means by chemical for these using boxplots. We then compare the values by grouping by both data set and propType value. We first examine the vapor pressures for all the chemicals in each list. We then graph these, grouped by propType and pooled together in separate plots. For this we will use boxplots. Group first by DTXSID. ccl4_vapor_all &lt;- ccl4_phys_chem[propertyId %in% &#39;vapor-pressure&#39;, .(mean_vapor_pressure = sapply(.SD, mean)), .SDcols = c(&#39;value&#39;), by = .(dtxsid)] natadb_vapor_all &lt;- natadb_phys_chem[propertyId %in% &#39;vapor-pressure&#39;, .(mean_vapor_pressure = sapply(.SD, mean)), .SDcols = c(&#39;value&#39;), by = .(dtxsid)] Then group by DTXSID and then by property type. ccl4_vapor_grouped &lt;- ccl4_phys_chem[propertyId %in% &#39;vapor-pressure&#39;, .(mean_vapor_pressure = sapply(.SD, mean)), .SDcols = c(&#39;value&#39;), by = .(dtxsid, propType)] natadb_vapor_grouped &lt;- natadb_phys_chem[propertyId %in% &#39;vapor-pressure&#39;, .(mean_vapor_pressure = sapply(.SD, mean)), .SDcols = c(&#39;value&#39;), by = .(dtxsid, propType)] Then examine the summary statistics of the data. summary(ccl4_vapor_all) #&gt; dtxsid mean_vapor_pressure #&gt; Length:88 Min. : 0.000 #&gt; Class :character 1st Qu.: 0.000 #&gt; Mode :character Median : 0.001 #&gt; Mean : 211.735 #&gt; 3rd Qu.: 5.009 #&gt; Max. :6007.244 summary(ccl4_vapor_grouped) #&gt; dtxsid propType mean_vapor_pressure #&gt; Length:147 Length:147 Min. : 0.000 #&gt; Class :character Class :character 1st Qu.: 0.000 #&gt; Mode :character Mode :character Median : 0.060 #&gt; Mean : 252.599 #&gt; 3rd Qu.: 9.851 #&gt; Max. :7252.700 summary(natadb_vapor_all) #&gt; dtxsid mean_vapor_pressure #&gt; Length:151 Min. : 0.00 #&gt; Class :character 1st Qu.: 0.01 #&gt; Mode :character Median : 1.44 #&gt; Mean : 851.70 #&gt; 3rd Qu.: 116.65 #&gt; Max. :33917.88 summary(natadb_vapor_grouped) #&gt; dtxsid propType mean_vapor_pressure #&gt; Length:274 Length:274 Min. : 0.00 #&gt; Class :character Class :character 1st Qu.: 0.01 #&gt; Mode :character Mode :character Median : 1.68 #&gt; Mean : 587.02 #&gt; 3rd Qu.: 112.59 #&gt; Max. :33917.88 With such a large range of values covering several orders of magnitude, we log transform the data. Since these value are positive, we do not have to worry about illegal transformations. ccl4_vapor_all[, log_transform_mean_vapor_pressure := log(mean_vapor_pressure)] #&gt; dtxsid mean_vapor_pressure log_transform_mean_vapor_pressure #&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; #&gt; 1: DTXSID0020153 1.436092e+00 0.3619259 #&gt; 2: DTXSID0020446 1.211530e-06 -13.6236263 #&gt; 3: DTXSID0020573 1.016181e-08 -18.4046290 #&gt; 4: DTXSID0020600 1.060440e+03 6.9664394 #&gt; 5: DTXSID0020814 1.713402e-08 -17.8821999 #&gt; 6: DTXSID0021464 1.020300e+01 2.3226818 #&gt; 7: DTXSID0021541 3.623345e+03 8.1951530 #&gt; 8: DTXSID0021917 1.447043e+02 4.9746920 #&gt; 9: DTXSID0024052 2.579787e-07 -15.1703889 #&gt; 10: DTXSID0032578 3.546976e-05 -10.2468302 #&gt; 11: DTXSID1020437 2.153143e+02 5.3720986 #&gt; 12: DTXSID1021407 9.093477e-04 -7.0027830 #&gt; 13: DTXSID1021409 4.170000e-08 -16.9927647 #&gt; 14: DTXSID1021740 6.808950e+00 1.9182379 #&gt; 15: DTXSID1021798 5.779040e-02 -2.8509326 #&gt; 16: DTXSID1024174 5.616588e-06 -12.0897863 #&gt; 17: DTXSID1024338 7.125250e-08 -16.4570359 #&gt; 18: DTXSID1026164 2.444570e-01 -1.4087159 #&gt; 19: DTXSID1037484 4.124746e-07 -14.7010911 #&gt; 20: DTXSID1037486 4.206599e-07 -14.6814412 #&gt; 21: DTXSID1037567 4.633640e-08 -16.8873380 #&gt; 22: DTXSID2020684 3.733735e-03 -5.5903463 #&gt; 23: DTXSID2021028 1.127656e+00 0.1201408 #&gt; 24: DTXSID2021317 1.563302e+01 2.7493857 #&gt; 25: DTXSID2021731 2.360838e+02 5.4641866 #&gt; 26: DTXSID2022333 1.685188e+00 0.5218768 #&gt; 27: DTXSID2037506 8.393291e-06 -11.6880778 #&gt; 28: DTXSID2052156 3.878108e-09 -19.3679185 #&gt; 29: DTXSID3020203 1.876293e+03 7.5370534 #&gt; 30: DTXSID3020702 2.074800e+01 3.0324499 #&gt; 31: DTXSID3020833 2.420272e+02 5.4890503 #&gt; 32: DTXSID3020964 2.336795e-01 -1.4538048 #&gt; 33: DTXSID3024366 5.501190e+01 4.0075495 #&gt; 34: DTXSID3024869 1.885188e-02 -3.9711425 #&gt; 35: DTXSID3031864 2.479965e-06 -12.9072661 #&gt; 36: DTXSID3032464 1.727766e-06 -13.2686813 #&gt; 37: DTXSID3034458 6.440000e-11 -23.4659075 #&gt; 38: DTXSID3042219 3.106110e+00 1.1333711 #&gt; 39: DTXSID3074313 1.481081e-11 -24.9356641 #&gt; 40: DTXSID4020533 4.036368e+01 3.6979302 #&gt; 41: DTXSID4021503 1.722775e+02 5.1491065 #&gt; 42: DTXSID4022361 3.226160e-06 -12.6442179 #&gt; 43: DTXSID4022367 1.643378e-08 -17.9239267 #&gt; 44: DTXSID4022448 1.773129e-05 -10.9401797 #&gt; 45: DTXSID4022991 1.440972e-10 -22.6605333 #&gt; 46: DTXSID4032611 5.263403e-04 -7.5495627 #&gt; 47: DTXSID4034948 3.744327e-08 -17.1004389 #&gt; 48: DTXSID5020023 2.545125e+02 5.5393500 #&gt; 49: DTXSID5020576 5.737673e-09 -18.9762122 #&gt; 50: DTXSID5020601 1.428200e+00 0.3564147 #&gt; 51: DTXSID5021207 4.463567e+02 6.1011185 #&gt; 52: DTXSID5024182 7.171160e+00 1.9700674 #&gt; 53: DTXSID5039224 7.963770e+02 6.6800727 #&gt; 54: DTXSID50867064 1.522060e-03 -6.4876906 #&gt; 55: DTXSID6020301 6.007244e+03 8.7007213 #&gt; 56: DTXSID6020856 2.909907e-01 -1.2344638 #&gt; 57: DTXSID6021030 3.562765e-05 -10.2423885 #&gt; 58: DTXSID6021032 8.306000e-01 -0.1856069 #&gt; 59: DTXSID6022422 1.921524e-06 -13.1623921 #&gt; 60: DTXSID6024177 7.035691e-02 -2.6541743 #&gt; 61: DTXSID6037483 4.847560e-08 -16.8422053 #&gt; 62: DTXSID6037485 4.976030e-08 -16.8160484 #&gt; 63: DTXSID6037568 2.875770e-07 -15.0617752 #&gt; 64: DTXSID7020005 6.776980e-02 -2.6916386 #&gt; 65: DTXSID7020637 1.836866e+03 7.5158163 #&gt; 66: DTXSID7021029 3.323853e+00 1.2011248 #&gt; 67: DTXSID7024241 2.031631e-06 -13.1066715 #&gt; 68: DTXSID7047433 1.073489e-08 -18.3497663 #&gt; 69: DTXSID8020044 2.447645e+01 3.1977114 #&gt; 70: DTXSID8020090 5.073765e-01 -0.6785019 #&gt; 71: DTXSID8020597 2.037895e-01 -1.5906676 #&gt; 72: DTXSID8020832 1.520093e+03 7.3265271 #&gt; 73: DTXSID8021062 1.180510e-01 -2.1366388 #&gt; 74: DTXSID8022292 1.528695e-08 -17.9962663 #&gt; 75: DTXSID8022377 1.016181e-08 -18.4046290 #&gt; 76: DTXSID8023846 1.702425e-06 -13.2834569 #&gt; 77: DTXSID8023848 8.277807e-06 -11.7019325 #&gt; 78: DTXSID8025541 1.949228e-05 -10.8454918 #&gt; 79: DTXSID8031865 8.386721e-01 -0.1759355 #&gt; 80: DTXSID9020243 1.186626e-04 -9.0392263 #&gt; 81: DTXSID9021390 4.408660e+00 1.4835708 #&gt; 82: DTXSID9021427 3.637282e-01 -1.0113483 #&gt; 83: DTXSID9022366 1.506794e-09 -20.3132814 #&gt; 84: DTXSID9023380 1.149256e-08 -18.2815660 #&gt; 85: DTXSID9023914 3.131080e-04 -8.0689624 #&gt; 86: DTXSID9024142 2.739923e-09 -19.7153359 #&gt; 87: DTXSID9032113 2.981382e-08 -17.3282936 #&gt; 88: DTXSID9032329 7.041032e-07 -14.1663408 #&gt; dtxsid mean_vapor_pressure log_transform_mean_vapor_pressure ccl4_vapor_grouped[, log_transform_mean_vapor_pressure := log(mean_vapor_pressure)] #&gt; dtxsid propType mean_vapor_pressure log_transform_mean_vapor_pressure #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; #&gt; 1: DTXSID0020153 experimental 1.229990e+00 0.2070060 #&gt; 2: DTXSID0020153 predicted 1.504793e+00 0.4086556 #&gt; 3: DTXSID0020446 experimental 6.899220e-08 -16.4892724 #&gt; 4: DTXSID0020446 predicted 1.592376e-06 -13.3502831 #&gt; 5: DTXSID0020573 predicted 1.016181e-08 -18.4046290 #&gt; --- #&gt; 143: DTXSID9024142 predicted 2.059810e-09 -20.0006521 #&gt; 144: DTXSID9032113 experimental 1.279970e-08 -18.1738441 #&gt; 145: DTXSID9032113 predicted 3.548520e-08 -17.1541501 #&gt; 146: DTXSID9032329 experimental 8.000180e-07 -14.0386316 #&gt; 147: DTXSID9032329 predicted 6.721317e-07 -14.2128116 natadb_vapor_all[, log_transform_mean_vapor_pressure := log(mean_vapor_pressure)] #&gt; dtxsid mean_vapor_pressure log_transform_mean_vapor_pressure #&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; #&gt; 1: DTXSID0020153 1.436092e+00 0.3619259 #&gt; 2: DTXSID0020448 4.899910e+01 3.8918019 #&gt; 3: DTXSID0020523 2.824828e-04 -8.1718931 #&gt; 4: DTXSID0020529 7.603507e-04 -7.1817307 #&gt; 5: DTXSID0020600 1.060440e+03 6.9664394 #&gt; --- #&gt; 147: DTXSID9020293 3.124725e-02 -3.4658239 #&gt; 148: DTXSID9020299 1.177018e-06 -13.6525264 #&gt; 149: DTXSID9020827 2.930390e-06 -12.7403750 #&gt; 150: DTXSID9021138 3.619153e-03 -5.6215152 #&gt; 151: DTXSID9041522 6.433997e-05 -9.6513295 natadb_vapor_grouped[, log_transform_mean_vapor_pressure := log(mean_vapor_pressure)] #&gt; dtxsid propType mean_vapor_pressure log_transform_mean_vapor_pressure #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; #&gt; 1: DTXSID0020153 experimental 1.229990e+00 0.2070060 #&gt; 2: DTXSID0020153 predicted 1.504793e+00 0.4086556 #&gt; 3: DTXSID0020448 experimental 5.329670e+01 3.9758744 #&gt; 4: DTXSID0020448 predicted 4.756657e+01 3.8621301 #&gt; 5: DTXSID0020523 experimental 3.900320e-04 -7.8492818 #&gt; --- #&gt; 270: DTXSID9020299 experimental 2.199890e-06 -13.0271032 #&gt; 271: DTXSID9020299 predicted 8.360607e-07 -13.9945647 #&gt; 272: DTXSID9020827 predicted 2.930390e-06 -12.7403750 #&gt; 273: DTXSID9021138 predicted 3.619153e-03 -5.6215152 #&gt; 274: DTXSID9041522 predicted 6.433997e-05 -9.6513295 Now we plot the log transformed data. First plot the CCL4 data. ggplot(ccl4_vapor_all, aes(log_transform_mean_vapor_pressure)) + geom_boxplot() + coord_flip() ggplot(ccl4_vapor_grouped, aes(propType, log_transform_mean_vapor_pressure)) + geom_boxplot() Then plot the NATA data. ggplot(natadb_vapor_all, aes(log_transform_mean_vapor_pressure)) + geom_boxplot() + coord_flip() ggplot(natadb_vapor_grouped, aes(propType, log_transform_mean_vapor_pressure)) + geom_boxplot() Finally, we compare both sets simultaneously. We add in a column to each data.table denoting to which data set the rows correspond and then combine the rows from both data sets together using the function rbind(). ccl4_vapor_grouped[, set := &#39;CCL4&#39;] #&gt; dtxsid propType mean_vapor_pressure log_transform_mean_vapor_pressure set #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID0020153 experimental 1.229990e+00 0.2070060 CCL4 #&gt; 2: DTXSID0020153 predicted 1.504793e+00 0.4086556 CCL4 #&gt; 3: DTXSID0020446 experimental 6.899220e-08 -16.4892724 CCL4 #&gt; 4: DTXSID0020446 predicted 1.592376e-06 -13.3502831 CCL4 #&gt; 5: DTXSID0020573 predicted 1.016181e-08 -18.4046290 CCL4 #&gt; --- #&gt; 143: DTXSID9024142 predicted 2.059810e-09 -20.0006521 CCL4 #&gt; 144: DTXSID9032113 experimental 1.279970e-08 -18.1738441 CCL4 #&gt; 145: DTXSID9032113 predicted 3.548520e-08 -17.1541501 CCL4 #&gt; 146: DTXSID9032329 experimental 8.000180e-07 -14.0386316 CCL4 #&gt; 147: DTXSID9032329 predicted 6.721317e-07 -14.2128116 CCL4 natadb_vapor_grouped[, set := &#39;NATADB&#39;] #&gt; dtxsid propType mean_vapor_pressure log_transform_mean_vapor_pressure set #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID0020153 experimental 1.229990e+00 0.2070060 NATADB #&gt; 2: DTXSID0020153 predicted 1.504793e+00 0.4086556 NATADB #&gt; 3: DTXSID0020448 experimental 5.329670e+01 3.9758744 NATADB #&gt; 4: DTXSID0020448 predicted 4.756657e+01 3.8621301 NATADB #&gt; 5: DTXSID0020523 experimental 3.900320e-04 -7.8492818 NATADB #&gt; --- #&gt; 270: DTXSID9020299 experimental 2.199890e-06 -13.0271032 NATADB #&gt; 271: DTXSID9020299 predicted 8.360607e-07 -13.9945647 NATADB #&gt; 272: DTXSID9020827 predicted 2.930390e-06 -12.7403750 NATADB #&gt; 273: DTXSID9021138 predicted 3.619153e-03 -5.6215152 NATADB #&gt; 274: DTXSID9041522 predicted 6.433997e-05 -9.6513295 NATADB all_vapor_grouped &lt;- rbind(ccl4_vapor_grouped, natadb_vapor_grouped) Now we plot the combined data. First we color the boxplots based on the property type, with mean log transformed vapor pressure plotted for each data set and property type. vapor_box &lt;- ggplot(all_vapor_grouped, aes(set, log_transform_mean_vapor_pressure)) + geom_boxplot(aes(color = propType)) vapor_box Next we color the boxplots based on the data set. vapor &lt;- ggplot(all_vapor_grouped, aes(log_transform_mean_vapor_pressure)) + geom_boxplot((aes(color = set))) + coord_flip() vapor In the plots above, when we graph the data separated both by data set and property type as well as just by data set, we observe the general trend that the NATADB chemicals have a higher mean vapor pressure than the CCL4 chemicals. We also explore Henry’s Law constant and boiling point in a similar fashion. Group by DTXSID. ccl4_hlc_all &lt;- ccl4_phys_chem[propertyId %in% &#39;henrys-law&#39;, .(mean_hlc = sapply(.SD, mean)), .SDcols = c(&#39;value&#39;), by = .(dtxsid)] natadb_hlc_all &lt;- natadb_phys_chem[propertyId %in% &#39;henrys-law&#39;, .(mean_hlc = sapply(.SD, mean)), .SDcols = c(&#39;value&#39;), by = .(dtxsid)] Group by DTXSID and property type. ccl4_hlc_grouped &lt;- ccl4_phys_chem[propertyId %in% &#39;henrys-law&#39;, .(mean_hlc = sapply(.SD, mean)), .SDcols = c(&#39;value&#39;), by = .(dtxsid, propType)] natadb_hlc_grouped &lt;- natadb_phys_chem[propertyId %in% &#39;henrys-law&#39;, .(mean_hlc = sapply(.SD, mean)), .SDcols = c(&#39;value&#39;), by = .(dtxsid, propType)] Examine summary statistics. summary(ccl4_hlc_all) #&gt; dtxsid mean_hlc #&gt; Length:84 Min. :0.0000000 #&gt; Class :character 1st Qu.:0.0000000 #&gt; Mode :character Median :0.0000007 #&gt; Mean :0.0074491 #&gt; 3rd Qu.:0.0000171 #&gt; Max. :0.4922550 summary(ccl4_hlc_grouped) #&gt; dtxsid propType mean_hlc #&gt; Length:112 Length:112 Min. :0.0000000 #&gt; Class :character Class :character 1st Qu.:0.0000000 #&gt; Mode :character Mode :character Median :0.0000020 #&gt; Mean :0.0062734 #&gt; 3rd Qu.:0.0001211 #&gt; Max. :0.4922550 summary(natadb_hlc_all) #&gt; dtxsid mean_hlc #&gt; Length:145 Min. :0.0000000 #&gt; Class :character 1st Qu.:0.0000002 #&gt; Mode :character Median :0.0000310 #&gt; Mean :0.0085716 #&gt; 3rd Qu.:0.0011833 #&gt; Max. :0.4922550 summary(natadb_hlc_grouped) #&gt; dtxsid propType mean_hlc #&gt; Length:211 Length:211 Min. :0.0000000 #&gt; Class :character Class :character 1st Qu.:0.0000003 #&gt; Mode :character Mode :character Median :0.0001027 #&gt; Mean :0.0073001 #&gt; 3rd Qu.:0.0028065 #&gt; Max. :0.4922550 Again, we log transform the data as it is positive and covers several orders of magnitude. ccl4_hlc_all[, log_transform_mean_hlc := log(mean_hlc)] #&gt; dtxsid mean_hlc log_transform_mean_hlc #&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; #&gt; 1: DTXSID0020153 2.918720e-03 -5.8366101 #&gt; 2: DTXSID0020446 1.527830e-09 -20.2994174 #&gt; 3: DTXSID0020573 3.748870e-06 -12.4940561 #&gt; 4: DTXSID0020600 1.479610e-04 -8.8185618 #&gt; 5: DTXSID0020814 2.049460e-07 -15.4005193 #&gt; 6: DTXSID0021541 8.825620e-03 -4.7300964 #&gt; 7: DTXSID0021917 4.922550e-01 -0.7087584 #&gt; 8: DTXSID0024052 2.338400e-10 -22.1763840 #&gt; 9: DTXSID0032578 2.362830e-09 -19.8634058 #&gt; 10: DTXSID1020437 5.639950e-03 -5.1778801 #&gt; 11: DTXSID1021407 1.768600e-06 -13.2453223 #&gt; 12: DTXSID1021740 8.824295e-06 -11.6380018 #&gt; 13: DTXSID1021798 8.664450e-06 -11.6562821 #&gt; 14: DTXSID1024174 2.964310e-07 -15.0314514 #&gt; 15: DTXSID1024338 1.646800e-10 -22.5270169 #&gt; 16: DTXSID1026164 1.977445e-06 -13.1337050 #&gt; 17: DTXSID1037484 1.196270e-09 -20.5440575 #&gt; 18: DTXSID1037486 1.185530e-09 -20.5530759 #&gt; 19: DTXSID1037567 4.339640e-10 -21.5580595 #&gt; 20: DTXSID2020684 3.194620e-06 -12.6540424 #&gt; 21: DTXSID2021028 3.662790e-06 -12.5172854 #&gt; 22: DTXSID2021317 2.493330e-03 -5.9941361 #&gt; 23: DTXSID2021731 4.553975e-06 -12.2995101 #&gt; 24: DTXSID2022333 8.026420e-03 -4.8250167 #&gt; 25: DTXSID2037506 1.313660e-09 -20.4504487 #&gt; 26: DTXSID2052156 3.785200e-10 -21.6947522 #&gt; 27: DTXSID3020203 4.129470e-02 -3.1870211 #&gt; 28: DTXSID3020833 5.902805e-04 -7.4349127 #&gt; 29: DTXSID3020964 2.389895e-05 -10.6416760 #&gt; 30: DTXSID3024366 1.485460e-04 -8.8146159 #&gt; 31: DTXSID3024869 1.776260e-06 -13.2410005 #&gt; 32: DTXSID3031864 1.803350e-11 -24.7387900 #&gt; 33: DTXSID3032464 8.835860e-06 -11.6366921 #&gt; 34: DTXSID3042219 1.047975e-02 -4.5583105 #&gt; 35: DTXSID3074313 2.046920e-11 -24.6120998 #&gt; 36: DTXSID4020533 4.847905e-06 -12.2369639 #&gt; 37: DTXSID4021503 3.771570e-03 -5.5802639 #&gt; 38: DTXSID4022361 2.458220e-08 -17.5212432 #&gt; 39: DTXSID4022367 1.047830e-09 -20.6765445 #&gt; 40: DTXSID4022448 9.004310e-09 -18.5255625 #&gt; 41: DTXSID4022991 1.238900e-11 -25.1142121 #&gt; 42: DTXSID4032611 1.477090e-05 -11.1228515 #&gt; 43: DTXSID4034948 1.537700e-09 -20.2929780 #&gt; 44: DTXSID5020023 1.214215e-04 -9.0162426 #&gt; 45: DTXSID5020576 9.440170e-08 -16.1757068 #&gt; 46: DTXSID5020601 3.864950e-09 -19.3713171 #&gt; 47: DTXSID5021207 1.374810e-04 -8.8920248 #&gt; 48: DTXSID5024182 3.307185e-07 -14.9219983 #&gt; 49: DTXSID5039224 6.639505e-05 -9.6198881 #&gt; 50: DTXSID50867064 1.183860e-08 -18.2519005 #&gt; 51: DTXSID6020301 4.061025e-02 -3.2037348 #&gt; 52: DTXSID6020856 3.217130e-09 -19.5547762 #&gt; 53: DTXSID6021030 9.187830e-07 -13.9002159 #&gt; 54: DTXSID6021032 3.249700e-04 -8.0317777 #&gt; 55: DTXSID6022422 1.067460e-07 -16.0528137 #&gt; 56: DTXSID6024177 2.553800e-07 -15.1805132 #&gt; 57: DTXSID6037483 5.542140e-10 -21.3134702 #&gt; 58: DTXSID6037485 5.623550e-10 -21.2988878 #&gt; 59: DTXSID6037568 8.289390e-09 -18.6082895 #&gt; 60: DTXSID7020005 8.830950e-08 -16.2424181 #&gt; 61: DTXSID7020637 3.432355e-07 -14.8848490 #&gt; 62: DTXSID7021029 3.650960e-05 -10.2179353 #&gt; 63: DTXSID7024241 3.283070e-06 -12.6267316 #&gt; 64: DTXSID7047433 6.785440e-08 -16.5059016 #&gt; 65: DTXSID8020044 4.999450e-06 -12.2061827 #&gt; 66: DTXSID8020090 2.019270e-06 -13.1127745 #&gt; 67: DTXSID8020597 5.999740e-08 -16.6289646 #&gt; 68: DTXSID8020832 7.367270e-03 -4.9107081 #&gt; 69: DTXSID8021062 1.159700e-05 -11.3647641 #&gt; 70: DTXSID8022292 2.387120e-08 -17.5505931 #&gt; 71: DTXSID8022377 3.748870e-06 -12.4940561 #&gt; 72: DTXSID8023846 4.938440e-09 -19.1262163 #&gt; 73: DTXSID8023848 9.954840e-09 -18.4252070 #&gt; 74: DTXSID8025541 4.150640e-07 -14.6948331 #&gt; 75: DTXSID8031865 1.916920e-10 -22.3751312 #&gt; 76: DTXSID9020243 3.705160e-06 -12.5057841 #&gt; 77: DTXSID9021390 3.432295e-04 -7.9771112 #&gt; 78: DTXSID9021427 5.556870e-07 -14.4030607 #&gt; 79: DTXSID9022366 5.167830e-09 -19.0808130 #&gt; 80: DTXSID9023380 4.156230e-09 -19.2986574 #&gt; 81: DTXSID9023914 5.035540e-11 -23.7119153 #&gt; 82: DTXSID9024142 2.692760e-06 -12.8249439 #&gt; 83: DTXSID9032113 3.098320e-07 -14.9872356 #&gt; 84: DTXSID9032329 1.566850e-06 -13.3664433 #&gt; dtxsid mean_hlc log_transform_mean_hlc ccl4_hlc_grouped[, log_transform_mean_hlc := log(mean_hlc)] #&gt; dtxsid propType mean_hlc log_transform_mean_hlc #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; #&gt; 1: DTXSID0020153 predicted 2.91872e-03 -5.836610 #&gt; 2: DTXSID0020446 predicted 1.52783e-09 -20.299417 #&gt; 3: DTXSID0020573 predicted 3.74887e-06 -12.494056 #&gt; 4: DTXSID0020600 experimental 1.48000e-04 -8.818298 #&gt; 5: DTXSID0020600 predicted 1.47922e-04 -8.818825 #&gt; --- #&gt; 108: DTXSID9023914 experimental 5.03002e-11 -23.713012 #&gt; 109: DTXSID9023914 predicted 5.04106e-11 -23.710820 #&gt; 110: DTXSID9024142 predicted 2.69276e-06 -12.824944 #&gt; 111: DTXSID9032113 predicted 3.09832e-07 -14.987236 #&gt; 112: DTXSID9032329 predicted 1.56685e-06 -13.366443 natadb_hlc_all[, log_transform_mean_hlc := log(mean_hlc)] #&gt; dtxsid mean_hlc log_transform_mean_hlc #&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; #&gt; 1: DTXSID0020153 2.918720e-03 -5.836610 #&gt; 2: DTXSID0020448 2.806465e-03 -5.875830 #&gt; 3: DTXSID0020523 8.611785e-08 -16.267549 #&gt; 4: DTXSID0020529 5.417975e-08 -16.730959 #&gt; 5: DTXSID0020600 1.479610e-04 -8.818562 #&gt; --- #&gt; 141: DTXSID9020293 2.943670e-06 -12.735853 #&gt; 142: DTXSID9020299 4.846860e-10 -21.447520 #&gt; 143: DTXSID9020827 2.040880e-07 -15.404715 #&gt; 144: DTXSID9021138 5.672420e-08 -16.685065 #&gt; 145: DTXSID9041522 8.981400e-06 -11.620355 natadb_hlc_grouped[, log_transform_mean_hlc := log(mean_hlc)] #&gt; dtxsid propType mean_hlc log_transform_mean_hlc #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; #&gt; 1: DTXSID0020153 predicted 2.91872e-03 -5.836610 #&gt; 2: DTXSID0020448 experimental 2.82000e-03 -5.871018 #&gt; 3: DTXSID0020448 predicted 2.79293e-03 -5.880664 #&gt; 4: DTXSID0020523 experimental 8.59999e-08 -16.268920 #&gt; 5: DTXSID0020523 predicted 8.62358e-08 -16.266180 #&gt; --- #&gt; 207: DTXSID9020299 predicted 4.84686e-10 -21.447520 #&gt; 208: DTXSID9020827 experimental 2.03000e-07 -15.410060 #&gt; 209: DTXSID9020827 predicted 2.05176e-07 -15.399398 #&gt; 210: DTXSID9021138 predicted 5.67242e-08 -16.685065 #&gt; 211: DTXSID9041522 predicted 8.98140e-06 -11.620355 We compare both sets simultaneously. We add in a column to each data.table denoting to which set the rows correspond and then rbind() the rows together. Label and combine data. ccl4_hlc_grouped[, set := &#39;CCL4&#39;] #&gt; dtxsid propType mean_hlc log_transform_mean_hlc set #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID0020153 predicted 2.91872e-03 -5.836610 CCL4 #&gt; 2: DTXSID0020446 predicted 1.52783e-09 -20.299417 CCL4 #&gt; 3: DTXSID0020573 predicted 3.74887e-06 -12.494056 CCL4 #&gt; 4: DTXSID0020600 experimental 1.48000e-04 -8.818298 CCL4 #&gt; 5: DTXSID0020600 predicted 1.47922e-04 -8.818825 CCL4 #&gt; --- #&gt; 108: DTXSID9023914 experimental 5.03002e-11 -23.713012 CCL4 #&gt; 109: DTXSID9023914 predicted 5.04106e-11 -23.710820 CCL4 #&gt; 110: DTXSID9024142 predicted 2.69276e-06 -12.824944 CCL4 #&gt; 111: DTXSID9032113 predicted 3.09832e-07 -14.987236 CCL4 #&gt; 112: DTXSID9032329 predicted 1.56685e-06 -13.366443 CCL4 natadb_hlc_grouped[, set := &#39;NATADB&#39;] #&gt; dtxsid propType mean_hlc log_transform_mean_hlc set #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID0020153 predicted 2.91872e-03 -5.836610 NATADB #&gt; 2: DTXSID0020448 experimental 2.82000e-03 -5.871018 NATADB #&gt; 3: DTXSID0020448 predicted 2.79293e-03 -5.880664 NATADB #&gt; 4: DTXSID0020523 experimental 8.59999e-08 -16.268920 NATADB #&gt; 5: DTXSID0020523 predicted 8.62358e-08 -16.266180 NATADB #&gt; --- #&gt; 207: DTXSID9020299 predicted 4.84686e-10 -21.447520 NATADB #&gt; 208: DTXSID9020827 experimental 2.03000e-07 -15.410060 NATADB #&gt; 209: DTXSID9020827 predicted 2.05176e-07 -15.399398 NATADB #&gt; 210: DTXSID9021138 predicted 5.67242e-08 -16.685065 NATADB #&gt; 211: DTXSID9041522 predicted 8.98140e-06 -11.620355 NATADB all_hlc_grouped &lt;- rbind(ccl4_hlc_grouped, natadb_hlc_grouped) Plot data. hlc_box &lt;- ggplot(all_hlc_grouped, aes(set, log_transform_mean_hlc)) + geom_boxplot(aes(color = propType)) hlc_box hlc &lt;- ggplot(all_hlc_grouped, aes(log_transform_mean_hlc)) + geom_boxplot(aes(color = set)) + coord_flip() hlc Again, we observe that in both grouping by propType and aggregating all results together by data set, that the chemicals in NATADB have a generally higher mean Henry’s Law Constant value than those in CCL4. Finally, we consider boiling point. Group by DTXSID. ccl4_boiling_all &lt;- ccl4_phys_chem[propertyId %in% &#39;boiling-point&#39;, .(mean_boiling_point = sapply(.SD, mean)), .SDcols = c(&#39;value&#39;), by = .(dtxsid)] natadb_boiling_all &lt;- natadb_phys_chem[propertyId %in% &#39;boiling-point&#39;, .(mean_boiling_point = sapply(.SD, mean)), .SDcols = c(&#39;value&#39;), by = .(dtxsid)] Group by DTXSID and property type. ccl4_boiling_grouped &lt;- ccl4_phys_chem[propertyId %in% &#39;boiling-point&#39;, .(mean_boiling_point = sapply(.SD, mean)), .SDcols = c(&#39;value&#39;), by = .(dtxsid, propType)] natadb_boiling_grouped &lt;- natadb_phys_chem[propertyId %in% &#39;boiling-point&#39;, .(mean_boiling_point = sapply(.SD, mean)), .SDcols = c(&#39;value&#39;), by = .(dtxsid, propType)] Calculate summary statistics. summary(ccl4_boiling_all) #&gt; dtxsid mean_boiling_point #&gt; Length:95 Min. : -34.92 #&gt; Class :character 1st Qu.: 167.40 #&gt; Mode :character Median : 306.38 #&gt; Mean : 348.58 #&gt; 3rd Qu.: 390.63 #&gt; Max. :3377.66 summary(ccl4_boiling_grouped) #&gt; dtxsid propType mean_boiling_point #&gt; Length:147 Length:147 Min. : -40.78 #&gt; Class :character Class :character 1st Qu.: 117.29 #&gt; Mode :character Mode :character Median : 210.08 #&gt; Mean : 317.60 #&gt; 3rd Qu.: 382.80 #&gt; Max. :4825.00 summary(natadb_boiling_all) #&gt; dtxsid mean_boiling_point #&gt; Length:153 Min. :-38.45 #&gt; Class :character 1st Qu.: 85.98 #&gt; Mode :character Median :185.60 #&gt; Mean :183.61 #&gt; 3rd Qu.:265.50 #&gt; Max. :584.48 summary(natadb_boiling_grouped) #&gt; dtxsid propType mean_boiling_point #&gt; Length:296 Length:296 Min. :-87.78 #&gt; Class :character Class :character 1st Qu.: 82.73 #&gt; Mode :character Mode :character Median :179.83 #&gt; Mean :177.10 #&gt; 3rd Qu.:254.77 #&gt; Max. :685.00 Since some of the boiling point values have negative values, we cannot log transform these values. If we try, as you will see below, there will be warnings of NaNs produced. ccl4_boiling_all[, log_transform := log(mean_boiling_point)] #&gt; Warning in log(mean_boiling_point): NaNs produced #&gt; dtxsid mean_boiling_point log_transform #&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; #&gt; 1: DTXSID0020153 179.515667 5.1902625 #&gt; 2: DTXSID0020446 287.518667 5.6612878 #&gt; 3: DTXSID0020573 398.205250 5.9869676 #&gt; 4: DTXSID0020600 14.330786 2.6624101 #&gt; 5: DTXSID0020814 395.859500 5.9810594 #&gt; 6: DTXSID0021464 129.352000 4.8625374 #&gt; 7: DTXSID0021541 -17.681056 NaN #&gt; 8: DTXSID0021917 69.413390 4.2400798 #&gt; 9: DTXSID0024052 371.968000 5.9188078 #&gt; 10: DTXSID0032578 382.199000 5.9459414 #&gt; 11: DTXSID1020437 58.122722 4.0625567 #&gt; 12: DTXSID1021407 226.088833 5.4209280 #&gt; 13: DTXSID1021409 418.990000 6.0378471 #&gt; 14: DTXSID1021740 117.328000 4.7649734 #&gt; 15: DTXSID1021798 237.391429 5.4697104 #&gt; 16: DTXSID1024174 381.107333 5.9430811 #&gt; 17: DTXSID1024207 3377.660000 8.1249384 #&gt; 18: DTXSID1024338 360.932000 5.8886896 #&gt; 19: DTXSID1026164 203.990250 5.3180722 #&gt; 20: DTXSID1031040 2160.993333 7.6783233 #&gt; 21: DTXSID1037484 374.193000 5.9247717 #&gt; 22: DTXSID1037486 374.766500 5.9263032 #&gt; 23: DTXSID1037567 401.491667 5.9951868 #&gt; 24: DTXSID2020684 276.241250 5.6212746 #&gt; 25: DTXSID2021028 181.991333 5.2039591 #&gt; 26: DTXSID2021317 130.530000 4.8716031 #&gt; 27: DTXSID2021731 53.497800 3.9796405 #&gt; 28: DTXSID2022333 174.655667 5.1628164 #&gt; 29: DTXSID2024169 1514.066667 7.3225545 #&gt; 30: DTXSID2037506 329.383000 5.7972212 #&gt; 31: DTXSID2040282 1741.490000 7.4624963 #&gt; 32: DTXSID2052156 414.215250 6.0263858 #&gt; 33: DTXSID3020203 1.270948 0.2397629 #&gt; 34: DTXSID3020702 242.027667 5.4890520 #&gt; 35: DTXSID3020833 55.992829 4.0252236 #&gt; 36: DTXSID3020964 210.665625 5.3502722 #&gt; 37: DTXSID3024366 89.453218 4.4937158 #&gt; 38: DTXSID3024869 200.921000 5.3029118 #&gt; 39: DTXSID3031864 195.536286 5.2757460 #&gt; 40: DTXSID3032464 389.168333 5.9640120 #&gt; 41: DTXSID3034458 472.612000 6.1582748 #&gt; 42: DTXSID3042219 162.024857 5.0877498 #&gt; 43: DTXSID3073137 497.040000 6.2086705 #&gt; 44: DTXSID3074313 453.378333 6.1167269 #&gt; 45: DTXSID4020533 102.426556 4.6291460 #&gt; 46: DTXSID4021503 70.806838 4.2599556 #&gt; 47: DTXSID4022361 383.492250 5.9493194 #&gt; 48: DTXSID4022367 386.137750 5.9561942 #&gt; 49: DTXSID4022448 353.556250 5.8680426 #&gt; 50: DTXSID4022991 654.551500 6.4839503 #&gt; 51: DTXSID4032611 306.376333 5.7248142 #&gt; 52: DTXSID4034948 427.915500 6.0589257 #&gt; 53: DTXSID5020023 53.273463 3.9754383 #&gt; 54: DTXSID5020576 410.481250 6.0173303 #&gt; 55: DTXSID5020601 284.046000 5.6491362 #&gt; 56: DTXSID5021207 37.409986 3.6219377 #&gt; 57: DTXSID5024182 122.936286 4.8116662 #&gt; 58: DTXSID5039224 22.668140 3.1209604 #&gt; 59: DTXSID50867064 270.425500 5.5999966 #&gt; 60: DTXSID6020301 -34.922650 NaN #&gt; 61: DTXSID6020856 201.126625 5.3039347 #&gt; 62: DTXSID6021030 327.147500 5.7904111 #&gt; 63: DTXSID6021032 213.844200 5.3652477 #&gt; 64: DTXSID6022422 352.294375 5.8644671 #&gt; 65: DTXSID6024177 223.431333 5.4091041 #&gt; 66: DTXSID6037483 392.672667 5.9729764 #&gt; 67: DTXSID6037485 392.092000 5.9714965 #&gt; 68: DTXSID6037568 377.399500 5.9333043 #&gt; 69: DTXSID7020005 202.910286 5.3127639 #&gt; 70: DTXSID7020637 -14.211583 NaN #&gt; 71: DTXSID7021029 154.793429 5.0420915 #&gt; 72: DTXSID7024241 373.244000 5.9222324 #&gt; 73: DTXSID7047433 392.270750 5.9719523 #&gt; 74: DTXSID8020044 94.263900 4.5460983 #&gt; 75: DTXSID8020090 185.053250 5.2206436 #&gt; 76: DTXSID8020597 185.598000 5.2235830 #&gt; 77: DTXSID8020832 7.726314 2.0446319 #&gt; 78: DTXSID8021062 210.866600 5.3512257 #&gt; 79: DTXSID8022292 383.472400 5.9492676 #&gt; 80: DTXSID8022377 398.205250 5.9869676 #&gt; 81: DTXSID8023846 313.631500 5.7482187 #&gt; 82: DTXSID8023848 351.873750 5.8632724 #&gt; 83: DTXSID8025541 344.445667 5.8419364 #&gt; 84: DTXSID8031865 190.962750 5.2520784 #&gt; 85: DTXSID8052483 482.980000 6.1799752 #&gt; 86: DTXSID9020243 351.292333 5.8616187 #&gt; 87: DTXSID9021390 155.947500 5.0495194 #&gt; 88: DTXSID9021427 172.769429 5.1519579 #&gt; 89: DTXSID9022366 418.984000 6.0378327 #&gt; 90: DTXSID9023380 384.037500 5.9507402 #&gt; 91: DTXSID9023914 330.833000 5.8016137 #&gt; 92: DTXSID9024142 452.011333 6.1137073 #&gt; 93: DTXSID9032113 384.574500 5.9521375 #&gt; 94: DTXSID9032119 736.490000 6.6018957 #&gt; 95: DTXSID9032329 448.429667 6.1057519 #&gt; dtxsid mean_boiling_point log_transform ccl4_boiling_grouped[, log_transform := log(mean_boiling_point)] #&gt; Warning in log(mean_boiling_point): NaNs produced #&gt; dtxsid propType mean_boiling_point log_transform #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; #&gt; 1: DTXSID0020153 experimental 178.9778 5.187262 #&gt; 2: DTXSID0020153 predicted 180.1880 5.194001 #&gt; 3: DTXSID0020446 experimental 182.5000 5.206750 #&gt; 4: DTXSID0020446 predicted 340.0280 5.829028 #&gt; 5: DTXSID0020573 predicted 398.2052 5.986968 #&gt; --- #&gt; 143: DTXSID9024142 predicted 452.0113 6.113707 #&gt; 144: DTXSID9032113 predicted 384.5745 5.952138 #&gt; 145: DTXSID9032119 experimental 990.0000 6.897705 #&gt; 146: DTXSID9032119 predicted 482.9800 6.179975 #&gt; 147: DTXSID9032329 predicted 448.4297 6.105752 natadb_boiling_all[, log_transform := log(mean_boiling_point)] #&gt; Warning in log(mean_boiling_point): NaNs produced #&gt; dtxsid mean_boiling_point log_transform #&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; #&gt; 1: DTXSID0020153 179.51567 5.190262 #&gt; 2: DTXSID0020448 96.89345 4.573612 #&gt; 3: DTXSID0020523 286.48440 5.657684 #&gt; 4: DTXSID0020529 301.62250 5.709176 #&gt; 5: DTXSID0020600 14.33079 2.662410 #&gt; --- #&gt; 149: DTXSID9020299 347.78160 5.851575 #&gt; 150: DTXSID9020827 372.92120 5.921367 #&gt; 151: DTXSID9021138 265.49863 5.581610 #&gt; 152: DTXSID9021261 584.48000 6.370723 #&gt; 153: DTXSID9041522 336.67514 5.819118 natadb_boiling_grouped[, log_transform := log(mean_boiling_point)] #&gt; Warning in log(mean_boiling_point): NaNs produced #&gt; dtxsid propType mean_boiling_point log_transform #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; #&gt; 1: DTXSID0020153 experimental 178.97780 5.187262 #&gt; 2: DTXSID0020153 predicted 180.18800 5.194001 #&gt; 3: DTXSID0020448 experimental 96.14168 4.565823 #&gt; 4: DTXSID0020448 predicted 97.64523 4.581341 #&gt; 5: DTXSID0020523 experimental 113.00000 4.727388 #&gt; --- #&gt; 292: DTXSID9021138 predicted 263.44175 5.573832 #&gt; 293: DTXSID9021261 experimental 685.00000 6.529419 #&gt; 294: DTXSID9021261 predicted 483.96000 6.182002 #&gt; 295: DTXSID9041522 experimental 340.00000 5.828946 #&gt; 296: DTXSID9041522 predicted 334.18150 5.811684 We compare both sets simultaneously. We add in a column to each data.table denoting to which set the rows correspond and then rbind() the rows together. We use the values as is rather than transforming them. Label and combine data. ccl4_boiling_grouped[, set := &#39;CCL4&#39;] #&gt; dtxsid propType mean_boiling_point log_transform set #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID0020153 experimental 178.9778 5.187262 CCL4 #&gt; 2: DTXSID0020153 predicted 180.1880 5.194001 CCL4 #&gt; 3: DTXSID0020446 experimental 182.5000 5.206750 CCL4 #&gt; 4: DTXSID0020446 predicted 340.0280 5.829028 CCL4 #&gt; 5: DTXSID0020573 predicted 398.2052 5.986968 CCL4 #&gt; --- #&gt; 143: DTXSID9024142 predicted 452.0113 6.113707 CCL4 #&gt; 144: DTXSID9032113 predicted 384.5745 5.952138 CCL4 #&gt; 145: DTXSID9032119 experimental 990.0000 6.897705 CCL4 #&gt; 146: DTXSID9032119 predicted 482.9800 6.179975 CCL4 #&gt; 147: DTXSID9032329 predicted 448.4297 6.105752 CCL4 natadb_boiling_grouped[, set := &#39;NATADB&#39;] #&gt; dtxsid propType mean_boiling_point log_transform set #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID0020153 experimental 178.97780 5.187262 NATADB #&gt; 2: DTXSID0020153 predicted 180.18800 5.194001 NATADB #&gt; 3: DTXSID0020448 experimental 96.14168 4.565823 NATADB #&gt; 4: DTXSID0020448 predicted 97.64523 4.581341 NATADB #&gt; 5: DTXSID0020523 experimental 113.00000 4.727388 NATADB #&gt; --- #&gt; 292: DTXSID9021138 predicted 263.44175 5.573832 NATADB #&gt; 293: DTXSID9021261 experimental 685.00000 6.529419 NATADB #&gt; 294: DTXSID9021261 predicted 483.96000 6.182002 NATADB #&gt; 295: DTXSID9041522 experimental 340.00000 5.828946 NATADB #&gt; 296: DTXSID9041522 predicted 334.18150 5.811684 NATADB all_boiling_grouped &lt;- rbind(ccl4_boiling_grouped, natadb_boiling_grouped) Plot the data. boiling_box &lt;- ggplot(all_boiling_grouped, aes(set, mean_boiling_point)) + geom_boxplot(aes(color = propType)) boiling_box boiling &lt;- ggplot(all_boiling_grouped, aes(mean_boiling_point)) + geom_boxplot(aes(color = set)) + coord_flip() boiling A visual inspection of this set of graphs is not as clear as in the previous cases. Note that the predicted values for each data set tend to be higher than the experimental. The mean of CCL4, by predicted and experimental appears to be greater than the corresponding means for NATADB, as does the overall mean, but the interquartile ranges of these different groupings yield slightly different results. This gives us a sense that the picture for boiling point is not as clear cut between experimental and predicted for these two data sets as it was in the previous cases of physico-chemical properties we investigated. Answer to Environmental Health Question 2 Through inspecting the last several plots, we can answer Environmental Health Question 2: The physico-chemical property data are reported with both experimental and predicted values present for many chemicals. Are there differences between the mean predicted and experimental results for a variety of physico-chemical properties? Answer: There are indeed differences between the mean values of various physico-chemical properties when grouped by predicted or experimental. In the case of “vapor-pressure”, the means of predicted values tend to be a little lower than experimental, though they are much closer in the case of NATADB than CCL4. The trend of lower predicted means compared to experimental means is more clearly demonstrated for “henrys-law” values in both data sets. In the case of “boiling-point”, the predicted values are greater than the experimental values, though this is much more pronounced in CCL4 while the set of means for NATADB are again fairly close. Hazard Data: Genotoxicity Now, having examined some of the distributions of the physico-chemical properties of the two lists, aggregated between predicted and experimental, we move towards learning more about these chemicals beyond physico-chemical properties. Specifically, we will examine their genotoxicity. Using the standard CompTox Chemicals Dashboard approach to access genotoxicity, one would again navigate to the individual chemical page Once one navigates to the genotoxicity tab highlighted in the previous page, the following is displayed as seen here: This page includes two sets of information, the first of which provides a summary of available genotoxicity data while the second provides the individual reports and samples of such data. We again use the CTX APIs to streamline the process of retrieving this information in a programmatic fashion. To this end, we will use the genotoxicity endpoints found within the Hazard endpoints of the CTX APIs. Pictured below is the particular set of genotoxicity resources available in the Hazard endpoints of the CTX APIs. There are both summary and detail resources, reflecting the information one can find on the CompTox Chemicals Dashboard Genotoxicity page for a given chemical. To access the genetox endpoint, we will use the function get_genetox_summary(). Since we have a list of chemicals, rather than searching individually for each chemical, we use the batch search version of the function, named get_genetox_summary_batch(). We will examine this and then access the details. Grab the data using the APIs. ccl4_genotox &lt;- get_genetox_summary_batch(DTXSID = ccl4$dtxsid) natadb_genetox &lt;- get_genetox_summary_batch(DTXSID = natadb$dtxsid) Examine the dimensions. dim(ccl4_genotox) #&gt; [1] 71 7 dim(natadb_genetox) #&gt; [1] 153 7 Examine the column names and data from the first six chemicals with genetox data from CCL4. colnames(ccl4_genotox) #&gt; [1] &quot;id&quot; &quot;dtxsid&quot; &quot;reportsPositive&quot; &quot;reportsNegative&quot; &quot;reportsOther&quot; #&gt; [6] &quot;ames&quot; &quot;micronucleus&quot; head(ccl4_genotox) #&gt; id dtxsid reportsPositive reportsNegative reportsOther ames micronucleus #&gt; &lt;int&gt; &lt;char&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; #&gt; 1: 518256 DTXSID0020153 26 5 1 &lt;NA&gt; positive #&gt; 2: 522227 DTXSID0020446 0 12 0 &lt;NA&gt; negative #&gt; 3: 523058 DTXSID0020573 3 14 0 &lt;NA&gt; negative #&gt; 4: 525830 DTXSID0020600 20 0 1 &lt;NA&gt; positive #&gt; 5: 525597 DTXSID0020814 1 0 0 &lt;NA&gt; &lt;NA&gt; #&gt; 6: 520045 DTXSID0021464 9 11 0 &lt;NA&gt; positive The information returned is of the first variety highlighted in the image above, that is, the summary data on the available genotoxicity data for each chemical. Observe that we have information on 71 chemicals from the CCL4 data and 153 from the NATA data. We note the chemicals not included in the results and then dig into the returned results. ccl4[!(dtxsid %in% ccl4_genotox$dtxsid), .(dtxsid, casrn, preferredName, molFormula)] #&gt; dtxsid casrn preferredName molFormula #&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; #&gt; 1: DTXSID001024118 77238-39-2 Microcystin &lt;NA&gt; #&gt; 2: DTXSID0024052 55290-64-7 Dimethipin C6H10O4S2 #&gt; 3: DTXSID0032578 59669-26-0 Thiodicarb C10H18N4O4S3 #&gt; 4: DTXSID1037484 194992-44-4 Acetochlor OA C14H19NO4 #&gt; 5: DTXSID1037486 171262-17-2 2-[(2,6-Diethylphenyl)(me C14H19NO4 #&gt; 6: DTXSID1037567 171118-09-5 Metolachlor ESA C15H23NO5S #&gt; 7: DTXSID2022333 135-98-8 sec-Butylbenzene C10H14 #&gt; 8: DTXSID2031083 143545-90-8 Cylindrospermopsin C15H21N5O7S #&gt; 9: DTXSID2037506 16655-82-6 3-Hydroxycarbofuran C12H15NO4 #&gt; 10: DTXSID2052156 517-09-9 Equilenin C18H18O2 #&gt; 11: DTXSID3021857 25154-52-3 n-Nonylphenol C15H24O #&gt; 12: DTXSID3034458 99129-21-2 Clethodim C17H26ClNO3S #&gt; 13: DTXSID3042219 103-65-1 Propylbenzene C9H12 #&gt; 14: DTXSID3073137 14866-68-3 Chlorate ClO3 #&gt; 15: DTXSID3074313 35523-89-8 Saxitoxin C10H17N7O4 #&gt; 16: DTXSID4022448 51218-45-2 Metolachlor C15H22ClNO2 #&gt; 17: DTXSID4032611 13194-48-4 Ethoprop C8H19O2PS2 #&gt; 18: DTXSID4034948 112410-23-8 Tebufenozide C22H28N2O2 #&gt; 19: DTXSID50867064 64285-06-9 Anatoxin a C10H15NO #&gt; 20: DTXSID6024177 10265-92-6 Methamidophos C2H8NO2PS #&gt; 21: DTXSID6037483 187022-11-3 Acetochlor ESA C14H21NO5S #&gt; 22: DTXSID6037485 142363-53-9 Alachlor ESA C14H21NO5S #&gt; 23: DTXSID6037568 152019-73-3 Metolachlor OA C15H21NO4 #&gt; 24: DTXSID7024241 42874-03-3 Oxyfluorfen C15H11ClF3NO4 #&gt; 25: DTXSID7047433 474-86-2 Equilin C18H20O2 #&gt; 26: DTXSID8022377 57-91-0 17alpha-Estradiol C18H24O2 #&gt; 27: DTXSID8052483 7440-56-4 Germanium Ge #&gt; 28: DTXSID9032113 107534-96-3 Tebuconazole C16H22ClN3O #&gt; 29: DTXSID9032329 741-58-2 Bensulide C14H24NO4PS3 #&gt; dtxsid casrn preferredName molFormula natadb[!(dtxsid %in% natadb_genetox$dtxsid), .(dtxsid, casrn, preferredName, molFormula)] #&gt; dtxsid casrn preferredName molFormula #&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; #&gt; 1: DTXSID00872421 NOCAS_872421 Lead &amp; Lead Compounds &lt;NA&gt; #&gt; 2: DTXSID1020273 7782-50-5 Chlorine Cl2 #&gt; 3: DTXSID10872417 NOCAS_872417 Cadmium &amp; Cadmium Compoun &lt;NA&gt; #&gt; 4: DTXSID30872414 NOCAS_872414 Antimony &amp; Antimony Compo &lt;NA&gt; #&gt; 5: DTXSID30872419 NOCAS_872419 Cobalt &amp; Cobalt Compounds &lt;NA&gt; #&gt; 6: DTXSID40872425 NOCAS_872425 Nickel &amp; Nickel Compounds &lt;NA&gt; #&gt; 7: DTXSID5024267 1336-36-3 Polychlorinated biphenyls &lt;NA&gt; #&gt; 8: DTXSID7020687 608-73-1 1,2,3,4,5,6-Hexachlorocyc C6H6Cl6 #&gt; 9: DTXSID7023984 NOCAS_23984 Coke oven emissions &lt;NA&gt; #&gt; 10: DTXSID90872415 NOCAS_872415 Arsenic &amp; Arsenic Compoun &lt;NA&gt; Now, we access the genotoxicity details of the chemicals in each data set using the function get_genetox_details_batch(). We explore the dimensions of the returned queries, the column names, and the first few lines of the data. Grab the data from the CTX APIs. ccl4_genetox_details &lt;- get_genetox_details_batch(DTXSID = ccl4$dtxsid) natadb_genetox_details &lt;- get_genetox_details_batch(DTXSID = natadb$dtxsid) Examine the dimensions. dim(ccl4_genetox_details) #&gt; [1] 1041 10 dim(natadb_genetox_details) #&gt; [1] 2647 10 Look at the column names and the first six rows of the data from the CCL4 chemicals. colnames(ccl4_genetox_details) #&gt; [1] &quot;id&quot; &quot;source&quot; &quot;year&quot; &quot;dtxsid&quot; #&gt; [5] &quot;strain&quot; &quot;species&quot; &quot;assayCategory&quot; &quot;assayType&quot; #&gt; [9] &quot;metabolicActivation&quot; &quot;assayResult&quot; head(ccl4_genetox_details) #&gt; id source year dtxsid strain species #&gt; &lt;int&gt; &lt;char&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; #&gt; 1: 1182642 CSCL_ISHL NA DTXSID0020153 &lt;NA&gt; &lt;NA&gt; #&gt; 2: 1182633 CSCL_ISHL NA DTXSID0020153 &lt;NA&gt; &lt;NA&gt; #&gt; 3: 1182649 eChemPortal 1976 DTXSID0020153 &lt;NA&gt; Saccharomyces cerevisiae #&gt; 4: 1182644 eChemPortal 1976 DTXSID0020153 &lt;NA&gt; S. typhimurium TA 1535, T #&gt; 5: 1182645 eChemPortal 1976 DTXSID0020153 &lt;NA&gt; S. typhimurium TA 1538 #&gt; 6: 1182650 eChemPortal 1982 DTXSID0020153 other: Tuck To (outbred) mouse #&gt; 4 variable(s) not shown: [assayCategory &lt;char&gt;, assayType &lt;char&gt;, metabolicActivation &lt;char&gt;, assayResult &lt;char&gt;] We examine the information returned for the first chemical in each set of results, which is DTXSID0020153. Notice that the information is identical in each case as this information is chemical specific and not data set specific. Look at the dimensions first. dim(ccl4_genetox_details[dtxsid %in% &#39;DTXSID0020153&#39;, ]) #&gt; [1] 28 10 dim(natadb_genetox_details[dtxsid %in% &#39;DTXSID0020153&#39;, ]) #&gt; [1] 28 10 Now examine the first few rows. head(ccl4_genetox_details[dtxsid %in% &#39;DTXSID0020153&#39;, ]) #&gt; id source year dtxsid strain species #&gt; &lt;int&gt; &lt;char&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; #&gt; 1: 1182642 CSCL_ISHL NA DTXSID0020153 &lt;NA&gt; &lt;NA&gt; #&gt; 2: 1182633 CSCL_ISHL NA DTXSID0020153 &lt;NA&gt; &lt;NA&gt; #&gt; 3: 1182649 eChemPortal 1976 DTXSID0020153 &lt;NA&gt; Saccharomyces cerevisiae #&gt; 4: 1182644 eChemPortal 1976 DTXSID0020153 &lt;NA&gt; S. typhimurium TA 1535, T #&gt; 5: 1182645 eChemPortal 1976 DTXSID0020153 &lt;NA&gt; S. typhimurium TA 1538 #&gt; 6: 1182650 eChemPortal 1982 DTXSID0020153 other: Tuck To (outbred) mouse #&gt; 4 variable(s) not shown: [assayCategory &lt;char&gt;, assayType &lt;char&gt;, metabolicActivation &lt;char&gt;, assayResult &lt;char&gt;] Observe that the data is the same for each data set when restricting to the same chemical. This is because the information we are retrieving is specific to the chemical and not dependent on the chemical lists to which the chemical may belong. identical(ccl4_genetox_details[dtxsid %in% &#39;DTXSID0020153&#39;, ], natadb_genetox_details[dtxsid %in% &#39;DTXSID0020153&#39;, ]) #&gt; [1] TRUE We now explore the assays present for chemicals in each data set. We first determine the unique values of the assayCategory column and then group by these values and determine the number of unique assays for each assayCategory value. Determine the unique assay categories. ccl4_genetox_details[, unique(assayCategory)] #&gt; [1] &quot;in vitro&quot; &quot;ND&quot; &quot;in vivo&quot; natadb_genetox_details[, unique(assayCategory)] #&gt; [1] &quot;in vitro&quot; &quot;ND&quot; &quot;in vivo&quot; Determine the unique assays for each data set and list them. ccl4_genetox_details[, unique(assayType)] #&gt; [1] &quot;InVivoMN&quot; #&gt; [2] &quot;Overall&quot; #&gt; [3] &quot;bacterial reverse mutation assay&quot; #&gt; [4] &quot;micronucleus assay&quot; #&gt; [5] &quot;Ames&quot; #&gt; [6] &quot;InVitroCA&quot; #&gt; [7] &quot;InVitroMLA&quot; #&gt; [8] &quot;InVitroMN&quot; #&gt; [9] &quot;Cell transformation, clonal assay&quot; #&gt; [10] &quot;Forward and reverse gene mutation, host-mediated assay&quot; #&gt; [11] &quot;Histidine reverse gene mutation, Ames assay&quot; #&gt; [12] &quot;Micronucleus test, chromosome aberrations&quot; #&gt; [13] &quot;Mitotic recombination or gene conversion&quot; #&gt; [14] &quot;Rec-assay, DNA effects (bacterial DNA repair)&quot; #&gt; [15] &quot;Rec-assay, spot test, DNA effects (bacterial DNA repair)&quot; #&gt; [16] &quot;Sister-chromatid exchange (SCE) in vitro&quot; #&gt; [17] &quot;Unscheduled DNA synthesis (UDS) in vitro, DNA effects&quot; #&gt; [18] &quot;In vivo carcinogenicity studies&quot; #&gt; [19] &quot;in vitro mammalian chromosome aberration test&quot; #&gt; [20] &quot;mammalian cell gene mutation assay&quot; #&gt; [21] &quot;DNA damage and repair assay, unscheduled DNA synthesis in mammalian cells in vitro&quot; #&gt; [22] &quot;in vivo micronucleus (mouse)&quot; #&gt; [23] &quot;in vivo micronucleus (rat)&quot; #&gt; [24] &quot;Sperm morphology&quot; #&gt; [25] &quot;InVivoCA&quot; #&gt; [26] &quot;InVivoUDS&quot; #&gt; [27] &quot;transgenic&quot; #&gt; [28] &quot;Chromosome aberrations&quot; #&gt; [29] &quot;Forward gene mutation at the HPRT locus&quot; #&gt; [30] &quot;Heritable translocation test, chromosome aberrations&quot; #&gt; [31] &quot;Reverse gene mutation&quot; #&gt; [32] &quot;Sex-linked recessive lethal gene mutation&quot; #&gt; [33] &quot;Sister-chromatid exchange (SCE) in vivo&quot; #&gt; [34] &quot;Dominant lethal test&quot; #&gt; [35] &quot;Unscheduled DNA synthesis (UDS) in vivo; DNA effects&quot; #&gt; [36] &quot;chromosome aberration assay&quot; #&gt; [37] &quot;mammalian germ cell cytogenetic assay&quot; #&gt; [38] &quot;bacterial forward mutation assay&quot; #&gt; [39] &quot;sister chromatid exchange assay in mammalian cells&quot; #&gt; [40] &quot;DNA Binding&quot; #&gt; [41] &quot;rodent dominant lethal assay&quot; #&gt; [42] &quot;unscheduled DNA synthesis&quot; #&gt; [43] &quot;Bacterial Mutagenesis&quot; #&gt; [44] &quot;Cytogenetics Other&quot; #&gt; [45] &quot;Cytotoxicity&quot; #&gt; [46] &quot;In Vitro Micronucleus&quot; #&gt; [47] &quot;bacterial gene mutation assay&quot; #&gt; [48] &quot;in vitro mammalian cell micronucleus test&quot; #&gt; [49] &quot;Aneuploidy, chromosome aberrations&quot; #&gt; [50] &quot;Chromosome aberrations in vivo&quot; #&gt; [51] &quot;sister chromatid exchange assay&quot; #&gt; [52] &quot;InVivoDNADamage&quot; #&gt; [53] &quot;Cell transformation, viral enhanced&quot; #&gt; [54] &quot;combined chromosome aberration and micronucleus assay&quot; #&gt; [55] &quot;Chromosome aberrations in vitro&quot; #&gt; [56] &quot;Forward gene mutation&quot; #&gt; [57] &quot;Forward gene mutation at the HPRT or ouabain locus&quot; #&gt; [58] &quot;Forward gene mutation at the thymidine kinase (TK) locus; chromosome aberrations&quot; #&gt; [59] &quot;Specific locus test, gene mutation&quot; #&gt; [60] &quot;Spot test, gene mutation&quot; #&gt; [61] &quot;In Vivo Non-mammalian Mutagenesis&quot; #&gt; [62] &quot;In Vivo Micronucleus&quot; #&gt; [63] &quot;mouse spot test&quot; #&gt; [64] &quot;transgenic rodent mutagenicity assay&quot; #&gt; [65] &quot;yeast cytogenetic assay&quot; #&gt; [66] &quot;Micronucleus and sister chromatid exchange&quot; #&gt; [67] &quot;in vivo comet (mouse)&quot; #&gt; [68] &quot;in vivo comet (rat)&quot; #&gt; [69] &quot;Gene mutation&quot; #&gt; [70] &quot;in vitro mammalian cell transformation assay&quot; #&gt; [71] &quot;Cell transformation&quot; #&gt; [72] &quot;Tryptophan reverse gene mutation&quot; #&gt; [73] &quot;Cell Transformation&quot; #&gt; [74] &quot;DNA Damage/Repair&quot; #&gt; [75] &quot;In Vitro Chromosome Aberration&quot; #&gt; [76] &quot;Mutation&quot; #&gt; [77] &quot;DNA Covalent Binding&quot; #&gt; [78] &quot;In Vivo Chromosome Aberration&quot; #&gt; [79] &quot;In Vivo Mammalian Mutagenesis&quot; #&gt; [80] &quot;in vitro chromosomal aberration study in mammalian cells&quot; #&gt; [81] &quot;Evaluation of metabolic activity of acute cytotoxicity&quot; #&gt; [82] &quot;In vitro mammalian chromosomal aberration test&quot; #&gt; [83] &quot;Mutation Other&quot; #&gt; [84] &quot;Forward and reverse gene mutation, body fluid assay&quot; #&gt; [85] &quot;Forward and reverse gene mutation, chromosome aberrations, mitotic recombination and gene conversion, DNA effects, host-mediated assay&quot; #&gt; [86] &quot;Chromosomal aberration assay&quot; #&gt; [87] &quot;Mitotic recombination&quot; #&gt; [88] &quot;Aneuploidy, sex chromosome gain, chromosome aberrations&quot; #&gt; [89] &quot;Aneuploidy, whole sex chromosome loss, chromosome aberrations&quot; #&gt; [90] &quot;fluctuation test&quot; natadb_genetox_details[, unique(assayType)] #&gt; [1] &quot;InVivoMN&quot; #&gt; [2] &quot;Overall&quot; #&gt; [3] &quot;bacterial reverse mutation assay&quot; #&gt; [4] &quot;micronucleus assay&quot; #&gt; [5] &quot;Ames&quot; #&gt; [6] &quot;InVitroCA&quot; #&gt; [7] &quot;InVitroMLA&quot; #&gt; [8] &quot;InVitroMN&quot; #&gt; [9] &quot;Cell transformation, clonal assay&quot; #&gt; [10] &quot;Forward and reverse gene mutation, host-mediated assay&quot; #&gt; [11] &quot;Histidine reverse gene mutation, Ames assay&quot; #&gt; [12] &quot;Micronucleus test, chromosome aberrations&quot; #&gt; [13] &quot;Mitotic recombination or gene conversion&quot; #&gt; [14] &quot;Rec-assay, DNA effects (bacterial DNA repair)&quot; #&gt; [15] &quot;Rec-assay, spot test, DNA effects (bacterial DNA repair)&quot; #&gt; [16] &quot;Sister-chromatid exchange (SCE) in vitro&quot; #&gt; [17] &quot;Unscheduled DNA synthesis (UDS) in vitro, DNA effects&quot; #&gt; [18] &quot;In vivo carcinogenicity studies&quot; #&gt; [19] &quot;DNA damage and repair assay, unscheduled DNA synthesis in mammalian cells in vitro&quot; #&gt; [20] &quot;rodent dominant lethal assay&quot; #&gt; [21] &quot;Chromosome aberrations&quot; #&gt; [22] &quot;Gene mutation&quot; #&gt; [23] &quot;InVivoUDS&quot; #&gt; [24] &quot;InVivoCA&quot; #&gt; [25] &quot;transgenic&quot; #&gt; [26] &quot;Forward gene mutation at the HPRT locus&quot; #&gt; [27] &quot;Heritable translocation test, chromosome aberrations&quot; #&gt; [28] &quot;Reverse gene mutation&quot; #&gt; [29] &quot;Sex-linked recessive lethal gene mutation&quot; #&gt; [30] &quot;Sister-chromatid exchange (SCE) in vivo&quot; #&gt; [31] &quot;Dominant lethal test&quot; #&gt; [32] &quot;Unscheduled DNA synthesis (UDS) in vivo; DNA effects&quot; #&gt; [33] &quot;Bacterial Mutagenesis&quot; #&gt; [34] &quot;Cytogenetics Other&quot; #&gt; [35] &quot;Cytotoxicity&quot; #&gt; [36] &quot;DNA Damage/Repair&quot; #&gt; [37] &quot;In Vitro Chromosome Aberration&quot; #&gt; [38] &quot;In Vitro Micronucleus&quot; #&gt; [39] &quot;In Vivo Non-mammalian Mutagenesis&quot; #&gt; [40] &quot;Mutation&quot; #&gt; [41] &quot;In Vivo Chromosome Aberration&quot; #&gt; [42] &quot;In Vivo Mammalian Mutagenesis&quot; #&gt; [43] &quot;In Vivo Micronucleus&quot; #&gt; [44] &quot;in vitro mammalian chromosome aberration test&quot; #&gt; [45] &quot;InVivoDNADamage&quot; #&gt; [46] &quot;Cell transformation, viral enhanced&quot; #&gt; [47] &quot;mammalian cell gene mutation assay&quot; #&gt; [48] &quot;in vivo micronucleus (mouse)&quot; #&gt; [49] &quot;Sperm morphology&quot; #&gt; [50] &quot;Forward and reverse gene mutation, mitotic recombination and gene conversion, host-mediated assay&quot; #&gt; [51] &quot;Spot test, gene mutation&quot; #&gt; [52] &quot;bacterial forward mutation assay&quot; #&gt; [53] &quot;sister chromatid exchange assay in mammalian cells&quot; #&gt; [54] &quot;DNA Binding&quot; #&gt; [55] &quot;unscheduled DNA synthesis&quot; #&gt; [56] &quot;bacteriophage induction in E. coli, gene mutation, UDS in mammalian cells, sex-linked recessive lethal mutations in Drosophila&quot; #&gt; [57] &quot;DNA damage, gene mutation, reverse mutation, gene conversion, DNA repair, chromosomal aberration, chromatid exchange, UDS&quot; #&gt; [58] &quot;chromosome aberration study in mammalian cells&quot; #&gt; [59] &quot;in vitro mammalian cell transformation assay&quot; #&gt; [60] &quot;Forward gene mutation at the thymidine kinase (TK) locus; chromosome aberrations&quot; #&gt; [61] &quot;Cell transformation&quot; #&gt; [62] &quot;Forward and reverse gene mutation, body fluid assay&quot; #&gt; [63] &quot;Forward gene mutation at the HPRT or ouabain locus&quot; #&gt; [64] &quot;chromosome aberration assay&quot; #&gt; [65] &quot;Drosophila SLRL assay&quot; #&gt; [66] &quot;Salmonella and Escherichia strains: bacterial reverse mutation assay (e.g. Ames test) ; Bacillus strains: recombination assay&quot; #&gt; [67] &quot;Cytogenetic assay in bone marrow cells&quot; #&gt; [68] &quot;in vivo comet (mouse)&quot; #&gt; [69] &quot;Chromosome aberrations in vitro&quot; #&gt; [70] &quot;Forward gene mutation&quot; #&gt; [71] &quot;Chromosome aberrations in vivo&quot; #&gt; [72] &quot;in vitro mammalian cell gene mutation tests using the thymidine kinase gene&quot; #&gt; [73] &quot;in vivo comet (rat)&quot; #&gt; [74] &quot;in vivo micronucleus (rat)&quot; #&gt; [75] &quot;mouse spot test&quot; #&gt; [76] &quot;Aneuploidy, whole sex chromosome loss, chromosome aberrations&quot; #&gt; [77] &quot;sister chromatid exchange assay&quot; #&gt; [78] &quot;Mouse Lymphoma Forward Mutation Assay&quot; #&gt; [79] &quot;mammalian erythrocyte micronucleus test&quot; #&gt; [80] &quot;Tryptophan reverse gene mutation&quot; #&gt; [81] &quot;bacterial gene mutation assay&quot; #&gt; [82] &quot;yeast forward mutation and mitotic gene conversion assays in Schizosaccharomyces pombe (P1 strain) and Saccharomyces cerevisiae (D4 strain)&quot; #&gt; [83] &quot;Micronucleus test in vitro, chromosome aberrations&quot; #&gt; [84] &quot;heritable translocation assay&quot; #&gt; [85] &quot;mitotic recombination assay with Saccharomyces cerevisiae&quot; #&gt; [86] &quot;Aneuploidy, chromosome aberrations&quot; #&gt; [87] &quot;cell transformation&quot; #&gt; [88] &quot;in vitro mammalian cell micronucleus test&quot; #&gt; [89] &quot;somatic mutation and recombination test in Drosophila&quot; #&gt; [90] &quot;transgenic rodent mutagenicity assay&quot; #&gt; [91] &quot;yeast cytogenetic assay&quot; #&gt; [92] &quot;Micronucleus and sister chromatid exchange&quot; #&gt; [93] &quot;in vitro mammalian cell gene mutation test using the Hprt and xprt genes&quot; #&gt; [94] &quot;bone marrow chromosome aberration assay and mammalian germ cell cytogenetic assay&quot; #&gt; [95] &quot;bacterial mutation&quot; #&gt; [96] &quot;bacterial reverse mutation assay (Salmonella typhimurium and Escherichia coli)&quot; #&gt; [97] &quot;Aneuploidy, partial sex chromosome loss, chromosome aberrations &quot; #&gt; [98] &quot;Chromosome aberrations, in vivo&quot; #&gt; [99] &quot;in vitro chromosome aberration study&quot; #&gt; [100] &quot;Cell transformation, focus assay&quot; #&gt; [101] &quot;Forward and reverse gene mutation, mitotic recombination and gene conversion, DNA effects, host-mediated assay&quot; #&gt; [102] &quot;gene mutation assay in fungi&quot; #&gt; [103] &quot;DNA adduct formation&quot; #&gt; [104] &quot;Cell Transformation&quot; #&gt; [105] &quot;DNA Covalent Binding&quot; #&gt; [106] &quot;mammalian comet assay&quot; #&gt; [107] &quot;Aneuploidy, sex chromosome gain, chromosome aberrations&quot; #&gt; [108] &quot;mammalian germ cell cytogenetic assay&quot; #&gt; [109] &quot;Forward and reverse gene mutation, chromosome aberrations, mitotic recombination and gene conversion, DNA effects, host-mediated assay&quot; #&gt; [110] &quot;E. coli K-12 DNA repair host-mediated assay&quot; #&gt; [111] &quot;Chromosomal aberration assay&quot; #&gt; [112] &quot;forward mutation&quot; #&gt; [113] &quot;mammalian cell gene mutation test&quot; #&gt; [114] &quot;Mitotic recombination&quot; Determine the number of assays per unique assayCategory value. ccl4_genetox_details[, .(Assays = length(unique(assayType))), by = .(assayCategory)] #&gt; assayCategory Assays #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: in vitro 65 #&gt; 2: ND 3 #&gt; 3: in vivo 22 natadb_genetox_details[, .(Assays = length(unique(assayType))), by = .(assayCategory)] #&gt; assayCategory Assays #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: in vitro 83 #&gt; 2: ND 3 #&gt; 3: in vivo 28 We can analyze these results more closely, counting the number of assay results and grouping by assayCategory, and assayType. We also examine the different numbers of assayCategory and assayTypes values used. ccl4_genetox_details[, .N, by = .(assayCategory, assayType, assayResult)] #&gt; assayCategory assayType assayResult N #&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: in vitro InVivoMN negative 10 #&gt; 2: ND Overall positive 5 #&gt; 3: in vitro bacterial reverse mutatio positive 39 #&gt; 4: in vivo micronucleus assay negative 36 #&gt; 5: in vivo micronucleus assay equivocal 1 #&gt; --- #&gt; 149: in vitro Heritable translocation t negative 1 #&gt; 150: in vitro Mitotic recombination positive 1 #&gt; 151: in vitro Aneuploidy, sex chromosom negative 1 #&gt; 152: in vitro Aneuploidy, whole sex chr positive 1 #&gt; 153: in vitro fluctuation test negative 2 ccl4_genetox_details[, .N, by = .(assayCategory)] #&gt; assayCategory N #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: in vitro 815 #&gt; 2: ND 38 #&gt; 3: in vivo 188 We look at the assayType values and numbers of each for the three different assayCategory values. ccl4_genetox_details[assayCategory == &#39;in vitro&#39;, .N, by = .(assayType)] #&gt; assayType N #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: InVivoMN 28 #&gt; 2: bacterial reverse mutatio 165 #&gt; 3: Ames 88 #&gt; 4: InVitroCA 31 #&gt; 5: InVitroMLA 24 #&gt; 6: InVitroMN 6 #&gt; 7: Cell transformation, clon 8 #&gt; 8: Forward and reverse gene 7 #&gt; 9: Histidine reverse gene mu 19 #&gt; 10: Micronucleus test, chromo 8 #&gt; 11: Mitotic recombination or 18 #&gt; 12: Rec-assay, DNA effects (b 15 #&gt; 13: Rec-assay, spot test, DNA 2 #&gt; 14: Sister-chromatid exchange 41 #&gt; 15: in vitro mammalian chromo 22 #&gt; 16: mammalian cell gene mutat 42 #&gt; 17: DNA damage and repair ass 21 #&gt; 18: Chromosome aberrations 2 #&gt; 19: Forward gene mutation at 5 #&gt; 20: Heritable translocation t 5 #&gt; 21: Reverse gene mutation 9 #&gt; 22: Sex-linked recessive leth 9 #&gt; 23: Sister-chromatid exchange 13 #&gt; 24: chromosome aberration ass 15 #&gt; 25: bacterial forward mutatio 1 #&gt; 26: sister chromatid exchange 11 #&gt; 27: Bacterial Mutagenesis 27 #&gt; 28: Cytogenetics Other 26 #&gt; 29: Cytotoxicity 21 #&gt; 30: In Vitro Micronucleus 4 #&gt; 31: bacterial gene mutation a 7 #&gt; 32: in vitro mammalian cell m 5 #&gt; 33: Aneuploidy, chromosome ab 5 #&gt; 34: sister chromatid exchange 3 #&gt; 35: Cell transformation, vira 12 #&gt; 36: combined chromosome aberr 1 #&gt; 37: Chromosome aberrations in 2 #&gt; 38: Forward gene mutation 5 #&gt; 39: Forward gene mutation at 6 #&gt; 40: Forward gene mutation at 2 #&gt; 41: Specific locus test, gene 1 #&gt; 42: Spot test, gene mutation 1 #&gt; 43: In Vivo Non-mammalian Mut 7 #&gt; 44: mouse spot test 2 #&gt; 45: transgenic rodent mutagen 1 #&gt; 46: yeast cytogenetic assay 1 #&gt; 47: Gene mutation 4 #&gt; 48: in vitro mammalian cell t 2 #&gt; 49: Cell transformation 5 #&gt; 50: Tryptophan reverse gene m 8 #&gt; 51: Cell Transformation 2 #&gt; 52: DNA Damage/Repair 8 #&gt; 53: In Vitro Chromosome Aberr 11 #&gt; 54: Mutation 3 #&gt; 55: in vitro chromosomal aber 1 #&gt; 56: Evaluation of metabolic a 1 #&gt; 57: In vitro mammalian chromo 2 #&gt; 58: Mutation Other 4 #&gt; 59: Forward and reverse gene 2 #&gt; 60: Forward and reverse gene 1 #&gt; 61: Chromosomal aberration as 2 #&gt; 62: Mitotic recombination 1 #&gt; 63: Aneuploidy, sex chromosom 1 #&gt; 64: Aneuploidy, whole sex chr 1 #&gt; 65: fluctuation test 2 #&gt; assayType N ccl4_genetox_details[assayCategory == &#39;ND&#39;, .N, by = .(assayType)] #&gt; assayType N #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: Overall 5 #&gt; 2: In vivo carcinogenicity s 23 #&gt; 3: transgenic 10 ccl4_genetox_details[assayCategory == &#39;in vivo&#39;, .N, by = .(assayType)] #&gt; assayType N #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: micronucleus assay 46 #&gt; 2: Unscheduled DNA synthesis 9 #&gt; 3: in vivo micronucleus (mou 19 #&gt; 4: in vivo micronucleus (rat 9 #&gt; 5: Sperm morphology 9 #&gt; 6: InVivoCA 14 #&gt; 7: InVivoUDS 11 #&gt; 8: Dominant lethal test 5 #&gt; 9: Unscheduled DNA synthesis 3 #&gt; 10: mammalian germ cell cytog 2 #&gt; 11: DNA Binding 1 #&gt; 12: rodent dominant lethal as 15 #&gt; 13: unscheduled DNA synthesis 6 #&gt; 14: Chromosome aberrations in 2 #&gt; 15: InVivoDNADamage 7 #&gt; 16: In Vivo Micronucleus 1 #&gt; 17: Micronucleus and sister c 2 #&gt; 18: in vivo comet (mouse) 1 #&gt; 19: in vivo comet (rat) 3 #&gt; 20: DNA Covalent Binding 12 #&gt; 21: In Vivo Chromosome Aberra 4 #&gt; 22: In Vivo Mammalian Mutagen 7 #&gt; assayType N Now we repeat this for NATADB. natadb_genetox_details[, .N, by = .(assayCategory, assayType, assayResult)] #&gt; assayCategory assayType assayResult N #&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: in vitro InVivoMN negative 40 #&gt; 2: ND Overall positive 16 #&gt; 3: in vitro bacterial reverse mutatio positive 93 #&gt; 4: in vivo micronucleus assay negative 76 #&gt; 5: in vivo micronucleus assay equivocal 4 #&gt; --- #&gt; 194: in vitro Heritable translocation t negative 2 #&gt; 195: in vivo mammalian comet assay equivocal 1 #&gt; 196: in vitro mammalian cell gene mutat positive 1 #&gt; 197: in vitro in vitro mammalian cell t positive 1 #&gt; 198: in vitro Mitotic recombination positive 1 natadb_genetox_details[, .N, by = .(assayCategory)] #&gt; assayCategory N #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: in vitro 2112 #&gt; 2: ND 100 #&gt; 3: in vivo 435 Examine the number of rows for each assayType value by each assaycategory value. natadb_genetox_details[assayCategory == &#39;in vitro&#39;, .N, by = .(assayType)] #&gt; assayType N #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: InVivoMN 89 #&gt; 2: bacterial reverse mutatio 362 #&gt; 3: Ames 258 #&gt; 4: InVitroCA 98 #&gt; 5: InVitroMLA 85 #&gt; 6: InVitroMN 20 #&gt; 7: Cell transformation, clon 14 #&gt; 8: Forward and reverse gene 17 #&gt; 9: Histidine reverse gene mu 55 #&gt; 10: Micronucleus test, chromo 33 #&gt; 11: Mitotic recombination or 47 #&gt; 12: Rec-assay, DNA effects (b 34 #&gt; 13: Rec-assay, spot test, DNA 6 #&gt; 14: Sister-chromatid exchange 98 #&gt; 15: DNA damage and repair ass 50 #&gt; 16: Chromosome aberrations 25 #&gt; 17: Gene mutation 20 #&gt; 18: Forward gene mutation at 12 #&gt; 19: Heritable translocation t 10 #&gt; 20: Reverse gene mutation 30 #&gt; 21: Sex-linked recessive leth 26 #&gt; 22: Sister-chromatid exchange 31 #&gt; 23: Bacterial Mutagenesis 41 #&gt; 24: Cytogenetics Other 41 #&gt; 25: Cytotoxicity 20 #&gt; 26: DNA Damage/Repair 27 #&gt; 27: In Vitro Chromosome Aberr 6 #&gt; 28: In Vitro Micronucleus 8 #&gt; 29: In Vivo Non-mammalian Mut 7 #&gt; 30: Mutation 6 #&gt; 31: in vitro mammalian chromo 91 #&gt; 32: Cell transformation, vira 46 #&gt; 33: mammalian cell gene mutat 104 #&gt; 34: Forward and reverse gene 4 #&gt; 35: Spot test, gene mutation 4 #&gt; 36: bacterial forward mutatio 4 #&gt; 37: sister chromatid exchange 50 #&gt; 38: bacteriophage induction i 1 #&gt; 39: DNA damage, gene mutation 1 #&gt; 40: chromosome aberration stu 1 #&gt; 41: in vitro mammalian cell t 2 #&gt; 42: Forward gene mutation at 6 #&gt; 43: Cell transformation 11 #&gt; 44: Forward and reverse gene 7 #&gt; 45: Forward gene mutation at 10 #&gt; 46: chromosome aberration ass 24 #&gt; 47: Drosophila SLRL assay 20 #&gt; 48: Salmonella and Escherichi 4 #&gt; 49: Cytogenetic assay in bone 1 #&gt; 50: Chromosome aberrations in 7 #&gt; 51: Forward gene mutation 18 #&gt; 52: in vitro mammalian cell g 2 #&gt; 53: mouse spot test 7 #&gt; 54: Aneuploidy, whole sex chr 4 #&gt; 55: sister chromatid exchange 7 #&gt; 56: Mouse Lymphoma Forward Mu 1 #&gt; 57: Tryptophan reverse gene m 18 #&gt; 58: bacterial gene mutation a 11 #&gt; 59: yeast forward mutation an 4 #&gt; 60: Micronucleus test in vitr 2 #&gt; 61: mitotic recombination ass 6 #&gt; 62: Aneuploidy, chromosome ab 8 #&gt; 63: cell transformation 2 #&gt; 64: in vitro mammalian cell m 13 #&gt; 65: somatic mutation and reco 3 #&gt; 66: transgenic rodent mutagen 2 #&gt; 67: yeast cytogenetic assay 2 #&gt; 68: in vitro mammalian cell g 2 #&gt; 69: bacterial mutation 1 #&gt; 70: bacterial reverse mutatio 5 #&gt; 71: Aneuploidy, partial sex c 2 #&gt; 72: in vitro chromosome aberr 1 #&gt; 73: Cell transformation, focu 2 #&gt; 74: Forward and reverse gene 1 #&gt; 75: gene mutation assay in fu 5 #&gt; 76: Cell Transformation 1 #&gt; 77: Aneuploidy, sex chromosom 1 #&gt; 78: Forward and reverse gene 1 #&gt; 79: E. coli K-12 DNA repair h 1 #&gt; 80: Chromosomal aberration as 2 #&gt; 81: forward mutation 1 #&gt; 82: mammalian cell gene mutat 1 #&gt; 83: Mitotic recombination 1 #&gt; assayType N natadb_genetox_details[assayCategory == &#39;ND&#39;, .N, by = .(assayType)] #&gt; assayType N #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: Overall 16 #&gt; 2: In vivo carcinogenicity s 66 #&gt; 3: transgenic 18 natadb_genetox_details[assayCategory == &#39;in vivo&#39;, .N, by = .(assayType)] #&gt; assayType N #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: micronucleus assay 105 #&gt; 2: Unscheduled DNA synthesis 27 #&gt; 3: rodent dominant lethal as 31 #&gt; 4: InVivoUDS 33 #&gt; 5: InVivoCA 37 #&gt; 6: Dominant lethal test 14 #&gt; 7: Unscheduled DNA synthesis 5 #&gt; 8: In Vivo Chromosome Aberra 5 #&gt; 9: In Vivo Mammalian Mutagen 6 #&gt; 10: In Vivo Micronucleus 11 #&gt; 11: InVivoDNADamage 23 #&gt; 12: in vivo micronucleus (mou 51 #&gt; 13: Sperm morphology 25 #&gt; 14: DNA Binding 1 #&gt; 15: unscheduled DNA synthesis 18 #&gt; 16: in vivo comet (mouse) 4 #&gt; 17: Chromosome aberrations in 9 #&gt; 18: in vivo comet (rat) 3 #&gt; 19: in vivo micronucleus (rat 9 #&gt; 20: mammalian erythrocyte mic 2 #&gt; 21: heritable translocation a 2 #&gt; 22: Micronucleus and sister c 2 #&gt; 23: bone marrow chromosome ab 1 #&gt; 24: Chromosome aberrations, i 2 #&gt; 25: DNA adduct formation 1 #&gt; 26: DNA Covalent Binding 1 #&gt; 27: mammalian comet assay 6 #&gt; 28: mammalian germ cell cytog 1 #&gt; assayType N Answer to Environmental Health Question 3 From these initial explorations of the data, we can answer Environmental Health Question 3: After pulling the genotoxicity data for the different environmental contaminant data sets, list the assays associated with the chemicals in each data set. How many unique assays are there in each data set? What are the different assay categories and how many unique assays for each assay category are there? Answer: There are 90 unique assays for CCl4 and 114 unique assays for NATADB. The different assay categories are “in vitro”, “ND”, and “in vivo”, with 65 unique “in vitro” assays for CCl4 and 83 for NATADB, 3 unique “ND” assays for CCL4 and 3 for NATADB, and 22 unique “in vivo” assays for CCL4 and 28 for NATADB. Next, we dig into the results of the assays. One may be interested in looking at the number of chemicals for which an assay resulted in a positive or negative result for instance. We group by assayResult and determine the number of unique dtxsid values associated with each assayResult value. ccl4_genetox_details[, .(DTXSIDs = length(unique(dtxsid))), by = .(assayResult)] #&gt; assayResult DTXSIDs #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: negative 64 #&gt; 2: positive 53 #&gt; 3: equivocal 15 natadb_genetox_details[, .(DTXSIDs = length(unique(dtxsid))), by = .(assayResult)] #&gt; assayResult DTXSIDs #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: negative 141 #&gt; 2: positive 130 #&gt; 3: equivocal 48 Answer to Environmental Health Question 4 With this data we may now answer Environmental Health Question 4: The genotoxicity data contains information on which assays have been conducted for different chemicals and the results of those assays. How many chemicals in each data set have a ‘positive’, ‘negative’, and ‘equivocal’ value for the assay result? Answer: For CCL4, there are 64 unique chemicals that have a negative assay result, 53 that have a positive result, and 15 that have an equivocal result. For NATADB, there are 141 unique chemicals that have a negative assay result, 130 that have a positive result, and 48 that have an equivocal result. Observe that since there are 72 unique dtxsid values with assay results in CCL4 and 153 in NATADB, there are several chemicals that have multiple assay results. We now determine the chemicals from each data set that are known to have genotoxic effects. For this, we look to see which chemicals produce at least one positive response in the assayResult column. ccl4_genetox_details[, .(is_positive = any(assayResult == &#39;positive&#39;)), by = .(dtxsid)][is_positive == TRUE, dtxsid] #&gt; [1] &quot;DTXSID0020153&quot; &quot;DTXSID0020573&quot; &quot;DTXSID0020600&quot; &quot;DTXSID0020814&quot; &quot;DTXSID0021464&quot; &quot;DTXSID0021541&quot; #&gt; [7] &quot;DTXSID0024341&quot; &quot;DTXSID1021407&quot; &quot;DTXSID1021740&quot; &quot;DTXSID1021798&quot; &quot;DTXSID1024338&quot; &quot;DTXSID1026164&quot; #&gt; [13] &quot;DTXSID1031040&quot; &quot;DTXSID2021028&quot; &quot;DTXSID2021317&quot; &quot;DTXSID2021731&quot; &quot;DTXSID3020203&quot; &quot;DTXSID3020702&quot; #&gt; [19] &quot;DTXSID3020833&quot; &quot;DTXSID3024869&quot; &quot;DTXSID3031864&quot; &quot;DTXSID4020533&quot; &quot;DTXSID4021503&quot; &quot;DTXSID4022361&quot; #&gt; [25] &quot;DTXSID4022367&quot; &quot;DTXSID5020023&quot; &quot;DTXSID5020576&quot; &quot;DTXSID5020601&quot; &quot;DTXSID5021207&quot; &quot;DTXSID5024182&quot; #&gt; [31] &quot;DTXSID5039224&quot; &quot;DTXSID6020301&quot; &quot;DTXSID6021030&quot; &quot;DTXSID6021032&quot; &quot;DTXSID6022422&quot; &quot;DTXSID7020005&quot; #&gt; [37] &quot;DTXSID7020215&quot; &quot;DTXSID7020637&quot; &quot;DTXSID7021029&quot; &quot;DTXSID8020044&quot; &quot;DTXSID8020090&quot; &quot;DTXSID8020832&quot; #&gt; [43] &quot;DTXSID8021062&quot; &quot;DTXSID8023846&quot; &quot;DTXSID8023848&quot; &quot;DTXSID8025541&quot; &quot;DTXSID8031865&quot; &quot;DTXSID9020243&quot; #&gt; [49] &quot;DTXSID9021390&quot; &quot;DTXSID9021427&quot; &quot;DTXSID9022366&quot; &quot;DTXSID9023380&quot; &quot;DTXSID9023914&quot; natadb_genetox_details[, .(is_positive = any(assayResult == &#39;positive&#39;)), by = .(dtxsid)][is_positive == TRUE, dtxsid] #&gt; [1] &quot;DTXSID0020153&quot; &quot;DTXSID0020448&quot; &quot;DTXSID0020523&quot; &quot;DTXSID0020529&quot; &quot;DTXSID0020600&quot; #&gt; [6] &quot;DTXSID0020868&quot; &quot;DTXSID0021381&quot; &quot;DTXSID0021383&quot; &quot;DTXSID0021541&quot; &quot;DTXSID0021834&quot; #&gt; [11] &quot;DTXSID0021965&quot; &quot;DTXSID0024187&quot; &quot;DTXSID0039227&quot; &quot;DTXSID0039229&quot; &quot;DTXSID1020148&quot; #&gt; [16] &quot;DTXSID1020302&quot; &quot;DTXSID1020306&quot; &quot;DTXSID1020431&quot; &quot;DTXSID1020512&quot; &quot;DTXSID1020516&quot; #&gt; [21] &quot;DTXSID1020566&quot; &quot;DTXSID1021374&quot; &quot;DTXSID1021798&quot; &quot;DTXSID1021827&quot; &quot;DTXSID1022057&quot; #&gt; [26] &quot;DTXSID1023786&quot; &quot;DTXSID1024045&quot; &quot;DTXSID1026164&quot; &quot;DTXSID1049641&quot; &quot;DTXSID2020137&quot; #&gt; [31] &quot;DTXSID2020262&quot; &quot;DTXSID2020507&quot; &quot;DTXSID2020682&quot; &quot;DTXSID2020844&quot; &quot;DTXSID2021284&quot; #&gt; [36] &quot;DTXSID2021286&quot; &quot;DTXSID2021319&quot; &quot;DTXSID2021658&quot; &quot;DTXSID2021731&quot; &quot;DTXSID2021781&quot; #&gt; [41] &quot;DTXSID3020203&quot; &quot;DTXSID3020257&quot; &quot;DTXSID3020413&quot; &quot;DTXSID3020415&quot; &quot;DTXSID3020596&quot; #&gt; [46] &quot;DTXSID3020679&quot; &quot;DTXSID3020702&quot; &quot;DTXSID3020833&quot; &quot;DTXSID3021431&quot; &quot;DTXSID3025091&quot; #&gt; [51] &quot;DTXSID3039242&quot; &quot;DTXSID4020161&quot; &quot;DTXSID4020298&quot; &quot;DTXSID4020402&quot; &quot;DTXSID4020533&quot; #&gt; [56] &quot;DTXSID4020583&quot; &quot;DTXSID4020874&quot; &quot;DTXSID4020901&quot; &quot;DTXSID4021006&quot; &quot;DTXSID4021056&quot; #&gt; [61] &quot;DTXSID4021395&quot; &quot;DTXSID4039231&quot; &quot;DTXSID5020023&quot; &quot;DTXSID5020027&quot; &quot;DTXSID5020029&quot; #&gt; [66] &quot;DTXSID5020071&quot; &quot;DTXSID5020316&quot; &quot;DTXSID5020449&quot; &quot;DTXSID5020491&quot; &quot;DTXSID5020601&quot; #&gt; [71] &quot;DTXSID5020607&quot; &quot;DTXSID5020865&quot; &quot;DTXSID5021124&quot; &quot;DTXSID5021207&quot; &quot;DTXSID5021380&quot; #&gt; [76] &quot;DTXSID5021386&quot; &quot;DTXSID5024055&quot; &quot;DTXSID5024059&quot; &quot;DTXSID5039224&quot; &quot;DTXSID6020145&quot; #&gt; [81] &quot;DTXSID6020307&quot; &quot;DTXSID6020353&quot; &quot;DTXSID6020432&quot; &quot;DTXSID6020438&quot; &quot;DTXSID6020515&quot; #&gt; [86] &quot;DTXSID6020569&quot; &quot;DTXSID6020981&quot; &quot;DTXSID6021828&quot; &quot;DTXSID6022422&quot; &quot;DTXSID6023947&quot; #&gt; [91] &quot;DTXSID6023949&quot; &quot;DTXSID7020005&quot; &quot;DTXSID7020009&quot; &quot;DTXSID7020267&quot; &quot;DTXSID7020637&quot; #&gt; [96] &quot;DTXSID7020689&quot; &quot;DTXSID7020710&quot; &quot;DTXSID7020716&quot; &quot;DTXSID7021029&quot; &quot;DTXSID7021100&quot; #&gt; [101] &quot;DTXSID7021106&quot; &quot;DTXSID7021318&quot; &quot;DTXSID7021360&quot; &quot;DTXSID7021368&quot; &quot;DTXSID7021948&quot; #&gt; [106] &quot;DTXSID7024166&quot; &quot;DTXSID7024370&quot; &quot;DTXSID7024532&quot; &quot;DTXSID7025180&quot; &quot;DTXSID7026156&quot; #&gt; [111] &quot;DTXSID8020090&quot; &quot;DTXSID8020173&quot; &quot;DTXSID8020250&quot; &quot;DTXSID8020599&quot; &quot;DTXSID8020759&quot; #&gt; [116] &quot;DTXSID8020832&quot; &quot;DTXSID8021195&quot; &quot;DTXSID8021197&quot; &quot;DTXSID8021432&quot; &quot;DTXSID8021434&quot; #&gt; [121] &quot;DTXSID8021438&quot; &quot;DTXSID8024286&quot; &quot;DTXSID9020168&quot; &quot;DTXSID9020243&quot; &quot;DTXSID9020247&quot; #&gt; [126] &quot;DTXSID9020293&quot; &quot;DTXSID9020827&quot; &quot;DTXSID9021138&quot; &quot;DTXSID9021261&quot; &quot;DTXSID9041522&quot; With so much genotoxicity data, let us explore this data for one chemical more deeply to get a sense of the assays and results present for it. We will explore the chemical with DTXSID0020153. We will look at the assays, the number of each type of result, and which correspond to “positive” results. To determine this, we group by assayResult and calculate .N for each group. We also isolate which were positive and output a data.table with the number of each type. ccl4_genetox_details[dtxsid == &#39;DTXSID0020153&#39;, .(Number = .N), by = .(assayResult)] #&gt; assayResult Number #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: negative 5 #&gt; 2: positive 22 #&gt; 3: equivocal 1 ccl4_genetox_details[dtxsid == &#39;DTXSID0020153&#39; &amp; assayResult == &#39;positive&#39;, .(Number_of_assays = .N), by = .(assayType)][order(-Number_of_assays),] #&gt; assayType Number_of_assays #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: bacterial reverse mutatio 3 #&gt; 2: Ames 3 #&gt; 3: InVitroCA 2 #&gt; 4: InVitroMLA 2 #&gt; 5: Rec-assay, DNA effects (b 2 #&gt; 6: Sister-chromatid exchange 2 #&gt; 7: Overall 1 #&gt; 8: InVitroMN 1 #&gt; 9: Cell transformation, clon 1 #&gt; 10: Histidine reverse gene mu 1 #&gt; 11: Mitotic recombination or 1 #&gt; 12: Rec-assay, spot test, DNA 1 #&gt; 13: Unscheduled DNA synthesis 1 #&gt; 14: In vivo carcinogenicity s 1 Answer to Environmental Health Question 5 With these data.tables, we may answer Environmental Health Question 5: Based on the genotoxicity data reported for the chemical with DTXSID identifier DTXSID0020153, how many assays resulted in a positive/equivocal/negative value? Which of the assays were positive and how many of each were there for the most reported assays? Answer: There were five assays that produced a negative result, 22 that produced a positive result, and one that produced an equivocal result. Of the 22 positive assays, “bacterial reverse mutation assay” and “Ames” were the most numerous, with three each. Hazard Resource Finally, we examine the hazard data associated with the chemicals in each data set. For each chemical, there will be potentially hundreds of rows of hazard data, so the returned results will be much larger than in most other API endpoints. ccl4_hazard &lt;- get_hazard_by_dtxsid_batch(DTXSID = ccl4$dtxsid) natadb_hazard &lt;- get_hazard_by_dtxsid_batch(DTXSID = natadb$dtxsid) We do some preliminary exploration of the data. First we determine the dimensions of the data sets. dim(ccl4_hazard) #&gt; [1] 39541 33 dim(natadb_hazard) #&gt; [1] 68500 33 Next we record the column names and display the first six results in the CCL4 hazard data. colnames(ccl4_hazard) #&gt; [1] &quot;id&quot; &quot;source&quot; &quot;year&quot; #&gt; [4] &quot;dtxsid&quot; &quot;exposureRoute&quot; &quot;toxvalNumeric&quot; #&gt; [7] &quot;toxvalNumericQualifier&quot; &quot;toxvalUnits&quot; &quot;studyType&quot; #&gt; [10] &quot;studyDurationClass&quot; &quot;studyDuractionValue&quot; &quot;studyDurationUnits&quot; #&gt; [13] &quot;strain&quot; &quot;sex&quot; &quot;population&quot; #&gt; [16] &quot;exposureMethod&quot; &quot;exposureForm&quot; &quot;media&quot; #&gt; [19] &quot;lifestage&quot; &quot;generation&quot; &quot;criticalEffect&quot; #&gt; [22] &quot;detailText&quot; &quot;supercategory&quot; &quot;speciesCommon&quot; #&gt; [25] &quot;humanEcoNt&quot; &quot;priorityId&quot; &quot;subsource&quot; #&gt; [28] &quot;sourceUrl&quot; &quot;subsourceUrl&quot; &quot;riskAssessmentClass&quot; #&gt; [31] &quot;toxvalType&quot; &quot;toxvalSubtype&quot; &quot;studyDurationValue&quot; head(ccl4_hazard) #&gt; id source year dtxsid exposureRoute toxvalNumeric toxvalNumericQualifier #&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: 150792 COSMOS 1978 DTXSID7020215 - 18.02470 = #&gt; 2: 11820 COSMOS 1986 DTXSID5039224 - 7.48901 = #&gt; 3: 11603 COSMOS 1980 DTXSID5039224 - 440.53000 = #&gt; 4: 150947 COSMOS 1980 DTXSID7020215 - 0.75000 = #&gt; 5: 150950 COSMOS 1978 DTXSID7020215 - 0.75000 = #&gt; 6: 150945 COSMOS 1979 DTXSID7020215 - 90123.50000 = #&gt; 26 variable(s) not shown: [toxvalUnits &lt;char&gt;, studyType &lt;char&gt;, studyDurationClass &lt;char&gt;, studyDuractionValue &lt;num&gt;, studyDurationUnits &lt;char&gt;, strain &lt;char&gt;, sex &lt;char&gt;, population &lt;char&gt;, exposureMethod &lt;char&gt;, exposureForm &lt;char&gt;, ...] We determine the number of unique values in the criticalEffect, supercategory, and toxvalType columns for each data set. The number of unique values for criticalEffect. length(ccl4_hazard[, unique(criticalEffect)]) #&gt; [1] 3252 length(natadb_hazard[, unique(criticalEffect)]) #&gt; [1] 2802 The number of unique values of supercategory. length(ccl4_hazard[, unique(supercategory)]) #&gt; [1] 12 length(natadb_hazard[, unique(supercategory)]) #&gt; [1] 12 The number of unique values for toxvalType. length(ccl4_hazard[, unique(toxvalType)]) #&gt; [1] 207 length(natadb_hazard[, unique(toxvalType)]) #&gt; [1] 463 Now we look at the number of entries per supercategory. ccl4_hazard[, .N, by = .(supercategory)] #&gt; supercategory N #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: Point of Departure 23011 #&gt; 2: Screening Level 564 #&gt; 3: Effect Concentration 3098 #&gt; 4: Effect Level 743 #&gt; 5: Lethality Effect Level 8119 #&gt; 6: other 1084 #&gt; 7: Inhibition Concentration 356 #&gt; 8: Effective Residue Level 14 #&gt; 9: Effect Dose 5 #&gt; 10: Effect Time 250 #&gt; 11: Exposure Limit 648 #&gt; 12: Toxicity Value 1649 natadb_hazard[, .N, by = .(supercategory)] #&gt; supercategory N #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: Point of Departure 22524 #&gt; 2: Screening Level 2012 #&gt; 3: Lethality Effect Level 24536 #&gt; 4: Effect Dose 19 #&gt; 5: Effect Concentration 6538 #&gt; 6: Effect Level 1393 #&gt; 7: Exposure Limit 2271 #&gt; 8: Inhibition Concentration 556 #&gt; 9: other 2760 #&gt; 10: Effective Residue Level 18 #&gt; 11: Effect Time 1043 #&gt; 12: Toxicity Value 4830 With over 22,000 results for the supercategory value “Point of Departure” for each data set, we dig into this further. We determine the number of rows grouped by toxvalType that have the “Point of Departure” supercategory value, and display this descending. ccl4_hazard[ supercategory %in% &#39;Point of Departure&#39;, .N, by = .(toxvalType)][order(-N),] #&gt; toxvalType N #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: NOEC 8295 #&gt; 2: LOEC 6092 #&gt; 3: NOAEL 1858 #&gt; 4: LEL 1519 #&gt; 5: LOAEL 1484 #&gt; 6: NOEL 1211 #&gt; 7: NEL 842 #&gt; 8: LOEL 688 #&gt; 9: NOAEC 365 #&gt; 10: LOAEC 180 #&gt; 11: HNEL 130 #&gt; 12: BMDL 101 #&gt; 13: BMD 71 #&gt; 14: BMDL (10) 69 #&gt; 15: LEC 30 #&gt; 16: BMCL 26 #&gt; 17: BMDL (05) 15 #&gt; 18: BMCL (10) 9 #&gt; 19: POD (screening chronic) 6 #&gt; 20: POD (screening subchronic 5 #&gt; 21: BMCL (10 HEC) 3 #&gt; 22: BMC (05) 2 #&gt; 23: NEC 2 #&gt; 24: BMDL (1SD) 2 #&gt; 25: BMCL (HEC) 2 #&gt; 26: BMC (20) 1 #&gt; 27: BMDL (ADJ) 1 #&gt; 28: BMDL (10 HED) 1 #&gt; 29: BMDL (05 HED) 1 #&gt; toxvalType N natadb_hazard[ supercategory %in% &#39;Point of Departure&#39;, .N, by = .(toxvalType)][order(-N),] #&gt; toxvalType N #&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: NOEC 7329 #&gt; 2: LOEC 4412 #&gt; 3: NOAEL 3150 #&gt; 4: LOAEL 1778 #&gt; 5: NOAEC 1037 #&gt; 6: LEL 1031 #&gt; 7: NOEL 934 #&gt; 8: NEL 668 #&gt; 9: LOEL 638 #&gt; 10: LOAEC 492 #&gt; 11: BMDL 360 #&gt; 12: BMD 245 #&gt; 13: HNEL 219 #&gt; 14: BMDL (10) 51 #&gt; 15: BMCL 50 #&gt; 16: LEC 43 #&gt; 17: BMCL (10 HEC) 13 #&gt; 18: POD (screening subchronic 11 #&gt; 19: BMDL (10 HED) 10 #&gt; 20: BMCL (10) 9 #&gt; 21: POD (screening chronic) 9 #&gt; 22: BMC 7 #&gt; 23: BMDL (1SD) 6 #&gt; 24: BMDL (05) 5 #&gt; 25: BMC (20) 3 #&gt; 26: BMC (10 HEC) 3 #&gt; 27: LOAEL (HED) 3 #&gt; 28: NEC 2 #&gt; 29: BMDL (HED) 2 #&gt; 30: BMDL (05 HEC) 1 #&gt; 31: BMD (2x-ADJ) 1 #&gt; 32: BMCL (HEC) 1 #&gt; 33: BMCL (Adjusted HE) 1 #&gt; toxvalType N We explore “NOEC” and “LOEC” further. Let us look at the the case when media value is either “salt water” or “fresh water”. For this, we will recover the minimum value of “NOEC” and “LOEC” for each chemical in each data set. First, we look at soil. We order by toxvalType and by the minimum toxvalNumeric value in each group, descending. ccl4_hazard[media %in% &#39;salt water&#39; &amp; toxvalType %in% c(&#39;LOEC&#39;, &#39;NOEC&#39;), .(toxvalNumeric = min(toxvalNumeric)), by = .(toxvalType, toxvalUnits, dtxsid)][order(toxvalType, -toxvalNumeric)] #&gt; toxvalType toxvalUnits dtxsid toxvalNumeric #&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; #&gt; 1: LOEC mg/m3 DTXSID2021731 1.18556e+07 #&gt; 2: LOEC mg/m3 DTXSID8020597 5.00000e+06 #&gt; 3: LOEC mg/m3 DTXSID4022991 1.51000e+04 #&gt; 4: LOEC mg/m3 DTXSID1021740 1.00000e+04 #&gt; 5: LOEC mg/m3 DTXSID9024142 1.90000e+03 #&gt; 6: LOEC mg/m3 DTXSID8023846 1.40000e+03 #&gt; 7: LOEC mg/m3 DTXSID1024207 1.00000e+03 #&gt; 8: LOEC mL/m3 DTXSID7020637 2.00000e+02 #&gt; 9: LOEC mg/kg-day DTXSID8031865 1.00000e+02 #&gt; 10: LOEC mg/m3 DTXSID3074313 9.37000e+01 #&gt; 11: LOEC ppm DTXSID7020637 5.50000e+01 #&gt; 12: LOEC mg/m3 DTXSID9032113 4.30000e+01 #&gt; 13: LOEC mg/m3 DTXSID4034948 2.20000e+01 #&gt; 14: LOEC mg/m3 DTXSID4022448 1.50000e+01 #&gt; 15: LOEC % v/v DTXSID8020597 1.00000e+01 #&gt; 16: LOEC mg/m3 DTXSID5020023 8.00000e+00 #&gt; 17: LOEC mg/m3 DTXSID9023914 6.00000e+00 #&gt; 18: LOEC mg/m3 DTXSID7020637 5.00000e+00 #&gt; 19: LOEC % v/v DTXSID2021731 5.00000e+00 #&gt; 20: LOEC % DTXSID2021731 4.00000e+00 #&gt; 21: LOEC mg/m3 DTXSID3021857 1.00000e+00 #&gt; 22: LOEC mg/m3 DTXSID6024177 1.00000e+00 #&gt; 23: LOEC mg/kg-day DTXSID5020576 1.00000e+00 #&gt; 24: LOEC mg/m3 DTXSID2040282 1.00000e+00 #&gt; 25: LOEC mg/m3 DTXSID4032611 6.20000e-01 #&gt; 26: LOEC mg/m3 DTXSID4022367 4.84000e-01 #&gt; 27: LOEC mg/m3 DTXSID1024174 3.40000e-01 #&gt; 28: LOEC mg/m3 DTXSID3032464 3.00000e-01 #&gt; 29: LOEC nM/g DTXSID0020573 3.00000e-01 #&gt; 30: LOEC mg/m3 DTXSID3031864 1.00000e-01 #&gt; 31: LOEC mg/m3 DTXSID1021409 9.00000e-02 #&gt; 32: LOEC mg/m3 DTXSID8031865 8.10000e-02 #&gt; 33: LOEC mg/kg-day DTXSID3021857 5.00000e-02 #&gt; 34: LOEC mg/m3 DTXSID8022292 5.00000e-02 #&gt; 35: LOEC mg/m3 DTXSID4022361 3.00000e-02 #&gt; 36: LOEC mL/L DTXSID8020597 2.60000e-02 #&gt; 37: LOEC mg/m3 DTXSID0020446 1.00000e-02 #&gt; 38: LOEC mg/kg-day DTXSID0020573 3.50000e-03 #&gt; 39: LOEC mg/m3 DTXSID0020573 3.00000e-03 #&gt; 40: LOEC ml/g DTXSID7020637 1.00000e-03 #&gt; 41: LOEC mg/m3 DTXSID5020576 1.00000e-03 #&gt; 42: LOEC mg/kg-day DTXSID3032464 3.50000e-04 #&gt; 43: LOEC L DTXSID0020573 2.00000e-04 #&gt; 44: NOEC mg/m3 DTXSID1024207 1.00000e+05 #&gt; 45: NOEC mg/m3 DTXSID2040282 3.00000e+04 #&gt; 46: NOEC mg/m3 DTXSID1021740 1.00000e+04 #&gt; 47: NOEC mg/m3 DTXSID4022991 4.90000e+03 #&gt; 48: NOEC mg/m3 DTXSID9024142 1.90000e+03 #&gt; 49: NOEC mg/m3 DTXSID8025541 1.00000e+03 #&gt; 50: NOEC mg/m3 DTXSID8023846 5.80000e+02 #&gt; 51: NOEC mg/m3 DTXSID3020964 3.60000e+02 #&gt; 52: NOEC mL/m3 DTXSID7020637 2.00000e+02 #&gt; 53: NOEC mg/m3 DTXSID2021731 1.00000e+02 #&gt; 54: NOEC mg/kg-day DTXSID8031865 1.00000e+02 #&gt; 55: NOEC mg/m3 DTXSID3074313 9.37000e+01 #&gt; 56: NOEC mg/kg-day DTXSID3021857 2.50000e+01 #&gt; 57: NOEC mg/m3 DTXSID4034948 2.20000e+01 #&gt; 58: NOEC ppm DTXSID7020637 2.00000e+01 #&gt; 59: NOEC mg/m3 DTXSID9032113 1.90000e+01 #&gt; 60: NOEC mg/m3 DTXSID6024177 1.00000e+01 #&gt; 61: NOEC mg/m3 DTXSID5020023 8.00000e+00 #&gt; 62: NOEC mg/m3 DTXSID7020637 5.00000e+00 #&gt; 63: NOEC % v/v DTXSID8020597 5.00000e+00 #&gt; 64: NOEC mg/m3 DTXSID9020243 3.10000e+00 #&gt; 65: NOEC mg/m3 DTXSID9023914 3.00000e+00 #&gt; 66: NOEC mg/kg-day DTXSID4022367 2.50000e+00 #&gt; 67: NOEC mg/m3 DTXSID4022448 2.00000e+00 #&gt; 68: NOEC % sat DTXSID7020637 2.00000e+00 #&gt; 69: NOEC mg/kg-day DTXSID5020576 1.00000e+00 #&gt; 70: NOEC nM/g DTXSID0020573 5.00000e-01 #&gt; 71: NOEC mg/m3 DTXSID4032611 3.60000e-01 #&gt; 72: NOEC mg/m3 DTXSID1024174 3.40000e-01 #&gt; 73: NOEC mg/m3 DTXSID4022361 3.00000e-01 #&gt; 74: NOEC mg/m3 DTXSID0021464 2.65000e-01 #&gt; 75: NOEC mg/m3 DTXSID3021857 1.80000e-01 #&gt; 76: NOEC mg/m3 DTXSID4022367 1.00000e-01 #&gt; 77: NOEC mg/m3 DTXSID3031864 1.00000e-01 #&gt; 78: NOEC mg/m3 DTXSID3032464 1.00000e-01 #&gt; 79: NOEC mg/m3 DTXSID1021409 4.70000e-02 #&gt; 80: NOEC mg/m3 DTXSID8022292 2.50000e-02 #&gt; 81: NOEC % v/v DTXSID2021731 2.00000e-02 #&gt; 82: NOEC mg/m3 DTXSID8031865 1.00000e-02 #&gt; 83: NOEC % DTXSID2021731 1.00000e-02 #&gt; 84: NOEC mg/m3 DTXSID0020446 2.00000e-03 #&gt; 85: NOEC mg/m3 DTXSID0020573 1.00000e-03 #&gt; 86: NOEC ml/g DTXSID7020637 1.00000e-03 #&gt; 87: NOEC ppm DTXSID3020702 5.00000e-04 #&gt; 88: NOEC mg/kg-day DTXSID3032464 2.20000e-04 #&gt; 89: NOEC L DTXSID0020573 2.00000e-04 #&gt; 90: NOEC mg/m3 DTXSID5020576 1.00000e-04 #&gt; 91: NOEC mg/kg-day DTXSID0020573 4.00000e-05 #&gt; toxvalType toxvalUnits dtxsid toxvalNumeric natadb_hazard[media %in% &#39;salt water&#39; &amp; toxvalType %in% c(&#39;LOEC&#39;, &#39;NOEC&#39;), .(toxvalNumeric = min(toxvalNumeric)), by = .(toxvalType, toxvalUnits, dtxsid)][order(toxvalType, -toxvalNumeric)] #&gt; toxvalType toxvalUnits dtxsid toxvalNumeric #&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; #&gt; 1: LOEC mg/m3 DTXSID2021731 1.18556e+07 #&gt; 2: LOEC mg/m3 DTXSID8020597 5.00000e+06 #&gt; 3: LOEC mg/m3 DTXSID7026156 5.08300e+05 #&gt; 4: LOEC mg/m3 DTXSID8020759 1.56000e+05 #&gt; 5: LOEC mg/m3 DTXSID3020415 2.50000e+04 #&gt; --- #&gt; 130: NOEC mg/m3 DTXSID9020827 5.00000e-03 #&gt; 131: NOEC mg/m3 DTXSID7021100 3.10000e-03 #&gt; 132: NOEC ml/g DTXSID7020637 1.00000e-03 #&gt; 133: NOEC ppm DTXSID3020702 5.00000e-04 #&gt; 134: NOEC mg/m3 DTXSID5020607 1.00000e-04 Next we look at fresh water, repeating the same grouping and ordering as in the previous case. ccl4_hazard[media %in% &#39;fresh water&#39; &amp; toxvalType %in% c(&#39;LOEC&#39;, &#39;NOEC&#39;), .(toxvalNumeric = min(toxvalNumeric)), by = .(toxvalType, toxvalUnits, dtxsid)][order(toxvalType, -toxvalNumeric)] #&gt; toxvalType toxvalUnits dtxsid toxvalNumeric #&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; #&gt; 1: LOEC mg/m3 DTXSID7020005 6.20000e+06 #&gt; 2: LOEC mg/m3 DTXSID4020533 5.75000e+05 #&gt; 3: LOEC mg/m3 DTXSID5024182 5.00000e+05 #&gt; 4: LOEC mg/m3 DTXSID1020437 5.00000e+05 #&gt; 5: LOEC mg/m3 DTXSID3020833 4.40748e+05 #&gt; --- #&gt; 185: NOEC % vol DTXSID2021731 4.00000e-04 #&gt; 186: NOEC mg/m3 DTXSID5020576 2.96409e-05 #&gt; 187: NOEC mg/m3 DTXSID0020573 2.70000e-05 #&gt; 188: NOEC L DTXSID2021731 1.00000e-06 #&gt; 189: NOEC mg/kg-day DTXSID001024118 6.00000e-08 natadb_hazard[media %in% &#39;fresh water&#39; &amp; toxvalType %in% c(&#39;LOEC&#39;, &#39;NOEC&#39;), .(toxvalNumeric = min(toxvalNumeric)), by = .(toxvalType, toxvalUnits, dtxsid)][order(toxvalType, -toxvalNumeric)] #&gt; toxvalType toxvalUnits dtxsid toxvalNumeric #&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; #&gt; 1: LOEC mg/m3 DTXSID7020005 6.200e+06 #&gt; 2: LOEC mg/m3 DTXSID6020515 3.000e+06 #&gt; 3: LOEC mg/m3 DTXSID6020981 6.125e+05 #&gt; 4: LOEC mg/m3 DTXSID4020533 5.750e+05 #&gt; 5: LOEC mg/m3 DTXSID1020437 5.000e+05 #&gt; --- #&gt; 284: NOEC % DTXSID2021731 5.000e-04 #&gt; 285: NOEC % vol DTXSID2021731 4.000e-04 #&gt; 286: NOEC L DTXSID0021383 2.340e-04 #&gt; 287: NOEC L DTXSID2021319 2.300e-05 #&gt; 288: NOEC L DTXSID2021731 1.000e-06 Now, let us restrict our attention to human hazard and focus on the exposure routes given by inhalation and oral. First, let us determine the exposure routes in general. ccl4_hazard[humanEcoNt %in% &#39;human health&#39;, unique(exposureRoute)] #&gt; [1] &quot;oral&quot; &quot;dermal&quot; &quot;inhalation&quot; &quot;radiation&quot; #&gt; [5] &quot;-&quot; &quot;soil&quot; &quot;in vitro&quot; &quot;intraperitoneal&quot; #&gt; [9] &quot;inhalation, dermal&quot; &quot;subcutaneous&quot; &quot;intramuscular&quot; &quot;injection&quot; #&gt; [13] &quot;implant&quot; &quot;minipump&quot; &quot;intraveneous&quot; natadb_hazard[humanEcoNt %in% &#39;human health&#39;, unique(exposureRoute)] #&gt; [1] &quot;dermal&quot; &quot;inhalation&quot; &quot;oral&quot; #&gt; [4] &quot;-&quot; &quot;soil&quot; &quot;tdermal&quot; #&gt; [7] &quot;injection&quot; &quot;subcutaneous&quot; &quot;intravenous&quot; #&gt; [10] &quot;intraperitoneal, subcutaneous&quot; &quot;inhalation, dermal&quot; &quot;other&quot; Then, let’s focus on the inhalation and oral exposure routes for human hazard. To answer this, filter the data into the corresponding exposure routes, then group by exposureRoute and riskAssessmentClass, and finally count the number of instances for each grouping. To determine the most represented class, one can order the results descending. ccl4_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% c(&#39;inhalation&#39;, &#39;oral&#39;), .(Hits = .N), by = .(exposureRoute, riskAssessmentClass)][order(exposureRoute, -Hits)] #&gt; exposureRoute riskAssessmentClass Hits #&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: inhalation acute 705 #&gt; 2: inhalation air quality standard 580 #&gt; 3: inhalation chronic 489 #&gt; 4: inhalation subchronic 321 #&gt; 5: inhalation short-term 236 #&gt; 6: inhalation reproduction 110 #&gt; 7: inhalation water quality standard 90 #&gt; 8: inhalation developmental 41 #&gt; 9: inhalation neurotoxicity 34 #&gt; 10: inhalation repeat dose other 32 #&gt; 11: inhalation immunotoxicity 14 #&gt; 12: inhalation clinical 4 #&gt; 13: oral chronic 2332 #&gt; 14: oral subchronic 1081 #&gt; 15: oral reproduction 840 #&gt; 16: oral acute 759 #&gt; 17: oral short-term 512 #&gt; 18: oral developmental 451 #&gt; 19: oral water quality standard 332 #&gt; 20: oral uterotrophic 78 #&gt; 21: oral drinking water standard 62 #&gt; 22: oral neurotoxicity 55 #&gt; 23: oral Hershberger 38 #&gt; 24: oral exposure limit 20 #&gt; 25: oral repeat dose other 16 #&gt; 26: oral special toxicology study 13 #&gt; 27: oral clinical 8 #&gt; 28: oral neurotoxicity subchronic 4 #&gt; 29: oral immunotoxicity 4 #&gt; 30: oral dose selection 3 #&gt; 31: oral reproduction developmenta 3 #&gt; 32: oral human 2 #&gt; 33: oral neurotoxicity short-term 1 #&gt; 34: oral neurotoxicity acute 1 #&gt; exposureRoute riskAssessmentClass Hits natadb_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% c(&#39;inhalation&#39;, &#39;oral&#39;), .(Hits = .N), by = .(exposureRoute, riskAssessmentClass)][order(exposureRoute, -Hits)] #&gt; exposureRoute riskAssessmentClass Hits #&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: inhalation acute 2125 #&gt; 2: inhalation air quality standard 1865 #&gt; 3: inhalation chronic 1665 #&gt; 4: inhalation subchronic 738 #&gt; 5: inhalation short-term 481 #&gt; 6: inhalation water quality standard 177 #&gt; 7: inhalation reproduction 150 #&gt; 8: inhalation repeat dose other 83 #&gt; 9: inhalation developmental 74 #&gt; 10: inhalation neurotoxicity 73 #&gt; 11: inhalation immunotoxicity 23 #&gt; 12: inhalation clinical 12 #&gt; 13: inhalation reproduction developmenta 5 #&gt; 14: inhalation repeat dose 2 #&gt; 15: oral chronic 3705 #&gt; 16: oral acute 1746 #&gt; 17: oral subchronic 1512 #&gt; 18: oral water quality standard 1043 #&gt; 19: oral short-term 598 #&gt; 20: oral developmental 289 #&gt; 21: oral drinking water standard 226 #&gt; 22: oral reproduction 199 #&gt; 23: oral repeat dose other 49 #&gt; 24: oral neurotoxicity 41 #&gt; 25: oral uterotrophic 27 #&gt; 26: oral special toxicology study 20 #&gt; 27: oral exposure limit 12 #&gt; 28: oral dose selection 11 #&gt; 29: oral Hershberger 11 #&gt; 30: oral clinical 8 #&gt; 31: oral neurotoxicity subchronic 6 #&gt; 32: oral immunotoxicity 6 #&gt; 33: oral neurotoxicity short-term 5 #&gt; 34: oral reproduction developmenta 4 #&gt; 35: oral neurotoxicity acute 2 #&gt; 36: oral mortality 2 #&gt; 37: oral repeat dose 2 #&gt; 38: oral air quality standard 1 #&gt; exposureRoute riskAssessmentClass Hits Answer to Environmental Health Question 6 With these results we may answer Environmental Health Question 6: After pulling the hazard data for the different data sets, list the different exposure routes for which there is data. What are the unique risk assessment classes for hazard values for the oral route and for the inhalation exposure route? For each such exposure route, which risk assessment class is most represented by the data sets? Answer: We listed the general exposure routes above for the hazard data associated with the chemicals in each data set. Restricting our attention to human hazard data, the “acute” riskAssessmentClass is most represented by the inhalation exposure route and “chronic” for the oral exposure route for both the CCL4 and NATADB data sets. We now drill down a little further before moving into a different path for data exploration. We explore the different types of toxicity values present in each data set for the inhalation and oral exposure routes, and then see which of these are common to both exposure routes for each data set. To answer this, we filter the rows to the “human health” humanEcoNT value and “inhalation” or “oral” exposureRoute value. Then we return the unique values that toxvalType takes. First we look at CCL4. ccl4_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% c(&#39;inhalation&#39;), unique(toxvalType)] #&gt; [1] &quot;cancer unit risk&quot; &quot;RfC&quot; #&gt; [3] &quot;RfD&quot; &quot;cancer slope factor&quot; #&gt; [5] &quot;NOAEL&quot; &quot;MRL&quot; #&gt; [7] &quot;OEHHA MADL&quot; &quot;REL&quot; #&gt; [9] &quot;OEHHA NSRL&quot; &quot;LC50&quot; #&gt; [11] &quot;MEG&quot; &quot;PAC-2&quot; #&gt; [13] &quot;PAC-1&quot; &quot;PAC-3&quot; #&gt; [15] &quot;-&quot; &quot;NOAEC&quot; #&gt; [17] &quot;LOAEC&quot; &quot;NOEC&quot; #&gt; [19] &quot;LOEL&quot; &quot;LC100&quot; #&gt; [21] &quot;LC0&quot; &quot;LCLo&quot; #&gt; [23] &quot;TClo&quot; &quot;LEC&quot; #&gt; [25] &quot;LOAEL&quot; &quot;LC&quot; #&gt; [27] &quot;BMCL (10)&quot; &quot;NEL&quot; #&gt; [29] &quot;NOEL&quot; &quot;LT50&quot; #&gt; [31] &quot;IC50&quot; &quot;LOEC&quot; #&gt; [33] &quot;RD50&quot; &quot;LEL&quot; #&gt; [35] &quot;MT0&quot; &quot;LD&quot; #&gt; [37] &quot;AEGL&quot; &quot;tolerable concentration in air&quot; #&gt; [39] &quot;BMDL (1SD)&quot; &quot;BMCL (HEC)&quot; #&gt; [41] &quot;BMCL (10 HEC)&quot; &quot;IDLH&quot; #&gt; [43] &quot;air contaminant limit&quot; &quot;medium spec. conc.&quot; #&gt; [45] &quot;POD (screening chronic)&quot; &quot;POD (screening subchronic)&quot; #&gt; [47] &quot;BMDL&quot; &quot;BMCL&quot; #&gt; [49] &quot;SRfCi&quot; &quot;BMD&quot; ccl4_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% c(&#39;oral&#39;), unique(toxvalType)] #&gt; [1] &quot;cancer slope factor&quot; &quot;RfD&quot; &quot;NOAEL&quot; #&gt; [4] &quot;MRL&quot; &quot;LOAEL&quot; &quot;MCL California&quot; #&gt; [7] &quot;OEHHA PHG&quot; &quot;OEHHA MADL&quot; &quot;MCL&quot; #&gt; [10] &quot;REL&quot; &quot;LD50&quot; &quot;BMDL&quot; #&gt; [13] &quot;LEL&quot; &quot;HNEL&quot; &quot;MEG&quot; #&gt; [16] &quot;LD0&quot; &quot;-&quot; &quot;NOEL&quot; #&gt; [19] &quot;LOEL&quot; &quot;LD100&quot; &quot;ALD&quot; #&gt; [22] &quot;LEC&quot; &quot;BMDL (10)&quot; &quot;ND50&quot; #&gt; [25] &quot;TClo&quot; &quot;UL&quot; &quot;TDI&quot; #&gt; [28] &quot;BMDL (ADJ)&quot; &quot;BMDL (05)&quot; &quot;BMDL (10 HED)&quot; #&gt; [31] &quot;BMDL (1SD)&quot; &quot;BMDL (05 HED)&quot; &quot;health advisory&quot; #&gt; [34] &quot;DWEL&quot; &quot;medium spec. conc.&quot; &quot;POD (screening subchronic)&quot; #&gt; [37] &quot;POD (screening chronic)&quot; &quot;SRfDo&quot; &quot;NEL&quot; #&gt; [40] &quot;HBSL&quot; &quot;HHBP&quot; &quot;BMD&quot; intersect(ccl4_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% &#39;inhalation&#39;, unique(toxvalType)], ccl4_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% &#39;oral&#39;, unique(toxvalType)]) #&gt; [1] &quot;RfD&quot; &quot;cancer slope factor&quot; &quot;NOAEL&quot; #&gt; [4] &quot;MRL&quot; &quot;OEHHA MADL&quot; &quot;REL&quot; #&gt; [7] &quot;MEG&quot; &quot;-&quot; &quot;LOEL&quot; #&gt; [10] &quot;TClo&quot; &quot;LEC&quot; &quot;LOAEL&quot; #&gt; [13] &quot;NEL&quot; &quot;NOEL&quot; &quot;LEL&quot; #&gt; [16] &quot;BMDL (1SD)&quot; &quot;medium spec. conc.&quot; &quot;POD (screening chronic)&quot; #&gt; [19] &quot;POD (screening subchronic)&quot; &quot;BMDL&quot; &quot;BMD&quot; Then we look at NATADB. natadb_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% c(&#39;inhalation&#39;), unique(toxvalType)] #&gt; [1] &quot;cancer unit risk&quot; &quot;cancer slope factor&quot; #&gt; [3] &quot;RfD&quot; &quot;RfC&quot; #&gt; [5] &quot;NOAEL&quot; &quot;MRL&quot; #&gt; [7] &quot;OEHHA MADL&quot; &quot;REL&quot; #&gt; [9] &quot;OEHHA NSRL&quot; &quot;LC50&quot; #&gt; [11] &quot;MEG&quot; &quot;PAC-2&quot; #&gt; [13] &quot;PAC-3&quot; &quot;PAC-1&quot; #&gt; [15] &quot;LOAEC&quot; &quot;LOEC&quot; #&gt; [17] &quot;NOEC&quot; &quot;NOAEC&quot; #&gt; [19] &quot;NOEL&quot; &quot;-&quot; #&gt; [21] &quot;LOEL&quot; &quot;LC0&quot; #&gt; [23] &quot;LC100&quot; &quot;TClo&quot; #&gt; [25] &quot;LOAEL&quot; &quot;LCLo&quot; #&gt; [27] &quot;LT50&quot; &quot;LEL&quot; #&gt; [29] &quot;TDLo&quot; &quot;LC95&quot; #&gt; [31] &quot;ID50&quot; &quot;LEC&quot; #&gt; [33] &quot;MT0&quot; &quot;T25&quot; #&gt; [35] &quot;IC50&quot; &quot;LT0&quot; #&gt; [37] &quot;NEL&quot; &quot;RD50&quot; #&gt; [39] &quot;EC50&quot; &quot;BMDL (10)&quot; #&gt; [41] &quot;LC80&quot; &quot;LD50&quot; #&gt; [43] &quot;LC10&quot; &quot;LD&quot; #&gt; [45] &quot;LD100&quot; &quot;LD0&quot; #&gt; [47] &quot;LC&quot; &quot;LD67&quot; #&gt; [49] &quot;LC30&quot; &quot;BMC&quot; #&gt; [51] &quot;LT100&quot; &quot;SCE50&quot; #&gt; [53] &quot;LD75&quot; &quot;RD10&quot; #&gt; [55] &quot;LT&quot; &quot;LC60&quot; #&gt; [57] &quot;ALC&quot; &quot;AEGL&quot; #&gt; [59] &quot;tolerable concentration in air&quot; &quot;BMDL (10 HED)&quot; #&gt; [61] &quot;BMCL (10 HEC)&quot; &quot;BMC (10 HEC)&quot; #&gt; [63] &quot;BMDL (05 HEC)&quot; &quot;BMDL (HED)&quot; #&gt; [65] &quot;BMDL (1SD)&quot; &quot;BMCL&quot; #&gt; [67] &quot;BMCL (HEC)&quot; &quot;IDLH&quot; #&gt; [69] &quot;air contaminant limit&quot; &quot;medium spec. conc.&quot; #&gt; [71] &quot;POD (screening chronic)&quot; &quot;cancer unit risk (provisional, screening)&quot; #&gt; [73] &quot;POD (screening subchronic)&quot; &quot;BMCL (Adjusted HE)&quot; #&gt; [75] &quot;BMDL&quot; &quot;LOAEL (HED)&quot; #&gt; [77] &quot;BMCL (10)&quot; &quot;SRfCi&quot; #&gt; [79] &quot;BMD&quot; natadb_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% c(&#39;oral&#39;), unique(toxvalType)] #&gt; [1] &quot;RfD&quot; &quot;cancer slope factor&quot; #&gt; [3] &quot;NOAEL&quot; &quot;MRL&quot; #&gt; [5] &quot;MCL California&quot; &quot;MCL Federal&quot; #&gt; [7] &quot;OEHHA PHG&quot; &quot;MCL&quot; #&gt; [9] &quot;REL&quot; &quot;OEHHA MADL&quot; #&gt; [11] &quot;OEHHA NSRL&quot; &quot;LD50&quot; #&gt; [13] &quot;BMDL&quot; &quot;LEL&quot; #&gt; [15] &quot;HNEL&quot; &quot;LOAEL&quot; #&gt; [17] &quot;MEG&quot; &quot;NOEL&quot; #&gt; [19] &quot;NOAEC&quot; &quot;LD100&quot; #&gt; [21] &quot;LEC&quot; &quot;LOEL&quot; #&gt; [23] &quot;-&quot; &quot;ALD&quot; #&gt; [25] &quot;LD0&quot; &quot;TClo&quot; #&gt; [27] &quot;LC50&quot; &quot;LOAEC&quot; #&gt; [29] &quot;TDLo&quot; &quot;LD37&quot; #&gt; [31] &quot;% mortality&quot; &quot;T25&quot; #&gt; [33] &quot;NEL&quot; &quot;BMDL (10)&quot; #&gt; [35] &quot;LT50&quot; &quot;UL&quot; #&gt; [37] &quot;TDI&quot; &quot;tolerable concentration in air&quot; #&gt; [39] &quot;ADI&quot; &quot;BMDL (1SD)&quot; #&gt; [41] &quot;BMD (2x-ADJ)&quot; &quot;BMDL (HED)&quot; #&gt; [43] &quot;BMDL (10 HED)&quot; &quot;BMDL (05)&quot; #&gt; [45] &quot;DWEL&quot; &quot;health advisory&quot; #&gt; [47] &quot;medium spec. conc.&quot; &quot;POD (screening subchronic)&quot; #&gt; [49] &quot;POD (screening chronic)&quot; &quot;BMC&quot; #&gt; [51] &quot;SRfDo&quot; &quot;HBSL&quot; #&gt; [53] &quot;HHBP&quot; &quot;BMD&quot; intersect(natadb_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% &#39;inhalation&#39;, unique(toxvalType)], natadb_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% &#39;oral&#39;, unique(toxvalType)]) #&gt; [1] &quot;cancer slope factor&quot; &quot;RfD&quot; #&gt; [3] &quot;NOAEL&quot; &quot;MRL&quot; #&gt; [5] &quot;OEHHA MADL&quot; &quot;REL&quot; #&gt; [7] &quot;OEHHA NSRL&quot; &quot;LC50&quot; #&gt; [9] &quot;MEG&quot; &quot;LOAEC&quot; #&gt; [11] &quot;NOAEC&quot; &quot;NOEL&quot; #&gt; [13] &quot;-&quot; &quot;LOEL&quot; #&gt; [15] &quot;TClo&quot; &quot;LOAEL&quot; #&gt; [17] &quot;LT50&quot; &quot;LEL&quot; #&gt; [19] &quot;TDLo&quot; &quot;LEC&quot; #&gt; [21] &quot;T25&quot; &quot;NEL&quot; #&gt; [23] &quot;BMDL (10)&quot; &quot;LD50&quot; #&gt; [25] &quot;LD100&quot; &quot;LD0&quot; #&gt; [27] &quot;BMC&quot; &quot;tolerable concentration in air&quot; #&gt; [29] &quot;BMDL (10 HED)&quot; &quot;BMDL (HED)&quot; #&gt; [31] &quot;BMDL (1SD)&quot; &quot;medium spec. conc.&quot; #&gt; [33] &quot;POD (screening chronic)&quot; &quot;POD (screening subchronic)&quot; #&gt; [35] &quot;BMDL&quot; &quot;BMD&quot; Answer to Environmental Health Question 7 With the results above, we may answer Environmental Health Question 7: There are several types of toxicity values for each exposure route. List the unique toxicity values for the oral and inhalation routes. What are the unique types of toxicity values for the oral route and for the inhalation route? How many of these are common to both the oral and inhalation routes for each data set? Answer: There are 21 toxicity value types shared between the oral and inhalation exposure routes for CCL4 and 36 for NATADB. The lists above indicate the variety of toxicity values present in the hazard data for the two different exposure routes we have considered. For the next data exploration, We will turn to the riskAssessmentClass value of “developmental”. We will examine the “NOAEL” and “LOAEL” values for chemicals with oral exposure, human hazard, and a riskAssessmentClass value of “developmental”. We also examine the units to determine whether any unit conversions are necessary to compare numeric values. ccl4_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% &#39;oral&#39; &amp; riskAssessmentClass %in% &#39;developmental&#39; &amp; toxvalType %in% c(&#39;NOAEL&#39;, &#39;LOAEL&#39;), ] #&gt; id source year dtxsid exposureRoute toxvalNumeric toxvalNumericQualifier #&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: 69722 ATSDR PFAS 2010 DTXSID8031865 oral 5.0 = #&gt; 2: 70774 ATSDR PFAS 2009 DTXSID8031865 oral 8.7 = #&gt; 3: 71567 ATSDR PFAS 1982 DTXSID8031865 oral 50.0 = #&gt; 4: 72842 ATSDR PFAS 2011 DTXSID8031865 oral 0.3 = #&gt; 5: 75135 ATSDR PFAS 2010 DTXSID8031865 oral 5.0 = #&gt; --- #&gt; 218: 293837 ToxRefDB 1985 DTXSID9032329 oral 80.0 = #&gt; 219: 124993 ToxRefDB 1996 DTXSID6024177 oral 0.2 = #&gt; 220: 166218 ToxRefDB 1979 DTXSID0032578 oral 100.0 &gt;= #&gt; 221: 194758 ToxRefDB 1995 DTXSID4022448 oral 100.0 = #&gt; 222: 195159 ToxRefDB 1985 DTXSID4022448 oral 1000.0 = #&gt; 26 variable(s) not shown: [toxvalUnits &lt;char&gt;, studyType &lt;char&gt;, studyDurationClass &lt;char&gt;, studyDuractionValue &lt;num&gt;, studyDurationUnits &lt;char&gt;, strain &lt;char&gt;, sex &lt;char&gt;, population &lt;char&gt;, exposureMethod &lt;char&gt;, exposureForm &lt;char&gt;, ...] ccl4_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% &#39;oral&#39; &amp; riskAssessmentClass %in% &#39;developmental&#39; &amp; toxvalType %in% c(&#39;NOAEL&#39;, &#39;LOAEL&#39;), unique(toxvalUnits)] #&gt; [1] &quot;mg/kg-day&quot; &quot;mg/kg&quot; &quot;-&quot; natadb_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% &#39;oral&#39; &amp; riskAssessmentClass %in% &#39;developmental&#39; &amp; toxvalType %in% c(&#39;NOAEL&#39;, &#39;LOAEL&#39;), ] #&gt; id source year dtxsid exposureRoute toxvalNumeric toxvalNumericQualifier #&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: 86910 EFSA2 - DTXSID8020913 oral 400.0 = #&gt; 2: 86732 EFSA2 - DTXSID8020913 oral 120.0 = #&gt; 3: 87341 EFSA2 - DTXSID8020913 oral 50.0 = #&gt; 4: 149371 HPVIS - DTXSID4020161 oral 500.0 = #&gt; 5: 69436 HPVIS - DTXSID0021834 oral 27.6 = #&gt; --- #&gt; 138: 330469 ToxRefDB 1984 DTXSID4021395 oral 500.0 &gt;= #&gt; 139: 330750 ToxRefDB 1984 DTXSID4021395 oral 100.0 = #&gt; 140: 229449 ToxRefDB 1993 DTXSID2021781 oral 630.0 = #&gt; 141: 330299 ToxRefDB 1983 DTXSID4021395 oral 500.0 &gt;= #&gt; 142: 330226 ToxRefDB 1983 DTXSID4021395 oral 500.0 = #&gt; 26 variable(s) not shown: [toxvalUnits &lt;char&gt;, studyType &lt;char&gt;, studyDurationClass &lt;char&gt;, studyDuractionValue &lt;num&gt;, studyDurationUnits &lt;char&gt;, strain &lt;char&gt;, sex &lt;char&gt;, population &lt;char&gt;, exposureMethod &lt;char&gt;, exposureForm &lt;char&gt;, ...] natadb_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% &#39;oral&#39; &amp; riskAssessmentClass %in% &#39;developmental&#39; &amp; toxvalType %in% c(&#39;NOAEL&#39;, &#39;LOAEL&#39;), unique(toxvalUnits)] #&gt; [1] &quot;mg/kg-day&quot; &quot;mg/kg&quot; &quot;-&quot; Observe that for both CCL4 and NATADB, the units are given by “mg/kg-day”, “mg/kg” and “-”. In this case, we treat “mg/kg-day” and “mg/kg” the same and exclude “-”. We group by DTXSID to find the lowest or highest value. ccl4_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% &#39;oral&#39; &amp; riskAssessmentClass %in% &#39;developmental&#39; &amp; toxvalType %in% c(&#39;NOAEL&#39;, &#39;LOAEL&#39;) &amp; (toxvalUnits != &#39;-&#39;), .(numeric_value = min(toxvalNumeric), units = toxvalUnits[[which.min(toxvalNumeric)]]), by = .(dtxsid, toxvalType)] #&gt; dtxsid toxvalType numeric_value units #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID8031865 LOAEL 0.300 mg/kg-day #&gt; 2: DTXSID8031865 NOAEL 1.000 mg/kg-day #&gt; 3: DTXSID3031864 LOAEL 0.800 mg/kg-day #&gt; 4: DTXSID3031864 NOAEL 0.100 mg/kg-day #&gt; 5: DTXSID1024174 NOAEL 1.000 mg/kg #&gt; 6: DTXSID7021029 LOAEL 0.025 mg/kg-day #&gt; 7: DTXSID8023846 NOAEL 3.000 mg/kg-day #&gt; 8: DTXSID3032464 NOAEL 30.000 mg/kg-day #&gt; 9: DTXSID8022292 LOAEL 150.000 mg/kg-day #&gt; 10: DTXSID8022292 NOAEL 50.000 mg/kg-day #&gt; 11: DTXSID9020243 NOAEL 10.000 mg/kg-day #&gt; 12: DTXSID4022361 NOAEL 50.000 mg/kg-day #&gt; 13: DTXSID9023914 LOAEL 1.000 mg/kg-day #&gt; 14: DTXSID9020243 LOAEL 30.000 mg/kg-day #&gt; 15: DTXSID6024177 NOAEL 0.200 mg/kg-day #&gt; 16: DTXSID7024241 NOAEL 10.000 mg/kg-day #&gt; 17: DTXSID8023846 LOAEL 10.000 mg/kg-day #&gt; 18: DTXSID7024241 LOAEL 30.000 mg/kg-day #&gt; 19: DTXSID8025541 NOAEL 0.100 mg/kg-day #&gt; 20: DTXSID8025541 LOAEL 0.100 mg/kg-day #&gt; 21: DTXSID1024338 LOAEL 6.000 mg/kg-day #&gt; 22: DTXSID9032329 NOAEL 5.500 mg/kg-day #&gt; 23: DTXSID9032329 LOAEL 23.000 mg/kg-day #&gt; 24: DTXSID6024177 LOAEL 0.650 mg/kg-day #&gt; 25: DTXSID1024174 LOAEL 7.000 mg/kg-day #&gt; 26: DTXSID8023848 LOAEL 190.000 mg/kg-day #&gt; 27: DTXSID4022448 NOAEL 20.000 mg/kg-day #&gt; 28: DTXSID0021464 LOAEL 7.500 mg/kg-day #&gt; 29: DTXSID4022448 LOAEL 100.000 mg/kg-day #&gt; 30: DTXSID3034458 NOAEL 25.000 mg/kg-day #&gt; 31: DTXSID3034458 LOAEL 100.000 mg/kg-day #&gt; 32: DTXSID9023914 NOAEL 0.500 mg/kg-day #&gt; 33: DTXSID1021409 LOAEL 0.300 mg/kg-day #&gt; 34: DTXSID4022361 LOAEL 200.000 mg/kg-day #&gt; 35: DTXSID0020446 LOAEL 50.000 mg/kg-day #&gt; 36: DTXSID0032578 LOAEL 1.000 mg/kg-day #&gt; 37: DTXSID0032578 NOAEL 0.500 mg/kg-day #&gt; 38: DTXSID1021409 NOAEL 0.100 mg/kg-day #&gt; 39: DTXSID1024338 NOAEL 2.000 mg/kg-day #&gt; 40: DTXSID3032464 LOAEL 60.000 mg/kg-day #&gt; 41: DTXSID0021464 NOAEL 3.000 mg/kg-day #&gt; 42: DTXSID4032611 LOAEL 9.000 mg/kg-day #&gt; 43: DTXSID8023848 NOAEL 50.000 mg/kg-day #&gt; 44: DTXSID0020446 NOAEL 10.000 mg/kg-day #&gt; 45: DTXSID4032611 NOAEL 2.000 mg/kg-day #&gt; dtxsid toxvalType numeric_value units natadb_hazard[humanEcoNt %in% &#39;human health&#39; &amp; exposureRoute %in% &#39;oral&#39; &amp; riskAssessmentClass %in% &#39;developmental&#39; &amp; toxvalType %in% c(&#39;NOAEL&#39;, &#39;LOAEL&#39;) &amp; (toxvalUnits != &#39;-&#39;), .(numeric_value = min(toxvalNumeric), units = toxvalUnits[[which.min(toxvalNumeric)]]), by = .(dtxsid, toxvalType)] #&gt; dtxsid toxvalType numeric_value units #&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID8020913 NOAEL 50.000 mg/kg-day #&gt; 2: DTXSID4020161 NOAEL 500.000 mg/kg #&gt; 3: DTXSID0021834 NOAEL 13.800 mg/kg #&gt; 4: DTXSID0020448 NOAEL 30.000 mg/kg #&gt; 5: DTXSID3022455 NOAEL 840.000 mg/kg #&gt; 6: DTXSID5020027 NOAEL 2.500 mg/kg #&gt; 7: DTXSID7021318 NOAEL 987.000 mg/kg #&gt; 8: DTXSID6020515 NOAEL 44.100 mg/kg #&gt; 9: DTXSID7021029 LOAEL 0.025 mg/kg-day #&gt; 10: DTXSID4020874 NOAEL 1.000 mg/kg-day #&gt; 11: DTXSID4021395 NOAEL 100.000 mg/kg-day #&gt; 12: DTXSID2021781 LOAEL 630.000 mg/kg-day #&gt; 13: DTXSID4021395 LOAEL 225.000 mg/kg-day #&gt; 14: DTXSID2021781 NOAEL 500.000 mg/kg-day #&gt; 15: DTXSID7021106 LOAEL 30.000 mg/kg-day #&gt; 16: DTXSID9020247 NOAEL 4.000 mg/kg-day #&gt; 17: DTXSID7021106 NOAEL 15.000 mg/kg-day #&gt; 18: DTXSID7021948 LOAEL 9.000 mg/kg-day #&gt; 19: DTXSID5020607 LOAEL 666.000 mg/kg-day #&gt; 20: DTXSID5020449 NOAEL 0.100 mg/kg-day #&gt; 21: DTXSID5020449 LOAEL 2.500 mg/kg-day #&gt; 22: DTXSID3022455 LOAEL 3570.000 mg/kg-day #&gt; 23: DTXSID2021105 LOAEL 125.000 mg/kg-day #&gt; 24: DTXSID2021105 NOAEL 12.500 mg/kg-day #&gt; 25: DTXSID9020247 LOAEL 30.000 mg/kg-day #&gt; 26: DTXSID5020607 NOAEL 357.000 mg/kg-day #&gt; 27: DTXSID7021948 NOAEL 3.000 mg/kg-day #&gt; 28: DTXSID8020913 LOAEL 50.000 mg/kg-day #&gt; 29: DTXSID9020243 NOAEL 10.000 mg/kg-day #&gt; 30: DTXSID9020243 LOAEL 30.000 mg/kg-day #&gt; dtxsid toxvalType numeric_value units Now, we also explore the values of “RfD”, “RfC”, and “cancer slope factor” of the toxvalType rows. We first determine the set of units for each, make appropriate conversions if necessary, and then make comparisons. ccl4_hazard[humanEcoNt %in% &#39;human health&#39; &amp; toxvalType %in% c(&#39;cancer slope factor&#39;, &#39;RfD&#39;, &#39;RfC&#39;), .N, by = .(toxvalType, toxvalUnits)][order(toxvalType, -N)] #&gt; toxvalType toxvalUnits N #&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: RfC mg/m3 178 #&gt; 2: RfC g/m3 2 #&gt; 3: RfC ug/m3 2 #&gt; 4: RfD mg/kg-day 300 #&gt; 5: RfD mg/kg 1 #&gt; 6: cancer slope factor (mg/kg-day)-1 149 #&gt; 7: cancer slope factor mg/kg-day 1 natadb_hazard[humanEcoNt %in% &#39;human health&#39; &amp; toxvalType %in% c(&#39;cancer slope factor&#39;, &#39;RfD&#39;, &#39;RfC&#39;), .N, by = .(toxvalType, toxvalUnits)][order(toxvalType, -N)] #&gt; toxvalType toxvalUnits N #&gt; &lt;char&gt; &lt;char&gt; &lt;int&gt; #&gt; 1: RfC mg/m3 476 #&gt; 2: RfC ppm 2 #&gt; 3: RfD mg/kg-day 776 #&gt; 4: RfD mg/kg 1 #&gt; 5: cancer slope factor (mg/kg-day)-1 460 #&gt; 6: cancer slope factor mg/kg-day 2 For CCL4, there are three inequivalent sets of units that need conversions. We convert to “mg/m3”, which means scaling values given in “g/m3” by 1E3 and values given in “ug/m3” by 1E-3. The Rfd units and cancer slope factor units require no conversions although we do remove the cancer slope factor unit of “mg/kg-day”. For NATADB, we need to convert RfC values from ppm to mg/m3, with a conversion factor that relies on the molecular weight of the chemical in question. We will remove these from consideration for now. The units for RfD and cancer slope factor require no conversions although we do remove the cancer slope factor unit of “mg/kg-day”. First, we filter and separate out the relevant data subsets. # Separate out into relevant data subsets ccl4_csf &lt;- ccl4_hazard[humanEcoNt %in% &#39;human health&#39; &amp; toxvalType %in% c(&#39;cancer slope factor&#39;) &amp; (toxvalUnits != &#39;mg/kg-day&#39;), ] ccl4_rfc &lt;- ccl4_hazard[humanEcoNt %in% &#39;human health&#39; &amp; toxvalType %in% c(&#39;RfC&#39;), ] ccl4_rfd &lt;- ccl4_hazard[humanEcoNt %in% &#39;human health&#39; &amp; toxvalType %in% c(&#39;RfD&#39;), ] Then we start to handle the unit conversions. # Set mass by volume units to mg/m3, so scale g/m3 by 1E3 and ug/m3 by 1E-3 ccl4_rfc[toxvalUnits == &#39;mg/m3&#39;, conversion := 1] #&gt; id source year dtxsid exposureRoute toxvalNumeric toxvalNumericQualifier #&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: 397149 Alaska DEC - DTXSID0021541 inhalation 9.0e-02 = #&gt; 2: 394651 Alaska DEC - DTXSID3042219 inhalation 3.5e-02 = #&gt; 3: 394579 Alaska DEC - DTXSID3020964 inhalation 2.0e-03 = #&gt; 4: 390390 Alaska DEC - DTXSID8020832 inhalation 5.0e-03 = #&gt; 5: 390677 Alaska DEC - DTXSID3020833 inhalation 3.0e+00 = #&gt; --- #&gt; 178: 382357 RSL - DTXSID2024169 - 5.0e-05 = #&gt; 179: 395836 RSL - DTXSID5021207 - 3.0e-02 = #&gt; 180: 380861 RSL - DTXSID2021731 - 2.0e+01 = #&gt; 181: 391656 RSL - DTXSID6022422 - 2.0e-02 = #&gt; 182: 391637 RSL - DTXSID6022422 inhalation 2.0e-02 = #&gt; 27 variable(s) not shown: [toxvalUnits &lt;char&gt;, studyType &lt;char&gt;, studyDurationClass &lt;char&gt;, studyDuractionValue &lt;num&gt;, studyDurationUnits &lt;char&gt;, strain &lt;char&gt;, sex &lt;char&gt;, population &lt;char&gt;, exposureMethod &lt;char&gt;, exposureForm &lt;char&gt;, ...] ccl4_rfc[toxvalUnits == &#39;g/m3&#39;, conversion := 1E3] #&gt; id source year dtxsid exposureRoute toxvalNumeric toxvalNumericQualifier #&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: 397149 Alaska DEC - DTXSID0021541 inhalation 9.0e-02 = #&gt; 2: 394651 Alaska DEC - DTXSID3042219 inhalation 3.5e-02 = #&gt; 3: 394579 Alaska DEC - DTXSID3020964 inhalation 2.0e-03 = #&gt; 4: 390390 Alaska DEC - DTXSID8020832 inhalation 5.0e-03 = #&gt; 5: 390677 Alaska DEC - DTXSID3020833 inhalation 3.0e+00 = #&gt; --- #&gt; 178: 382357 RSL - DTXSID2024169 - 5.0e-05 = #&gt; 179: 395836 RSL - DTXSID5021207 - 3.0e-02 = #&gt; 180: 380861 RSL - DTXSID2021731 - 2.0e+01 = #&gt; 181: 391656 RSL - DTXSID6022422 - 2.0e-02 = #&gt; 182: 391637 RSL - DTXSID6022422 inhalation 2.0e-02 = #&gt; 27 variable(s) not shown: [toxvalUnits &lt;char&gt;, studyType &lt;char&gt;, studyDurationClass &lt;char&gt;, studyDuractionValue &lt;num&gt;, studyDurationUnits &lt;char&gt;, strain &lt;char&gt;, sex &lt;char&gt;, population &lt;char&gt;, exposureMethod &lt;char&gt;, exposureForm &lt;char&gt;, ...] ccl4_rfc[toxvalUnits == &#39;ug/m3&#39;, conversion := 1E-3] #&gt; id source year dtxsid exposureRoute toxvalNumeric toxvalNumericQualifier #&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: 397149 Alaska DEC - DTXSID0021541 inhalation 9.0e-02 = #&gt; 2: 394651 Alaska DEC - DTXSID3042219 inhalation 3.5e-02 = #&gt; 3: 394579 Alaska DEC - DTXSID3020964 inhalation 2.0e-03 = #&gt; 4: 390390 Alaska DEC - DTXSID8020832 inhalation 5.0e-03 = #&gt; 5: 390677 Alaska DEC - DTXSID3020833 inhalation 3.0e+00 = #&gt; --- #&gt; 178: 382357 RSL - DTXSID2024169 - 5.0e-05 = #&gt; 179: 395836 RSL - DTXSID5021207 - 3.0e-02 = #&gt; 180: 380861 RSL - DTXSID2021731 - 2.0e+01 = #&gt; 181: 391656 RSL - DTXSID6022422 - 2.0e-02 = #&gt; 182: 391637 RSL - DTXSID6022422 inhalation 2.0e-02 = #&gt; 27 variable(s) not shown: [toxvalUnits &lt;char&gt;, studyType &lt;char&gt;, studyDurationClass &lt;char&gt;, studyDuractionValue &lt;num&gt;, studyDurationUnits &lt;char&gt;, strain &lt;char&gt;, sex &lt;char&gt;, population &lt;char&gt;, exposureMethod &lt;char&gt;, exposureForm &lt;char&gt;, ...] ccl4_rfc[toxvalUnits %in% c(&#39;mg/m3&#39;, &#39;g/m3&#39;, &#39;ug/m3&#39;), units := &#39;mg/m3&#39;] #&gt; id source year dtxsid exposureRoute toxvalNumeric toxvalNumericQualifier #&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: 397149 Alaska DEC - DTXSID0021541 inhalation 9.0e-02 = #&gt; 2: 394651 Alaska DEC - DTXSID3042219 inhalation 3.5e-02 = #&gt; 3: 394579 Alaska DEC - DTXSID3020964 inhalation 2.0e-03 = #&gt; 4: 390390 Alaska DEC - DTXSID8020832 inhalation 5.0e-03 = #&gt; 5: 390677 Alaska DEC - DTXSID3020833 inhalation 3.0e+00 = #&gt; --- #&gt; 178: 382357 RSL - DTXSID2024169 - 5.0e-05 = #&gt; 179: 395836 RSL - DTXSID5021207 - 3.0e-02 = #&gt; 180: 380861 RSL - DTXSID2021731 - 2.0e+01 = #&gt; 181: 391656 RSL - DTXSID6022422 - 2.0e-02 = #&gt; 182: 391637 RSL - DTXSID6022422 inhalation 2.0e-02 = #&gt; 28 variable(s) not shown: [toxvalUnits &lt;char&gt;, studyType &lt;char&gt;, studyDurationClass &lt;char&gt;, studyDuractionValue &lt;num&gt;, studyDurationUnits &lt;char&gt;, strain &lt;char&gt;, sex &lt;char&gt;, population &lt;char&gt;, exposureMethod &lt;char&gt;, exposureForm &lt;char&gt;, ...] # Set mass by mass units to mg/kg ccl4_rfd[toxvalUnits %in% c(&#39;mg/kg-day&#39;, &#39;mg/kg&#39;), conversion := 1] #&gt; id source year dtxsid exposureRoute toxvalNumeric toxvalNumericQualifier #&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: 385099 Alaska DEC - DTXSID6021030 dermal 0.0050 = #&gt; 2: 376617 Alaska DEC - DTXSID1021740 dermal 0.0500 = #&gt; 3: 381896 Alaska DEC - DTXSID8020597 dermal 1.0000 = #&gt; 4: 385648 Alaska DEC - DTXSID9024142 dermal 0.0030 = #&gt; 5: 392604 Alaska DEC - DTXSID1020437 dermal 0.2000 = #&gt; --- #&gt; 297: 387188 RSL - DTXSID0024052 - 0.0220 = #&gt; 298: 374437 RSL - DTXSID4022448 oral 0.1500 = #&gt; 299: 381916 RSL - DTXSID2021731 oral 2.0000 = #&gt; 300: 389619 RSL - DTXSID1031040 oral 0.0003 = #&gt; 301: 376317 RSL - DTXSID1021740 - 0.1000 = #&gt; 27 variable(s) not shown: [toxvalUnits &lt;char&gt;, studyType &lt;char&gt;, studyDurationClass &lt;char&gt;, studyDuractionValue &lt;num&gt;, studyDurationUnits &lt;char&gt;, strain &lt;char&gt;, sex &lt;char&gt;, population &lt;char&gt;, exposureMethod &lt;char&gt;, exposureForm &lt;char&gt;, ...] ccl4_rfd[toxvalUnits %in% c(&#39;mg/kg-day&#39;, &#39;mg/kg&#39;), units := &#39;mg/kg&#39;] #&gt; id source year dtxsid exposureRoute toxvalNumeric toxvalNumericQualifier #&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: 385099 Alaska DEC - DTXSID6021030 dermal 0.0050 = #&gt; 2: 376617 Alaska DEC - DTXSID1021740 dermal 0.0500 = #&gt; 3: 381896 Alaska DEC - DTXSID8020597 dermal 1.0000 = #&gt; 4: 385648 Alaska DEC - DTXSID9024142 dermal 0.0030 = #&gt; 5: 392604 Alaska DEC - DTXSID1020437 dermal 0.2000 = #&gt; --- #&gt; 297: 387188 RSL - DTXSID0024052 - 0.0220 = #&gt; 298: 374437 RSL - DTXSID4022448 oral 0.1500 = #&gt; 299: 381916 RSL - DTXSID2021731 oral 2.0000 = #&gt; 300: 389619 RSL - DTXSID1031040 oral 0.0003 = #&gt; 301: 376317 RSL - DTXSID1021740 - 0.1000 = #&gt; 28 variable(s) not shown: [toxvalUnits &lt;char&gt;, studyType &lt;char&gt;, studyDurationClass &lt;char&gt;, studyDuractionValue &lt;num&gt;, studyDurationUnits &lt;char&gt;, strain &lt;char&gt;, sex &lt;char&gt;, population &lt;char&gt;, exposureMethod &lt;char&gt;, exposureForm &lt;char&gt;, ...] Then aggregate the data. # Run data aggregations grouping by dtxsid and taking either the max or the min # depending on the toxvalType we are considering. ccl4_csf[,.(numeric_value = max(toxvalNumeric), units = toxvalUnits[which.max(toxvalNumeric)]), by = .(dtxsid)][order(-numeric_value),] #&gt; dtxsid numeric_value units #&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID2021028 1.50e+02 (mg/kg-day)-1 #&gt; 2: DTXSID7021029 1.02e+02 (mg/kg-day)-1 #&gt; 3: DTXSID0020573 3.90e+01 (mg/kg-day)-1 #&gt; 4: DTXSID9021390 3.00e+01 (mg/kg-day)-1 #&gt; 5: DTXSID6021032 2.80e+01 (mg/kg-day)-1 #&gt; 6: DTXSID3020702 1.72e+01 (mg/kg-day)-1 #&gt; 7: DTXSID2020684 6.49e+00 (mg/kg-day)-1 #&gt; 8: DTXSID3020203 3.40e+00 (mg/kg-day)-1 #&gt; 9: DTXSID1021798 3.00e+00 (mg/kg-day)-1 #&gt; 10: DTXSID8021062 2.10e+00 (mg/kg-day)-1 #&gt; 11: DTXSID1021409 1.83e+00 (mg/kg-day)-1 #&gt; 12: DTXSID6022422 1.60e+00 (mg/kg-day)-1 #&gt; 13: DTXSID9021427 1.00e+00 (mg/kg-day)-1 #&gt; 14: DTXSID0020600 3.10e-01 (mg/kg-day)-1 #&gt; 15: DTXSID5021207 2.40e-01 (mg/kg-day)-1 #&gt; 16: DTXSID1026164 1.80e-01 (mg/kg-day)-1 #&gt; 17: DTXSID0020153 1.70e-01 (mg/kg-day)-1 #&gt; 18: DTXSID9024142 1.10e-01 (mg/kg-day)-1 #&gt; 19: DTXSID4020533 1.00e-01 (mg/kg-day)-1 #&gt; 20: DTXSID7024241 7.32e-02 (mg/kg-day)-1 #&gt; 21: DTXSID7020005 7.00e-02 (mg/kg-day)-1 #&gt; 22: DTXSID5020601 4.50e-02 (mg/kg-day)-1 #&gt; 23: DTXSID0024341 3.90e-02 (mg/kg-day)-1 #&gt; 24: DTXSID1021407 3.40e-02 (mg/kg-day)-1 #&gt; 25: DTXSID4032611 2.81e-02 (mg/kg-day)-1 #&gt; 26: DTXSID2021317 2.60e-02 (mg/kg-day)-1 #&gt; 27: DTXSID7020637 2.10e-02 (mg/kg-day)-1 #&gt; 28: DTXSID6021030 1.96e-02 (mg/kg-day)-1 #&gt; 29: DTXSID0021541 1.63e-02 (mg/kg-day)-1 #&gt; 30: DTXSID1024338 1.20e-02 (mg/kg-day)-1 #&gt; 31: DTXSID5039224 1.00e-02 (mg/kg-day)-1 #&gt; 32: DTXSID8023846 8.70e-03 (mg/kg-day)-1 #&gt; 33: DTXSID8020090 5.70e-03 (mg/kg-day)-1 #&gt; 34: DTXSID1020437 5.70e-03 (mg/kg-day)-1 #&gt; 35: DTXSID9020243 2.30e-03 (mg/kg-day)-1 #&gt; 36: DTXSID3020833 2.25e-03 (mg/kg-day)-1 #&gt; 37: DTXSID7020215 2.00e-04 (mg/kg-day)-1 #&gt; dtxsid numeric_value units ccl4_rfc[,.(numeric_value = min(toxvalNumeric*conversion), units = units[which.min(toxvalNumeric*conversion)]), by = .(dtxsid)][order(numeric_value),] #&gt; dtxsid numeric_value units #&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID1031040 6.0e-12 mg/m3 #&gt; 2: DTXSID5020023 2.0e-05 mg/m3 #&gt; 3: DTXSID3020702 3.0e-05 mg/m3 #&gt; 4: DTXSID7021029 4.0e-05 mg/m3 #&gt; 5: DTXSID2024169 5.0e-05 mg/m3 #&gt; 6: DTXSID0024341 7.0e-05 mg/m3 #&gt; 7: DTXSID8020044 1.0e-04 mg/m3 #&gt; 8: DTXSID2040282 1.0e-04 mg/m3 #&gt; 9: DTXSID9021390 3.0e-04 mg/m3 #&gt; 10: DTXSID8020090 1.0e-03 mg/m3 #&gt; 11: DTXSID0020153 1.0e-03 mg/m3 #&gt; 12: DTXSID3020964 2.0e-03 mg/m3 #&gt; 13: DTXSID3020203 2.0e-03 mg/m3 #&gt; 14: DTXSID1024207 2.0e-03 mg/m3 #&gt; 15: DTXSID8020832 5.0e-03 mg/m3 #&gt; 16: DTXSID3024366 7.0e-03 mg/m3 #&gt; 17: DTXSID5024182 7.0e-03 mg/m3 #&gt; 18: DTXSID5039224 9.0e-03 mg/m3 #&gt; 19: DTXSID7020637 9.8e-03 mg/m3 #&gt; 20: DTXSID6022422 2.0e-02 mg/m3 #&gt; 21: DTXSID5021207 3.0e-02 mg/m3 #&gt; 22: DTXSID4020533 3.0e-02 mg/m3 #&gt; 23: DTXSID0020600 3.0e-02 mg/m3 #&gt; 24: DTXSID3042219 3.5e-02 mg/m3 #&gt; 25: DTXSID2022333 3.5e-02 mg/m3 #&gt; 26: DTXSID4021503 4.0e-02 mg/m3 #&gt; 27: DTXSID0021541 9.0e-02 mg/m3 #&gt; 28: DTXSID8020597 4.0e-01 mg/m3 #&gt; 29: DTXSID1020437 5.0e-01 mg/m3 #&gt; 30: DTXSID0021917 7.0e-01 mg/m3 #&gt; 31: DTXSID3020833 3.0e+00 mg/m3 #&gt; 32: DTXSID2021731 4.0e+00 mg/m3 #&gt; 33: DTXSID6020301 5.0e+01 mg/m3 #&gt; dtxsid numeric_value units ccl4_rfd[,.(numeric_value = min(toxvalNumeric*conversion), units = units[which.min(toxvalNumeric*conversion)]), by = .(dtxsid)][order(numeric_value),] #&gt; dtxsid numeric_value units #&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID7021029 0.000004 mg/kg #&gt; 2: DTXSID3031864 0.000020 mg/kg #&gt; 3: DTXSID8031865 0.000020 mg/kg #&gt; 4: DTXSID9023914 0.000030 mg/kg #&gt; 5: DTXSID1024174 0.000030 mg/kg #&gt; 6: DTXSID6024177 0.000050 mg/kg #&gt; 7: DTXSID4032611 0.000065 mg/kg #&gt; 8: DTXSID2040282 0.000070 mg/kg #&gt; 9: DTXSID5020601 0.000080 mg/kg #&gt; 10: DTXSID8025541 0.000100 mg/kg #&gt; 11: DTXSID1021407 0.000100 mg/kg #&gt; 12: DTXSID3032464 0.000120 mg/kg #&gt; 13: DTXSID8023846 0.000300 mg/kg #&gt; 14: DTXSID1031040 0.000300 mg/kg #&gt; 15: DTXSID3020964 0.000485 mg/kg #&gt; 16: DTXSID5020023 0.000500 mg/kg #&gt; 17: DTXSID9024142 0.000800 mg/kg #&gt; 18: DTXSID3020702 0.000900 mg/kg #&gt; 19: DTXSID8020832 0.001000 mg/kg #&gt; 20: DTXSID5021207 0.001000 mg/kg #&gt; 21: DTXSID1021409 0.001000 mg/kg #&gt; 22: DTXSID4022361 0.001200 mg/kg #&gt; 23: DTXSID0032578 0.001500 mg/kg #&gt; 24: DTXSID0020446 0.002000 mg/kg #&gt; 25: DTXSID0020153 0.002000 mg/kg #&gt; 26: DTXSID9021390 0.004000 mg/kg #&gt; 27: DTXSID8020044 0.004000 mg/kg #&gt; 28: DTXSID6021030 0.005000 mg/kg #&gt; 29: DTXSID9032329 0.005000 mg/kg #&gt; 30: DTXSID1024207 0.005000 mg/kg #&gt; 31: DTXSID5024182 0.005000 mg/kg #&gt; 32: DTXSID8020090 0.007000 mg/kg #&gt; 33: DTXSID2020684 0.008000 mg/kg #&gt; 34: DTXSID3042219 0.010000 mg/kg #&gt; 35: DTXSID2022333 0.010000 mg/kg #&gt; 36: DTXSID4021503 0.010000 mg/kg #&gt; 37: DTXSID0021464 0.016000 mg/kg #&gt; 38: DTXSID8023848 0.020000 mg/kg #&gt; 39: DTXSID4034948 0.020000 mg/kg #&gt; 40: DTXSID1026164 0.020000 mg/kg #&gt; 41: DTXSID0024052 0.021800 mg/kg #&gt; 42: DTXSID2024169 0.024000 mg/kg #&gt; 43: DTXSID0021541 0.025700 mg/kg #&gt; 44: DTXSID9032113 0.029000 mg/kg #&gt; 45: DTXSID2021317 0.030000 mg/kg #&gt; 46: DTXSID4020533 0.030000 mg/kg #&gt; 47: DTXSID7024241 0.040000 mg/kg #&gt; 48: DTXSID1021740 0.050000 mg/kg #&gt; 49: DTXSID8022292 0.050000 mg/kg #&gt; 50: DTXSID0021917 0.060000 mg/kg #&gt; 51: DTXSID9020243 0.100000 mg/kg #&gt; 52: DTXSID4022448 0.100000 mg/kg #&gt; 53: DTXSID1020437 0.143000 mg/kg #&gt; 54: DTXSID1024338 0.160000 mg/kg #&gt; 55: DTXSID7020637 0.200000 mg/kg #&gt; 56: DTXSID3034458 0.300000 mg/kg #&gt; 57: DTXSID2021731 0.500000 mg/kg #&gt; 58: DTXSID8020597 0.800000 mg/kg #&gt; 59: DTXSID3020833 0.857000 mg/kg #&gt; dtxsid numeric_value units Repeat the process for NATADB, first separating out the relevant subsets of the data. # Separate out into relevant data subsets natadb_csf &lt;- natadb_hazard[humanEcoNt %in% &#39;human health&#39; &amp; toxvalType %in% c(&#39;cancer slope factor&#39;) &amp; (toxvalUnits != &#39;mg/kg-day&#39;), ] natadb_rfc &lt;- natadb_hazard[humanEcoNt %in% &#39;human health&#39; &amp; toxvalType %in% c(&#39;RfC&#39;), ] natadb_rfd &lt;- natadb_hazard[humanEcoNt %in% &#39;human health&#39; &amp; toxvalType %in% c(&#39;RfD&#39;), ] Now handle the unit conversions. # Set mass by mass units to mg/kg. Note that ppm is already in mg/kg natadb_rfc &lt;- natadb_rfc[toxvalUnits != &#39;ppm&#39;,] natadb_rfd[, units := &#39;mg/kg-day&#39;] #&gt; id source year dtxsid exposureRoute toxvalNumeric toxvalNumericQualifier #&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: 384500 Alaska DEC - DTXSID5020607 dermal 0.0038 = #&gt; 2: 398757 Alaska DEC - DTXSID3020679 oral 0.0005 = #&gt; 3: 392604 Alaska DEC - DTXSID1020437 dermal 0.2000 = #&gt; 4: 376366 Alaska DEC - DTXSID2021284 oral 0.2000 = #&gt; 5: 380884 Alaska DEC - DTXSID8020597 oral 2.0000 = #&gt; --- #&gt; 773: 399356 RSL - DTXSID3020596 oral 0.0500 = #&gt; 774: 392719 RSL - DTXSID2021993 oral 0.0010 = #&gt; 775: 395204 RSL - DTXSID5020316 oral 0.0200 = #&gt; 776: 395902 RSL - DTXSID2021159 - 2.0000 = #&gt; 777: 396090 RSL - DTXSID8021438 oral 0.0500 = #&gt; 27 variable(s) not shown: [toxvalUnits &lt;char&gt;, studyType &lt;char&gt;, studyDurationClass &lt;char&gt;, studyDuractionValue &lt;num&gt;, studyDurationUnits &lt;char&gt;, strain &lt;char&gt;, sex &lt;char&gt;, population &lt;char&gt;, exposureMethod &lt;char&gt;, exposureForm &lt;char&gt;, ...] Finally, aggregate the data. # Run data aggregations grouping by dtxsid and taking either the max or the min # depending on the toxvalType we are considering. natadb_csf[, .(numeric_value = max(toxvalNumeric), units = toxvalUnits[which.max(toxvalNumeric)]), by = .(dtxsid)][order(-numeric_value),] #&gt; dtxsid numeric_value units #&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID2020137 5.00e+02 (mg/kg-day)-1 #&gt; 2: DTXSID8020173 2.20e+02 (mg/kg-day)-1 #&gt; 3: DTXSID4021006 1.20e+02 (mg/kg-day)-1 #&gt; 4: DTXSID7021029 1.02e+02 (mg/kg-day)-1 #&gt; 5: DTXSID8020599 6.50e+01 (mg/kg-day)-1 #&gt; 6: DTXSID9020168 4.00e+01 (mg/kg-day)-1 #&gt; 7: DTXSID5020071 2.10e+01 (mg/kg-day)-1 #&gt; 8: DTXSID3020702 1.72e+01 (mg/kg-day)-1 #&gt; 9: DTXSID8021197 1.40e+01 (mg/kg-day)-1 #&gt; 10: DTXSID1020512 1.30e+01 (mg/kg-day)-1 #&gt; 11: DTXSID1020148 1.30e+01 (mg/kg-day)-1 #&gt; 12: DTXSID5024059 1.10e+01 (mg/kg-day)-1 #&gt; 13: DTXSID3020413 7.00e+00 (mg/kg-day)-1 #&gt; 14: DTXSID4021056 6.70e+00 (mg/kg-day)-1 #&gt; 15: DTXSID3020679 6.25e+00 (mg/kg-day)-1 #&gt; 16: DTXSID5020491 4.60e+00 (mg/kg-day)-1 #&gt; 17: DTXSID5020027 4.50e+00 (mg/kg-day)-1 #&gt; 18: DTXSID4020402 4.00e+00 (mg/kg-day)-1 #&gt; 19: DTXSID7020687 4.00e+00 (mg/kg-day)-1 #&gt; 20: DTXSID0039227 3.80e+00 (mg/kg-day)-1 #&gt; 21: DTXSID3020203 3.40e+00 (mg/kg-day)-1 #&gt; 22: DTXSID2020682 3.20e+00 (mg/kg-day)-1 #&gt; 23: DTXSID1021798 3.00e+00 (mg/kg-day)-1 #&gt; 24: DTXSID0021383 2.67e+00 (mg/kg-day)-1 #&gt; 25: DTXSID3020415 2.50e+00 (mg/kg-day)-1 #&gt; 26: DTXSID8021195 2.40e+00 (mg/kg-day)-1 #&gt; 27: DTXSID6020307 2.40e+00 (mg/kg-day)-1 #&gt; 28: DTXSID7021368 2.20e+00 (mg/kg-day)-1 #&gt; 29: DTXSID5024267 2.00e+00 (mg/kg-day)-1 #&gt; 30: DTXSID6022422 1.60e+00 (mg/kg-day)-1 #&gt; 31: DTXSID3025091 1.60e+00 (mg/kg-day)-1 #&gt; 32: DTXSID8021434 1.50e+00 (mg/kg-day)-1 #&gt; 33: DTXSID5020865 1.50e+00 (mg/kg-day)-1 #&gt; 34: DTXSID7020267 1.30e+00 (mg/kg-day)-1 #&gt; 35: DTXSID6020432 1.20e+00 (mg/kg-day)-1 #&gt; 36: DTXSID5020029 1.00e+00 (mg/kg-day)-1 #&gt; 37: DTXSID7020710 8.70e-01 (mg/kg-day)-1 #&gt; 38: DTXSID0020529 8.00e-01 (mg/kg-day)-1 #&gt; 39: DTXSID8021438 6.00e-01 (mg/kg-day)-1 #&gt; 40: DTXSID2021319 5.40e-01 (mg/kg-day)-1 #&gt; 41: DTXSID7021106 4.00e-01 (mg/kg-day)-1 #&gt; 42: DTXSID0020600 3.10e-01 (mg/kg-day)-1 #&gt; 43: DTXSID5020449 2.90e-01 (mg/kg-day)-1 #&gt; 44: DTXSID7021318 2.86e-01 (mg/kg-day)-1 #&gt; 45: DTXSID2021105 2.60e-01 (mg/kg-day)-1 #&gt; 46: DTXSID5021207 2.40e-01 (mg/kg-day)-1 #&gt; 47: DTXSID8020250 2.00e-01 (mg/kg-day)-1 #&gt; 48: DTXSID1022057 1.82e-01 (mg/kg-day)-1 #&gt; 49: DTXSID1026164 1.80e-01 (mg/kg-day)-1 #&gt; 50: DTXSID0020153 1.70e-01 (mg/kg-day)-1 #&gt; 51: DTXSID2021286 1.60e-01 (mg/kg-day)-1 #&gt; 52: DTXSID7020683 1.56e-01 (mg/kg-day)-1 #&gt; 53: DTXSID8020913 1.20e-01 (mg/kg-day)-1 #&gt; 54: DTXSID9020299 1.10e-01 (mg/kg-day)-1 #&gt; 55: DTXSID3039242 1.00e-01 (mg/kg-day)-1 #&gt; 56: DTXSID4020533 1.00e-01 (mg/kg-day)-1 #&gt; 57: DTXSID0020448 9.19e-02 (mg/kg-day)-1 #&gt; 58: DTXSID6020438 9.10e-02 (mg/kg-day)-1 #&gt; 59: DTXSID1020306 8.05e-02 (mg/kg-day)-1 #&gt; 60: DTXSID1020566 8.00e-02 (mg/kg-day)-1 #&gt; 61: DTXSID5020607 7.37e-02 (mg/kg-day)-1 #&gt; 62: DTXSID5021380 7.04e-02 (mg/kg-day)-1 #&gt; 63: DTXSID5021386 7.00e-02 (mg/kg-day)-1 #&gt; 64: DTXSID7020005 7.00e-02 (mg/kg-day)-1 #&gt; 65: DTXSID7020716 6.00e-02 (mg/kg-day)-1 #&gt; 66: DTXSID4020583 4.80e-02 (mg/kg-day)-1 #&gt; 67: DTXSID5020601 4.50e-02 (mg/kg-day)-1 #&gt; 68: DTXSID7020689 4.00e-02 (mg/kg-day)-1 #&gt; 69: DTXSID1020431 4.00e-02 (mg/kg-day)-1 #&gt; 70: DTXSID7026156 3.90e-02 (mg/kg-day)-1 #&gt; 71: DTXSID0021965 2.90e-02 (mg/kg-day)-1 #&gt; 72: DTXSID2020507 2.70e-02 (mg/kg-day)-1 #&gt; 73: DTXSID4039231 2.10e-02 (mg/kg-day)-1 #&gt; 74: DTXSID7020637 2.10e-02 (mg/kg-day)-1 #&gt; 75: DTXSID0021541 1.63e-02 (mg/kg-day)-1 #&gt; 76: DTXSID0020868 1.40e-02 (mg/kg-day)-1 #&gt; 77: DTXSID1021374 1.32e-02 (mg/kg-day)-1 #&gt; 78: DTXSID3020596 1.10e-02 (mg/kg-day)-1 #&gt; 79: DTXSID5039224 1.00e-02 (mg/kg-day)-1 #&gt; 80: DTXSID4020161 8.00e-03 (mg/kg-day)-1 #&gt; 81: DTXSID4021395 7.70e-03 (mg/kg-day)-1 #&gt; 82: DTXSID8020090 5.70e-03 (mg/kg-day)-1 #&gt; 83: DTXSID1020437 5.70e-03 (mg/kg-day)-1 #&gt; 84: DTXSID1020302 3.63e-03 (mg/kg-day)-1 #&gt; 85: DTXSID7021948 3.52e-03 (mg/kg-day)-1 #&gt; 86: DTXSID9020243 2.30e-03 (mg/kg-day)-1 #&gt; 87: DTXSID3020833 2.25e-03 (mg/kg-day)-1 #&gt; 88: DTXSID8020759 1.90e-03 (mg/kg-day)-1 #&gt; dtxsid numeric_value units natadb_rfc[, .(numeric_value = min(toxvalNumeric), units = toxvalUnits[which.min(toxvalNumeric)]), by = .(dtxsid)][order(numeric_value),] #&gt; dtxsid numeric_value units #&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID1020516 0.00000200 mg/m3 #&gt; 2: DTXSID7026156 0.00000800 mg/m3 #&gt; 3: DTXSID4024143 0.00001000 mg/m3 #&gt; 4: DTXSID7025180 0.00002000 mg/m3 #&gt; 5: DTXSID5020023 0.00002000 mg/m3 #&gt; 6: DTXSID4020874 0.00002000 mg/m3 #&gt; 7: DTXSID9020293 0.00003000 mg/m3 #&gt; 8: DTXSID3020702 0.00003000 mg/m3 #&gt; 9: DTXSID7021029 0.00004000 mg/m3 #&gt; 10: DTXSID8042476 0.00010000 mg/m3 #&gt; 11: DTXSID1020273 0.00014501 mg/m3 #&gt; 12: DTXSID2020688 0.00020000 mg/m3 #&gt; 13: DTXSID0039229 0.00020000 mg/m3 #&gt; 14: DTXSID3020413 0.00020000 mg/m3 #&gt; 15: DTXSID5021380 0.00020000 mg/m3 #&gt; 16: DTXSID3021932 0.00020000 mg/m3 #&gt; 17: DTXSID2021157 0.00030000 mg/m3 #&gt; 18: DTXSID0024260 0.00030000 mg/m3 #&gt; 19: DTXSID4020161 0.00040000 mg/m3 #&gt; 20: DTXSID5020449 0.00050000 mg/m3 #&gt; 21: DTXSID7020267 0.00070000 mg/m3 #&gt; 22: DTXSID7024166 0.00070000 mg/m3 #&gt; 23: DTXSID8020090 0.00100000 mg/m3 #&gt; 24: DTXSID4039231 0.00100000 mg/m3 #&gt; 25: DTXSID1020566 0.00100000 mg/m3 #&gt; 26: DTXSID1023786 0.00100000 mg/m3 #&gt; 27: DTXSID0020153 0.00100000 mg/m3 #&gt; 28: DTXSID3020964 0.00200000 mg/m3 #&gt; 29: DTXSID0021383 0.00200000 mg/m3 #&gt; 30: DTXSID0021965 0.00200000 mg/m3 #&gt; 31: DTXSID3020415 0.00200000 mg/m3 #&gt; 32: DTXSID5020029 0.00200000 mg/m3 #&gt; 33: DTXSID3020203 0.00200000 mg/m3 #&gt; 34: DTXSID8020913 0.00300000 mg/m3 #&gt; 35: DTXSID8021432 0.00300000 mg/m3 #&gt; 36: DTXSID0020448 0.00400000 mg/m3 #&gt; 37: DTXSID8020832 0.00500000 mg/m3 #&gt; 38: DTXSID1020148 0.00500000 mg/m3 #&gt; 39: DTXSID5020027 0.00600000 mg/m3 #&gt; 40: DTXSID3024366 0.00700000 mg/m3 #&gt; 41: DTXSID6020438 0.00700000 mg/m3 #&gt; 42: DTXSID2021658 0.00800000 mg/m3 #&gt; 43: DTXSID4020583 0.00800000 mg/m3 #&gt; 44: DTXSID5039224 0.00900000 mg/m3 #&gt; 45: DTXSID7020637 0.00980000 mg/m3 #&gt; 46: DTXSID1049641 0.01400000 mg/m3 #&gt; 47: DTXSID1022057 0.02000000 mg/m3 #&gt; 48: DTXSID5020316 0.02000000 mg/m3 #&gt; 49: DTXSID2021159 0.02000000 mg/m3 #&gt; 50: DTXSID6020981 0.02000000 mg/m3 #&gt; 51: DTXSID6020569 0.02000000 mg/m3 #&gt; 52: DTXSID2020711 0.02000000 mg/m3 #&gt; 53: DTXSID6022422 0.02000000 mg/m3 #&gt; 54: DTXSID9021261 0.02000000 mg/m3 #&gt; 55: DTXSID3039242 0.03000000 mg/m3 #&gt; 56: DTXSID5021207 0.03000000 mg/m3 #&gt; 57: DTXSID7020689 0.03000000 mg/m3 #&gt; 58: DTXSID4020533 0.03000000 mg/m3 #&gt; 59: DTXSID6020515 0.03000000 mg/m3 #&gt; 60: DTXSID0020600 0.03000000 mg/m3 #&gt; 61: DTXSID2021319 0.04000000 mg/m3 #&gt; 62: DTXSID4020298 0.05000000 mg/m3 #&gt; 63: DTXSID7020009 0.06000000 mg/m3 #&gt; 64: DTXSID8021434 0.08000000 mg/m3 #&gt; 65: DTXSID1021827 0.09000000 mg/m3 #&gt; 66: DTXSID0021541 0.09000000 mg/m3 #&gt; 67: DTXSID1020306 0.09765240 mg/m3 #&gt; 68: DTXSID2021446 0.10000000 mg/m3 #&gt; 69: DTXSID8020250 0.10000000 mg/m3 #&gt; 70: DTXSID6023949 0.10000000 mg/m3 #&gt; 71: DTXSID8021438 0.20000000 mg/m3 #&gt; 72: DTXSID3021431 0.20000000 mg/m3 #&gt; 73: DTXSID5021124 0.20000000 mg/m3 #&gt; 74: DTXSID8020597 0.40000000 mg/m3 #&gt; 75: DTXSID1020437 0.50000000 mg/m3 #&gt; 76: DTXSID0020868 0.60000000 mg/m3 #&gt; 77: DTXSID6023947 0.70000000 mg/m3 #&gt; 78: DTXSID0021917 0.70000000 mg/m3 #&gt; 79: DTXSID2020844 0.70000000 mg/m3 #&gt; 80: DTXSID5021889 0.80000000 mg/m3 #&gt; 81: DTXSID1020431 0.80000000 mg/m3 #&gt; 82: DTXSID3020596 1.00000000 mg/m3 #&gt; 83: DTXSID2021284 1.00000000 mg/m3 #&gt; 84: DTXSID8020759 2.00000000 mg/m3 #&gt; 85: DTXSID0021381 2.20000000 mg/m3 #&gt; 86: DTXSID3020833 3.00000000 mg/m3 #&gt; 87: DTXSID1020302 4.00000000 mg/m3 #&gt; 88: DTXSID2021731 4.00000000 mg/m3 #&gt; 89: DTXSID7021360 5.00000000 mg/m3 #&gt; dtxsid numeric_value units natadb_rfd[, .(numeric_value = min(toxvalNumeric), units = units[which.min(toxvalNumeric)]), by = .(dtxsid)][order(numeric_value),] #&gt; dtxsid numeric_value units #&gt; &lt;char&gt; &lt;num&gt; &lt;char&gt; #&gt; 1: DTXSID7021029 0.000004 mg/kg-day #&gt; 2: DTXSID2020682 0.000010 mg/kg-day #&gt; 3: DTXSID9020827 0.000020 mg/kg-day #&gt; 4: DTXSID1024382 0.000020 mg/kg-day #&gt; 5: DTXSID3020679 0.000030 mg/kg-day #&gt; --- #&gt; 103: DTXSID5021124 0.270000 mg/kg-day #&gt; 104: DTXSID2021731 0.500000 mg/kg-day #&gt; 105: DTXSID8020597 0.800000 mg/kg-day #&gt; 106: DTXSID3020833 0.857000 mg/kg-day #&gt; 107: DTXSID2021159 2.000000 mg/kg-day Answer to Environmental Health Question 8 With these results, we may answer Environmental Health Question 8: When examining different toxicity values, the data may be reported in multiple units. To assess the relative hazard from this data, it is important to take into account the different units and adjust accordingly. List the units reported for the cancer slope factor, reference dose, and reference concentration values associated with the oral and inhalation exposure routes for human hazard. Which chemicals in each data set have the highest cancer slope factor, lowest reference dose, and lowest reference concentration values? Answer: The units for these three toxicity value types for CCL4 are given by “mg/m3”, “g/m3”, “ug/m3” for RfC, “mg/kg-day”, “mg/kg” for RfD, and “(mg/kg-day)-1” for Cancer Slope Factor. For NATADB, the units for RfC are given by “mg/m3” and “ppm”, for RfD by “mg/kg-day”, “mg/kg”, and for Cancer Slope Factor by “(mg/kg-day)-1”. For CCL4, the chemical DTXSID2021028 has the highest CsF at 150 (mg/kg-day)-1, the chemical DTXSID1031040 has the lowest RfC value at 6.0e-12 mg/m3, and the chemical DTXSID7021029 has the lowest RfD value at 4e-6 mg/kg. For NATADB, the chemical DTXSID2020137 has the highest CsF at 500 (mg/kg-day)-1, the chemical DTXSID1020516 has the lowest RfC value at 2.0e-6 mg/m3, and the chemical DTXSID7021029 had the lowest RfD at 4e-6 mg/kg-day. Concluding Remarks In conclusion, we explored how one can access publicly available data from the CompTox Chemicals Dashboard programmatically using the CTX APIs via the ctxR R package. In the examples above, we investigated different types of data associated with chemicals, visualized and aggregated the data, and employed different data wrangling techniques using data.tables to answer the proposed environmental health questions. With these tools, one can build workflows that take in a list of chemicals and gather and process data associated with those chemicals through the CTX APIs. Consider how you might use this functionality for building models in your own work. Test Your Knowledge Try running the same analysis of physical-chemical properties, genotoxicity data, and hazard data on a different pair of data sets available from the CCD. For instance, try pulling the data set ‘BIOSOLIDS2021’ and work through the same steps we completed in this module to investigate the chemicals in this list to gain a better understanding of the associated data. "],["database-integration-air-quality-mortality-and-environmental-justice-data.html", "7.4 Database Integration: Air Quality, Mortality, and Environmental Justice Data Introduction to Training Module Introduction to Exposure and Health Databases Analyzing Relationships between PM2.5 and Mortality Environmental Justice Considerations Concluding Remarks", " 7.4 Database Integration: Air Quality, Mortality, and Environmental Justice Data The development of this training module was led by Cavin Ward-Caviness with contributions from Alexis Payton. Disclaimer: The views expressed in this document are those of the author and do not necessarily reflect the views or policies of the U.S. EPA. Introduction to Training Module This training module provides an example analysis based on the integration of data across multiple environmental health databases. This module specifically guides trainees through an explanation of how the data were downloaded and organized, and then details the loading of required packages and datasets. Then, code is provided for visualizing county-level air pollution measures, including PM2.5, NO2, and SO2. These measures were obtained through U.S. EPA monitoring stations are visualized here as the yearly average. Air pollution concentrations are then evaluated for potential relationship to the health outcome, mortality. Specifically, age-adjusted mortality rates are organized and statistically related to PM2.5 concentrations through linear regression modeling. Crude statistical models are first provided that do not take into account the influence of potential confounders. Then, statistical models are used that adjust for potential confounders, including adult smoking rates, obesity, food environment indicators, physical activity, employment status, rural vs urban living percentages, sex, ethnicity, and race. Results from these models point to the finding that areas with higher percentages of African-Americans may be experiencing higher impacts from PM2.5 on mortality. This relationship is of high interest, as it represents a potential environmental justice issue. Introduction to Exposure and Health Databases In this training module, we will use publicly available exposure and health databases to examine associations between air quality and mortality across the entire U.S. Specific databases that we will query include the following: EPA Air Quality data: As an example air pollutant exposure dataset, 2016 annual average data from the EPA Air Quality System database will be analyzed, using data downloaded and organized from the following website: https://aqs.epa.gov/aqsweb/airdata/annual_conc_by_monitor_2016.zip CDC Health Outcome data: As an example health outcome dataset, the 2016 CDC Underlying Cause of Death dataset, from the WONDER (Wide-ranging ONline Data for Epidemiologic Research) website will be analyzed, using All-Cause Mortality Rates downloaded and organized from the following website: https://wonder.cdc.gov/ucd-icd10.html Human covariate data: The potential influence of covariates (e.g., race) and other confounders will be analyzed using data downloaded and organized from the following 2016 county-level resource: https://www.countyhealthrankings.org/explore-health-rankings/rankings-data-documentation/national-data-documentation-2010-2019 Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: What areas of the U.S. are most heavily monitored for air quality? Is there an association between long-term, ambient PM2.5 concentrations and mortality at the county level? Stated another way we are asking: Do counties with higher annual average PM2.5 concentrations also have higher all-cause mortality rates? What is the difference when running crude statistical models vs. statistical models that adjust for potential confounding, when evaluating the relationship between PM2.5 and mortality? Do observed associations differ when comparing between counties with a higher vs. lower percentage of African-Americans which can indicate environmental justice concerns? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;sf&quot;)) install.packages(&quot;sf&quot;); if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); if (!requireNamespace(&quot;ggthemes&quot;)) install.packages(&quot;ggthemes&quot;); Loading R packages required for this session library(sf) library(tidyverse) library(ggthemes) library(DT) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Let’s start by loading the datasets needed for this training module. As detailed in the introduction, these data were previously downloaded and organized, and specifically made available for this training exercise as a compiled RDataset, containing organized dataframes ready to analyze. We can now read in these organized data using the load() function. load(&quot;Module7_4_Input/Module7_4_InputData.RData&quot;) First let’s take a look at the geographic data, starting with the county-level shapefile (counties_shapefile). This dataframe contains the location information for the counties which we will use for visualizations. head(counties_shapefile) #&gt; Simple feature collection with 6 features and 9 fields #&gt; Geometry type: MULTIPOLYGON #&gt; Dimension: XY #&gt; Bounding box: xmin: -102.042 ymin: 37.38839 xmax: -84.79633 ymax: 43.49961 #&gt; Geodetic CRS: NAD83 #&gt; STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND AWATER #&gt; 1 19 107 00465242 0500000US19107 19107 Keokuk 06 1500067253 1929323 #&gt; 2 19 189 00465283 0500000US19189 19189 Winnebago 06 1037261946 3182052 #&gt; 3 20 093 00485011 0500000US20093 20093 Kearny 06 2254696689 1133601 #&gt; 4 20 123 00485026 0500000US20123 20123 Mitchell 06 1817632928 44979981 #&gt; 5 20 187 00485055 0500000US20187 20187 Stanton 06 1762104518 178555 #&gt; 6 21 005 00516849 0500000US21005 21005 Anderson 06 522745702 6311537 #&gt; geometry #&gt; 1 MULTIPOLYGON (((-92.41199 4... #&gt; 2 MULTIPOLYGON (((-93.97076 4... #&gt; 3 MULTIPOLYGON (((-101.5419 3... #&gt; 4 MULTIPOLYGON (((-98.49007 3... #&gt; 5 MULTIPOLYGON (((-102.0419 3... #&gt; 6 MULTIPOLYGON (((-85.16919 3... These geographic data are represented by the following columns. (Some columns are not described below, since we don’t need them for these analyses.): STATEFP: State FIPS code (1 or 2 digits) COUNTYFP: County FIPS Code (3 digits) GEOID: Geographic identifier that combines the state and county FIPS codes NAME: County name geometry: Latitude and longitude coordinates Now let’s view the EPA Air Quality Survey (AQS) data (epa_ap_county) collected from 2016. This dataframe represents county-level air quality measures, as detailed above. This dataframe is in melted (or long) format, meaning that different air quality measures are organized across rows, with variable measure indicators in the Parameter.Name, Units.of.Measure, and County_Avg columns. head(epa_ap_county) #&gt; State.Code State.Name County.Code County.Name State_County_Code Parameter.Name #&gt; 1 1 Alabama 3 Baldwin 1003 PM25 #&gt; 2 1 Alabama 27 Clay 1027 PM25 #&gt; 3 1 Alabama 33 Colbert 1033 PM25 #&gt; 4 1 Alabama 49 DeKalb 1049 PM25 #&gt; 5 1 Alabama 55 Etowah 1055 PM25 #&gt; 6 1 Alabama 69 Houston 1069 PM25 #&gt; Units.of.Measure County_Avg #&gt; 1 Micrograms/cubic meter (LC) 7.226446 #&gt; 2 Micrograms/cubic meter (LC) 7.363793 #&gt; 3 Micrograms/cubic meter (LC) 7.492241 #&gt; 4 Micrograms/cubic meter (LC) 7.695690 #&gt; 5 Micrograms/cubic meter (LC) 8.195575 #&gt; 6 Micrograms/cubic meter (LC) 7.061538 These air quality data are represented by the following columns: State.Code: State FIPS code (1 or 2 digits) State.Name: State name County.Code: County FIPS code (1-3 digits) County.Name: County name State_County_Code: Combined state and county code (separated by a 0) Parameter.Name: Name of the air pollutant Units.of.Measure: Units of measurement County_Avg: County average These data can be restructured to view air quality measures as separate variables labeled across columns using: # transform from the &quot;long&quot; to &quot;wide&quot; format for the pollutants epa_ap_county &lt;- epa_ap_county %&gt;% select(-Units.of.Measure) %&gt;% unique() %&gt;% tidyr::spread(Parameter.Name, County_Avg) head(epa_ap_county) #&gt; State.Code State.Name County.Code County.Name State_County_Code NO2 PM25 SO2 #&gt; 1 1 Alabama 3 Baldwin 1003 NA 7.226446 NA #&gt; 2 1 Alabama 27 Clay 1027 NA 7.363793 NA #&gt; 3 1 Alabama 33 Colbert 1033 NA 7.492241 NA #&gt; 4 1 Alabama 49 DeKalb 1049 NA 7.695690 NA #&gt; 5 1 Alabama 55 Etowah 1055 NA 8.195575 NA #&gt; 6 1 Alabama 69 Houston 1069 NA 7.061538 NA Note that we can now see the specific pollutant variables NO2, PM25, and SO2 on the far right. Population-Weighted vs. Unweighted Exposures Here we pause briefly to speak on population-weighted vs unweighted exposures. The analysis we will be undertaking is known as an “ecological” analysis where we are looking at associations by area, e.g. county. When studying environmental exposures by area a common practice is to try to weight the exposures by the population so that exposures better represent the “burden” faced by the population. Ideally for this you would want a systematic model or assessment of the exposure that corresponded with a fine-scale population estimate so that for each county you could weight exposures within different areas of the county by the population exposed. This sparse monitor data (we will examine the population covered later in the tutorial) is not population weighted, but should you see similar analyses with population weighting of exposures you should simply be aware that this better captures the “burden” of exposure experienced by the population within the area estimated, typically zip code or county. Now let’s view the CDC’s mortality dataset collected from 2016 (cdc_mortality): head(cdc_mortality) #&gt; Notes County County.Code Deaths Population Crude.Rate Age.Adjusted.Rate #&gt; 1 NA Autauga County, AL 1001 520 55416 938.36 884.40 #&gt; 2 NA Baldwin County, AL 1003 1974 208563 946.48 716.92 #&gt; 3 NA Barbour County, AL 1005 256 25965 985.94 800.68 #&gt; 4 NA Bibb County, AL 1007 239 22643 1055.51 927.67 #&gt; 5 NA Blount County, AL 1009 697 57704 1207.89 989.37 #&gt; 6 NA Bullock County, AL 1011 133 10362 1283.54 1063.00 #&gt; Age.Adjusted.Rate.Standard.Error #&gt; 1 39.46 #&gt; 2 16.58 #&gt; 3 51.09 #&gt; 4 61.03 #&gt; 5 38.35 #&gt; 6 93.69 These mortality data are represented by the following columns. We’ll just ignore the Notes column for our purposes: County: County name with the state abbreviation County.Code: Combined state and county code (separated by a 0) Deaths: Number of deaths in 2016 Population: County population in 2016 Crude.Rate: Death rate (\\(\\frac{Number~of~Deaths}{Population}* 100,000\\)) Age.Adjusted.Rate: age-adjusted death rate (\\(\\sum{(Age~Specific~Death~Rate * Standard~Population~Weight)} * 100,000\\)) Age.Adjusted.Rate.Standard.Error: Standard error of the age-adjusted rate We can create a visualization of the age-adjusted death rate and air pollutants throughout the U.S. to further inform what these data look like: # Can merge them by the FIPS county code which we need to create for the counties_shapefile counties_shapefile$State_County_Code &lt;- as.character(as.numeric(paste0(counties_shapefile$STATEFP, counties_shapefile$COUNTYFP))) # Let&#39;s merge in the air pollution and mortality data and plot it counties_shapefile &lt;- merge(counties_shapefile, epa_ap_county, by.x = &quot;State_County_Code&quot;, by.y = &quot;State_County_Code&quot;, all.x=TRUE) counties_shapefile &lt;- merge(counties_shapefile, cdc_mortality, by.x = &quot;State_County_Code&quot;, by.y = &quot;County.Code&quot;) # Will remove alaska and hawaii just so we can look at the continental USA county_plot &lt;- subset(counties_shapefile, !STATEFP %in% c(&quot;02&quot;,&quot;15&quot;)) # We can start with a simple plot of age-adjusted mortality rate, PM2.5, NO2, and SO2 levels across the U.S. plot(county_plot[,c(&quot;Age.Adjusted.Rate&quot;,&quot;PM25&quot;,&quot;NO2&quot;,&quot;SO2&quot;)]) You can see that these result in the generation of four different nationwide plots, showing the distributions of age-adjusted mortality rates, PM2.5 concentrations, NO2 concentrations, and SO2 concentrations, averaged per county. Let’s make a nicer looking plot with ggplot(), looking just at PM2.5 levels: ggplot(data = county_plot) + geom_sf(aes(fill = PM25)) + # Changing colors scale_fill_viridis_c(option =&quot;plasma&quot;, name =&quot;PM2.5&quot;, guide = guide_colorbar( direction = &quot;horizontal&quot;, barheight = unit(2, units = &quot;mm&quot;), barwidth = unit(50, units = &quot;mm&quot;), draw.ulim = F, title.position = &#39;top&#39;, # some shifting around title.hjust = 0.5, label.hjust = 0.5)) + ggtitle(expression(2016~Annual~PM[2.5]~EPA~Monitors)) + theme_map() + theme(plot.title = element_text(hjust = 0.5, size = 22)) Answer to Environmental Health Question 1 With this, we can answer Environmental Health Question #1: What areas of the U.S. are most heavily monitored for air quality? Answer: We can tell from the PM2.5 specific plot that air monitors are densely located in California, and other areas with high populations (including the East Coast), while large sections of central U.S. lack air monitoring data. Analyzing Relationships between PM2.5 and Mortality Crude Model Now the primary question is whether counties with higher PM2.5 also have higher mortality rates. To answer this question, first we need to perform some data merging in preparation for this analysis. # Merging mortality and air pollution data model_data &lt;- merge(epa_ap_county, cdc_mortality, by.x = &quot;State_County_Code&quot;, by.y = &quot;County.Code&quot;) As we saw in the above plot, only a portion of the USA is covered by PM2.5 monitors. Let’s see what our population coverage is sum(model_data$Population, na.rm = TRUE) #&gt; [1] 232169063 sum(cdc_mortality$Population, na.rm = TRUE) #&gt; [1] 323127513 sum(model_data$Population, na.rm = TRUE)/sum(cdc_mortality$Population, na.rm = TRUE)*100 #&gt; [1] 71.8506 We can do a quick visual inspection of this using a scatter plot which will also let us check for unexpected distributions of the data (always a good idea) ggplot(model_data) + geom_point(aes(x = Age.Adjusted.Rate, y = PM25)) + ggtitle(expression(PM[2.5]~by~Mortality~Rate)) + xlab(&#39;Age-adjusted Mortality Rate&#39;) + ylab(expression(PM[2.5])) # changing axis labels The univariate correlation is a simple way of quantifying this potential relationship, though it does not nearly tell the complete story. Just as a starting point, let’s run this simple univariate correlation calculation using the cor() function. cor(model_data$Age.Adjusted.Rate, model_data$PM25, use = &quot;complete.obs&quot;) #&gt; [1] 0.178549 Now, let’s obtain a more complete estimate of the data through regression modeling. As an initial starting point, let’s run this model without any confounders (also known as a ‘crude’ model). A simple linear regression model in R can be carried out using the lm() function. Here, we are evaluating age-adjusted mortality rate (age.adjusted.rate) as the dependent variable, and PM2.5 as the independent variable. Values used in evaluating this model were weighted to adjust for the fact that some counties have higher precision in their age-adjusted mortality rate (represented by a smaller age-adjusted rate standard error). # running the linear regression model m &lt;- lm(Age.Adjusted.Rate ~ PM25, data = model_data, weights = 1/model_data$Age.Adjusted.Rate.Standard.Error) # viewing the model results through the summary function summary(m) #&gt; #&gt; Call: #&gt; lm(formula = Age.Adjusted.Rate ~ PM25, data = model_data, weights = 1/model_data$Age.Adjusted.Rate.Standard.Error) #&gt; #&gt; Weighted Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -110.096 -9.792 8.363 25.090 84.316 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 661.038 22.528 29.343 &lt; 2e-16 *** #&gt; PM25 8.956 2.878 3.112 0.00195 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 29.93 on 608 degrees of freedom #&gt; (88 observations deleted due to missingness) #&gt; Multiple R-squared: 0.01568, Adjusted R-squared: 0.01406 #&gt; F-statistic: 9.685 on 1 and 608 DF, p-value: 0.001945 Shown here are summary level statistics summarizing the results of the linear regression model. In the model summary we see several features: Estimate: the regression coefficient which tells us the relationship between a 1 ug/m3 change (elevation) in PM2.5 and the age-adjusted all-cause mortality rate Std. Error: the standard error of the estimate t value: represents the T-statistic which is the test statistic for linear regression models and is simply the Estimate divided by Std. Error. This t value is compared with the Student’s T distribution in order to determine the p-value (Pr(&gt;|t|)). The residuals are the difference between the predicted outcome (age-adjusted mortality rate) and known outcome from the data. For linear regression to be valid this should be normally distributed. The residuals from a linear model can be extracted using the residuals() function and plotted to see their distribution. Answer to Envrionmental Health Question 2 With this, we can answer Environmental Health Question #2: Is there an association between long-term, ambient PM2.5 concentrations and mortality at the county level? Answer: Based on these model results, there may indeed be an association between PM2.5 concentrations and mortality (p=0.0019) Adjusting for Covariates To more thoroughly examine the potential relationship between PM2.5 concentrations and mortality it is absolutely essential to adjust for confounders. Let’s start by viewing the human covariate data that contains some confounders of interest. datatable(county_health) These covariate data are represented by the following columns: State.FIPS.Code: Single digit code assigned to each state County.FIPS.Code: FIPS code unique to counties within a particular state (1 digit) County.5.digit.FIPS.Code: Unique county FIPS code (5 digits) State.Abbreviation: State abbreviation Name: County Name (or state name that contains the state average) Release.Year: Year the data was publicly released Various confounders indicative of race, ethnicity, socioeconomic status, population density, education, health-related variables in columns 7-36 # Merging the covariate data in with the AQS data model_data &lt;- merge(model_data, county_health, by.x = &quot;State_County_Code&quot;, by.y = &quot;County.5.digit.FIPS.Code&quot;, all.x = TRUE) # Now we add some relevant confounders to the linear regression model m &lt;- lm(Age.Adjusted.Rate ~ PM25 + Adult.smoking + Adult.obesity + Food.environment.index + Physical.inactivity + High.school.graduation + Some.college + Unemployment + Violent.crime + Percent.Rural + Percent.Females + Percent.Asian + Percent.Non.Hispanic.African.American + Percent.American.Indian.and.Alaskan.Native + Percent.NonHispanic.white, data = model_data, weights = 1/model_data$Age.Adjusted.Rate.Standard.Error) # And finally we check to see if the statistical association persists summary(m) #&gt; #&gt; Call: #&gt; lm(formula = Age.Adjusted.Rate ~ PM25 + Adult.smoking + Adult.obesity + #&gt; Food.environment.index + Physical.inactivity + High.school.graduation + #&gt; Some.college + Unemployment + Violent.crime + Percent.Rural + #&gt; Percent.Females + Percent.Asian + Percent.Non.Hispanic.African.American + #&gt; Percent.American.Indian.and.Alaskan.Native + Percent.NonHispanic.white, #&gt; data = model_data, weights = 1/model_data$Age.Adjusted.Rate.Standard.Error) #&gt; #&gt; Weighted Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -70.059 -7.055 0.960 8.119 64.052 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 616.47976 157.20842 3.921 1.00e-04 *** #&gt; PM25 3.84851 1.69321 2.273 0.023444 * #&gt; Adult.smoking 859.27337 137.81312 6.235 9.43e-10 *** #&gt; Adult.obesity 605.84315 97.71952 6.200 1.16e-09 *** #&gt; Food.environment.index -28.65536 3.93359 -7.285 1.22e-12 *** #&gt; Physical.inactivity 117.60916 91.41519 1.287 0.198834 #&gt; High.school.graduation 55.14446 40.78490 1.352 0.176944 #&gt; Some.college -244.41255 49.78764 -4.909 1.23e-06 *** #&gt; Unemployment 97.98159 161.89025 0.605 0.545290 #&gt; Violent.crime 0.07551 0.01516 4.982 8.62e-07 *** #&gt; Percent.Rural -12.55308 20.84834 -0.602 0.547364 #&gt; Percent.Females -271.45504 299.16925 -0.907 0.364640 #&gt; Percent.Asian 74.54327 55.89262 1.334 0.182897 #&gt; Percent.Non.Hispanic.African.American 153.39910 39.49623 3.884 0.000116 *** #&gt; Percent.American.Indian.and.Alaskan.Native 200.70010 102.75028 1.953 0.051329 . #&gt; Percent.NonHispanic.white 240.17976 26.30538 9.130 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 14.22 on 514 degrees of freedom #&gt; (168 observations deleted due to missingness) #&gt; Multiple R-squared: 0.7841, Adjusted R-squared: 0.7778 #&gt; F-statistic: 124.4 on 15 and 514 DF, p-value: &lt; 2.2e-16 Answer to Envrionmental Health Question 3 With this, we can answer Environmental Health Question #3: What is the difference when running crude statistical models vs. statistical models that adjust for potential confounding, when evaluating the relationship between PM2.5 and mortality? Answer: The relationship between PM2.5 and mortality remains statistically significant when confounders are considered (p=0.023), though is not as significant as when running the crude model (p=0.0019). Environmental Justice Considerations Environmental justice is the study of how societal inequities manifest in differences in environmental health risks either due to greater exposures or a worse health response to exposures. Racism and racial discrimination are major factors in both how much pollution people are exposed to as well what their responses might be due to other co-existing inequities (e.g. poverty, access to healthcare, food deserts). Race is a commonly used proxy for experiences of racism and racial discrimination. Here we will consider the race category of Non-Hispanic African-American to investigate if pollution levels differ by percent African-Americans in a county and if associations between PM2.5 and mortality also differ by this variable, which could indicate environmental justice-relevant issues revealed by this data. We will specifically evaluate data distributions across counties with the highest percentage of African-Americans (top 25%) vs. lowest percentage of African-Americans (bottom 25%). First let’s visualize the distribution of African-American percentage in these data ggplot(model_data) + geom_histogram(aes(x = Percent.Non.Hispanic.African.American*100)) + ggtitle(&quot;Percent African-American by County&quot;) + xlab(&#39;Percent&#39;) Let’s look at a summary of the data summary(model_data$Percent.Non.Hispanic.African.American) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; 0.00056 0.01220 0.03995 0.09415 0.12282 0.70458 67 We can compute quartiles of the data model_data$AA_quartile &lt;- with(model_data, cut(Percent.Non.Hispanic.African.American, breaks = quantile(Percent.Non.Hispanic.African.American, probs = seq(0,1, by = 0.25), na.rm = TRUE), include.lowest = TRUE, ordered_result = TRUE, labels = FALSE)) Then we can use these quartiles to see that as the Percent African-American increases so does the PM2.5 exposure by county AA_summary &lt;- model_data %&gt;% filter(!is.na(Percent.Non.Hispanic.African.American)) %&gt;% group_by(AA_quartile) %&gt;% summarise(Percent_AA = mean(Percent.Non.Hispanic.African.American, na.rm = TRUE), Mean_PM25 = mean(PM25, na.rm = TRUE)) AA_summary #&gt; # A tibble: 4 × 3 #&gt; AA_quartile Percent_AA Mean_PM25 #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 0.00671 5.97 #&gt; 2 2 0.0238 7.03 #&gt; 3 3 0.0766 7.93 #&gt; 4 4 0.269 8.05 Now that we can see this trend, let’s add some statistics. Let’s specifically compare the relationships between PM2.5 and mortality within the bottom 25% AA counties (quartile 1); and also the highest 25% AA counties (quartile 4) # first need to subset the data by these quartiles of interest low_AA &lt;- subset(model_data, AA_quartile == 1) high_AA &lt;- subset(model_data, AA_quartile == 4) # then we can run the relevant statistical models m.low &lt;- lm(Age.Adjusted.Rate ~ PM25 + Adult.smoking + Adult.obesity + Food.environment.index + Physical.inactivity + High.school.graduation + Some.college + Unemployment + Violent.crime + Percent.Rural + Percent.Females + Percent.Asian + Percent.American.Indian.and.Alaskan.Native + Percent.NonHispanic.white, data = low_AA, weights = 1/low_AA$Age.Adjusted.Rate.Standard.Error) m.high &lt;- lm(Age.Adjusted.Rate ~ PM25 + Adult.smoking + Adult.obesity + Food.environment.index + Physical.inactivity + High.school.graduation + Some.college + Unemployment + Violent.crime + Percent.Rural + Percent.Females + Percent.Asian + Percent.American.Indian.and.Alaskan.Native + Percent.NonHispanic.white, data = high_AA, weights = 1/high_AA$Age.Adjusted.Rate.Standard.Error) # We see a striking difference in the associations rbind(c(&quot;Bottom 25% AA Counties&quot;,round(summary(m.low)$coefficients[&quot;PM25&quot;,c(1,2,4)],3)), c(&quot;Top 25% AA Counties&quot;,round(summary(m.high)$coefficients[&quot;PM25&quot;,c(1,2,4)],3))) #&gt; Estimate Std. Error Pr(&gt;|t|) #&gt; [1,] &quot;Bottom 25% AA Counties&quot; &quot;4.782&quot; &quot;3.895&quot; &quot;0.222&quot; #&gt; [2,] &quot;Top 25% AA Counties&quot; &quot;14.552&quot; &quot;4.13&quot; &quot;0.001&quot; Answer to Envrionmental Health Question 4 With this, we can answer Environmental Health Question #4: Do observed associations differ when comparing between counties with a higher vs. lower percentage of African-Americans which can indicate environmental justice concerns? Answer: Yes. Counties with the highest percentage of African-Americans (top 25%) demonstrated a highly significant association between PM2.5 and age-adjusted mortality, even when adjusting for confounders (p=0.001), meaning that the association between PM2.5 and mortality within these counties may be exacerbated by factors relevant to race. Conversely, counties with the lowest percentages of African-Americans (bottom 25%) did not demonstrate a significant association between PM2.5 and age-adjusted mortality, indicating that these counties may have lower environmental health risks due to factors correlated with race. Concluding Remarks In conclusion, this training module serves as a novel example data integration effort of high relevance to environmental health issues. Databases that were evaluated here span exposure data (i.e., Air Quality System data), health outcome data (i.e., mortality data), and county-level characteristics on healthcare, food environment, and other potentially relevant confounders (i.e., county-level variables that may impact observed relationships), and environmental justice data (e.g., race). Many different visualization and statistical approaches were used, largely based on linear regression modeling and county-level characteristic stratifications. These example statistics clearly support the now-established relationship between PM2.5 concentrations in the air and mortality. Importantly, these related methods can be tailored to address new questions to increase our understanding between exposures to chemicals in the environment and adverse health outcomes, as well as the impact of different individual or area characteristics on these relationships - particularly those that might relate to environmental justice concerns. Test Your Knowledge This training module provided some examples looking at PM2.5 concentration data. Using the same “Module7_4_InputData.data” file (also saved as “Module7_4_TYKInput.data”), let’s ask similar questions but now looking at NO2 concentration data. Is there an association between long-term, ambient NO2 concentrations and age-adjusted mortality at the county level (i.e., the crude model results)? After adjusting for covariates, is there an association between long-term, ambient NO2 concentrations and age-adjusted mortality at the county level (i.e., the adjusted model results)? "]]
